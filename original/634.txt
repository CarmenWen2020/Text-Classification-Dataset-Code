Abstract
The Time-Sensitive Networking (TSN) set of standards introduces in IEEE 802.1 switches and end stations novel features to meet the requirements of a broad spectrum of applications that are characterized by time-sensitive and mission-critical traffic flows. In particular, the IEEE802.1Qbv-2015 amendment introduces enhancements that provide temporal isolation for scheduled traffic, i.e., a traffic class that requires transmission based on a known timescale, while the IEEE802.1Qbu-2016 introduces preemption as a mechanism to allow time-critical messages to interrupt ongoing non time-critical transmissions. Both amendments, that are now enrolled in the IEEE802.1Q-2018 standard, are very important for industrial networks, where scheduled traffic and low-latency real-time flows have to coexist, on the same network, with best-effort transmissions.

In this context, this work presents a response time analysis of TSN networks that encompasses the enhancements for scheduled traffic and preemption, in various combinations. The paper presents the proposed analysis and a performance comparison between the response times calculated by the analysis and the response times obtained through OMNeT++ simulations in three different scenarios.



Keywords
Time-sensitive networking
Schedulability analysis
Response time analysis
Scheduled traffic
Simulation



1. Introduction
The road to real-time Ethernet finds its roots in the IEEE Audio Video Bridging (AVB) set of standards [1], [16], that introduced in IEEE 802.1 networks several innovative features, such as, protocols to achieve network-wide time synchronization, credit-based shaping and bandwidth reservation, to guarantee low and bounded latency in industrial and automotive communications [2], [3]. Although these features allow for real-time operations over IEEE 802.1 networks, they are not enough to support the time-sensitive traffic flows that are typically found in industrial automation networks [25]. Novel mechanisms and augmented capabilities were therefore needed in IEEE 802.1 switches and end stations to guarantee not only bounded end-to-end latency, but also temporal isolation for scheduled traffic (i.e., a traffic class that requires transmission based on a known timescale), reliability, and zero congestion loss. The Time-Sensitive Networking (TSN) set of standards offer these additional capabilities building upon the AVB standards and adding a large number of novel features.

TSN is a flexible toolbox, from which one can pick only what is needed for the targeted applications. In this work we focus on two standards of the TSN family, that are relevant to the enhancements to support scheduled traffic (i.e., the IEEE 802.1Qbv-2015 amendment [17]) and to frame preemption of low-critical frames by high-critical frames (i.e., the IEEE 802.1Qbu-2016 amendment [18]), respectively. Both amendments, now enrolled into the IEEE 802.1Q-2018 standard [19], are very suitable for industrial networks, where scheduled traffic and real-time flows have to coexist on the same network with best-effort transmissions. In fact, the IEEE 802.1Qbv enhancements provide scheduled traffic with temporal isolation exploiting time-aware shaping, through a gate associated to each of the queues of a switch port, while the preemption capabilities specified by the IEEE 802.1Qbu amendment allow to reduce the latency of the other traffic classes.

1.1. Motivation and contribution
The TSN standards are becoming accepted solutions in several automotive and automation domains in which the complexity of time-critical software and systems is drastically growing, due to the high number of components and the integration of complex functionalities in the system. For time-critical systems it is essential to guarantee that the timing requirements will always be met, in other words, such systems require hard real-time guarantees. These systems should be able to provide correct functionalities within a correct timing, i.e., the response time of the functionalities should always be less than certain deadlines that are specified by the system designer, otherwise a catastrophe can occur [9], i.e. the system fails in accomplishing its mission. Therefore, it is crucial to verify the timing requirements after the deployment phase of the time-critical system. In general, there are two ways to perform the timing verification, also known as schedulability test, either using analytical or simulation methods. In the analytical method, the worst-case response time (WCRT) is computed considering the worst-case scenarios that can occur in the system. The worst-case response time analysis [9] is an example of the analytical-based method with relatively low run-time complexity. As the applications of the TSN standards are time-critical applications, such as those found in industrial and automotive contexts, we only focused on timing verification of such applications based on the WCRT analysis. We performed the simulation experiments to show the applicability of the analytical method on use cases rather than showing the performance of TSN network itself.

We would like to highlight that the schedulability analysis proposed in this paper is generic and all parameters in the theoretical equations are defined such that any value can be assigned to them. The analysis has generic validity for any system in which both preemptive and non-preemptive scheduling co-exist between multiple different traffic classes. The novelty of the analysis is exactly this one, as so far analysis was available only for preemptive or non preemptive, not for both in the same system. In this paper we tailored this specific analysis to the TSN standards.

In this paper, the target audience is time-critical system designers who not only need to verify the correctness of functionalities, but also to verify the timing requirements of the system, i.e., to check whether all deadlines are met. To this aim, we provide a methodology based on analytical worst-case response time analysis for TSN networks that encompasses credit-based shaping, time-aware shaping and preemption support, in various combinations according to the IEEE802.1Qbu and IEEE802.1Qbv amendments (now enrolled in the IEEE 802.1Q-2018 standard). Note that, in practice, the proposed methodology will be integrated into software development tools, such as Rubus-ICE1 and SymTA/S,2 to help the time-critical system designers in verifying the timing requirements of the system. An example of such integration into tools is presented in [6].

To the best of our knowledge, no previous work on worst-case response time analysis of TSN networks is able to include all these aspects together in the response time calculation of individual messages. In addition, the paper presents a comparison between the response times calculated by the analysis and the response times obtained through OMNeT++ simulations in three realistic industrial scenarios in order to show the applicability of the analysis on realistic settings.

1.2. Paper organization
The paper is organized as follows. Section 2 gives an overview on the IEEE 802.1Q standard. Section 3 positions our work in the context of the existing literature. Section 4 presents our system model, while Section 5 deals with the response time analysis proposed in this work. Section 6 presents a performance evaluation of TSN networks that assesses the correspondence between the results obtained through our analysis and those obtained by simulation in a realistic industrial scenario. Finally Section 7 gives our conclusions and hints for future work.

2. IEEE 802.1Q standard recap
The IEEE 802.1Q standard provides a number of interesting features, that are listed below.

•
A reservation mechanism, known as Stream Reservation Protocol (SRP), that allows for bandwidth reservation within the switches along the path between a sender and a receiver.

•
The Queuing and Forwarding mechanism, that distinguishes between critical and non-critical traffic classes and applies strict priority scheduling between them. The critical traffic classes are the Stream Reservation (SR) Class A (the highest priority) and Class B, while non-critical traffic is best-effort (BE).

•
A credit-based shaper (CBS) algorithm, to prevent traffic bursts in transmission. According to the CBS algorithm, which applies to stream reservation classes only, i.e., Classes A and B, each SR traffic class has an associate credit parameter. Whenever there is a pending message in a queue corresponding to a traffic class, the transmission can occur only if the associated credit is zero or higher. During the transmission, the credit is consumed at a constant rate, known as sendSlope. The credit is replenished with a constant rate, known as idleSlope, in two cases: (i) when the messages of the queue are pending for transmission, and (ii) when there are no more pending messages in the queue, but the credit is negative. If there is no message in the queue and the credit is positive then the credit is immediately reset to zero.

•
Enhancements for scheduled traffic (ST) implementing time-aware traffic shaping, that provide temporal isolation for scheduled traffic. In particular, transmission gates are associated with each queue of a switch port, and transmission from a queue is only allowed if the relevant gate is open. The gates operation follows a list, called Gate Control List (GCL) that periodically repeats with a period of OperCycleTime. The gate mechanism allows to temporally isolate ST traffic from the other traffic classes by creating a so-called protected window for ST transmission. This is achieved thanks to the gates operation, that is, if there is a scheduled close event for a non-ST class, then a non-ST frame is transmitted only if its transmission will be completed before the gate for ST traffic opens. According to Clause 8.6.8.2 in the IEEE 802.1Qbv standard, the credit for the SR classes is accumulated only while the transmission gates for the relevant queues are open, and the idleSlope for SR traffic increases with a constant rate that is higher than that defined in the CBS algorithm. In fact, the idleSlope is multiplied by the duty cycle for the transmission gate. Conversely, during the PW the idleSlope remains constant, as the gates for SR traffic are closed. Finally, the SR credit decreases with the rate of sendSlope during transmissions. Fig. 1-a shows the operation of CBS and gate mechanisms.

In this example, during the BE transmission, the messages from classes A and B become ready. However, as the port is busy, the credits of classes A and B increase. When the transmission of BE is finished,  starts transmitting, as it comes from the higher priority queue and the credit is positive. Afterwards, as there is an ST message () scheduled for transmission soon, neither  nor  can start transmitting, as their transmissions would interfere with the ST message, thus the transmission gates associated to their queues are closed. During the transmission of , the credits for classes A and B do not vary. Once the ST transmission is completed,  starts transmitting, as the credit for Class B is positive. The transmission of  follows, as the credit for Class A in the meantime has reached a positive value.

•
Preemption support, that introduces express and preemptable traffic classes. Express traffic can preempt preemptable traffic, but it cannot be preempted itself. Preemption support can be combined with the CBS and gate mechanisms. For instance, the ST queue can be set as express, while all the other ones are preemptable. In this case, the guard band is shorter than in the case without preemption, but it is still required because, according to the IEEE 802.1Qbu, a frame less than 123 Bytes cannot be preempted. Fig. 1-b shows an example of how the preemption mechanism works in the same case as in Fig. 1-a. After the transmission of ,  can be transmitted. However, it gets preempted 123Bytes before the transmission of , to prevent any interference. The transmission of  is resumed after the ST transmission. When preemption is supported, there are two modes of operation, i.e., with and without the Hold and Release mechanism, respectively. As these two modes slightly affect the analysis, they will be explained in detail in Section 4.


Table 1. List of acronyms.

Acronym	Description
TSN	Time-Sensitive Networking
AVB	Audio Video Bridging
CBS	Component-based Software Engineering
SR class	Stream Reservation class (A and B)
BE class	Best Effort traffic
ST	Scheduled Traffic
CBS	Credit-based Shaper
PW	Protected Window for the ST traffic transmission
3. Related work
Various schedulability analysis techniques have been proposed in the literature so far that compute the worst-case delay of traffic from classes A and B under credit-based shaping. Among them, the analysis presented in [20] adopts delay computation, while the ones described in [21] and [13] exploit network calculus. However, these analysis techniques aim at computing the worst-case per traffic class, not per individual message. As it is vital for many application domains (e.g., in automation) to know the worst-case delay of individual messages in the network, other analysis techniques have been proposed to bound the delay of each message. For instance, a schedulability analysis is given in [14] and further improved upon in [8]. The work in [23] presents a schedulability analysis to compute delays of messages from classes A and B using the trajectory approach, that obtains a tighter bound compared to the technique in [14]. The analysis presented in [30] calculates the message delay using Modular Performance Analysis (MPA). A more recent work in [10] presents the notion of Eligible Interval to compute the bounded delay per message, which delivers tighter bounds on the delay compared to previous techniques. However, none of the above mentioned analysis techniques takes into account scheduled traffic and the effect of time-aware shaping.

The work in [28] presents an analysis to compute the worst-case delay of traffic in TSN considering time-aware shaping for single-switch networks. Other works dealing with analysis techniques for time-aware shaping are [32], [34] and [33]. Nevertheless, all the above mentioned works mainly address the effects on the analysis of time-aware shaping, while preemption was not the main focus.

More similarities can be found between this work and the one in [31], that first develops an analysis for preemption support under the IEEE802.3br standard and then extends the analysis to the IEEE 802.1Qbv standard, under the assumption that scheduled traffic (time-triggered traffic in the paper) is of the express type (i.e., preempting), while the other traffic types are preemptable. However, the analysis presented in  [31] does not have any notion of offset and does not take into account the schedule of scheduled traffic considering different offsets in multi-hop architecture. Conversely, in the analysis proposed in this work we show that the express traffic and non-preempting high priority traffic do not behave in the same way and therefore their effect on the lower priority class becomes complex to analyze. The analysis in this work also considers the effect of the Hold and Release mechanism, that determines whether preemption occurs before or after the time at which the transmission gate for scheduled traffic opens. All these factors have a significant impact on the delays of traffic classes, in particular in multi-hop networks, where at each hop a specific offset is specified for the scheduled traffic for scheduling optimization.

The work in [7] presents a response time analysis for traffic classes A and B that considers credit-based shaping, time-aware shaping, and the scheduled traffic model given in [4]. The contribution of the work in [7] is different from this one, for two main reasons. In fact, the scheduled traffic model used in that work is different from the model in the IEEE802.1Qbv-2016 standard, that is the one assumed here. Moreover, the analysis in [7] does not deal with preemption.

Summarizing, this paper presents an analysis that encompasses credit-based shaping, time-aware shaping (i.e., the transmission gate mechanism) and preemption, in various combinations, according to the IEEE802.1Q-2018 standard. Comparing with previous works, the analysis technique here proposed has a more general validity and may nicely fit a number of time-sensitive applications, including industrial use cases.

4. System model
4.1. Network model
The TSN switches are considered to be full-duplex, i.e., the input and output of a switch port are isolated. Therefore, frame reception does not interfere with the transmission of other frames from the same physical switch port. A connection between a node and a switch, as well as connection of two switches is defined as a link. A link, denoted by , is a directional connection, which means that for each physical port two links are associated, one for sending and one for receiving. Moreover, each switch has a constant fabric latency due to the hardware configuration, which is denoted by . This delay may vary in each switch fabrication and it accounts for the time that it takes from receiving a frame through a switch port till inserting the frame into the output port queue. The link delay due to the wire and its physical characteristics is assumed to be negligible. The total network bandwidth is denoted by  and the value is the same in all network links.

The TSN standard considers various types of traffic shaping algorithms [16]. Here we focus on the credit-based shaping, that applies on SR classes only, and on the time-aware one, that affects all traffic classes. For SR traffic class X (i.e., Class A or B), on a link , the credit replenishment rate (idleSlope) is denoted by 
, while the credit consumption rate (sendSlope) is indicated as 
. Note that best-effort (BE) frames do not undergo credit-based shaping. Time-aware shaping opens/closes the transmission gates of the different queues of a link following the Gate Control List (GCL), an ordered list of gate operations that changes the state for the transmission gate associated with each of the Port’s traffic class queues and allows associated control operations to be scheduled. The GCL cyclically repeats.

4.2. Traffic model
In this paper we use the real-time periodic model for all classes of traffic, including the ST class. A set of  frames is characterized as follows: (1)
In this model we assume that each flow can be fragmented into multiple Ethernet frames, thus 
 represents the transmission time of frame 
, that depends on the frame size and network bandwidth. Moreover, the header of each Ethernet frame is included in the frame transmission time (
) and the header transmission time is denoted by , a constant value that is independent of the frame class.

In order to simplify the notation for the analysis, for each scheduled frame we convert the length of the frame into the transmission time, therefore, we identify the transmission time by 
. Consequently, 
 is the transmission time of 
 for all classes.

Moreover, 
 and 
 denote the period and relative deadline of the frame. We also assume a constrained deadline model, i.e., 
. The deadline is defined for a frame through all its links, thus the deadline decomposition per link for 
 is required, which can be achieved either dividing 
 for the number of links that 
 traverses or distributing it according to the load of each link. In this paper we propose a method to obtain a deadline per link proportional to the total load on the link. Note that the SR traffic classes can be initiated periodically or sporadically. In case of sporadic transmissions, 
 represents the minimum inter-arrival time. The class of a frame is denoted by 
 and we consider classes ST, A, B, and BE. The priority level is the highest for the ST class and the lowest for the BE class. In this paper, 
, 
 and 
 are the sets of frames with lower, same and higher priority than that of 
, respectively. As a frame may cross several links, the set of links that 
 traverses is specified by 
. The links in the set are ordered based on the order of frame transmission, i.e., 
 means that 
 crosses first link 
 and then 
. Offsets are used to accommodate ST frames in the transmission schedule. The offset for each ST frame is defined per link and the set of offsets for all links that 
 crosses is specified by 
, e.g., 
. We assume that the offsets are given, as scheduling optimization for ST frames is out of the scope of this paper. There is no offset defined for SR frames, so for SR frames 
 is an empty set. In addition, 
 is the queuing jitter of the frame 
.

In this model we consider the combination of preemption and scheduled traffic transmission in two modes, i.e., “with” and “without” the Hold and Release mechanism. In both operation modes, the traffic classes can be classified as express or preemptable. In our system model, the ST class is express, whereas all the other classes are preemptable. This means that the ST frames can preempt any transmission from the other classes, but cannot preempt other ST frames. For the analysis in this paper, we consider the following two models:

•
With Hold and Release mechanism: The Hold and Release mechanism provided by preemption allows to implement an explicit “guard band” before a transmission window for ST traffic, that is rather smaller than would otherwise be needed. The combination of scheduling, Hold and Release and preemption makes the ST traffic transmission window completely protected from interference from preemptable traffic, while at the same time reducing the impact of the protected window on the amount of bandwidth that is available to preemptable traffic. The guard band is specified by .

•
Without Hold and Release mechanism: in this mode, the preemptable traffic is allowed to continue transmitting up to 123 bytes even after the transmission gate of the express traffic has opened. In this mode, there is no guard band required and in fact the ST traffic may be delayed due to not fully protecting the interference. In the analysis when the mechanism is not enabled, the over-run of transmission after the gate open for express traffic (i.e., ST traffic) has to be considered.

In both operation modes, the preemptable traffic is resumed after preemption, but with a new header. This means that, for instance, when a frame is preempted and is split into two fragments, the second fragment also gets a header and a CRC, that should be accounted for frame overhead in this case. This has an effect on the analysis for SR classes, which will be discussed later in the analysis section.

The response time of frame 
 in this system model is specified as the interval between the time at which the frame is inserted in the queue of its source node and the time at which the frame is delivered to its destination node. The response time of 
 is denoted by 
. Moreover, the response time of 
 on link  is specified by 
. Note that we assume that the output port of a source node has a network interface that implements the same rules as the TSN switch output ports, i.e., the same shapers apply in the source nodes. Table 2 summarizes the notation in this paper.


Table 2. List of notations.

Notation	Description
A link in the network
The switch fabric latency
The total network bandwidth
The idleSlope for class  on link 
The sendSlope for class  on link 
The transmission time of 
The period of 
The deadline of 
The priority class of 
The set of offsets for 
 on its links
The set of links that 
 crosses
The queuing jitter of 
The Ethernet frame header
The guard interval
5. Response time analysis
This section presents a response time analysis for traffic classes A, B and ST. We first describe the overall approach to the analysis, then in the following subsections the detailed analysis is presented.

5.1. Analysis overall approach
In order to develop a response time analysis we consider each class of traffic separately. The main reason for this is that the effects on the response time of each class are different due to the shaping policies applied by the shapers. The traffic class ST has the highest priority in this model and the transmissions are offline scheduled. In this sub-section, we present the overall analysis when preemption is enabled and the Hold and Release mechanism is in use, hence there is a guard band to prevent any blocking on the ST traffic transmission. This way, ST frames are transmitted without any delay, so the response time is equal to the transmission time of ST frames.

Class A has the second priority in the system, i.e., lower than the ST class, but higher than the other classes, therefore, the higher priority interference for the Class A comes from the ST class. Note that the ST class can also preempt the transmission of frames from Class A. Consequently, in order to compute the response time of frames in Class A we need to consider the maximum possible number of preemption occurrences due to the ST frames. In addition, since in this model Class A is not preemptive, if frames from the lower priority classes are being transmitted, a Class A frame should wait for their entire transmission. This entails a blocking by the lower priority for Class A frames. Summarizing, both ST preemption and blocking by lower priority frames affect the response time of Class A frames.

Class B has lower priority than classes A and ST, hence frames from these classes can interfere with the transmission of Class B frames. However, the model enforces a special behavior from higher priority interference. Class ST can preempt Class B frames, whereas Class A cannot preempt Class B. This makes a scheme as a combination of preemptive and non-preemptive scheduling. In this paper, we follow a two-phase analysis in order to include the effects of both preemptive and non-preemptive behaviors from higher priority, as it will be described in the following sections. Similarly to Class A frames, Class B frames may also experience blocking due to the lower priority, which in this case is only best effort traffic.

Fig. 2 presents the described effects in an example. In this example there are four traffic classes on a port where the ST frames can preempt the other classes of traffic. Thanks to the gate mechanism, a Protected Window (PW) can be defined to avoid any ST frame to be blocked by other traffic classes, which are also shown in the figure for each and every ST frame. As it can be seen, the ST frames are transmitted without any interference, as they are scheduled offline and protected by the gate mechanism. The Class A frame that arrives after the BE one has to wait for the BE frame as it is already being transmitted, thus the Class A frame is blocked by the BE frame. Note that during the BE transmission an ST frame preempts the transmission as well. Class A frame can be preempted by the ST frames multiple times before it is fully transmitted. The Class B frame that arrived later is delayed by the Class A frame non-preemptively. Moreover, the Class B frame is delayed due to the ST frame preemption when the gates for all classes (including Class B) are closed for transmission. After the Class A frame is fully transmitted, as there is no other pending frame in Class A and assuming that the credit for Class B is zero or positive, the Class B frame can be transmitted. However, still due to the preemption by the ST frames and gate mechanism, the Class B frame can be delayed. It can be clearly seen in this example that the Class B frame is delayed by two types of higher priority, i.e., by Class A frame non-preemptively and by ST frames preemptively via the gate mechanism. In this paper, we take a two-phase method to consider the mentioned interference on Class B. In the first phase the interference from both Class A and preemption by the ST traffic is considered, while the second phase starts from the time in which there is no interference from Class A anymore but the Class B frame can still be preempted by the ST frames.

5.2. Response time analysis for Class A
This section presents the response time analysis for Class A frames when the transmission of scheduled traffic and preemption are enabled. For the response time analysis of frames in Class A four elements are required. These elements are: (i) interference from the higher priority frames, which in this case is only due to the ST frames, (ii) interference from the same priority frames in the FIFO queue, (iii) blocking by lower priority frames, and (iv) traffic shaper effect. We present each of these elements separately for one link, only. Then, we extend the analysis for multi-hop networks.

5.2.1. Interference from higher priority frames
As it is mentioned in the model, the ST frames are of express type, i.e., they can preempt Class A frames. Moreover, an offset per link is defined for each ST frame. Therefore, an offset-based analysis should be adopted for Class A. In the context of real-time tasks, the main challenge of the response time analysis for tasks with offsets is to find a critical instant. A critical instant for a task is an instant at which the task has the worst-case response time. For example, in the tasks scheduling without offsets the critical instant for a task is when all higher priority tasks are released at the same time as the task under analysis [24]. An exact response time analysis for the tasks with offsets are presented in [26], [29]. Here we first briefly review the analysis for tasks with offsets, then we show its adoption in the analysis for Class A frames in the TSN networks, where the higher priority ST frames are scheduled offline with offsets.

The analysis presented in [26] defines a transaction consisting of several periodic (or sporadic) tasks where the periods (or the minimum inter-arrival times) of all tasks are equal. Each task in the transaction has a specified offset such that the tasks in the transaction are not activated at the same time. Let us assume a task 
, where  is the index of the task belonging to transaction . Moreover, let us assume another task 
 which has lower priority than the tasks in transaction  and it is under analysis. It has been proved in [29] that activation of all tasks in transaction  at the same time as 
 may be a critical instant, hence activation of all tasks in the transaction  at the same time as 
 has to be verified as critical instant candidate. Then, the response time of 
 has to be computed taking all critical instant candidates into account where the larger one is the worst-case response time of 
. The interfering instances of 
 on 
 can be categorized into two sets: (i) activations that occur before the critical instant but they are delayed such that the activation coincide with the critical instant, (ii) activations that occur after the critical instant. Note that jitter in this case is the variation of delays in execution of tasks. It has been proved in [29] that the worst-case interference of 
 on 
 occurs when the activation of the former set (i) are delayed by an amount of jitter so that they all occur at the critical instant and when the activation of the latter set (ii) has zero jitter, i.e., they coincide with the critical instant. In order to compute the response time for each critical instant candidate, a phasing between two tasks should be considered. The phasing between 
 and the critical instant candidate 
, both belonging to transaction , is defined in Eq. (2) [26]. (2)
where, 
 and 
 are the offsets of 
 and 
, respectively. 
 is the jitter of 
 and 
 is the period of transaction .

Then, the total interference that transaction  imposes to 
 is computed in Eq. (3) [26], where 
 is the set of higher priority tasks than 
 in transaction . (3)
 
 

In case of Class A frames with class ST as high priority interference, we map the model in this paper to the transaction model explained above. As it is mentioned in the system model, the ST frames are scheduled cyclically. Let us denote the cycle by 
 on link . This cycle is the Least Common Multiple (LCM) of ST frames’ periods, i.e., 
. Then, the ST frames within time interval of 
 are scheduled with offsets, such that they are not activated at the same time. However, since we consider the LCM of the periods, some of the ST frames may be activated multiple times within the LCM. Considering the similarities of the ST frame scheduling with offsets and the transaction model, we can observe that the LCM can be seen as the transaction period in which several ST frames are scheduled with offsets. The only difference is that in the transaction model all periods of tasks are equal, so they are activated once within the transaction period, whereas during one LCM an ST frame might be activated multiple times. Fig. 3 shows an example of a transaction and an example of ST frames. In the transaction example, there are two tasks (
 and 
) with two different offsets (
 and 
) within the transaction period. Moreover, in the ST frames example there are two ST frames (
 and 
) with two different offsets (
 and 
) within the LCM of periods. As it can be seen in the figure, since 
 has a shorter period than the LCM (in fact, half of the period of 
), it will be activated twice within the LCM. In this case, the second instance of 
 can be seen as a “third” frame in the LCM with offset of 
.

Lemma 1

In case of ST frames, critical instant candidates are all activation times of ST frames within an LCM.

Proof

In the analysis for the transaction model the critical instant candidates are the activation times of tasks within the transaction period. In order to show that all activation times within one LCM should be counted as critical instant candidates, we create an example.

Fig. 4 shows an example of considering two scenarios for critical instants. In this example, there are two ST frames, where 
 has offset of zero and 
 has offset of 
. Moreover, the period of 
 is 5 time units and the period of 
 is 10 time units, thus the LCM is 10 time units. We assume to have a frame in Class A (depicted by ) and we are interested in its response time. The transmission times of 
, 
 and  are 1, 2 and 3 time units, respectively. In the first scenario (Scenario I in the figure) we consider that the critical instant is the time at which the first instance of 
 is activated. In this scenario the response time of frame  becomes 4 time units. In the second scenario (Scenario II in the figure) we assume that the critical instant is when the second instance of 
 within the LCM is activated. As it can be seen in the figure, the response time of  becomes 7 time units, which is larger than in the first scenario. This example shows that multiple instances of ST frames within an LCM should be considered as critical instant candidates. In other words, any activation of ST frames within an LCM can lead to a worst-case interference for a lower priority frame.  □


Download : Download high-res image (154KB)
Download : Download full-size image
Fig. 4. An example of two scenarios for critical instant candidates.

Another important observation is that the ST frame transmission time, depending on the Hold and Release mechanism, may include a guard band. The main difference between using and not using the Hold and Release mechanism is that without the mechanism there is no guard band, hence the SR traffic is allowed to continue transmission even after the gate is open for ST traffic. This can potentially make the ST frames being delayed. However, with the Hold and Release mechanism enabled the guard band prevents the transmission of SR traffic before the transmission of ST. The details of such difference is described below:

•
Without Hold and Release: In this mode, a frame from Class A can be transmitted up to 123B even after the transmission gate of the express traffic has opened. The critical instant candidate in this mode is the point at which the ST frame is ready for transmission. Assume in an example that a Class A frame is activated slightly before the ST frame becomes ready. This case is illustrated in Scenario I of Fig. 5. Therefore, the Class A frame can be transmitted pushing the transmission of ST frame further, only by 123B. In Scenario II, the Class A frame is activated just after the ST frame activation. In this case the Class A frame should wait until the ST frame transmission ends. We consider the critical instant at the point of ST frame activation making the worst-case situation for Class A frames.

•
With Hold and Release: In this mode, the mechanism prevents the transmission of SR classes before the ST activation, to make sure that the ST transmission occurs at its activation time, i.e., without any delay. Let us consider the example illustrated in Fig. 6. In Scenario I a Class A frame is ready right at the time when the mechanism “holds” the transmission of other classes than class ST, i.e., keeping a guard band shown as GB in the figure. Thus, the Class A frame transmission should wait for the guard band and the ST transmission. The Class A frame can also be activated right at the time of ST frame transmission, which is shown in Scenario II of the figure. In this case also the Class A frame transmission should wait until the ST transmission is done and the mechanism “releases” the transmission of other classes than the ST class. It can be clearly seen that considering the critical instant at the beginning of the guard band leads to the worst-case situation that makes the response time (RT in the figure) larger. Therefore, when the Hold and Release mechanism is enabled, all critical instant candidates at the beginning of the guard band should be taken into account for the analysis.

Now, assuming that the critical instant candidate is 
 the phasing between 
 and 
 on link  is computed in Eq. (5) following Eq. (2), where 
 contains the th member in the set 
. Note that 
 is the frame that we are considering to determine its interference on a frame in Class A under analysis. Therefore, there is no need to evaluate different instances of that, i.e., only multiple instances of the critical instant candidates should be evaluated in deriving the phasing. Thus, 
 is required as offset for 
, while 
 is required as offset for 
. (5)

The ST frames are scheduled offline, therefore no queuing jitter can be imposed on them by other frames and the notation of jitter is removed from the phasing calculation. Then, according to the worst-case interference calculation imposed by a transaction in Eq. (3), the worst-case interference imposed by class ST frames during time interval  when 
 is the critical instant candidate is calculated by Eq. (6). (6)
 
 

As it is mentioned in the system model (and according to the standard), when a Class A frame is being resumed after a preemption by an ST frame, a header and CRC is added to the frame again. Therefore, depending on the number of preemptions by the ST frames, the transmission time of the Class A frame under analysis may increase by several headers and CRC. In order to account for the effect of increasing the frame size in each preemption, we need to compute the number of preemptions on the Class A frame under analysis. The number of preemptions is equal to the number of ST frame instances during time interval of . Therefore, there is one header and CRC per ST frame instance (possible preemption) to be accounted for. The overhead based on the number of ST frame instances during the interval of , when 
 is a critical instant candidate, is shown in Eq. (7). (7)
 
 

5.2.2. Blocking by lower priority frames
It has been shown in [8] that considering at most one lower priority frame for the blocking is not enough. This is due to the traffic shaper behavior, as, on every replenishment of the credit, one frame from the lower priority may be ready for transmission. We show the insufficiency of considering one lower priority frame using an example. In this example,  is the frame from Class A under analysis, while B and A are lower and same priority frames, respectively. Initially, the credit increases as a lower priority frame (
) is transmitted on the link. Afterwards, a frame with the same priority, which is ahead of  in the FIFO queue, is transmitted. After the transmission of the same priority frame, the credit becomes negative, therefore there is room for transmitting another lower priority frame (
) with enough credit for transmission. Finally,  has a chance for transmission, as the credit is positive. In this example,  experiences blocking by two lower priority frames, i.e., 
 and 
.

However, it has been proven in [8] that considering an inflation factor for the same priority frames in the analysis makes it sufficient to take one lower priority frame for the blocking term. We use the following lemma to show that the same proof holds in the case of ST frames as well.

Lemma 2

It is sufficient to take one lower priority frame if the same priority frames are inflated by: (8)
 

Proof

We use the same methodology as presented in [8]. In order to show the idea we use an example. Let us assume a scheduling scenario for frame  depicted in Fig. 7. An interval of time is defined as the duration between the time at which the credit is zero and the time at which the credit is replenished to zero again, after the transmission of the ready frames. In order to show that inflation of the same priority frame covers the blocking by the lower priority frame in presence of ST frames, we define an interval , during which frames from Classes A, B and ST are transmitted. In this example, we assume that the ST frame is not preempting other frames, i.e., 
 finishes just before and  becomes active in the middle of the ST frame transmission. Note that the credit during the ST frame transmission is constant.

As the interval is defined between two zero credits, the total credit value remains zero. Thus, the credit value for the interval becomes: (10)

Deriving  from the above equation, and inserting it to the interval length calculation (Eq. (9)) we have the following: (11)
 

As it can be seen, the length of the interval does not depend on the transmission time of lower priority frames. However, the interval depends on the same priority frame  while it is inflated by 
 
. Moreover, the transmission time of the ST frame appears in the interval calculation. Note that the effect of the ST frames is already considered in the analysis, hence it can be neglected in the same priority interference calculation.  □

According to Lemma 2, only blocking by one frame from lower priority is sufficient, which is the largest frame among the lower priority frames than Class A, as shown in Eq. (12). (12)
 

5.2.3. Interference from same priority frames
In order to capture the worst-case scenario, we assume that all same priority frames in the FIFO queue are ahead of the frame under analysis. As we assumed the model to be constrained deadline, in a schedulable system, only one instance of the same priority frames can be ahead of the frame under analysis [12]. Moreover, as we consider to inflate all same priority frames to cover the need for accounting for lower priority frames, the same priority interference is shown in Eq. (13). (13)
 

5.2.4. Traffic shaper effect
In the worst-case scenario the credit of the traffic shaper must be considered to be as negative as possible when the frame under analysis is ready for transmission. In this case, the traffic shaper blocks the frame until the credit increases to zero. However, as it was discussed in the same priority interference calculation, all the frames in the same FIFO queue as the frame under analysis are inflated to cover the replenishment time of the credit. Consequently, as the replenishment times of all same priority frames are taken into account, the negative credit is also considered.

5.2.5. Response time calculation
The response time of 
 on link  when th instance of 
 is considered as a critical instant candidate is calculated by summation of all effects, iteratively, as shown in Eq. (14). The iteration starts from 
 and terminates when 
 or the response time becomes larger than the deadline defined for link . Note that  is the number of iterations. (14)

The largest response time among all critical instant candidates is the worst-case response time of 
, as computed in Eq. (15). (15)
 

Note that the equations for computing the worst-case response time are linear and iterative. The algorithm to compute such a response time requires a loop on the number of frames , number of critical instant candidates to verify , number of instances in the critical candidates  and number of links , i.e., . However, the time complexity depends on the value of inputs, which makes the equation pseudo polynomial time complexity.

5.3. Response time analysis for Class B
In case of Class B not only ST frames, but also Class A frames can interfere with the transmission. However, the higher priority interference has two different models. ST frames can preempt Class B frames, whereas Class A frames cannot preempt Class B frames. This is due to the fact that the ST frames are of express type, while Class A and B frames are of preemptable type. Furthermore, considering one instance of the frame under analysis is not sufficient. Instead, the response time of several instances of the frame during a busy period [22] must be calculated and the maximum among them is the worst-case response time. The busy period is the maximum time interval during which the resource is busy. Note that in the TSN networks the resource is busy either when there is an ongoing transmission on the link or when the queue is not empty, but the transmission is prevented due to a negative credit. The reason behind the need for considering multiple instances is the non-preemptive nature of the transmission in classes A and B, which is thoroughly discussed in the Controller Area Network (CAN) response time analysis [11].

Under high network utilization, a frame may delay subsequent transmission of the higher priority frames. Thus, the higher priority interference may be pushed through into the next period of the frames, causing larger response time in the next instance. Let us consider an example in Fig. 8, where we are interested in computing the response time for 
. In this example, 
 and 
 are higher priority frames than 
. Moreover, 
 has period of 4 time units, while the period of the other frames is 6 time units. The transmission time of 
 and 
 is 2 time units and for 
 is 1 time unit. The first instance of 
 is completely sent at time 5, hence its worst-case response time is 5 time units if we consider the first instance only. However, 
 is ready at time 4, but it cannot preempt 
 due to the non-preemptive nature of the transmission. Thus, its transmission starts at time 5 and its third transmission starts at time 8, thus pushing the transmission of 
. Then, transmission of the second instance of 
 starts at time 10 and completes at time 12, thus making the worst-case response time for the second instance equal to 6 time units instead of 5. Therefore, in the calculation of the worst-case response time, several instances should be examined.

Similar to the analysis for Class A, four elements should be considered, which include: (i) interference from higher priority frames (classes A and ST), (ii) interference from same priority frames (Class B), (iii) blocking by the lower priority frames (BE), and (iv) the traffic shaper effect. Given the th instance of frame 
 from Class B under analysis in the busy period, we compute the queuing delay. The queuing delay is the largest time from the start of the busy period until the beginning of the transmission of the th instance of the frame. However, after the busy period when th instance of 
 gets a chance for transmission, due to the preemption mechanism of the ST frames, the frame under analysis may further get preempted. Fig. 9 illustrates a scenario for frame . Assume that frame  is under analysis. The busy period is the time that the transmission of  is blocked due to blocking by the lower priority frames, negative credit, interference from ST, and Class A frames. For simplicity of illustration, the blocking and interference from all sources during the busy period is shown as a gray box. When the busy period is finished, the frame initiates its transmission. However, the ST frames can still appear to preempt the transmission of . This occurs due to the two types of higher priority interference, i.e., non-preemptive from Class A and preemptive from class ST.


Download : Download high-res image (55KB)
Download : Download full-size image
Fig. 8. An example of multiple instances.

In order to calculate the response time in this model, the response time analysis should be done in two phases. In the first phase, the total amount of busy period should be calculated considering interference and blocking from all sources. In the second phase, the busy period should be added to the frame transmission time and possible preemptions by the ST frames after the busy period. The following lemma proves that in the second phase only preemption can delay the transmission.


Download : Download high-res image (117KB)
Download : Download full-size image
Fig. 9. Illustration of the busy period.

Lemma 3

After the busy period, the frame under analysis can be only interfered by multiple preemptions from ST frames.

Proof

The interference to a Class B frame is from lower priority frames, same priority frames and higher priority frames from Classes A and ST. After the busy period the only chance to get interference is during the preemption time interval. We define various cases to show whether the interference can occur during the preemption time interval. There are four cases that can occur during the preemption time interval: (i) traffic shaper interference, (ii) low priority blocking; (iii) same priority interference; (iv) higher priority interference from Class A.

•
Case 1 — traffic shaper: during the preemption time interval, the credit can become negative. However, according to the standards, the frame which is preempted will be resumed even if the credit is not zero or positive. Therefore, the traffic shaper cannot affect again during the preemption time interval.

•
Case 2 — lower priority blocking: the only chance for the lower priority frames to initiate the transmission is when the credit for higher priority frames is negative. However, in this case the credit can be negative during the preemption time interval, while according to the standards the frame transmission should be resumed. Therefore, the lower priority frames will not get a chance to start during the preemption time interval.

•
Case 3 — same priority interference: when the frame in a queue is ready for transmission then it has to be the first in the queue due to the FIFO queue model. During the preemption time interval, the frame in the FIFO queue is still not fully transmitted, hence other frames in the same queue will not start. Consequently, the same priority interference will not occur during the preemption time interval.

•
Case 4 — high priority interference of Class A: it can occur that, during the preemption time interval, the credit for Class B becomes negative, while there is a pending frame in Class A with positive credit. Fig. 10 shows this scenario when the Hold and Release mechanism is enabled. The interference from other classes is shown by a dotted box on Class B transmission. After the interference, the frame in Class B is ready for transmission. However, during its transmission it is preempted by an ST frame. The credit of Class B becomes negative because of the transmission, while credit for Class A becomes positive as it becomes ready. Note that during the ST transmission the credits are frozen. As the frame from Class A is ready for the transmission (the arrow shows the activation in the Class A transmission) and the credit is positive, it should thus get the channel. However, according to the standards, the frame which is suspended should be able to resume first. Therefore, in this case Class B frame will be resumed after preemption.

In the following, we compute the source of delays separately and then we present two phases of the response time calculation.

5.3.1. Interference from class ST frames
Similar to the analysis for Class A frames, the ST frames are scheduled with offsets, thus the offset-based analysis should be taken into account. The reasoning for the critical instant candidates is still valid and the worst-case interference imposed by class ST frames can be calculated as in Eq. (6). Moreover, when the ST frames preempt Class B frames, a new header and CRC are attached to the frame. Therefore, the effect of increasing the frame size due to a new header in each preemption should be considered, which is again computed in Eq. (7).

5.3.2. Interference from Class A frames
The high priority interference within the busy period is calculated in Eq. (16), the same way as high interference calculation for non-preemptive model [11]. The equation is shown for th instance of 
 and th instance of critical instant candidate 
 on link . The queuing delay is denoted by 
. Note that the queuing jitter of Class A frames on link  is denoted by 
, which is described later in this section. (16)
 

5.3.3. Blocking by the lower priority frames
As it is mentioned in the analysis for Class A, considering at most one lower priority frame for the blocking is sufficient if the inflation factor is applied on the same priority frames. Therefore, the blocking term is shown in Eq. (12).

5.3.4. Interference from same priority frames
Similar to the analysis presented for Class A, the worst-case scenario occurs when all same priority frames are ahead of the frame under analysis. Moreover, the same priority frames should be inflated to cover for multiple lower priority blocking. Therefore, the interference is calculated as shown in Eq. (17), where multiple instances of 
 should be considered in case the queuing delay exceeds one instance, i.e., . (17)
 
 

5.3.5. Traffic shaper effect
In case of Class B analysis, the worst-case situation occurs when credit is as negative as possible when the frame under analysis is ready for transmission. Similar to the Class A analysis, since we consider the replenishment time of the credits for all same priority frames, there is no need for any negative credit.

5.3.6. Response time calculation
As it is mentioned before, the analysis should be done in two phases. We start with the first phase, which is calculating the queuing delay during the busy period. The queuing delay is the time duration in which all sources of delay can postpone the transmission of 
 and the bus is not idle. Therefore, the queuing delay on link  for th instance of 
 when th instance of 
 is considered as the critical instant candidate is computed, iteratively, in Eq. (18). (18) 
 

Note that the last term in the equation is to accommodate several instances of frame 
 itself in the queuing delay, in case the computation exceeds more than one instance.

In the second phase, we compute the response time by adding the queuing delay to the transmission time of 
 and possible preemption after the busy period. It is important to mention that the calculation is iterative and the iteration starts from 
 that is computed from Eq. (18) and terminates when 
. (19) 
 

The above equation contains five terms. The first term is the queuing delay during the busy period, as depicted in Fig. 9. The second term is the interference from the ST frames preempting 
, while the third term represents the increasing of frame size due to adding a new header and CRC at each preemption. The fourth term is the transmission time of the frame itself. Finally, the last term is the number of periods for frame 
 that have passed during the busy period. The calculation should be done for the range of instances 
, where the maximum result among the range is the response time for the critical instant candidate. (20)
 

The maximum number of times 
 that the response time should be evaluated is derived as the smallest positive integer value from Eq. (21). The left side of the equation is the length of the busy period, hence dividing that by 
 gives the maximum number of instances that have passed during the busy period. To compute the length of the busy period, the interference and blocking from all sources should be added to the transmission time of the frame under analysis. (21) 
 
 

Then, the worst-case response time for 
 in Class B on link  is computed in Eq. (22) as the largest response time among all critical candidates. (22)
 

Note that the equations for computing the worst-case response time are linear and iterative. The algorithm to compute such a response time requires a loop on the number of frames , number of critical instant candidates to verify , number of instances in the critical candidates  and number of links , i.e., . However, the time complexity depends on the value of inputs, which makes the equation pseudo polynomial time complexity.

5.3.7. Queuing jitter for Class A
In the analysis presented in [7] the need for considering jitter when computing the response time in multi-hop networks is discussed. The queuing jitter of Class A can affect the response time of a frame in Class B. In this section, we present the effect of queuing jitter from Class A on the Class B analysis.

In this work, we apply jitter similarly to the other response time analysis for switched Ethernet networks, e.g., [27], by adding it to the calculation of busy period. In order to compute the queuing jitter of a frame in Class A, we need to find the difference between the worst-case and the best-case response times of the frame from its source node to the link on which we are calculating the response time of 
 in Class B. Eq. (23) derives the queuing jitter of 
 from Class A on link , where 
 is the best-case response time of 
 on link . (23) 
 

5.4. Response time of multi-hop architectures
In a multi-switch TSN architecture, frames are buffered in the queues of each switch through their route. Thus, the worst-case response time of a frame traversing multiple switches is the sum of the per-hop response times, as shown in Eq. (24). Note that the wire latency is neglected in this calculation, whereas the switch fabric latency for each hop is considered for each link. Eq. (24) is used for both classes A and B. (24)

5.5. Response time of ST traffic in multi-hop architecture
The ST traffic is scheduled offline with an offset. The scheduling scheme affects the response time of ST frames on each link. Here we present the response time of ST frames for two cases of with and without Hold and Release mechanism. First, we start with the Hold and Release mechanism being enabled.

In order to present the response time of ST frames independent of the scheduling scheme we define a new notion of gate open time. Gate open time is defined as the time in which a gate for frame 
 is open regardless of its arrival time, and we denote it as 
 per link . We also denote a time in which a frame 
 arrives to an ST queue by 
. Note that the release time of frame 
 is synchronized with its initial offset on the first link. Therefore, the delay of an ST frame is computed from the time it releases, i.e., it counts after the defined offset. In consequence, the delay of crossing the first link for 
 is equal to 
. The interframe gap (IFG) is not considered in the calculation of 
.

On the following links, however, the delay depends on the arrival time to the queue, i.e., 
, and the gate open time, i.e., 
. This means a frame can stay in the queue until the gate is open, which this delay is 
. Moreover, the frame is delayed by the fabric latency . Therefore, on links other than the first link the time that the frame is fully transmitted is computed as: (25)

Fig. 11 illustrates an example of two links to show the above equation. The same calculation is valid for links until the last link where the destination node is connected.

The response time of 
 is computed by Eq. (26), where  when the Hold and Release mechanism is enabled. (26)

5.6. Deadline decomposition
According to the system model in this paper, the deadline is defined as an end-to-end deadline, i.e., the deadline is defined for a frame through all its links. Therefore, the deadline decomposition is required to obtain a specific deadline per link. This can be done in several ways. In a simple method the end-to-end deadline can be divided equally among the links, which is not efficient because of different load on each link. In this section we describe a method to divide the deadline proportional to the load of each link. We specify the deadline per link by 
. Therefore, the proportion of the load on link  compared to the total load on all links that frame 
 crosses can give the link deadline. We use Eq. (27) to derive the deadline for link , where 
 denotes the load on link  besides frame 
. (27)
 

In order to compute the load per link, we have to consider the nominal utilization of traffic on the link. The nominal utilization for a frame 
 is computed as 
 
. Thus, the total load on link  is computed as summation of nominal utilization from all possible traffic passing through link , which is shown in Eq. (28). (28)
 

In the above equation, the first term shows the nominal utilization used by the lower priority message. As there is only one frame (and the maximum frame size) can cross the link, thus the maximum utilization among all lower priority frames is taken into account. Moreover, the second term shows the nominal utilization used by higher priority and same priority frames, while the last term shows the nominal utilization used by the ST frames.

5.7. Effects of bandwidth on the response times
This section describes the effects of allocating various bandwidth on the worst-case response time of a frame.

The total bandwidth is denoted by  and its direct effect is on the transmission time of frames 
. By increasing  the size of a frame and its transmission time decreases. Therefore, the delays on frame 
, which is due to the number of frames that interfere with frame 
 decrease.

However, the network bandwidth allocation can have a more complicated effect. According to the standard, the total bandwidth is equal to the summation of idleSlope and sendSlope, i.e., 
. If the idleSlope increases proportionally to the bandwidth, then the same priority interference, according to Eq. (13), does not change. This is because the same priority interference depends on the inflation factor 
 
. The inflation factor can be written as below considering that the summation of idleSlope and sendSlope is equal to the total bandwidth. (29)
 
 
 

Above, if the idleSlope increases proportionally with , then the inflation factor remains the same value, hence the same priority interference remains the same. In contrast, if the idleSlope allocation increases considering a constant value for , the inflation factor decreases, thus the same priority interference decreases. Similarly, if the total bandwidth increases without any change in the idleSlope, the inflation factor increases making the same priority interference larger (according to Eq. (13)). This shows that by only increasing the total bandwidth we cannot obtain the shorter response time and the value of idleSlope plays a significant role.

6. Simulative assessment
This section presents simulative assessments of TSN in three realistic scenarios (i.e., A, B and C) with different traffic patterns and topologies. In particular, scenario A addresses a line topology and provides simulation results at 1 Gbps and 100 Mbps. Scenario B addresses an industrial use case in which periodic real-time data flows are mapped onto the SR classes, while the Scenario C addresses an automotive scenario with a high number of ST flows.

The simulation model was implemented using the OMNeT++ framework and the INET libraries. The physical layer was modified in order to support preemption as defined in the IEEE 802.1Qbu standard. The MAC layer modules supporting the IEEE 802.1Q-2018 [16], including both the enhancements for scheduled traffic and preemption support, were implemented from scratch.

No clock synchronization protocol was implemented in the simulation model, as the clocks are assumed to be synchronized.

Statistics on the delays are taken at the application level. No processing time on the nodes is assumed (only the switch fabric latency is considered).

The simulation model was validated comparing the timing parameters calculated under predictable test scenarios with those obtained in the simulation.

6.1. Scenario A
Fig. 12 shows the simulated network, that includes six switches connected in a line topology and eleven host nodes, ten of which are the talkers (i.e., N1, …, N10) and one is the listener (C).

The envisaged scenario simulates a network with flows typical of industrial scenarios [15], such as, periodic real-time sensor data (with and without low jitter requirements), encoded camera streams at 30 frames per second (FPS), and best-effort traffic for the transmission of logs and configuration files. In the simulated scenario, multiple traffic types are transmitted to a controller.

Nodes N4, N5, N8 and N9 transmit sensor data with strict jitter requirements and periods in the order of hundreds of microseconds. These flows are mapped onto the Scheduled Traffic class. Nodes N1, N7 transmit video streams at 30 FPS, in which each application message (i.e., one video frame) is 42 000 bytes long. Video streams are mapped onto Class A. In order to transmit Class A video flows while meeting the constraints imposed by the Stream Reservation Protocol, the video streams were reshaped and each application message was split into multiple Ethernet frames. In particular, to comply with the Stream Reservation Protocol, each Class A frame is transmitted to the Ethernet port every μ. Here the adopted solution is to split the application message into  Ethernet frames of length equal to 168 bytes, as presented in Table 4.


Table 3. Scenario A: Application flow parameters.

Flow	Source	Payload (B)	Period (μ)	Class
ST1	N4	64	200	ST
ST2	N5	100	500	ST
ST3	N8	64	200	ST
ST4	N9	100	500	ST
A1	N1	42 000	31 333	A
A2	N7	42 000	31 333	A
B1	N2	200	100 000	B
B2	N6	1000	200 000	B
BE1	N3	125 000	1 000 000	BE (Priority 1)
BE2	N10	42 000	50 000	BE (Priority 2)
Nodes N2 and N6 transmit no jitter-constrained sensor data. To comply with the Stream Reservation Protocol, for SR Class B flows, each frame is transmitted to the Ethernet port every μ. Finally, the transmissions of log and configuration files from nodes N3 and N10 are mapped onto the best-effort class.

Two cases were simulated to compare the maximum response time measured in the simulation with the worst-case response time calculated by the proposed analysis. The first case refers to a 100 Mbps network, whereas the second case to a 1 Gbps one. In both cases, we assume preemption enabled with the Hold and Release mechanism and the switch fabric latency is set to 5.2 us.


Table 4. Scenario A: SR flow parameters.

Flow	Class	Num. frames	Frame size (B)
A1	A	250	168
A2	A	250	168
B1	B	4	50
B2	B	20	50
The ST flows are scheduled according to an offset-based scheduling algorithm. The size of the protected window is set so as to accommodate the maximum frame size plus the inter-frame gap (IFG, 12 bytes) and the preamble (PRE, 8 bytes), according to Eq. (30). (30)
 

The protected window (PW) duration for each flow and the offset (at the talker) are shown in Table 5, respectively. At each hop of the ST flow, the protected window offset is increased by 25 us for 100 Mbps simulations and by 10 us for 1 Gbps simulations, so as to always obtain the same spacing between two consecutive PWs.

Table 6 shows the idleSlope parameters, calculated according the IEEE 802.1Q standard, Sect. 8.6.8.2 [19], for all links of the example scenario. Note that the values are the same in both simulation cases.


Table 5. Scenario A: Parameters of the ST flows.

Scen.	Flow	ST1	ST2	ST3	ST4
100 Mbps	
 (μ)	8.48	11.36	8.48	11.36
Offset (μ)	25	50	75	325
1 Gbps	
 (μ)	0.848	1.136	0.848	1.136
Offset (μ)	10	60	140	290
Table 7 presents the results of both simulation cases, i.e., at 100 Mbps and 1 Gbps, indicated as Case 1 and Case 2, respectively. The table shows the maximum response time of real-time frames measured in the simulation (Sim) and the worst-case response time calculated using the proposed analysis (Calc). Moreover, it shows the average and minimum frame delays obtained in the simulations. Simulation results for the BE traffic are not reported in the table, as such a traffic does not require any response time guarantee, so no comparison was possible. The BE traffic was introduced in the simulation to make the preemption scenario more realistic and interesting, as the BE traffic introduces blocking for the SR traffic, i.e., if there is an ongoing BE transmission, the SR frames have to wait until the end of BE frame transmission. As it can be seen from Table 7, for the ST frames in this case the measured values and the analysis results are the same, as these frames experience no interference (by design, being the highest priority class) and no blocking (thanks to the Hold and Release mechanism). For Class A frames, the response times obtained through simulation and the ones calculated through the analysis are very close to each other, the analysis values being slightly larger. Conversely, the results for Class B frames show a pessimism in the analysis results comparing with the simulation ones. This result for Class B frames in this example was somehow expected and is easily explainable. In fact, the pessimism in both classes A and B analysis stems from several factors. The ST class is of express type and can therefore preempt the other classes. In order to account for the preemption effect in the analysis, we calculated the worst-case number of preemptions, that may not actually occur in the simulation. Moreover, in order to compute the same priority effect in a FIFO queue, we considered the effect of traffic shaping (i.e., inflating the same priority frames), which is another source of pessimism. However, it can be observed that the pessimism is higher for Class B response time calculation. This is not only due to the pessimism derived from the worst-case number of preemptions by class ST, but also due to the pessimism in calculating the interference from Class A. In addition, we calculated the queuing jitter coming from Class A on Class B, which inherits the pessimism from the worst-case response time calculation of Class A. These factors accumulate at each and every link in the response time calculation. We intend to investigate in more detail and refine the analysis in order to reduce the pessimism in future work.


Table 6. Scenario A: Link parameters.

Link	L0	L1	L2	L3	L4	L5	L6	L7
idleSlope (A) Mb/s	13.44	–	–	13.44	–	–	13.44	–
idleSlope (B) Mb/s	–	7.75	–	7.75	–	–	7.75	33.44
Link	L8	L9	L10	L11	L12	L13	L14	L15
idleSlope (A) Mb/s	13.44	13.44	26.88	–	–	26.88	–	26.88
idleSlope (B) Mb/s	41.09	–	41.09	–	–	41.09	–	41.09

Table 7. Response time of real-time traffic in Scenario A.

Flow	Single frame worst response time (μ)	Average Sim (μ)	Min. Sim (μ)
Case 1 Sim	Case 1 Calc	Case 2 Sim	Case 2 Calc	Case 1	Case 2	Case 1	Case 2
ST1	133	133	51	51	133	51	133	51
ST2	136	136	52	52	136	52	136	52
ST3	58	58	21	21	58	21	58	21
ST4	61	61	22	22	61	22	61	22
A1	1279	1338	119	175	517	87	142	84
A2	853	854	58	99	425	24	79	22
B1	898	5112	278	288	608	160	212	46
B2	805	4657	67	136	674	65	653	65
Despite the pessimism found for Class B, the proposed analysis is a useful tool for network designers to configure their TSN networks according to the traffic properties. This analysis can be a guideline for the network designers to verify the allocation of bandwidth (idleSlopes) on each link considering a set of traffic flows. Moreover, the analysis can be integrated into software development tools, such as Rubus-ICE3 similarly to the work in [5], in which a response time analysis for AVB traffic was integrated into Rubus-ICE. Such a tool is used by software developers to design systems for industrial applications.

6.2. Scenario B
The second assessed scenario (Scenario B) is shown in Fig. 13. This scenario was derived from the Industrial Use Cases IEC/IEEE 60802. An industrial production cell was simulated. The simulated production cell includes three machinery (A, B, and C), each one with 5 sensor/actuator nodes (i.e., N_Ax, N_Bx, and N_Cx, where x is the node number), one PLC that is the controller and one monitoring station (HMI) in which the production can be monitored and some configuration can be done. All of these nodes transmit sampled data to the PLC, while video streams are transmitted to HMI. Moreover, the HMI transmits configuration data and commands to the PLC.

This scenario provides four kinds of traffic.

•
Isochronous control loop traffic with guaranteed low latency: Isochronous applications are synchronized to the network access.

•
Cyclic real-time control loop traffic with bounded latency: Cyclic traffic pattern for non-isochronous applications, which are not synchronized to the network access, but are synchronized to a local timescale.

•
Video stream: Periodic traffic with bounded latency.

•
Best Effort traffic: Traffic for configuration, management and logging.

Table 8 shows the parameters of the flows.

In this scenario, the isochronous traffic is mapped onto the ST class and consists of small frames transmitted according to a fixed scheduling. The cyclic real-time control traffic (i.e., the flows A1–A3 and B1–B3) is mapped onto the SR Class A and Class B, according to the relative deadline, that in this scenario is set equal to the flow period. Note that this traffic flows are not streams, so the idleSlope values for these flows cannot be calculated according to the SRP. Hence, the analysis proposed in this paper is useful to tune the idleSlope values in order to guarantee the deadline requirements. The video streams (i.e., the flows B4–B6) are mapped onto Class B. In this case the video streams were reshaped to meet the requirements imposed by the SRP. Each application message was split into multiple Ethernet frames according to the same approach used in Section 6.1. The parameters for the SR flows are shown in Table 9.


Table 8. Scenario B: application flows.

Flow	Source	Dest.	Payload (B)	Period (ms)	Relative deadline (ms)	Class
ST1	N_A4	PLC	100	4	4	ST
ST2	N_B4	PLC	100	4	4	ST
ST3	N_C4	PLC	100	4	4	ST
ST4	N_A5	PLC	50	1	1	ST
ST5	N_B5	PLC	50	1	1	ST
ST6	N_C5	PLC	50	1	1	ST
A1	N_A1	PLC	600	30	30	A
A2	N_B1	PLC	600	30	30	A
A3	N_C1	PLC	600	30	30	A
B1	N_A2	PLC	900	60	60	B
B2	N_B2	PLC	900	60	60	B
B3	N_C2	PLC	900	60	60	B
B4	N_A3	HMI	42 000	33.33	100	B
B5	N_B3	HMI	42 000	33.33	100	B
B6	N_C3	HMI	42 000	33.33	100	B
BE1	N_A1	PLC	1000	500	–	BE (Priority 4)
BE2	N_B1	PLC	1000	500	–	BE (Priority 4)
BE3	N_C1	PLC	1000	500	–	BE (Priority 4)
BE4	HMI	PLC	125 000	1000	–	BE (Priority 1)
Finally, there are four best effort traffic flows. These flows are added to make the simulation more realistic, as the BE traffic may delay the transmission of the SR traffic as explained in Section 5.


Table 9. Scenario B: SR flow parameters.

Flow	Class	Num. frames	Frame size (B)
A1	A	1	600
A2	A	1	600
A3	A	1	600
B1	B	1	900
B2	B	1	900
B3	B	1	900
B4	B	120	350
B5	B	120	350
B6	B	120	350
The configuration parameters for the ST flows are shown in Table 10.

In this scenario the protected window for each flow was sized according to Eq. (30). The ST flows are scheduled according to an offset-based scheduling algorithm. In this case the offset at each hop is not increased, so ST frames are immediately transmitted when they arrive to the port queue.


Table 10. Scenario B: ST flow parameters.

Flow	ST1	ST2	ST3	ST4	ST5	ST6
 (μ)	11.36	11.36	11.36	7.36	7.36	7.36
Offset (μ)	150	300	450	600	750	900
Table 11 shows the idleSlope parameters for all the links of the example scenario.

Table 12 shows the comparative results between the maximum response time obtained through simulations (Sim) and through the analysis (Calc). Moreover, the average and minimum delays obtained through simulations are shown.


Table 11. Scenario B: Link parameters.

Link						
idleSlope (A) Mb/s	20	20	20	–	–	–
idleSlope (B) Mb/s	–	–	–	17	17	17
Link						
idleSlope (A) Mb/s	–	–	–	20	20	20
idleSlope (B) Mb/s	13	13	13	30	30	30
Link					–	–
idleSlope (A) Mb/s	–	20	20	–	–	–
idleSlope (B) Mb/s	13	43	55	39	–	–
For the ST flows there is no difference between the calculated response times and the ones obtained through simulations, as the ST traffic does not experience any interference or blocking.


Table 12. Single frame response time of real-time traffic in the considered Scenario B.

Flow	ST1	ST2	ST3	ST4	ST5	ST6	A1	A2	A3	B1	B2	B3	B4	B5	B6
Sim μ	41	57	57	57	57	57	276	542	786	427	684	834	585	532	384
Calc μ	41	57	57	57	57	57	1059	1546	1546	1101	1658	1658	751	746	746
Sim. Avg. μ	41	57	57	57	57	57	165	444	694	295	445	582	197	108	138
Sim. Min. μ	41	57	57	57	57	57	161	335	592	284	383	533	182	101	101

Table 13. Scenario C: Application flow parameters.

No.	Talker	Listener	Period (ms)	Size (Bytes)	Class
ST1	DA-Cam	HU	1000	46	ST
ST2	DA-Cam	HU	200	46	ST
ST3	DA-Cam	CU	1000	46	ST
ST4	DA-Cam	CU	200	46	ST
ST5	HU	CU	5	46	ST
ST6	HU	CU	50	46	ST
ST[7–8]	HU	CU	100	46	ST
ST9	HU	CU	200	46	ST
ST10	HU	CU	500	46	ST
ST[11–12]	HU	CU	1000	46	ST
ST13	HU	DA-Cam	100	46	ST
ST[14–15]	HU	DA-Cam	200	46	ST
ST16	CU	HU	100	46	ST
ST17	CU	HU	200	46	ST
ST[18-19]	CU	HU	500	46	ST
ST20	CU	HU	1000	46	ST
ST21	CU	DA-Cam	10	46	ST
ST22	CU	DA-Cam	1000	46	ST
A[1–4]	Cam[1–4]	DA-Cam	33.33	42 000	A
A5	DA-Cam	HU	33.33	42 000	A
A6	Telem.	RSE	0.625	600	A
B1	Telem.	HU	5	400	B
B2	cd/dvd	RSE	33.33	42 000	B
B3	cd/dvd	RSE	0.25	80	B
BE1	Telem	HU	1000	250 000	BE (Prio 0)
As far as the cyclic real-time control flows (i.e., A1–A3, B1–B3) are concerned, the calculated response times are higher compared to the ones obtained through simulation. In particular, for the SR Class A flows the pessimism is higher than in the Scenario A. This is mainly due to the calculation of the interference from the same priority frames, which in this scenario is larger than in the Scenario A, as here the SR Class A frames are larger.

The response time difference for video streams (i.e., B4–B6) is very low, as in this scenario video streams do not suffer from the interference of Class A flows and therefore the analysis is less pessimistic. As far as the average and minimum delays are concerned, the difference between the average delays and the maximum delays obtained in the simulations is higher for the SR class B flows, as they suffer from the interference of class A flows.

6.3. Scenario C
The third assessed scenario (Scenario C) is shown in Fig. 14. This scenario is an automotive in-vehicle network that handles the communications of two different domains, i.e., Automated Driver Assistance (ADAS) and multimedia/infotainment. Nodes are connected to switches based on their position and to balance the number of required ports among switches. The network datarate is set at 100 Mb/s and preemption is enabled in HOLD/RELEASE mode.

The ADAS system consists of four cameras (i.e., Cam1-Cam4 in Fig. 14), which transmit video streams to the DA-Cam. The DA-Cam is a specialized Electronic Control Unit (ECU) that processes video streams and produces aggregated video and navigation warnings. The data produced by the DA-Cam is sent to both the Head Unit (HU), that displays visual information for driver assistance, and the Control Unit (CU). The CU acquires sensor data (e.g., from ultrasonic sensors), merges this data with the information received from HU and DA-Cam and sends real-time control messages to the DA-Cam and to the HU.


Download : Download high-res image (216KB)
Download : Download full-size image
Fig. 14. Scenario C. Network Topology.

This scenario also includes a multimedia/infotainment system that consists of a DVD/Audio-CD node that transmits both video and audio stream to the Rear Seat Entertainment (RSE) system. Finally, a telematic subsystem transmits information, such as GPS, traffic alerts, maps, etc. to the HU and to the RSE. Multimedia/infotainment traffic is real-time, but not safety-critical.

Table 13 shows the parameters of the flows.

The control frames transmitted by the ADAS system are mapped onto the ST class, while video streams are mapped onto the ST Class A, as these flows do not impose strict jitter requirements provided that each video frame is received within 33 ms. Conversely, the multimedia/infotainment flows are mapped onto SR Class B, as their deadlines are higher than those of ADAS traffic.

Table 14 shows the configuration parameters for each SR flow. The last column shows the number of frames the message is split into, while the fourth column shows the size of each segment (in bytes).

The idleSlope parameters for each link are shown in Table 15.


Table 14. Scenario C: SR flows parameters.

Flow	Class	Frame size	Num. frames
A[1–5]	A	168	250
A6	A	120	5
B1	B	50	8
B2	B	350	120
B3	B	80	1
The comparative results between the analysis and the simulations are shown in Table 16.


Table 15. Scenario C: Link parameters.

Parameter	Value
Cam[1–3]  Switch1 idleSlope(A)	13.45 Mbps
Cam[4]  Switch2 idleSlope(A)	13.45 Mbps
DA-Cam  Switch1 idleSlope(A)	13.45 Mbps
Switch2  Switch1 idleSlope(A)	13.45 Mbps
Switch1  DA-Cam idleSlope(A)	53.77 Mbps
Switch1  HU idleSlope(A)	13.45 Mbps
Telem.  Switch2 idleSlope(A)	10.37 Mbps
Switch2  RSE idleSlope(A)	10.37 Mbps
Switch1  HU idleSlope(B)	12.95 Mbps
Telem.  Switch2 idleSlope(B)	12.95 Mbps
Switch2  Switch1 idleSlope(B)	12.95 Mbps
cd/dvd  Switch2 idleSlope(B)	16.45 Mbps
Switch2  RSE idleSlope(B)	20 Mbps
The maximum single frame response time results obtained in simulation for the ST flows (i.e., ST1–ST22) are the same as the ones calculated through the analysis, thus demonstrating that these flows are transmitted in a deterministic way and therefore it is possible to accurately calculate the response times.


Table 16. Worst-case Response time of a single real-time traffic frame in Scenario C.

Flow	ST1	ST2	ST3	ST4	ST5	ST6	ST7	ST8	ST9	ST10	ST11
Sim μ	17	17	28	28	28	28	28	28	28	28	28
Calc μ	17	17	28	28	28	28	28	28	28	28	28
Flow	ST12	ST13	ST14	ST15	ST16	ST17	ST18	ST19	ST20	ST21	ST22
Sim μ	28	17	17	17	28	28	28	28	28	28	28
Calc μ	28	17	17	17	28	28	28	28	28	28	28
Flow	A1	A2	A3	A4	A5	A6	B1	B2	B3	–	–
Sim μ	37	54	77	94	37	37	55	66	76	–	–
Calc μ	185	185	185	402	276	185	772	111	210	–	–
As far as the SR flows (i.e., A1–A6, B1–B3) are concerned, the calculated max response time is significantly higher than the simulated one, and this result is due to the pessimism introduced in each link. The difference between the calculated and the simulated values is comparable for all the SR flows, with the exception of the SR flows that traverse two switches (i.e., A4 and B1).

Table 17 shows the average and minimum delay results for SR traffic in scenario C. The results for ST traffic are not shown, as there is no variability in the delay of ST traffic.

In this scenario it is interesting to highlight that, as the workload of SR traffic in each link is very low, the delay variability is also very low. Consequently, the maximum difference between the maximum delay and the minimum delay that we obtained is 53 μ and it refers to the flow B3.


Table 17. Single frame average and minimum response time of SR real-time traffic in the considered Scenario C obtained through simulations.

Flow	A1	A2	A3	A4	A5	A6	B1	B2	B3
Avg. μ	37	53	70	87	37	32	47	66	39
Min. μ	37	53	70	87	37	29	42	66	23

Table 18. Frame generation intervals set in each case.

Case 1	Case 2	Case 3	Case 4	Case 5
Frame Gen. Interval (ms)	2	[2, 4]	[2, 6]	[2, 8]	[2, 10]
6.4. Pessimism study of the proposed analysis
This sub-section aims to show to what extent the results of the proposed analysis are affected by varying network parameters. Here we focus on the network topology in Scenario A, that is shown in Fig. 12. We also consider the same set of traffic flows given in Table 3. We present two experiments. In the first one we vary the utilization ratio between class A and class B traffic, whereas in the second one we introduce traffic with varying bit-rate.

In the first experiment we consider five cases with different utilization of class B traffic, i.e., with different periods of class B flows. In this experiment class A traffic flows (A1 and A2) have a fixed payload (i.e., 168B) and a fixed period (125 μ). Conversely, for Class B traffic flows (B1 and B2) the payload is fixed (i.e., 500B), while in each case a different period was set, i.e., 2000 μ, 2500 μ, 5000 μ, 8000 μ, and 10 000 μ. In each case, as we change the utilization, we also need to change the allocated idleSlope accordingly. The idleSlope is computed considering the utilization and the gates operation according to the standard. For each case we compute the worst-case response time using the proposed analysis and we measure the maximum response time through simulation. Fig. 15 shows the results of the first experiment. In the figure, the -axis represents the utilization ratio, while the -axis shows the normalized difference between the computed worst-case response time and the maximum measured response time in simulation, i.e., 
 
. As it can be seen, the normalized difference for class A traffic is constant (close to 28%) and does not vary while changing the utilization ratio. This is because the analysis takes into account only the worst-case blocking from class B, i.e., one maximum frame size only. However, the normalized difference varies for class B when the utilization ratio changes. In fact, the normalized difference increases from around 70% to 90% when the utilization ratio grows from 7 to 32. This result depends on the high pessimism in the analysis for class B, that mostly derives from considering the worst-case interference from classes A and ST. Note that, although the worst-case situation may not occur in simulation, it gives a safe bound on the results anyway. The pessimism is lower for class A analysis, as the only interference on such a class comes from the class ST.

This happens as simulation picks the shortest period within the defined ranges according to the uniform randomness selection, thus selecting 2 ms from the [2;4] ms range. We also know that the shortest period leads to a high worst-case response time according to Eq. (17). Therefore, no matter how much variation is defined in the traffic bit-rate, the difference between the computed worst-case response time and the maximum measured response time is based on the shortest generation interval, leading to a constant normalized difference.

6.5. Lessons in a glimpse
This paper proposed a schedulability analysis for TSN networks that considers various traffic shapers and presented several experiments that show how to apply the analysis to three use cases. We identify several takeaway lessons from this work, that are listed below:

•
The first lesson is that a two-phase analysis technique can be helpful to adapt traditional response time analysis to systems with more complex interference model than the ones that are usually considered in the literature. In particular, in the model here addressed, the high priority interference has both preemptive and non-preemptive behaviors at the same time. In fact, the transmission of class ST frames is based on preemptive scheduling, whereas, at the same time, the transmission of class A frames is based on non-preemptive scheduling. This means that, on a switch port, an ST frame can preempt a class B frame whereas a class A frame cannot preempt the same class B frame. As a result, computing the worst-case response time for class B frames on a port is a non-trivial challenge. We proposed a method to solve it.

•
The second lesson refers to the effect of some network parameters on the pessimism of the analysis and it can be beneficial to real-time system designers. In particular, we presented the effect of varying two parameters. The first one is the ratio between the load of low priority traffic (i.e., traffic class B) over the load of high priority one (i.e., traffic class A). By varying such a ratio, we captured the pessimism, in terms of the normalized difference between the computed worst-case response time and the measured one. We showed that, when the low priority traffic load is low (e.g., when larger periods are used for generating traffic), the pessimism in the analysis of the low priority traffic is higher. This information can be helpful in designing the system. The second network parameter that we investigated is the presence of sporadic traffic with variable bit-rate. We showed that the introduction of such a traffic does not affect the pessimism level compared to the case of fixed-rate periodic traffic.

•
The third lesson is for the system designers who can use the proposed methodology to verify their time-critical systems. We show the process of system development and the applicability of the analysis in Fig. 17. In this process, first the designers develop the system in an abstract way, considering the communication and software components (SW) (step a), then they set the timing parameters in the model (step b). Finally, the designers run the tool using the proposed schedulability analysis to verify the timing requirements (steps c and d). This will also give the designer some hints on the bottlenecks, such as which traffic is not schedulable on which port in the network. This is possible because the proposed analysis can compute the worst-case response time of each individual message on each port.

7. Conclusion and future work
This work presented a schedulability analysis for TSN networks that allows for the response time calculation of individual frames encompassing credit-based shaping, time-aware shaping and preemption support, in various combinations (i.e., with and without the Hold and Release mechanisms). To the best of our knowledge, so far no previous work on response time analysis of TSN networks has addressed all these aspects together. The proposed analysis can be used in Component-based Software developing tools, in simulation and network deployment tools to tune the configuration parameters of a TSN network, and to provide a safe bound on the flow response times.

In addition, the paper presented a comparison between the response times calculated by the proposed analysis and the response times obtained through OMNeT++ simulations in three different scenarios. The performance assessment demonstrated that the proposed analysis introduces a level of pessimism, in particular in the response time calculation of Class B frames. However, in the assessed scenarios the pessimism resulted always orders of magnitude lower than the period and constraints (i.e., relative deadlines) of the flows, thus it is acceptable. The investigation about how to improve the pessimism in the proposed analysis is a non-trivial task, as several factors play a role in it. Some sources of pessimism include the calculation of the jitter from Class A over Class B, of the worst-case number of preemptions by class ST over the other classes, and of the traffic shaper effect. A detailed investigation of these aspects and possible optimizations will be explored in future work. Moreover, another future direction of research is to adapt the analysis and model in the software development modeling tools.