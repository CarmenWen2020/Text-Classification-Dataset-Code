Abstract
In this paper, the parallel machine scheduling problem with DeJong's learning effect, is addressed. The objective function to be minimized is the makespan. This problem is proofed to be NP-Hard in the strong sense. This is the challenging theoretical side of the studied problem. Furthermore, several real-life situations in manufacturing and computer science are modeled using the current problem. Several algorithms intended to solve the studied problem within a reasonable computing time are proposed in literature. Among these algorithms the exact methods, which failed to solve the studied problem to optimality even for small size instances. In this paper several new heuristics and meta-heuristics are proposed. These heuristics are classified into three types. The first type is based on Longest Processing Time (LPT) rule. The innovation is the modification of the LPT rule in a way to cope efficiently with the learning effect concept, by randomizing the selection of the next scheduled job. The second type of heuristics, is taking advantage of an exact Branch and Bound algorithm, developed originally for the classical parallel machine scheduling problem. The contribution for this kind of heuristics is lying in the modification of the processing time values, according to a prefixed selected functions. The third type of methods is based in an adaptation of the Genetic Algorithm to the learning effect concept. This adaptation consists in enlarging the area of selection of the parameters values. To assess the performance and the efficiency of the proposed heuristics, a newly developed lower bound is proposed. This lower bound is based on a relaxation of the studied problem, which allows to obtain a minimum cost flow problem.

Finally, an extensive experimental study is conducted over a benchmark test problems. The obtained results provide strong evidence that the proposed procedures outperform the earlier existing ones.


Keywords
Parallel machine
DeJong's learning effect
Lower bound
Heuristic

1. Introduction
Learning phenomena are impacting several real-life systems as production processes. This impact is a positive one, since it allows reducing the total production cost ([40], [5], [4]). Taking into account the learning effect in the decision-making process complicates the managers' task. In addition, the optimization procedure becomes more difficult. More specifically, the scheduling with the learning effect consideration increases the hardness of the studied problems compared to those without learning effect. In scheduling theory, the learning effect is modeled as a varying processing time which decreases with respect to the number of the already treated jobs by the same processor, this is the position-dependent job processing time. In this context, several mathematical models have been proposed ([3], [34], [10], [6], [40]). It is worth noting that the learning effect concept can be detected with humans and machines ([10]). For more details, the reader is referred to the latest surveys on scheduling problems with learning effect ([2], [9]).

The current studied problem models several real-life applications in different areas. Indeed, the impact of learning effect within a parallel environment on productivity, is investigated in several areas such as industry. In this context, servicing industries (traveling..) and manufacturing (assembly lines..) had received a lot of attention ([39], [27]). In addition, the studied problem models many practical situations in information systems. In this area, the huge amount of daily treated data is performed over parallel processors based computers, this is the parallel computing ([30]). Furthermore, cloud computing and distributed systems ([11]) are utilized to speed up the treatment of this data. The used parallel processors computers are equipped with learning software (machine learning), allowing the involved computers to gain experience and knowledge and therefore reducing the processing times of the treated tasks ([1]). Scheduling tasks in parallel processors computers under learning effect considerations is an important problem from optimization point view ([41]). In this work, the purpose is to propose and develop efficient algorithms that minimize the maximum completion time in parallel environment under linear effect considerations, this is the contribution of this work to the area of information systems.

The determination of accurate learning effect curves (models) for real-life situations, is the topic of plenty of research works in different areas. In this context, the learning curves are generally determined according to two different methods: (a) Comparing the collected data with existing (assumed) theoretical learning curves (log linear model, S-curve, Standford-B model, DeJong's learning formula, Levy's adaptation function, Glover's learning formula….) and selecting the best one. (b) Analyzing the collected data using different statistical methods (as regression analysis method) and determining an approximated learning effect curve. For these statistical methods and once again, there are some assumed curves to be used (linear regression, linear combination, quantile regression, nonparametric regression……).

In this paper, the DeJong's learning curve is selected in order to mainly allow the comparison with existing works with the same learning curve ([36]). The scope of this paper is to propose efficient algorithms solving the studied problem. Addressing a case study in which data is collected and analyzed using statistical methods to determine the learning curve could be a future research direction that might be the subject of an entire research work.

To tackle complex scheduling problems as some variants of flow shop and parallel machines, meta-heuristics are proposed literature. Indeed, these meta-heuristics are based on evolutionary concepts. In this context, authors in ([22]) addressed a blocking lot-streaming flow shop (BLSFS) scheduling problems with machine breakdowns. In the latter work, an evolutionary multi-objective algorithm is proposed. A flow shop scheduling problem with blocking is addressed in ([25]). In the latter paper, a discrete artificial bee colony procedure is proposed to solve the studied problem. Stochastic lot-streaming flow shop scheduling with blocking is studied in ([23]), and a multi-objective migrating birds optimization algorithm is proposed to solve the studied problem. In ([33]) authors treat the problem of vehicle routing problems with time windows and synchronized visit constraints. For this complex problem authors developed a meta-heuristic procedure, which is experimentally proofed to be efficient. The blocking flow shop scheduling problem is studied in ([21]), and a modified fruit fly optimization algorithm is used to solve it. Authors in ([20]) addressed the blocking lot-streaming flow shop scheduling with interval processing time. This problem is solved using a new designed evolutionary multi-objective optimization algorithm. The multi-objective lot-streaming flow shop problem is studied in ([24]), and an improved NSGA-II algorithm is proposed to provide a near optimal solution. The blocking lot-streaming flow shop scheduling problem is addressed in ([17]), and a novel hybrid multi-objective artificial bee colony algorithm is proposed. In this study, an evolutionary algorithm is considered and adapted to solve the parallel machine scheduling problem with learning effect, it is the Genetic Algorithm (GA) with four variants.

In the sequel, a brief literature review related to the parallel machine scheduling problems with the position-dependent job learning effect is presented. This kind of scheduling problem appeared in 1996 and was firstly addressed in ([16]) by Gawiejnowicz. The single machine problem with learning effect, was addressed in ([8]).

The parallel machine scheduling problem with learning effect is introduced and studied firstly by Mosheiov in ([35]). The latter research considered the total completion time as an objective function to be minimized. For this objective function, the treated scheduling problem was proved to be polynomially solvable. The parallel machine scheduling problem with learning effect and with maximum lateness as objective function was addressed in ([13]). In this research work, several heuristics and mathematical programming formulations were developed in order to solve the studied problem, and the makespan minimization is considered as a particular case. The presented results in ([13]) were enhanced throughout ([42]). The parallel machine scheduling problem with learning effect and maximum completion time minimization were studied ([36]). For the latter work, DeJong's learning curve was adopted and an exact procedure based on the branch and bound algorithm was developed and analyzed. The presented exact procedure has two versions: the serial and the parallel one.

In ([32]), the uniform parallel machine scheduling problem with learning effect and makespan minimization was addressed. In this work, different learning curves were considered and practical swarm method, genetic algorithm and other heuristics were developed to tackle the studied problem. Recently, the author in ([38]) studied the parallel machine scheduling problem with learning and aging effects. In order to solve the latter problem, a dynamic programming algorithm is proposed. The experimental study was carried out with a number of machines equal to two. The numerical results prove the performance of the developed procedure.

Assessing the proposed heuristics, meta-heuristics and mathematical programming requires efficient lower bounds. This assessment is performed throughout the relative gap. In addition, speeding up the exact procedures based on the branch and bound algorithm (B&B) is requested and developing efficient lower bounds contributes to achieving this goal. To the best of our knowledge, the best existing lower bounds were presented in ([36]), but still not enough efficient to be embedded in a branch and bound exact procedure. This could be observed in ([36]), where the proposed B&B fails to solve small size instances even if it is parallelized. This drawback will be addressed in this paper by developing new efficient lower bounds. These lower bounds are based on the maximum authorized positions that can be occupied by a job in an optimal schedule.

In addition, several new heuristics will be developed for the studied problem. One type of these heuristics, is motivated by extending some algorithms, developed originally for the classical parallel machine problem, to the one with learning effect. The parallel machine scheduling problem with learning effect is known to be more difficult to be solved than the classical one ([36]). Thus, extending existing procedures, that solve the classical parallel machine problem, in order to develop new ones for the studied problem with learning effect, will be adopted in this work. In this context, a B&B algorithm that solves the classical parallel machine is used. This B&B solves the classical problem (without learning effect) and the obtained schedule will be considered as an initial feasible one for the parallel machine problem with the learning effect. This initial feasible schedule will be improved and adapted for the studied problem.

Moreover, several papers treating the parallel machine scheduling problem with learning effect ([36], [38]), assessed experimentally the efficiency of the Longest Processing Time rule (LPT). Indeed, in the classical parallel machine scheduling problem, the LPT is performing well and a worst-case theoretical result is proposed in ([18]). In this work, several efficient heuristics based on the LPT rule, for the studied problem, will be proposed and assessed. Furthermore, Genetic Algorithms (GA) are developed for the studied scheduling problem. This is motivated by the successful experimental results for other scheduling problems.

The rest of this paper is structured as follows. The studied problem is formulated and some of its important proprieties are presented in the next section. In section 3, existing lower bounds are presented and new ones are proposed. The details of proposed heuristics are presented in section 4. An extensive experimental study assessing the performance and efficiency of the proposed procedures is carried out in section 5. Finally, a conclusion is presented in the last section of this paper.

2. Problem formulation and properties
2.1. Problem formulation
The addressed scheduling problem is defined as follows. A set of m identical parallel machines 
 has to process a set of n jobs . Because of the learning effect, the processing time of a job  is given by a function 
 related to its position k in the sequence on the machine 
 where it is processed. In this case,  jobs have been treated by 
 before job j.

In this work, 
 is selected as 
 where 
 is the normal processing time and  is a curve given by: 
. In the expression of  the parameter M  is the incompressibility factor and the scalar a  is the learning index. The already presented curve  is the DeJong's learning curve ([6]). The selection of such a curve is justified by: 1)  is polynomial and a decreasing curve, 2) when k goes to infinity  converges toward M and therefore 
 converges to the constant 
 ≠0. This kind of learning effect is more realistic while modeling real-life situations than other ones. For example, if 
 ([35], [8]) then 
 converges to 0 when k goes to infinity, which involves that by learning effect a machine is able to perform the requested tasks in zero unites of time, which is not realistic.

The processing of the jobs is performed under the following assumptions. The machines are ready for treating jobs from time zero onward and a machine treats only one job at most at the same time. The preemption is not allowed and all the jobs are available for processing from time zero. The normal processing times 
 are deterministic and positive integer.

For a feasible schedule, let 
 be the completion time of the job . The objective function to be minimized is the maximum completion time (makespan), which is defined and denoted as: 
  
, and the optimal makespan value is 
⁎
. The proposed three-field notation in ([19]) for the studied problem is: 
.

An important property of the studied problem is presented over the following proposition.

Proposition 2.1

In any optimal schedule for 
, the jobs assigned to each machine are scheduled in increasing order of their normal processing time 
  (Shortest Processing Time: SPT).

Proof 1

By contradiction, assume there exists a machine 
 in which the jobs are not assigned according to SPT rule. Then there exist at least two jobs k and l such that:

•
,

•
jobs k and l are assigned to positions s and t, respectively with .

Consequently, the completion time in machine 
 is 
 with A the load of the rest of scheduled jobs in this machine. In addition,  since the learning effect curve  is a decreasing function. Now, if jobs k and l are permuted then the completion time in machine 
 will be 
. Therefore, 
. Which yields, 
 and consequently the assumption is wrong and the jobs are scheduled according to the SPT rule in machine 
.
It is worth mentioning that the above proof could be seen as an immediate consequence of the presented result in ([36]), which states that the scheduling problem 
 is optimally solved using the SPT rule.

Another important and useful propriety for the studied problem, about the limitation of the positions that could be occupied by a job j in an optimal schedule, is presented over the following lemma.

Lemma 2.2

In an optimal schedule, the position l  of the job 
, having the 
 smallest normal processing time 
 in a machine satisfies . In other term the position l of the job having the 
 does not exceed k.

Proof 2

By contradiction, assume that 
 is scheduled at a position 
, then 
 jobs with normal processing times less or equal to 
, are scheduled on the same machine before 
. This is because in an optimal schedule the assigned jobs in the same machine are arranged according to the SPT rule. Thus, 
 jobs have a normal processing time less than 
, with 
, which is in contradiction with the fact that the job 
 has the 
 normal processing time 
.

3. Lower bounds
In this section existing lower bounds and new developed ones are presented.

3.1. Existing lower bounds
To the best of our knowledge, the only existing lower bounds for the problem 
 are those given in ([36]), and their expressions are presented in the following lemma.

Lemma 3.1

If 
 is the 
 smallest 
 and 
⁎
 is the optimal solution of 
, then:
⁎
 

⁎
 
 
⁎
Proof 3

•
 proof: For an optimal schedule, the job with largest normal processing time 
 
, is scheduled in a position k , therefore, 
⁎
. Since,  is a decreasing curve regarding the position, then . Combining the two last observations, one could have 
⁎
.

•
 proof: In an optimal schedule the processing time of the job 
 having the 
 smallest normal processing time is scheduled at position 
. The contribution of 
 in the total load is denoted 
. Since 
 with 
 then 
 (
 then 
). Denoting 
 the total load in machine 
. Clearly, 
⁎
 and  
 
 
 
 then 
 
 
⁎
 which proofs that 
 is a lower bound for the studied problem.

•
Trivially, 
 is a valid lower bound for the studied problem.

3.2. Proposed lower bounds
The first enhancement of the existing lower bound 
 is exhibited in the following lemma.

Lemma 3.2

 
 is a valid lower bound for the 
 problem.

Proof 4

For an optimal schedule, the job 
 with the 
 smallest normal processing time 
 is scheduled in a position 
, (based on Lemma 2.2). Consequently, 
⁎
 ( is a decreasing function). Therefore, 
⁎
 
.

The latter lower bound might be enhanced by observing that there is an upper bound h of the maximum positions that could be assigned in each machine for an optimal schedule. This result is presented in the following lemma.

Lemma 3.3

There exists a positive integer h such that in any optimal schedule we have 
, where 
 is the number of the scheduled jobs in machine 
.

Proof 5

Let UB be an upper bound for the problem 
. Without loss of generality we assume that the list of 
 is sorted in the increasing order. Let h be the maximum number of the jobs scheduled on a machine according to the SPT rule, without exceeding UB. By contradiction let proof that h is a candidate satisfying the previous lemma. Assume that there is an optimal schedule for which the number of the scheduled jobs 
 in a machine 
 satisfies 
. These 
 jobs are scheduled according to the SPT rule in machine 
 (according to Proposition 2.1). Let 
 be the set of these jobs and 
, therefore 
 
 for 
 this is because 
 is sorted in the increasing order.

Thus, the completion time 
 of the last scheduled job 
 in 
 is 
 
 
 
 
 
 
 (since 
 
 and 
). Consequently, 
 
 
 
⁎
 (definition of h) which is in contradiction with the fact that the schedule is optimal.

An immediate consequence of the latter lemma is an improvement of 
, which is the content of the following proposition.

Proposition 3.4

 
 is a valid lower bound for the problem 
.

Proof 6

According to the latter lemma, the maximum position to be filled in an optimal schedule is h.

Another type of lower bounds is based on the computation of the minimum sum of processing time (total load) and we have the following obvious result.

Remark 3.5

A valid lower bound for the problem 
 is 
 
 where LD a lower bound of the total load.

In order to determine a lower bound LD of the total load, the following binary linear programming (P) is presented. The decision variables for (P) are defined as: 
 if job i is placed at position j and 0 otherwise .(1)
 
 
(2)
 
(3)
 
(4)
 
(5)

The objective function (1) consists in minimizing the total load. The constraints (2) provide that each job is placed only in a unique position. In addition, the constraints (3) involve that the total placed jobs in a position is not exceeding the capacity m. While constraints (4) mean that at least one machine is fully occupied from position 1 to position h (each position should be filled with at least one job). Constraints (5) mean that the decision variables are binary.

Proposition 3.6

The optimal solution for the latter binary linear program (P) is a lower bound for the total load.

Proof 7

The optimal solution of (P) is the minimum load satisfying all the constraints: (2)-(5).

The time complexity for the resolution of (P) (as well as the complexity of LB) is the topic of the following proposition.
Proposition 3.7

(P) is a polynomially solvable binary linear program.

Proof 8

It turns out that the (P) linear program could be reformulated as a minimum cost flow problem with bounded capacity where the associated graph  is detailed in the sequel. The set of nodes N is composed of:

•
A source node S and a sink node T.

•
The jobs nodes 
.

•
The positions nodes 
.

Thus 
.

The set of the arcs V is composed of:

•
An arc 
 with unit capacity and cost 0, from S to node 
.

•
An arc 
 from node 
 to position node 
 such that 
. The arc 
 is with unit capacity and a cost 
.

•
An arc 
 from node 
 to node T with cost 0, a minimum capacity of 1 and a maximum capacity of m.

Therefore,

.

Notation 3.8

Let 
⁎
 be the optimal value of the (P) the obtained lower bound is denoted
⁎
 
and the final obtained lower bound is

It is worth mentioning that the development of a lower bound LB is useful for:

•
Measuring the performance of a heuristic H with makespan UB throughout the relative gap rg given by: . The rg is measuring the maximum relative distance between the optimal solution value 
⁎
 and the lower bound LB. Indeed, 
⁎
 then 
⁎
.

•
Building Branch and Bound (B&B) exact procedures by adapting the lower bound LB for each node of the search tree. The efficiency of the B&B is strongly related to the quality of the adapted lower bound.

In this study, the main objective is measuring the efficiency of the large variety of proposed heuristics and meta-heuristics.

4. Heuristics
In the first part of this section, the main motivation is the extension (if possible) of some heuristics, developed originally for the classical parallel machine scheduling problem to the studied problem. In this context, the Shortest Processing Time SPT based heuristic is considered. In addition, the Longest Processing Time (LPT) heuristic is a very simple and efficient one for the classical problem. Indeed, if 
 and 
⁎
 denote the LPT schedule and an optimal schedule respectively, then ([18]) proved that in the worst-case 
⁎
 
 
 
 
. Unfortunately, authors in ([36]) demonstrated that the latter result is no longer valid for the studied problem. Furthermore, experimental results in ([38]) provide strong evidence that the LPT is worse than the optimal solution about 30%–40%. Therefore, other approaches are requested to overcome these drawbacks and to extend these heuristics.

4.1. Shortest processing time SPT heuristic (SPT)
The Shortest Processing Time (SPT) based heuristic is scheduling at iteration k the 
 smallest normal processing time's job at the most available machine. This heuristic was investigated in ([36]). In this study the SPT is reconsider to be compared to the other proposed heuristics.

The complexity of the SPT algorithm is .

The next subsection provides an adaptation of the LPT to the studied problem.

4.2. Randomized selected position LPT heuristic ()
In this subsection, an adaptation of the LPT heuristic is performed. This adaptation consists of enlarging the selection field of the next candidate to be scheduled in the following way. Instead of selecting the first job having the largest processing time, the next one is selected randomly between the two first unscheduled jobs. The obtained upper bound is denoted 
. An experimental study, assessing the efficiency of 
 is carried out over a data set of 1320 instances. The results are reported in Table 1, where nb and  are respectively the numbers and the percentage of the instances satisfying the column headers conditions.


Table 1. Comparison LPT and 
.

nb	39	1272	9
Perc	2.95%	96.36%	0.68%
In this study, one could observe that in 96.36% of the instances, 
. Encouraged by the already obtained results, a question arises: what can occur in terms of improvement, if the random selection is extended to the three first unscheduled jobs and so on. Let 
 be the obtained upper bound if the random selection for LPT is applied to the s first unscheduled jobs (). Let  be the percentage of instances where 
 is equal to the minimum of 
 with . An experimental study is carried out and a summary of the obtained results is displayed in the following figure (Fig. 2).

Fig. 2
Download : Download high-res image (151KB)
Download : Download full-size image
Fig. 2. Experimental determination of the best s. (For interpretation of the colors in the figure(s), the reader is referred to the web version of this article.)

According to these results, we observe that a maximum is reached for , but there are some residual percents for . An additional experimental study, comparing the contributions in the upper bounds 
 (the next job is selected randomly between the s first unscheduled jobs) suggested that stopping in 7 iterations is worthy. The results are displayed in Table 2.


Table 2. Exploring number of iterations.

s	3	4	5	6	7	8	9	10
nbr	657	411	293	197	134	80	65	59
Perct	49.77%	31.14%	22.20%	14.92%	10.15%	6.06%	4.92%	4.47%
Let 
 
. In the above table nbr denotes the number of times where 
 (i.e.: the exclusive improvement of 
). The  denotes the corresponding percentage given by 
 
.

In addition, the random selection between the first s  unscheduled jobs is performed as follows. A step ST is fixed and a uniform random generator is utilized to generate random number Z within the discrete interval . If  where , then the 
 unscheduled job is selected. The ST is fixed based on an experimental study where the results are summarized and presented in the following table (Table 3).


Table 3. Selection of the step size ST.

ST	5	20	50
nb	488	503	535
Percg	36.97%	38.11%	40.53%
For this table  and ST , nb is the number the obtained heuristic value is reaching exclusively the minimum. The previous study, suggests that .

The complexity of the  algorithm is , it is the required time to sort the processing times in the increasing order.

4.3. 
 based heuristics
Clearly, the problem 
 is easier than the problem with learning effect 
 in terms of resolution effort ([36], [38]). Therefore, the goal of this subsection will be the extension of some valid procedures, originally developed for 
 (heuristics, exact methods...) in appropriate way to solve the problem 
. More precisely, in this subsection an efficient exact procedure for 
 problem will be adapted to provide a solution for 
.

In this procedure, a position  is fixed and an instance of n jobs with processing times 
  is set up. For that instance, an exact Branch and Bound (B&B) algorithm in ([26]), developed for the problem 
, is applied. The obtained schedule is reorganized as follows. The jobs scheduled in each machine are rescheduled according to the SPT rule. The learning effect is considered and the maximum completion time 
 is calculated. The question is what is the best value of α to be selected? For that aim, an experimental study is performed on a selected data set, where α is varied in . Denoting 
 
 the minimum obtained value. A comparison study is performed according to the number of jobs n. The obtained results are displayed in Fig. 3.

Fig. 3
Download : Download high-res image (212KB)
Download : Download full-size image
Fig. 3. Selection criteria between positions.

In Fig. 3, the curve presents the percentage (Per) of times when 
 regarding α. According to this figure, the highest percentage is concentrated around  and . Following the latter observation, the first heuristic in this subsection is derived. This heuristic is denoted 
 where the corresponding obtained value is 
.

The latter result is a motivation to experiment the B&B algorithm for 
 on the following instances. The set of jobs is partitioned into two subsets 
 and 
 where 
 and 
. The parameter 
 
 represents the percentage of the jobs to be included in 
. The processing times are:
 

The presented curves in Fig. 4, report the percentage of times 
 with respect to k. Based on Fig. 4, one could observe that the maximum contributions are reached for  or . Combining the two cases  and  by considering the minimum between 
, 
 and noting 
. This heuristic is denoted by 
. The results of the latter combination are reported in Table 4.


Table 4. Selection of the k value parameter.

n	k = 4	k = 5	k = 4 and 5
10	60.0%	61.3%	63.8%
20	36.3%	30.0%	45.0%
50	43.8%	40.0%	52.5%
100	41.7%	48.3%	58.3%
150	42.5%	43.8%	55.0%
200	41.9%	43.8%	53.8%
300	38.8%	43.1%	51.3%
500	37.5%	45.6%	53.1%
1000	37.0%	46.6%	53.4%

Overall	41.3%	44.8%	53.9%
Results in Table 4 confirm the selection of the couple  and .

A third heuristic is derived based on Fig. 3, which suggests that the most percentage is concentrated around the positions  and . This result encourages us to experiment the following combination: 
 where  and . Several values of γ are experimented and the results are reported in Fig. 5.

Fig. 5
Download : Download high-res image (165KB)
Download : Download full-size image
Fig. 5. Selection of the parameter γ.

According to Fig. 5, the maximum percentage is reached for . The corresponding heuristic and its value are denoted 
 and 
, respectively.

The complexity of the current heuristic is the same as 
, which is NP-Hard.

4.4. Random machine selection based heuristic (RMS)
In this subsection, an additional greedy heuristic will be presented. This heuristic is based on a random selection of the machine to process the first unscheduled job. The overall algorithm is displayed as follows.


Algorithm 1
Download : Download high-res image (53KB)
Download : Download full-size image
Algorithm 1. Random machine selection algorithm.

It is worth noting that step 4 is performed to avoid the scheduling of a lot of jobs on a particular machine and this could conduct a non-diversification of the machines. The  is a rule to decide about the machine to be excluded from 
. The  belongs to one of the three following ways.

• Way1:
.

• Way2:
 is the machine on which is scheduled the last job

• Way3:
 is the most loaded machine.

Therefore, three versions have been experimented and the best one is kept for each instance. The previous procedure is repeated 500 times for each Way separately, and the best value U is preserved. The number of 500 is retrieved from an experimental study.

Noting 
 with  the value of the heuristic corresponding to Way i. All three previous ways are tested and the results are reported in Table 5.


Table 5. Random machine selection based heuristics.

Uway1	Uway2	Uway3
Overall	7.70%	7.50%	92.90%
In Table 5, 
 is denoting the minimum value between the three heuristics. The percentage of times a heuristic is participating in UU is analyzed and 
 is dominating in 92.9% of cases.

Clearly, the complexity of the current heuristic RMS is . Indeed, the required effort is the same as the number of jobs.

4.5. Random jobs selection based heuristic (RJS)
The basic idea of the current heuristic is to pick randomly and uniformly a job j among the unscheduled jobs. Let 
 and 
 be the availability of the machine 
  and the number of the scheduled jobs on it, respectively. The selected job j is assigned to a machine according to a strategy among the three following ones.

• Strategy 1:
The machine 
 such that 
 
 is the least one.

• Strategy 2:
The picked job j is assigned to the machine 
 having the smallest 
.

• Strategy 3:
Select the machine having the smallest 
.

The resulted heuristic is considering the minimum among the three described strategies. In this heuristic, the iterations are repeated 500 times for each strategy.

The complexity of the RJS heuristic is also .

4.6. Genetic algorithm
The Genetic Algorithm (GA) is one of the most familiar and used meta-heuristic in order to provide a near-optimal solution in reasonable time for the NP-Hard problems ([15], [7]). The basics of the GA were presented firstly by Holland ([28]). A genetic algorithm consists of starting with an initial set of feasible solutions (initial population) and substitute it iteratively by a new one. The GA requires a suitable representation (encoding) of a feasible solution (chromosome or individual). In addition, the quality evaluation of each encoded solution requires a fitness function. The generation of the new population depends on the reproduction mechanism. This reproduction mechanism requires the selection of the parents and the generation of the offsprings using a crossover operator. The generated offsprings are subject to a mutation operator in order to diversify them ([14], [31]).

The main steps of the proposed GA are summarized as follows.

•
Representation of a feasible solution (encoding): a feasible solution is represented by a sequence of jobs or a permutation 
. The decoding is performed following two different strategies. In the first one (S1), the first unscheduled job in the permutation is assigned to the most available machine. In the second strategy (S2), the first unscheduled job in the permutation is assigned to the machine that gives the least completion time when the job is scheduled.

Seeking more clarity about the implementation of the strategies S1 and S2 in the decoding phase, the following example is considered.

Example 4.1

For this example, , , 
, 
, 
, 
, and 
. The values of learning parameters are set to  and . Let  be a permutation that encodes (represents) a solution.

For strategy S1, jobs 3 and 5 are placed in the first machine while jobs 2, 1, and 4 are assigned to the second machine. If the second strategy S2 is considered, then the jobs 3 and 4 are placed in the first machine while jobs 2, 1, and 5 are assigned to the second machine.

In the above example, job 5 is placed in the second machine for strategy S1, and the same job is assigned to the first machine for strategy S2.

•
Population size pop: The size of the produced population at each iteration is a parameter that should be fixed at the beginning of the implementation of the GA. Based on an experimental study, the population size pop belongs to 50, 100, and 200.

•
Initial population: For a given population size (pop), the initial population is composed of: 1) one solution derived either from LPT or , and 2)  randomly generated solutions. This is an initial population with pop solutions. It is worth noting that there is no relationship between the randomization in generating an initial solution with  and the  random generated solutions.

•
Fitness function: The makespan is the objective function to be minimized. For each sequence of jobs (permutation), the schedule on the machines is performed following either the encoding strategy S1 or the encoding strategy S2. Once the schedule is obtained, the maximum completion time (makespan) is calculated as: 
  
. The fitness function to be consider is the makespan.

•
Crossover: The crossover is an important step in GA implementation. During this step, the offspring (produced solutions) are created from two parents (two existing solutions) by exchanging genes (jobs). In this study, the two crossover points technique is adopted. For this technique, two different points are randomly selected and the genes between these two points are exchanged to create new offspring. In this work, the crossover rate  belongs to  and the experimental study determines the appropriate values.

•
Mutation: In order to preserve enough diversity among the new generated offspring (solutions), mutations with low rate (probability) are stimulated within the new generated offspring. This allows avoiding a premature convergence. In the current work, the insertion mutation is utilized as a mutation operator. Preliminary experimental study suggests the mutation rate (mut) to be set at 5%.

•
Selection: the selection of offspring from the current population for the reproduction of next generation is performed according to the tournament selection with a size of the tournament group .

•
Stopping rule: The GA is halted after 100 repetitions.

In any GA, the selection of the parameters , is performed at the beginning of the implementation, over an experimental study. For that aim, an experimental study is conducted over a limited data set.

This data set is considering the learning parameters  and . The number of jobs is  and the number of machines , which results in 520 instances for each parameter a.

The tested parameters are: pop (initial population size),  (crossover) and mut (mutation). The considered values of the latter parameters are: ,  and . The combination of these parameters provides 243 alternatives ().

For each combination  and each instance the genetic algorithm value 
 (where  stands respectively for ) is computed. Let 
 denote 
 
, , , .

Experiments show that 10 combinations of  are the most relevant. These combinations are: , , , and .

Based on Fig. 6, one could observe that the maximum percentage is reached for the last . Moreover, the trend is increasing as the  increases.

Furthermore, the percentages are not exceeding 20.5%, which obliges us to select more than one configuration of parameters .

The selection of these parameters consists of collecting those representing a local maximum. This strategy results in the selection of 10 combinations. This selection is for one hand less time consuming than considering all the combinations and for the other hand, it is guarantying the enhancement of 
 compared to considering only one combination.

It is worth noting that four versions of this GA are developed. These versions are related to the way the initial population is generated (LPT or ), and the encoding strategy (S1 or S2). These versions are:

•
: the initial population is obtained using LPT rule and the encoding strategy is S1.

•
: the initial population is obtained using LPT rule and the encoding strategy is S2.

•
: the initial population is obtained using RSPL rule and the encoding strategy is S2.

•
: the initial population is obtained using RSPL rule and the encoding strategy is S2.

5. Experimental results
In order to evaluate the performance of the developed lower bounds, heuristics, and meta-heuristics, an extensive experimental study is carried out. All the proposed procedures are coded in C++ and run on Intel(R) Xeon(R) CPU E5-2687W v4 @ 3.00 GHz and 64 GB RAM. The proposed procedures are tested on a set of test problems that are given below.

5.1. Test problems
The learning effect parameters a and M are selected respectively as: a  and . In this experimental part, the values of incompressibility factor M and learning index a are selected according to some recommendations as follows. The value  corresponds to the particular case of log-linear learning effect ([8]). However, the value  was proposed by ([37]) and it presents the machine-intensive processes. The value  is selected to be close to the classical case where there is no learning effect. The value  is chosen because it corresponds to a learning rate of 80% and this particular value is commonly used in literature ([8], [35]).

The number of jobs n and the number of machines m are correlated and picked as follows:

•
If  then .

•
If  then .

•
For the large number of jobs  we select .

The normal processing times 
 are generated according to different probably distributions. Each one of the probability distributions represents a class. The classes are:

•
Class 1: 
 is generated from the discrete uniform distribution .

•
Class 2: 
 is generated from the discrete uniform distribution .

•
Class 3: 
 is generated from the discrete uniform distribution .

•
Class 4: 
 is generated from the normal distribution .

For each combination a, M, n, m, 
, 10 random instances are generated, therefore a set of 3040 test problems is obtained. It is worth noting that the test problems of class 1 and class 2 are those proposed in [36]. The remaining classes (class 3 and class 4) are proposed in this work to guarantee the diversification of the test problems.

5.2. Performance assessment
This subsection is intended to analyze the performance of the proposed lower bounds and the developed heuristics.

5.2.1. Lower bounds evaluation
The lower bound 
 is the best proposed lower bound in literature (to the best of our knowledge ([36])).

It is worth noting that the core part in developing the lower bound LB is the parameter h representing the maximum number of jobs scheduled on one machine. The latter one is obtained using a UB makespan value. In this study, . This choice is justified by the low time complexity of the heuristics , RMS, and RJS.

In the current study analysis a pairwise comparison between LB and 
 is conducted. This comparison study will be evaluated over the following measures of performance:

•
: is the percentage of instances where 
.

•
GAP: is the average of 
 
 for the considered class of instances, where rg measures the relative distance between LB and 
.

•
: is the average computational time of LB (in seconds: s).

In addition the (−) denotes the case where the time is strictly less than 0.001 s.
An overall view of the obtained results is presented in Table 6.


Table 6. Results overall view.

LB > LB2	GAP	Time
All instances	100%	13.4%	-
Based on Table 6, we observe that in 100% of the instances, 
. Moreover, the average gap GAP is 13.4% and the consumed time  is less than 0.001 s. These results prove that the proposed lower bound LB is outperforming the existing one 
, which provides strong evidences of the performance of the proposed lower bound. Hereafter, the attention will be focused on the GAP performance measure since  and  s for the general results.

A more detailed analysis with respect to some parameters is carried out. The first detailed result is presented with respect to the learning effect parameters a and M. The related results are provided in Table 7.


Table 7. Results regarding learning effect parameters a and M.

M = 0	M = 0.5	M = 0	M = 0.5
LB > LB2	100%	100%	100%	100%
GAP	9.7%	3.8%	33.1%	7.0%
Time	-	-	-	-
According to Table 7, the average gap (GAP) is increasing as  increases. In contrast, the average gap (GAP) is decreasing when the incompressibility parameter M increases. The maximum relative gap (GAP) 33.1% is obtained for the learning effect parameters  and , which is the case where the learning effect is more important. In this case ( and ), LB is largely outperforming 
. An additional detailed analysis of the behavior of LB, with respect to the number of machines m is conducted and displayed in Table 8.


Table 8. Behavior of LB according to m.

m	GAP	LB > LB2	Time
2	8.4%	100%	-
3	11.8%	100%	-
5	15.8%	100%	-
10	23.6%	100%	-
Based on Table 8, the average relative gap (GAP) for the proposed lower bound LB is increasing as the number of machines m increases. For , the minimum value of the GAP is reached and . The maximum value of the GAP is obtained for  and . The impact of the way the normal processing times 
 is generated (classes 1,2,3,4) on the GAP is studied and the related results are presented in Table 9.


Table 9. Behavior of GAP according to Classes.

Class	LB > LB2	GAP	Time
1	100%	10.2%	-
2	100%	10.0%	-
3	100%	16.8%	-
4	100%	16.6%	-
Table 9 exhibits a more important performance of LB in term of GAP, for the classes class 3 and class 4. For these two classes, the GAP value is around 17%. These two classes are including only large normal processing times 
. For classes 1 and 2, where there is a mixing of small and large normal processing times, the GAP value is about 10%.

The presented curve in Fig. 7 expresses the variation of the GAP with respect to n. Based on Fig. 7, we observe that an absolute minimum of 10% is reached for  and an absolute maximum of 15% is obtained for . Moreover, an asymptotic behavior is exhibited and when n goes to infinity the GAP converges toward 14.7%. More detailed results combining the simultaneous effect of n and m on GAP are displayed in Table 10.


Table 10. Effect of n and m on GAP.

n	m	LB > LB2	GAP	Time
10	2	100%	8.5%	-
3	100%	11.4%	-

50	2	100%	8.5%	-
3	100%	12.1%	-

100	2	100%	8.5%	-
3	100%	12.0%	-
5	100%	16.0%	-

200	2	100%	8.3%	-
3	100%	11.9%	-
5	100%	15.9%	-
10	100%	23.8%	-

300	2	100%	8.2%	-
3	100%	11.8%	-
5	100%	15.7%	-
10	100%	23.7%	-

500	2	100%	8.1%	-
3	100%	11.6%	-
5	100%	15.5%	-
10	100%	23.4%	-
Based on Table 10, for each fixed n, the GAP increases as m increases. Now, for a fixed m, we observe that there is a slight decreasing of the GAP when n increases.

5.2.2. Heuristics assessment
In this work, several heuristics and meta-heuristics have been developed. The performance of these procedures is assessed using the following performance measurements:

•
GAP: is the average of the relative gap 
 
 where U is the upper bound (heuristic's value) to be assessed and LB is the developed lower bound. It is worth noting that the relative gap  is the maximum relative deviation of U from the optimal solution (which is unknown).

•
: the required time to obtain U (in seconds s).

Assume UB is the minimum value of the developed heuristics for each instance. The performance of UB is illustrated in Table 11, where GAP and  are presented for each couple of learning effect parameters . In addition, the average relative gap GAP and the average computation time  are given for all the instances.


Table 11. GAP and Time according to a and M.

Total
M = 0	M = 0.5	M = 0	M = 0.5
GAP	3.3%	1.4%	12.9%	2.6%	5.0%
Time	17.15	17.39	16.52	17.33	17.10
According to Table 11, the minimum GAP is reached for  and , with a value of 1.4%. Whereas the hardest instances are detected for the class where  and  with a value of 12.9%. Moreover, the maximum mean time is 17.39 s for  and . The GAP is increasing as  increases, and decreasing as M increases. Remarkably, the  is exhibiting the inverse behavior of GAP. A detailed analysis of the performance of UB with respect to the number of jobs n is presented over Table 12.


Table 12. Performance of UB with respect to the number of jobs n.

n	GAP	Time
10	2.8%	0.59
50	2.4%	0.28
100	4.2%	1.20
200	6.1%	6.37
300	6.1%	16.13
500	6.1%	57.39
Based on Table 12, we observe that the  increases dramatically as n increases to reach 57.39 s for . In addition, the GAP is converging toward a maximum value of 6.1% from . The behavior of both GAP and , with regard to the number of machines m, is given in Table 13.


Table 13. The behavior of both GAP and Time, with regard to the number of machines m.

m	GAP	Time
2	1.1%	10.94
3	3.8%	11.91
5	7.7%	20.48
10	11.7%	36.69
According to Table 13, the GAP and  are increasing as m increases. The maximum GAP value 11.7%, and the maximum  value 36.69 s are reached for . The effect of the classes (class 1,2,3,4: generation of 
) on both GAP and  is displayed in Table 14.


Table 14. Effect of the classes on GAP and Time.

Class	GAP	Time
1	8.0%	16.713
2	8.2%	17.600
3	1.9%	16.748
4	2.1%	18.220
The displayed results in Table 14 show that the hardest instances belong to classes 1 and 2 with a GAP of almost 8%. While the more easy instances are from classes 3 and 4 with a GAP of almost 2%. The  is within the interval .

The assessment of the performance of each one of the proposed heuristics will be presented in the next stage. This assessment is carried out first throughout the percentage  of times the studied heuristic is equal to UB. The related results are presented in Table 15.


Table 15. Perc performance analysis of the proposed heuristics regarding a and M (%).

RMS	RJS	RSPL	GA1	GA2	GA3	GA4	SPT
M = 0	3.29	2.89	2.63	5.13	0.53	11.32	50.53	55.13	19.74	19.34	0.00
M = 0.5	4.87	4.47	4.08	4.61	1.18	12.11	54.47	50.79	22.11	21.18	0.00

M = 0	1.05	0.92	0.53	5.26	0.26	10.53	51.32	56.32	19.34	19.61	0.00
M = 0.5	1.71	2.11	0.92	5.53	0.53	10.39	52.11	51.97	19.61	18.82	0.00

2.73	2.60	2.04	5.13	0.63	11.09	52.11	53.55	20.20	19.74	0.00
Based on Table 15, we observe that the best contribution in UB is detected for 
 with a percentage of 53.55%. A moderate impact is observed for the  with a percentage of 11.09%. Remarkably, for  and  the 
 is delivering a 56.32% of contribution in UB. The least participation is observed for the RJS heuristic with a percentage of 0.63%. The general remark is that there is no dominance between all of the proposed heuristics.

The required average time running the proposed heuristics , is presented in Table 16.


Table 16. Time performance analysis of the proposed heuristics regarding a and M.

RMS	RJS	RSPL	GA1	GA2	GA3	GA4	SPT
M = 0	0.76	0.80	0.40	0.35	0.73	0.89	30.71	46.31	39.55	51.01	0.00
M = 0.5	0.80	0.80	0.40	0.36	0.73	0.89	30.98	47.29	41.55	50.15	0.00

M = 0	0.75	0.70	0.40	0.35	0.73	0.89	30.16	44.45	38.91	47.90	0.00
M = 0.5	0.73	0.77	0.37	0.35	0.73	0.89	31.65	46.32	41.18	50.29	0.00

0.76	0.77	0.39	0.35	0.73	0.89	30.88	46.09	40.29	49.84	0.00
In Table 16, the 
 is reaching the maximum  of 49.84 s, while the RMS has the least meantime 0.35 s and the  requires in average 0.89 s.

In the next part, we are interested in the percentage of exclusive contribution  of each one of the proposed heuristics in the UB value. This exclusive participation of each one of the proposed heuristics is displayed in Table 17.


Table 17. Exclusive participation EPerc of heuristics (%).

RMS	RJS	RSPL	GA1	GA2	GA3	GA4	SPT
M = 0	0.00	0.00	0.00	0.00	0.00	0.00	34.21	38.55	4.87	5.26	0.00
M = 0.5	0.00	0.00	0.00	0.00	0.00	0.00	35.79	32.37	6.58	5.39	0.00

M = 0	0.00	0.00	0.00	0.00	0.00	0.00	33.55	38.68	4.47	4.74	0.00
M = 0.5	0.00	0.00	0.00	0.00	0.00	0.00	36.45	35.92	5.53	5.53	0.00

0.00	0.00	0.00	0.00	0.00	0.00	35.00	36.38	5.36	5.23	0.00
Based on Table 17 the 
 is strictly dominating all the other heuristics in 36.38% of the cases. This suggested that the dominance of one of the proposed heuristics is out of reach.

In order to get a more precise idea about the behavior of all the proposed heuristics, the following comparison is performed. This comparison consists of computing the average relative gap of each one of the proposed heuristic relatively to UB. The relative gap of a heuristic with a value U is given as 
 
. The average relative gap is denoted GAP. Another criterion to assess a heuristic is the average time performing the heuristic . The results of the latter study are reported in Table 18.


Table 18. Performance of each heuristic relatively to UB.

RMS	RJS	RSPL	GA1	GA2	GA3	GA4	SPT
GAP(%)	3.55	2.79	3.87	1.00	163.32	0.04	0.00	0.00	0.00	0.00	10.84
Time	0.76	0.77	0.39	0.35	0.73	0.89	30.88	46.09	40.29	49.84	0.00
Based on Table 18, the least contributing heuristic is the RJS with a mean gap equal to 163.32%. The  is performing well since the mean gap is 0.04% and the mean consumed time is 0.89 s. Surprisingly, the RMS has a mean gap of 1.00% and a mean time of 0.35 s, this is close to the performance of the  heuristic. The GA based heuristics are performing well in term of mean gap equal to 0.00%, but still remain a consuming time procedures with a mean time equal to 40 s. The 
 based heuristics displayed a mean gap of almost 3%, which is not far from the GA, , RMS. The mean time for the latter heuristics (
 based ones) is 0.7 s. All the retrieved conclusions suggested that all the developed heuristics are useful in improving the value of UB except the RJS and SPT that could be dropped.

A more detailed analysis of the performance of the proposed heuristics is presented. This insight analysis aims to characterize the situations (subclasses) in which each heuristic could be useful. In addition, this insight analysis is intended to identify if it is “possible”, the reasons that make some heuristics better than others. Recall that the heuristics are assessed over three performance measures which are , GAP, and . The detailed results for each combination of n, m, , a, and M are presented in Table 19, Table 20, Table 21, Table 22 in Appendix.

According to these Table 19, Table 20, Table 21, Table 22 in Appendix:

•
The RJS is the worst one over all subclasses with  and an average gap . These results are due to the random selection of the next job to be scheduled among a large number of jobs, this affects the quality of the obtained solution. This kind of heuristic is useless whatever the test problem subclass.

•
The SPT based heuristic is generally useless whatever the data subclass since  and the average gap GAP is almost 11%, despite the average computing time is almost 0 s.

•
The heuristics 
, 
, and 
 which are based on the exact solution of the parallel machine, are with moderate performance and do not participate in UB especially for classes 1 and 2, low number of machines m, large number of jobs n, high a, and low M. Indeed, for these subclasses . This is due to the fact that including the learning effect makes the problem more difficult and the old techniques do not apply. More specific techniques should be developed to cope with learning effect compared to the classical problem (without learning effect).

•
The RMS heuristic presents a percentage of participation in UB  for all subclasses except for the small number of jobs () where  might reach 100%. Interestingly, the RMS heuristic presents a low average gap GAP, which is not exceeding 1%. Furthermore, the average computing time is low and  s. Because of the two latter results, this heuristic is useful when low computing time is required. The most hard instances for this heuristic are detected for large number of jobs () and large number of machines (). For these subclasses GAP is reaching 6.8%. The latter result is due to the random selection of the next machine to be loaded, among a large number of machines.

•
The  heuristic a very efficient for small size problems () where  is greater than 90% and  in the majority of the cases. Furthermore,  is an efficient heuristic for all subclasses since the average gap GAP is not exceeding 0.1% and the time is not exceeding 4.6 s for large instances (). Interestingly and unlike the other heuristics (except GA), the  presents a non zero  (even with low percentage) for class 1 (small processing times) low number of machines (), high M and low a. For the latter subclasses . The random selection among a low number of jobs for the next one to be scheduled is the main reason for the satisfactory results of the  heuristic.

•
The 
 meta-heuristics are providing the best performance in terms of  and GAP. For these meta-heuristics, the computation time  is the largest one compared to the rest of heuristics as expected. A little advantage is observed for 
 and 
. Indeed, for all subclasses, 
 except for 4 instances out of 3040 ones. In almost all subclasses, the genetic algorithms based meta-heuristic 
 and 
 are useful. In contrast, 
 and 
 are exhibiting a low  performance for almost all the subclasses. A slight advantage is observed for 
 over 
 in terms of GAP for almost all subclasses.

Remarkably, 
 and 
 which start with an initial population based on the LPT rule, are more efficient' than 
 and 
 which have an initial population based on the  heuristic, despite  is more efficient than LPT. Indeed, the topic of selecting the best population size and the initial population is subject of several researches. These researches indicate that there is no general rule about the relationship between the efficiency of the GA and the quality of initial population ([12], [29]). Generally, an experimental study is required to retrieve conclusions about the effect of the quality of the initial population on the efficiency of the proposed GA.

6. Conclusion
In this paper, the parallel machine scheduling problem with DeJong's learning effect is addressed. The performance to be minimized is the maximum completion time of the jobs. The learning effect is modeled throughout varying processing times which decreases when the position of the treated job increases. This problem is an NP-Hard problem which is more difficult than the classical one in terms of resolution. Previous works confirm this hardness and moderate results were provided, especially for the exact methods. This is due basically to the weakness of the proposed lower bounds and the shortage of dominance rules, that are intended to reduce the size of the search tree. To overcome this drawback, several lower bounds, heuristics, and meta-heuristics are proposed in this work. The proposed lower bounds are classified into two classes. The first class is based on the maximum position that could be assigned to a job in an optimal schedule. The second class relies on a lower bound of the minimum load (minimum sum of the processing times). The proposed heuristics in this work are subdivided into three categories. The first category is based on modified dispatching rules as the LPT. The second type of heuristics allows us to take advantage of some procedures, which are developed originally for the classical parallel machine scheduling problem and modified in an appropriate way to fit with the problem with the learning effect. The last category is composed of four variants of the Genetic Algorithm. An intensive experimental study allows to assess the performance and the efficiency of the proposed procedures. This study provides strong evidence of the dominance of the proposed lower bounds compared to the existing ones. On the other hand, the developed heuristics are evaluated over the average relative gap which provides satisfactory results. The developed lower and upper bound stimulate their integration in the exact B&B algorithm as a future research area. In addition, several new evolutionary based meta-heuristics might be explored and adopted to provide a near optimal solution for the studied problem. As examples of these meta-heuristics, the artificial bee colony, modified fruit fly optimization algorithm, and migrating birds optimization algorithm.

