Abstract
Convolutional neural networks have been proposed as an approach for classifying data corresponding to a variety of datasets. Indeed, developments in data diversity and information technology have increased the complexity of deep learning algorithms. Numerous trained models have been proposed for supporting complex algorithms and data detachment with high accuracy. Convolutional operations increase when the convolution depth of neural networks increases. Thus, employing deep convolutional networks is challenging regarding energy consumption, bandwidth, memory requirements, and memory access. Different types of on-chip communication platforms and traffic distribution methods are effective in the improvement of memory access and energy consumption induced by data transfer. Also, dataflow mapping methods have an impressive effect on reducing or increasing delay and energy consumption caused by exchanging data between cores of a communication network. Different methods have been proposed to dataflow mapping on various networks for reducing total hop counts that led to improve performance and cost. Dataflow mapping approach can affect performance improvement of the inference phase in neural networks. This paper proposes various traffic patterns by considering different memory access mechanisms for traffic distribution of a trained AlexNet model on mesh topology. We propose a flow mapping method (FMM) on the mesh to determine the data flow efficiency of different traffic patterns on energy consumption. The FMM reduced energy and total flow by approximately 17.86% and 34.16%, respectively, using different traffic patterns. Thus, the FMM improved the performance of AlexNet traffic distribution while the impact on data flow reduced energy consumption.

Previous
Next 
Keywords
Convolutional neural network (CNN)

Flow mapping

Traffic pattern

Traffic distribution

1. Introduction
The development of information technology and various data types for datasets have rendered data classification challenging. Convolutional neural networks (CNNs) have been proposed as a machine-learning algorithm to solve the challenging problem posed by increasing data diversity by classifying data with high accuracy [15], [35], [41], [43], [48], [58]. CNN is able to categorize the data using different methods, such as labeling, semantic segmentation, and classification based on input data features of the dataset [15], [27], [42], [60]. A CNN is a powerful approach for yielding enhancement in hierarchical labeling [45], [60]. The complexity of deep learning algorithms is attributed to the development of applications based on emerging information technology, Internet of Things, and web search engines for achieving accurate results [18]. Employing modern photo editing applications, such as iPhoto and Picasa, and the increasing accuracy of the image processing are effective in dealing with algorithm complexity [18], [35], [36], [42], [48]. The enhancement of data variety and algorithm complexity are effective in increasing the convolution depth of neural networks (NN).

Different deep learning accelerators (DLA) have been proposed for supporting and solving the challenges faced by CNN. These challenges are dependent and centralized on the special components of the DLA. On-chip communication platforms, memory access mechanisms, parallel and pipeline units for computational operations, and memories with different manufacturing processes have been proposed to solve CNN challenges [8], [42], [48], [65]. Nevertheless, the memory access and energy consumption of CNNs are persistent problems.

CNNs are used for ML-based applications related to in edge nodes of internet of things (IoT), web search engines, and speech and face recognitions that increases the depth and complexity of CNNs. Increasing the depth and computational operations of NNs are challenging for GPU-based systems due to performing parallel multiply operations of filter and ifmap matrices, which reduced energy efficiency with increasing memory access [5]. Numerous DLAs have been proposed for solving these challenges with presenting different approaches in order to dataflow mapping. MAESTRO evaluated total runtime, and bandwidth and memory requirements based on ShiDiaNao (shi), weight stationary (ws), DLA, no local reuse (NL), and row stationary (rs) dataflows, as an analytical tool [37], [39]. H Kwon et al. [40] also analyzed bandwidth requirement based on rs, ws, and output stationary (os) dataflows. Eyeriss [8] and Eyeriss v.2 [7] proposed row-stationary and row-stationary plus for improving memory access and bandwidth requirement, respectively. A Samajdar et al. [56] investigated the effect of os dataflow on memory and bandwidth requirements for systolic arrays with different dimensions using providing a python-based systolic CNN accelerator simulator.

NN maps data of CNNs on proposed NN structure based on detecting near weights using algorithmic structure and characteristics of CNN (filte, ifmap, channel, and kernel sizes) [49]. Analyzing different dataflows of DLA-based systems demonstrates that dataflow mapping method is a significant effect on improving the performance of inference phase of CNNs. So, we propose a flow mapping method which is different to dataflow mapping method. Dataflow mapping depends on computing sequences whereas flow mapping is not depended on computational operations. Main purpose is the mapping flows of trained models on a mesh network in order to reduce delay and energy consumption caused by transferring data between processing elements and also exchanging data between global buffer and shared bus.

A mesh topology has a suitable bisection bandwidth which has a positive impact on the performance of a NoC. A mesh is scalable which is able to support the many PEs without the performance loss compared to expandable interconnections such as a bus. A lot of different deadlock-free routing algorithms were proposed to a mesh. A mesh has a planar structure; thus, traffic could be distributed on a mesh from the all four directions of up, down, left, and right while the traffic distribution was restricted to one direction on the some topologies. And another characteristic of a mesh is high bandwidth-links between PEs [3], [30].

This paper considers mesh topology as an on-chip communication platform for AlexNet traffic distribution. We propose different traffic patterns for AlexNet traffic distribution on a mesh based on various memory access mechanisms. Multiply-accumulate operations are performed by the processing element (PE) units per mesh node. The PEs are connected by network-on-chip (NoC) interconnection. This paper describes patterns for transferring data between PEs according to input data weight, while the flow between PEs is an effective energy parameter. Accordingly, we provide a flow mapping method (FMM) on the mesh based on the impact of different data traffic flow patterns on energy consumption. Analyzing the mesh architecture and relationship between shared bus and source or destination nodes of various patterns can demonstrate memory access improvement. The FMM is effective in reducing energy consumption and the total AlexNet traffic flow distribution on the mesh. FMM also improves the performance of the trained AlexNet model.

The remainder of this paper is organized as follows. The second section reviews previous studies on DLAs and CNNs. The third section discusses the background of CNN concepts. The fourth section focuses on the system architecture while the fifth section analyzes data flow based on different traffic patterns. The sixth section describes different patterns and the FMM. The problem formulation is described in the seventh section. The experimental results are evaluated in the eighth section, and the ninth section concludes the paper.

2. Related work
Trained CNN and deep neural network (DNN) models have been proposed as machine-learning algorithms for supporting the various sizes of input datasets and improving data classification accuracy. Numerous DLA hardware has been provided to solve problems caused by the development of applications for machine-learning algorithms. This section reviews various approaches using DLA hardware and machine-learning algorithms proposed in previous works to improve convolutional operation performance.

AlexNet has been proposed as an eight-layer trained CNN model that can classify larger input datasets with higher accuracy than the trained ImageNet model [36]. The eight layers of AlexNet consist of five convolution layers and three fully-connected layers [36]. The number of layers and the per-node computational operations of AlexNet are less than those of trained VGG-Net and GoogleNet models [8], [44], [61]. AlexNet has reduced the number of connections in fully-connected layers using the state-of-the-art method for CNN known as dropout, which has induced decreased error rates compared with ImageNet. [36]. In 2012, comparison of the classification and single-object localization accuracy between AlexNet and VGG-Net demonstrated the improved accuracy of AlexNet compared with VGG-Net [54]. Investigating the characteristics of AlexNet, GoogleNet, and VGG-Net networks illustrates that the total number of multipliers, maximum weight, and number of activation layers in AlexNet are less than those of VGG-Net [51]. Numerous methods have been exploited to categorize the larger dataset aspect of data diversity for CNN; whereas data classification is similar between trained AlexNet, VGG-Net, and GoogleNet models [60]. Shellmer et al. [60] improved the performance of semantic segmentation by utilizing end-to-end and pixel-to-pixel methods. Evaluating end-to-end and pixel-to-pixel approaches of fully convolutional networks has demonstrated significant improvement in AlexNet performance compared with VGG-Net and GoogleNet [60]. Semantic matching is an approach to solve and address the challenge of the mutual model in CNN using the annotation method of one or multi-labels [65]. However, energy consumption, storage space, and memory access of CNNs are the challenges resulting from the increase in the number of CNN layers. Sparse CNN (SCNN) has been proposed as a novel dataflow approach for improving performance and energy factors [51]. SCNN utilizes the sparse retention of weights and activations in a compressed encoding method to evaluate unnecessary data transfers and reduce storage space [51].

Different models have been proposed to improve the performance of CNN and DNN. These include tuning the recurrent neural layer for detecting structured visual regions, as well as interest regions, regarding location and orientation, and indicating objects by the RGB-D [2], [45].

Different hardware DLAs have been proposed to solve problems with machine-learning algorithms (DNN and CNN) [48], [58]. A memory access mechanism and fabrication technology are effective characteristics of DLA hardware architecture for reducing the communication platform bandwidth, storage space, and memory access. The high throughput and low energy of three-dimensional (3D) memory based on TSV technology are rebalanced in the neural network design by dedicating large areas for processing elements (PEs) and fewer areas for SRAM buffers [18]. TETRIS [18] provided hybrid partitioning in which the partitions of neural network computations were located adjacent to the dynamic random-access memory (DRAM) and parallelized neural network computational operations over multiple DLAs. ShiDianNao [15] consumed 4700x less energy compared to a graphics processing unit (GPU) owing to the lack of DRAM access by exploiting static RAM (SRAM) and careful utilization of specific data access patterns. The main memory was removed from multichip systems based on deep learning owing to the dedicated embedded DRAM (eDRAM) per chip of the system. DaDianNao [48] is a machine-learning supercomputer for accelerating computing deep neural networks which improved the performance of CNN and DNN memory access by employing the eDRAM per chip of the systems based on deep learning. CNN-MERP [21] reduced on-chip and off-chip bandwidth by employing hierarchical memory characteristics and two-level reconfigurability.

An electrical–optical hybrid architecture was designed for the multichip deep-learning (DL) supercomputer, to decrease the energy consumption and increase speed by 656.63x compared to a GPU owing to the enhanced optical data transfer rate [48]. DaDianNao has employed on-chip optical interconnections and off-chip electrical connections to perform data transfer on mesh topology. Nanophotonic circuits have increased the speed of computational operations by optical multipliers while maintaining input energy efficiency [45]. Exploiting optical communication platforms is challenging due to high sensitivity of TSV links that leads different proposed approaches for protecting links. S Huang et al. [29] proposed a method to protect multi-link random failures in optical mesh network. This method improved performance compared with protecting links in terms of single, dual, and sequential failures by improving the performance of sparse recourse efficiency [29]. Detecting link failures and recovery affect on improving performance of optical-based deep learning accelerator using reducing failure rate. Some proposed DLAs utilized high bandwidth of fiber links to reduce delay of data transfer between off-chip DRAM and on-chip memory, which an elastic optical networks can reduce delay caused by memory access using provide bandwidth-variable spectrum selection [46]. X Li et al. [46] proposed a cost-efficient method for reducing spectrum consumption induced by multicast services in optical networks. This approach employed optical networks and various spectrum that can improve bandwidth requirement in optical-based DLAs.

An on-chip communication platform type is an effective method for achieving memory access and bandwidth requirements. Eyeriss [8] proposed a novel NoC architecture for implementing CNN, which includes a two-dimensional (2D) shared bus, PEs, and multicast controllers. The Eyeriss architecture implemented CNN algorithms with low energy consumption and high efficiency by reducing memory access and bandwidth requirements [8]. Indeed, data compression and reusability of the input feature map (Ifmap), partial sum (Psum), and filter data based on row stationary reduced off-chip DRAM access, thereby increasing data traffic between a global buffer (GB) and PEs [8]. Eyeriss has improved the performance of DNNs and SCNNs. However, memory access, memory requirements, and energy consumption are problems for sparse DNN (SDNN) owing to the variety of batch sizes and complexity of SDNN [9]. Eyeriss v.2 [9] provided an approach based on an energy-aware algorithm, weight, and activation memory for reducing energy consumption. Dataflow mapping, likewise, has analyzed the energy efficiency of the Eyeriss architecture, which illustrates energy improvements based on the row stationary method [6]. AlexNet traffic distribution has been evaluated in terms of unicast, multicast, broadcast, and gather patterns based on various stationary [40]. Hyoukjun et al. [40] improved the energy efficiency of convolution operations by enabling microswitches as an on-chip communication platform. A hybrid mesh of wireless, wireline, and 3D NoC technologies have been proposed as on-chip communication platforms for GPU–CPU based heterogeneous many-core systems to improve the performance of trained CNN models [12], [32]. The latency and throughput of a hybrid mesh were reduced by approximately 1.8x and 2.2x, respectively, compared with a wireline mesh for trained CNN models [12]. B Kumar et al. [32] achieved reduction of thermal hotspots using 3D mesh, whereas maximum temperature was reduced by approximately 22% with negligible loss in performance.

Reconfigurable, flexible, pipeline, and parallel PE unit architectures have been proposed to improve the performance of machine-learning algorithms. Seongwook et al. [52] exploited the reconfigurable architecture and parallelism of semantic inference engines per layer, which increased the speed of big data processing and reduced SRAM bandwidth requirements. A flexible DLA architecture is an effective method to decrease power consumption using three parallel processing units and the tile technique for localization of DL applications [64]. Fathom [1] has been used to analyze the application-level parallelism for DL networks; data-level parallelism on-kernel, off-kernel, and a hybrid of both were evaluated by C-Brain [63]. Angel-Eye [20] has provided a flexible and programmable architecture on the Zynq-XC7Z045 platform for improving the energy efficiency of CNN. L Jing et al. [31] also proposed a deep learning inference accelerator based on model comparison on FPGA that employed shift and accumulation (SAC) operations for calculations of convolutional and fully connected layers. The majority of DLA hardware has been limited to implementing specific machine-learning algorithms. Minerva, however, can support any DNN algorithm and reduce power consumption by 8.1x over an accelerator baseline without degrading DNN accuracy by presenting the highly automated codesign of accelerators [53].

Systolic arrays have been employed to improve the computational operations of NNs in some DLA architectures. Tensor processing units (TPUs) [62] have been proposed as a custom ASIC design for NN machine learning by Google. The TPU architecture consists a matrix multiplication unit, local unified buffer, DRAM, and accumulators, where the matrix multiplication unit is a systolic array. The TPU accelerates the inference phase of NNs owing to the lower precision compared with GPU and CPU. TPU performs high-volume and low-precision computations owing to the lack of texture mapping and eight-bit multiplies [62]. Experimental results demonstrate the increase in processing speed of TPU compared with GPU and CPU in some NN applications. The TPU is faster than contemporary GPU and CPU by about 15x–30x [62].

Computational complexity is increased for deep neural networks (DNN) compared with neural networks. DNNs involve different layer shapes and sizes that tend to be dense or sparse in deep neural networks. Many hardware accelerators do not support trained models based on DNN because they lack the flexibility and variety of layer shapes and sizes that DNN requires [7], [38]. The trend toward deep neural networks increases the amount of research in the field of hardware accelerators with high performance for supporting DNN-trained models [38]. Different methods have been proposed for partitioning layers. Layer-by-layer partitioning is a classic method for mapping within the substrate; however, the layer-by-layer method is not suitable for DNNs. Partitioning based on weight and output reuse is employed by DNN accelerators; however, DRAM access remains challenging owing to computational complexity, latency, and energy efficiency [38]. MAERI [38] reduced latency and area consumption compared with Eyeriss, systolic array and micro-switches array accelerator by providing a different architecture for a DNN accelerator [40], [62]. Hyoukjun et al. [38] employed a fat tree as an interconnection network between PEs and the prefetch buffer, and an augmented reduction tree for accumulating Psums. Tiny switches in the MAERI tree and high-bandwidth extra links improved the performance of DNN trained models [38]. Unfortunately, the required number of PEs remains a challenge for designing an accelerator owing to different layer shapes and sizes of DNN. MAESTRO [39] determined the required number of PEs for DNN trained models by presenting an open-source infrastructure for modeling dataflows within Deep Learning Accelerators based on kernel, ifmap, and filter sizes.

NN trained models possess three layers: of convolution, pooling, and fully-connected. Some DNN trained models, such as GoogleNet, not only possess these three layers but also contain a reduction layer; in addition to the three original layers, the DNN accelerator should be capable of supporting a reduction layer [7]. Eyeriss.v2 investigated quality performance and identified loss performance of DNN processors that support DNN trained models, such as AlexNet, GoogleNet, and MobileNet. Y Chen et al. [7] improved performance by approximately 10.4x–17.9x compared with Eyeriss by presenting row-stationary plus (RS). Eyeriss v.2 used a hierarchical mesh for supporting RS which provided a link bandwidth based on the data reuse rate. However, RS and the delivery of the data reuse rate increased multiplier–accumulator (MAC) complexity of PEs [7].

Some DLAs have been proposed for supporting the specific goals of improving the real-time scene labeling on embedded platforms and energy-aware layer-by-layer pruning of the algorithm for reducing the energy and weight of AlexNet as a trained CNN model [4], [66]. A text-mining accelerator has been proposed to decrease bandwidth requirements, power consumption, and delay using reconfigurable architecture and field-programmable gate arrays [34].

GPU-based systems perform computational operations of NNs in parallel that reduce energy efficiency due to increase memory access in order to read and write main memory [5]. Numerous DLAs have been proposed to improve energy consumption as a challenge of GPU-based systems, which consists ASIC, FPGA, and NoC-based designs [5]. Simulation result and case study demonstrate that NoC-based DLAs have higher flexible structure than ASIC and FPGA designs due to flexibility of interconnections in on-chip communication networks [5]. These accelerators exploited different approaches for improving performance of inference phase of DNNs. Different mapping approaches have been proposed in order to increase energy efficiency and reduce delay. A Firuzan et al. [17] proposed a reconfigurable architecture of NoC for accelerating large-scale neural networks using 3D technology, multicasting and mapping methods. DNN pruning methods have been also proposed to improve performance of NNs which proposed tiny trained models and pruning before dataflow mapping on NoC reduced energy consumption and total delay caused by inference phase [59], [67]. FlexFlow [47] proposed a flexible dataflow architecture that was adapted to growing computing engine scale and multiple mixtures of parallelisms in computational operations. W Lu et al. [47] demonstrated that tiling method had an impressive effect in increasing performance of LeNet-5 compared to systolic and 2D-Mapping method. Architecture of FlexFlow consists neuron buffers 1–2, kernel buffer, arithmetic logic units (ALU), and a set of PEs that is flexible based on dataflow and introduced loop unrolling convolutional layers [47]. The flexibility of FlexFlow have a significant effect on improving performance of large-scale convolutional neural network.

Development of evolutionary algorithms of artificial intelligence is challenging for implementing adaptive general-purpose intelligent systems that are required to learn autonomously. GeneSys proposed a hardware–software (HW–SW) approach for solving the challenge of backpropagation in the training phase using a closed loop engine [55]. Indeed, GeneSys [55] employed an HW–SW prototype and two different engines for inference and training phases for supporting evolutionary algorithms in modern deep learning systems.

3. CNN background
This paper proposes a flow mapping approach to improve the performance and energy efficiency of convolution operations. The previous section presented different DLA methods and machine-learning algorithms, and the fact that DLA hardware discussions depend on the basic concepts of NN trained models. This section describes the background of NNs, which includes machine-learning algorithm classification, AlexNet as a trained CNN model, and pooling and convolution operations of CNN.

CNNs and DNNs are machine-learning algorithms, whereas CNNs and DNNs are machine-learning algorithms that belong to the family of multilayer perceptrons (MLPs) [8], [15], [18]. CNNs and DNNs are known as four-layer state-of-the-art algorithms consisting of convolution, pooling, normalization, and classification layers [15]. DNNs are more complex than CNNs; thus, they can support broader applications despite the fact that both DNN and CNN possess a similar number of main layers [13], [28], [48]. The Ifmap, filter, and Psum are the reusable synaptic weights of CNN, which are shared by certain neurons [15]. However, the synaptic weights of DNN convolution layers are not reusable [15]. The reusability of CNN synaptic weights is an effective characteristic for improving memory access and the reducing bandwidth requirement. Hence, we evaluate AlexNet traffic distribution as a CNN trained model for the flow mapping approach, where the reusability of CNN synaptic weights is effective in improving memory access and energy efficiency.

A CNN was designed as an MLPS to detect 2D shapes with a high degree of variance for translation, scaling, skewing, and other forms of distortion [22]. Feature mapping (fmap) is an effective method to design a network for learning, owing to the benefits of structural constraints, such as shift invariance, reduced number of free parameters, and subsampling [22]. The computational layers of NNs include multiple fmaps, where each feature map is a plan of single neurons and limited to sharing the identical sets of synaptic weights [22]. Increasing the depth of the convolutional layer hierarchy increases the classification accuracy of state-of-the-art CNNs, which leads to an fmap with high-level abstraction [19], [23], [57].

The Ifmap, filter, and Psum as the synaptic weights of CNN are described as follows. The Ifmap consists of various input data categorized in the form of a matrix determined based on semantic matching and other characteristics with specific size and six-bit parameters [8]. A filter is a matrix for categorizing Ifmap data in the range of the filter by multiplying the Ifmap and filter, with each multiplication result generating a Psum [8].

Convolution operations categorize input data per layer and label the input features. Special characteristics of input datasets per convolution layer are recognized using convolution operation [8].

AlexNet is an eight-layer trained CNN model with five convolution layers and three fully-connected layers [36]. The error rate of AlexNet is lower compared with ImageNet, whereas the number of ImageNet layers is greater than that of AlexNet [36]. The analysis of error rates for image classification and single-object localization based on the ImageNet Large Scale Visual Recognition Challenge illustrates the enhanced accuracy of AlexNet compared with VGG-Net in 2012 [54]. In AlexNet, convolution sizes are 1127, and  for CONV1, CONV2, and CONV3–CONV5, respectively [8]. This paper analyzes AlexNet traffic distribution on mesh topology with high accuracy and a lower number of layers compared with the trained model [51].

Fig. 1 shows the details of the AlexNet architecture as a trained CNN model. Pooling operations combine the outputs of neuron clusters per layer into a single neuron in the next layer by re-eliminating some parameters per layer [28], [36]. The zero-padding operation is often removed from the pooling layer [36].

Stride size is a parameter of CNN algorithms for determining the number of features, and dropout from width and height in order to reduce input depth. Indeed, matrix arrays were shifted to the left based on stride size for the rotary matrix multiplication [8], [36]. Stride parameters are shown in Fig. 1.


Download : Download high-res image (269KB)
Download : Download full-size image
Fig. 1. AlexNet architecture [36].

4. System architecture
Mesh topology was employed as a communications platform for transferring data in AlexNet convolution operations; the energy consumption was investigated in unicast and multicast patterns. We analyzed energy consumption based on various memory access mechanisms and different configurations in order to connect the shared bus, mesh, and partitioning nodes. The DLA architecture consists of two storage units and a mesh topology as the interconnection between PE units. The storage units include a GB for transferring local data and reusable synaptic weights of the CNN algorithm, an off-chip DRAM for data storage of all layers of AlexNet convolution operations, and a mesh topology as an interconnection of PE units. One hundred sixty-eight mesh nodes were designated as destination nodes for convolution operations. The shared bus transferred data between the sources or destination nodes and GB.

We employed unicast and multicast routing methods for data transfer between mesh nodes.

Unicast method: data is only transferred between two nodes at any moment.

Multicast method: data is transferred between sets of nodes on the mesh in parallel at any moment.

In the system architecture section, interconnection and node structure of the mesh topology were investigated as part of the proposed system architecture. This section describes AlexNet traffic distribution per layer on the mesh according to CNN background concepts.

4.1. Overview
We describe the system as a set of integrated components for transferring and distributing data. Fig. 3(a) shows a schematic of proposed DLA including a shared bus, a global buffer (GB), switch selector, and a mesh topology whereas the dimension of the mesh and partitioning on  dimension are changed based on traffic patterns, which will describe in Section 6. Switch selector directed received flits from GB to destination nodes using enabling a signal for determining destination node due to reading address field which this signal connected to our proposed switches. Switch selector is the same as the arbiter unit in the baseline router that is simpler than the arbiter unit due to the simple routing. Data is distributed to destination nodes using connected address lines to switches, which will be described in subsection B. Fig. 3(b) shows the schematic of a  2D mesh as the basic mesh for traffic distribution of AlexNet convolution operations. The node structure includes a router and PE units, as a shown in Fig. 3(b).

4.2. Router
Fig. 4 demonstrates a schematic of the designed router in this paper. We designed the router with a proposed simple architecture for the switches owing to a simple routing algorithm. The routing algorithm is based on multicast XY with an on/off buffer backpressure flow control mechanism [14], [16]. The router includes four in/out full-duplex ports and one injection and ejection port between the PE and router, and each port has a buffer. A multicast buffer was designed to copy and send data for multicast routing. First, each input flit is transferred on the multicast buffer and is sent on output links for data multicasting. The router assignment and data transfer on a port or switching on the output port of the router were based on a two-stage pipeline. The two-stage pipeline includes the buffer write (BW) and switch transmission (ST). Route computation delay and switch transmission (ST) are not significant because the architecture of the designed switches and switch selector is very simple. RC mechanism is performing into switch selector.

Fig. 4 (a) shows a schematic of a router that includes a simple switch. Fig. 4(b) illustrates a schematic of the switch designed in this study. We designed the switch based on various mechanisms and patterns of memory access and data transfer. Accordingly, the switch has a different architecture and mechanism compared with the base switch of the mesh and does not consider the circuits in the arbitration process of the switch. The switch can be multicasting flit on three links based on address lines (S0, S1, S2, S3, and S4) and enable port (EN).

We design a switch selector for selecting switches, as shown in Fig. 4(c). Switch selector includes two active low decoders of R-decoder and C-decoder for selecting switches on rows and columns of the mesh. The selector switch can be multicasting flit on the different row and column switches of the mesh based on the address lines (S0, S1, S2, S3, and S4).

4.3. Implementation of AlexNet convolution on mesh topology
Fig. 5(a)–(e) show destination nodes on a  2D mesh for CONV1, CONV2, CONV3, CONV4, and CONV5, respectively, in the trained AlexNet model. The destination nodes are dedicated to convolution operations and are denoted as gray-colored nodes on the mesh topology. The shared-bus transfers the read data from the GB to destination nodes by considering the address. Traffic is distributed per convolution owing to stride size, with CONV1 traffic distribution based on a stride size of 4, as shown in Fig. 5(a). The stride size is 2 for CONV2–CONV5. However, we considered stride size 1 for CONV2–CONV5 to reach maximum utilization of the mesh node for AlexNet traffic distribution; stride size 1 did not cause a negative impact on rectified linear unit computation [8]. Fig. 5 illustrates the location of destination nodes in order to describe and analyze the presented patterns.

5. Row–column stationary
The mesh topology, shared bus, and GB were introduced as the components of the designed DLA architecture in the previous section. We analyzed data transfer between the shared bus and mesh, as well as different patterns of AlexNet traffic distribution based on various memory access mechanisms as the effective parameters for evaluating energy consumption.

This section investigates different dataflow approaches based on data placement-stationary discussed in previous papers and the proposed dataflow approach in this paper.

5.1. Introducing previous stationary
Utilizing synaptic weights, the reusability of CNN and ASIC special-purpose DNN algorithms [33] were categorized as local reusable (LR) and not local reusable (NLR) synaptic weights.

NLR synaptic weights: Lu et al. and Jouppi et al. employed the NLR approach for transferring data in order to increase processing speed owing to the proposed architecture structure [33], [48].

Dataflow based on LR approaches include the following:

Weight stationary (WS): One weight element is received from the GB and broadcasted on PEs, whereas the other weight element is not received during convolution operations of PEs [11], [40], [58].

Output stationary (OS): An output-stationary DLA maps the received output or weights and input activations of GB, and sends Psum to the GB [15], [40], [50].

Row stationary (RS): The Ifmap and filter are transferred from the GB to PE units horizontally, whereas Psums are accumulated vertically by a local on-chip network, and the accumulated Psums are transferred to the GB [8].

Eyeriss proposed a row stationary approach for computing and transferring data between GB and PE units, and analyzed energy consumption based on dataflow. Analyzing dataflow in stationary row, weights, NLR, and output illustrates the maximum improvement of dataflow based on row stationary compared with other stationaries [8].

5.2. Proposed row-column stationary
We propose row–column stationary (RCS) dataflow as a state-of-the-art approach for AlexNet traffic distribution and flow mapping based on different patterns and memory access mechanisms. An accelerator can transfer data on sets of nodes based on RCS dataflow in the vertical and horizontal positions, as well as in parallel simultaneously. The influence of RCS on energy consumption will be analyzed in the next section. Fig. 6 shows the RCS method for transferring Ifmap, filter, and Psum data on a set of nodes.

6. Different patterns for AlexNet traffic distribution on mesh
The previous section described RCS dataflow as a state-of-the-art approach for AlexNet traffic distribution. This section evaluates proposed traffic patterns based on different memory access mechanisms and the various architectures of connected shared buses and mesh. We describe different architectures based on different communication patterns of connected shared bus to global buffers. The flow mapping method was investigated based on energy consumption and total flow on the mesh per pattern, where patterns are in a multicast distribution.

6.1. Different architecture based on various patterns
Sixty-four-bit parameters of Ifmap and filter matrices were transferred to PEs for convolutional operations owing to different memory access mechanisms and interconnection between the shared bus and source or destination nodes. Different patterns were proposed based on the architecture and connections between the mesh, shared bus, and GB with mesh size changing with the patterns. We consider the minimum number of hop counts between the destination nodes for reducing energy consumption induced by accumulating Psums.

Fig. 7 shows a  2D mesh where  partition nodes1 are destination2 or source nodes3 for the  partition. AlexNet traffic was scattered on the  mesh, as shown in Fig. 7.

Fig. 9 shows a  2D mesh. The location of partitioning on the mesh is different in Fig. 8, Fig. 9. The nodes are partitioned at the up of the  mesh, as shown in Fig. 9(b) and (c).

As shown in Fig. 10(b), AlexNet traffic is scattered on the  mesh while Fig. 10(a) shows a 6 2D mesh. The  mesh is divided into similar partitions vertically, and two  partitions6 are connected to the left and right side of the  mesh.

Fig. 11 illustrates a  2D mesh and AlexNet traffic distribution on the  mesh. One  is arranged to the left and two  partitions are arranged on the up and down sides of the  mesh. The up and down- partition nodes are the destination or source nodes for the up and down-partitions, respectively, as shown in Fig. 11(b) and (c). Note that there is a difference between Fig. 11(b) and (c) is the number of partition nodes.


Download : Download high-res image (304KB)
Download : Download full-size image
Fig. 10. (a) 6 2D mesh (b) the number of nodes per-partitions is 84 and the  partition nodes on the left are the source or destination nodes for the left partition;  partition nodes on the right are the source or destination nodes for the right partition.

Two  and two  partitions are connected to the left side, right side, up, and down of the  mesh, respectively, as shown in Fig. 12(b) and (c). Thus, Fig. 12 shows a 2D  mesh as well as AlexNet traffic distribution on the  mesh. The up and down  partition nodes are the destination or source nodes for the up and down partition7 s, respectively. Note that there is a difference between Fig. 12(b) and (c) is the number of partition nodes.


Download : Download high-res image (524KB)
Download : Download full-size image
Fig. 11. (a)  2D mesh; (b) the number of partition nodes is 46 and  partition nodes are the source or destination nodes for partitioned-out nodes; the up- partition and down- partition are the source or destination nodes for up and down-partitions, respectively; (c) the number of partition nodes is 56 and  partition nodes are the source or destination nodes for partitioned-out nodes; the up  partition nodes and down- partition nodes are the source or destination nodes for up and down-partitions, respectively.

A multicast method was employed for routing and traffic distribution per pattern; the mesh topology was partitioned according to the hop counts between the destination and source nodes, as well as different architectures of on-chip communications between the shared bus, mesh, and GB. The RCS dataflow approach was utilized to transfer data between the source and destination nodes. Dataflow was based on RCS for traffic patterns on meshes with dimensions 1315, and . The energy consumed by the traffic distribution was analyzed based on various patterns provided and an FMM was proposed to evaluate energy and total flow on the mesh. Specific equations describe the proposed traffic patterns and analyze flow mapping. The next section provides mathematical descriptions of flow mapping and different patterns.


Download : Download high-res image (508KB)
Download : Download full-size image
Fig. 12. (a)  2D mesh. (b) The number of up and down-partition nodes is 46 and the number of left and right-partition nodes is 42. Left and right- partition nodes are the source or destination nodes for left and right-partitions, respectively. Up and down- partition nodes are the source or destination nodes for up and down-partitions, respectively. (c) The number of up and down-partition nodes is 56 and the number of left and right-partition nodes is 28. The left and right- partition nodes are the source or destination nodes for left and right partitions, respectively. The up and down- partition nodes are the source or destination nodes for up and down-partitions, respectively.


Table 1. A list of parameters used in this paper.

Parameter	Description	Depend on
The row of the source node	Traffic patterns
The row of the destination node
The row of the current node
The column of the source node
The column of the destination node
The column of the current node
R	A permanent value for the repeat number
r	Repeat number
TFM	Total flow of the mesh
Traffic per node
f	Flow
TAN	Total of Active Nodes
TNN	Total Number of Nodes
PAN	Percent of the active nodes
N  M	Mesh size
H  W	Ifmap size height/width	CNN of the AlexNet
s	Stride size
X,Y	X and Y dimensions of each convolution group for the partition
Number of columns for AlexNet traffic distribution on the mesh based on Ifmap size
Number of rows for AlexNet traffic distribution on the mesh based on Ifmap size
Partition size	Choice suitable partition
Maximum value of the row in a partition
Minimum value of the row in a partition
Maximum value of the column in a partition
Minimum value of the column in a partition
A permanent value
Hop count between source and destination (i, j)
The determiner parameter of the suitable partition
Primary determiner parameter of the suitable partition
Total hop count for all flows in partition
Percent of total flow in up partition	
Percent of total flow in central partition	Traffic distribution
Percent of total flow in down partition
7. Problem formulation
In this paper, we propose different patterns and flow mapping methods based on the energy consumption of AlexNet traffic distribution on mesh topology. The previous section analyzed various patterns due to different memory access mechanisms and the connection between the shared bus and mesh. This section provides mathematical descriptions of different patterns proposed for AlexNet traffic distribution on the mesh according to the hop counts between the source and destination nodes. We propose a model for partitioning nodes to evaluate total hop counts and the maximum distance between the source and destination nodes.

Table 1 lists the parameters used for the mathematical descriptions of different patterns, traffic distribution on a mesh, and the model for partitioning nodes.8 The parameters of the traffic distribution percentage on the up, down, and central partition9 s of a mesh are also presented in Table 1.

We first provide the primary definitions of parameters listed in Table 1.

Definition 1

Total flow for 
 traffic (
) per node of 
 matrix is a parameter dependent on traffic pattern and calculated for nodes along the path between a source and destination nodes where  is a matrix while 
 demonstrates an  mesh topology.

Definition 2

Total flow on 
 matrix on an  mesh is achieved based on dataflow of different proposed patterns.

Definition 3

Percentage of active nodes for various patterns is an effective characteristic for evaluating the relevance between the number of nodes and energy consumption, which will be described in subsection A.

Definition 4

AlexNet traffic distribution on a mesh is based on classifying the mesh into X and Y dimensions owing to the  Ifmap matrix.

Definition 5

Determining suitable partition is a model for the subset selection of nodes based on traffic patterns when the following conditions are met:

•
First step:  for different partitions with various dimensions while a subset of the source and destination nodes is similar.

•
Second step:  parameter is calculated, and suitable partition is determined based on  value that meets the condition .

Definition 6

, and  are obtained based on percentage of traffic distribution on the up, center, and down- partitions of the mesh, respectively. The purpose of calculating the percentage of traffic distribution is to evaluate the relationship between different proposed patterns and on-chip GB suitable location of the accelerator architecture where 
 is a matrix in order to illustrate a  2D mesh.

We map data per convolution on the mesh based on different patterns and model partitioning by the mathematical description of the AlexNet traffic distribution. Energy consumption and flow mapping are analyzed by calculating total flow and percentage of active nodes per pattern.

7.1. Mathematical analysis of proposed traffic patterns
This subsection describes various proposed patterns for AlexNet traffic distribution on a mesh. The 
 matrix is denoted as an N  M 2D mesh, and the A matrix elements are the traffic per node.10 In the previous section, six patterns were provided based on the different communication architectures between the shared bus, mesh, and GB. In this subsection, we analyze the  2D mesh traffic pattern.

The A matrix describes the N  M 2D mesh topology with the mathematical analysis of traffic patterns corresponding the  matrix. 
 
(1) 
  The total hop count between source and destination nodes is achieved by Formula (1).

The Z parameter is obtained from the hop counts between source and destination nodes where  and  are vertical and horizontal, respectively. (2)
 

The total flow is calculated based on current i and j values for 
 as the A matrix element, both horizontally and vertically, by Eqs. (2), (3), and (4), where flow is achieved based on the described conditions per the recursive equation, owing to the node position as an element of the A matrix. Flow is calculated per node based on the hop count between the source and destination nodes by Eq. (2) when   as a primary condition is met, where the conditions  and  are met to achieve flow between the source and destination nodes based on 
 and 
, respectively, owing to the value of parameter j. Flow increases for nodes between a source and destination by two units per flow when the condition 
 is met in Eq. (2); thus, 
 as the flow parameter is enhanced for the source and destination nodes by one unit per flow.

Eq. (2) describes the traffic pattern of the  2D mesh in Fig. 12(c) while the left and right  partition nodes are the source and destination nodes. (3)
 Flow is obtained based on Eq. (3) from the hop counts between the source and destination nodes when the primary condition, , is met for the source and destination nodes.

The flow is increased by two units per flow based on  owing to the value of parameter i for nodes between a source and destination. The flow parameter of 
 is enhanced when the condition, , is met in Eq. (3).

Eq. (3) describes the traffic pattern of the  2D mesh in Fig. 12(c) when the up- partition nodes are the source and destination nodes. (4)
  Eq. (4) describes the traffic pattern of the  mesh in Fig. 12(c) where the down- partition nodes are the source and destination nodes. The hop counts between the source and destination nodes are obtained based on Eq. (4) when the primary condition, , is met for the source and destination nodes.

The conditions,  and , are met to obtain flow between the source and destination nodes when 
 and 
, respectively, owing to the value of parameter i. The flow is increased by two units per flow for the nodes between a source and destination.

The condition, 
, is met in Eq. (4); thus, the flow parameter, 
, is enhanced for the source and destination nodes by one unit per flow. (5)

The total flow on a mesh is obtained using Eq. (5) after calculating flow for five convolutions. (6)
 
 The percentage of active nodes is obtained by Eq. (6) where 
 for 

7.2. Mathematical analysis of AlexNet traffic distribution on mesh
In this subsection, we describe convolution traffic of the trained AlexNet model based on the matrix dimensions of the Ifmap and filter. The mathematical description of the AlexNet traffic distribution was employed for all proposed traffic patterns in this paper. 
 (7)
The value of parameter, j, is obtained for the destination node in order to determine the AlexNet traffic distribution in the next convolution based on the convolution corresponding to the stride size using Eq. (7). (8) 
 
 
 

Eq. (8) describes the AlexNet traffic distribution method on a mesh for all convolutions based on Ifmap size, i.e., 
 and 
 values are determined as members of the X set based on the  Ifmap matrix, where X is a subset of the N set. (9)The value of parameter, y, is obtained based on Ifmap size by Eq. (9) with .

7.3. Analytical model for determining suitable partition
In this subsection, we propose a mesh partitioning model based on the total hop counts from a set of source and destination nodes. A partition is suitable when the total hop counts between a source and destination node are minimized compared with different partitions with various dimensions. (10) 
 The values of I and J are obtained as determiner parameters for the partition size by Eq. (10) where

(1)
The traffic pattern is similar for partitions with different dimensions.

(2)
I and J values are calculated by the maximum hop counts between source and destination nodes horizontally and vertically, respectively, for partitions with various dimensions while the traffic pattern is similar.

(3)
I and J values are measured for a subset of nodes when the data transfer between nodes is restricted to the inside of the partition. (11)
 

The primary condition for determining a suitable partition was investigated based on Eq. (11) to determine the value of the PDP parameter. The value of the  parameter is obtained when

(1)
The primary condition of  is met.

(2)
Eqs. (12), (13) are employed for detecting suitable partitions when two or more partitions are evaluated with different sizes.

However, the values of  and  parameters are not estimated when determining a suitable partition and the evaluation process for  ends. (12)
Total hop counts are calculated for the described flows of a partition based on the traffic pattern by Eq. (12). (13)
 
A suitable partition is modeled based on Eq. (13). Such that:

The values of the parameters, i and j, are permanent as the numerator and denominator of , respectively. Suitable partitions have a lower  value than other partitions.

7.4. Mathematical analysis of traffic distribution percentage on the mesh
The percentage of traffic distribution on the up, center, and down partitions of a mesh are effective parameters for analyzing the on-chip GB position of the proposed DLA architecture. Hence, we evaluated traffic distribution on partitions at the up, center, and down- of a  2D mesh. Matrix A demonstrates a  2D mesh for calculating the percentage of traffic distribution. 
 
(14)
 
 The percentage of traffic distribution on the up partition11 is obtained based on the 
 parameter for source and destination nodes of the  up partition compared with the total flow on the mesh by Eq. (14) where the positions of the source and destination nodes are  and  when 
 for 
, (15)
 
The percentage of traffic distribution on the center partition is obtained based on the total flow for the source and destination nodes of the  center partition compared with the total flow on the mesh by Eq. (15) when the values of parameters, i and j, are  and , respectively. (16)
 
The  parameter as a percentage of traffic distribution on the down-partition is obtained based on the parameter, 
, for the source and destination nodes of the  down-partition compared with the total flow on the mesh by Eq. (16) when the positions of the source and destination nodes are  and , and 
 for 
.

The problem formulation is described for the total flow estimation and flow mapping to determine the AlexNet traffic distribution on a mesh per each pattern. In the next section, we analyze different patterns based on energy consumption, problem formulation, and experimental results.

8. Experimental results
In this paper, we proposed various patterns based on memory access mechanisms and different interconnections of a shared bus, mesh, and GB for AlexNet traffic distribution on a mesh topology. The previous section described traffic patterns mathematically and proposed an analytical model for mesh partitioning based on the total hop count. The proposed patterns are based on an AlexNet traffic distribution on a mesh at the left, right, up, and down using a shared bus. In this section, we evaluate the total hop count, energy consumption, percentage of active nodes, total flow on a mesh, and percentage of AlexNet traffic distribution on the up, center, and down-partitions of the mesh.

The experiment includes an estimation of the energy consumption of the AlexNet traffic distribution on the mesh based on different memory access mechanisms. We developed a cycle-accurate simulation tool based on SystemC; the original format of this tool is inspired by the noxim tool [3], [10], [24]. Thus, we obtained the total energy of the AlexNet traffic distribution on the mesh per pattern using SystemC-based cycle-accurate tools furthermore we got a design verification using the switch synthesis by the VC707 device of Xilinx simulation tool [25]. AlexNet is a state-of-the-art eight-layer trained CNN model of a machine-learning algorithm. The eight layers of the AlexNet model consist of five convolution layers and three fully-connected layers [36]. We analyzed AlexNet traffic using an investigation of AlexNet trained model in the caffe. AlexNet had been investigated by us with getting a log of AlexNet project and analyzing frameworks [26]. This paper investigated the traffic distribution of five convolutions on a mesh.

Table 2 lists the chart labels for total hop count, energy consumption, total flow, percentage of active nodes, and traffic distribution based on the different patterns proposed in Section 6.

Fig. 13 shows total hop counts based on different patterns when the hop count was evaluated as an effective parameter of energy consumption. The total hop count was reduced for a pattern on the  mesh corresponding to Fig. 12(c) compared with other patterns. The total hop count reduction for mesh _p is approximately 31.15% compared with mesh , as shown in Fig. 13. Labels of mesh _p and mesh  correspond with the patterns of Figs. 12(c) and 7(b), respectively, as described in Table 2. The total hop count was obtained for different patterns, whereas traffic was distributed on a mesh in terms of multicasting per pattern. The Multicast traffic distribution is based on RCS and row stationary for patterns corresponding to Figs. 12(c) and 7(b), respectively.


Table 2. Description of labels used for figures in this subsection.

Chart labels	Corresponding pattern	Corresponding figure
Mesh 	 2D mesh	Fig. 7
Mesh _down_u	 2D mesh	Fig. 8(a)
Mesh _down	 2D mesh	Fig. 8(b)
Mesh _down_p	 2D mesh	Fig. 8(c)
Mesh _up	 2D mesh	Fig. 9(b)
Mesh _up_p	 2D mesh	Fig. 9(c)
Mesh 	 2D mesh	Fig. 10(b)
Mesh 	 2D mesh	Fig. 11(b)
Mesh _p	 2D mesh	Fig. 11(c)
Mesh 	 2D mesh	Fig. 12(b)
Mesh _p	 2D mesh	Fig. 12(c)

Download : Download high-res image (570KB)
Download : Download full-size image
Fig. 13. Total hop count.


Download : Download high-res image (271KB)
Download : Download full-size image
Fig. 14. Total energy (J).

The energy consumption influences AlexNet traffic distribution based on different patterns. AlexNet traffic is scattered on a mesh by multicasting for all proposed patterns except for the traffic distribution of u_mesh _down which is based on unicasting. Fig. 14 demonstrates total energy for the different patterns proposed in Section 6. The total energy of mesh _p is less than that of other patterns, whereas the energy consumption of u_mesh  is greater than the other patterns. The reduction in total energy of mesh _p is approximately 17.96% compared with mesh , as shown in Fig. 14. The multicast traffic distribution patterns of mesh _p and mesh  are based on RCS and row-stationary, respectively.

The evaluation of the total energy charts indicates that unicast traffic distribution causes a significant negative impact on energy consumption, while decreasing the total hop count reduces the total energy.

We illustrate total flow on a mesh in the form of a chart for the numerical description of the total flow per pattern to explain the concept of Fig. 17. Fig. 16 shows the total flow on a mesh corresponding to different patterns. The total flow of mesh _p is less than that of other patterns, whereas mesh _p corresponds to Fig. 12(c). Total flow decreased by approximately 34.16% for mesh _p compared with mesh  when the pattern of mesh  corresponds to Fig. 7(b). The multicast traffic distribution patterns of mesh _p and mesh  are based on RCS and row-stationary, respectively.

Analyzing the relationship between the four parameters of total hop count, percent of active nodes, total energy, and flow illustrate the following:

(1)
Reducing the total hop count and flow reduces the total energy per pattern.

(2)
Increasing the percentage of active nodes demonstrates scattering traffic distribution on a mesh when the total flow is reduced.

(3)
Enhancing the percentage of active nodes reduces the total energy.

Fig. 17 demonstrates the total flow on nodes using a color spectrum to represent different patterns. The brighter color spectrum indicates greater flow on a node; thus, the total flow for patterns corresponding to mesh with darker colors is lower compared with patterns corresponding to mesh with brighter colors. The total flow was reduced for patterns corresponding to Fig. 12(c) compared with other patterns owing to more coverage with darker colors.

We determined the percentage of traffic distribution on the up, center, and down  partitions of a  mesh using Eqs. (14), (15), and (16), respectively. Fig. 18 shows the percentage of AlexNet traffic distribution on the center, up and down -partitions of a  2D mesh with partitions consisting of 56 nodes. The  2D mesh corresponds to Fig. 1, Fig. 2. The traffic distribution percentage of the down partition is greater than those of the up and center  partitions of the  mesh. The percentage of traffic distribution is an effective parameter for on-chip GB positioning of the proposed DLA architecture, where one GB is employed for storage of Ifmap, filter, and Psum data.

AlexNet traffic was distributed on MAERI topology for comparing the simulation results of MAERI and the mesh _p pattern [38]. The simulation results illustrate that the energy consumption decreases and the total delay increases for mesh _p by approximately 43.66% and 2%, respectively, compared with MAERI, as shown in Fig. 19. FMM was evaluated on MAERI topology for the total traffic of five convolutional layers of AlexNet owing to MAERI possessed better latency and area consumption than Eyeriss, systolic array, and micro-switches array accelerators [38], [40], [62]. Therefore, the proposed mesh and FMM of this paper were compared with MAERI.


Download : Download high-res image (94KB)
Download : Download full-size image
Fig. 18. Percentage of AlexNet traffic distribution on up, center, and down-partitions of  mesh.

Fig. 20 illustrates the energy consumption of different components of the proposed mesh network for analyzing the energy contribution of various components and operations in estimated total energy caused by distributing traffic on our mesh network.


Download : Download high-res image (316KB)
Download : Download full-size image
Fig. 19. Comparing total energy (J) and total delay (Cycle) of Mesh _P and MAERI: (a) comparing total energy of Mesh _P and MAERI, (b) comparing total delay of Mesh _P and MAERI.

The experimental results demonstrate that the energy consumption and the total flow of mesh _p were reduced by approximately 17.86% and 34.16%, respectively, compared with mesh . Mesh _P corresponds to Fig. 12(c) when the  2D mesh was surrounded at the up, down, left, and right by a shared bus versus a  mesh covered only on the left side by a shared bus. Multi-side traffic distribution leads to a decrease in total energy, hop count, flow on a mesh, and increased percentage of active nodes. Dispersion of traffic distribution is an effective for reducing total hop count and flow on a mesh, and enhance the percentage of active nodes. Therefore, energy consumption was reduced by decreasing the total hop count and flow, and increasing the percentage of active nodes.

9. Conclusion and future work
In this paper, we investigated different approaches for improving memory access, energy consumption, and memory requirements of trained CNN models. Memory access and energy consumption of trained CNN models, such as AlexNet remain a challenge. This paper proposed an FMM based on an AlexNet traffic distribution on a mesh topology to solve problems with memory access and energy consumption. We provided various traffic patterns based on different memory access mechanisms in order to reduce the energy consumption of the AlexNet traffic distribution on mesh topology. We evaluated the parameters of total hop count, total energy, percentage of active nodes, total flow, and percentage of traffic distribution on the up, center, and down- partitions of a  mesh. The experimental results demonstrated a reduction of total energy and total flow by approximately 17.86% and 34.16%, respectively, by decreasing the total hop count and increasing the percentage of active nodes. Thus, we improved total energy and flow using the proposed FMM, partitioning, and different patterns for an AlexNet traffic distribution on mesh topology. Multi-sides and multicast traffic distribution a decrease in total energy hop count, flow on the mesh, and increased the percentage of active nodes. A 2D mesh was surrounded at the side of up, down, left, and right by a shared bus. Thus, distributed buffers with minimal storage size can effectively improve the performance of the proposed DLA.

The experimental results also demonstrated a higher percentage of traffic distribution at the down-partition compared with the up and center partitions of the mesh. The performance of the proposed DLA will be investigated when positioning GB at the down of the mesh to estimate the percentage of traffic distribution at the up, center, and down-partitions of the mesh. A 2D mesh was surrounded at the up, down, left, and right by a shared bus. Thus, the effective parameters of the AlexNet traffic distribution on mesh topology, such as total energy and flow, will be evaluated based on distributed memory with minimal storage size for DLA architectures where mesh topology is the communications platform.

