Abstract
Some Deep Neural Networks (DNN) have what we call lanes, or they can be reorganized as such. Lanes are paths in the network which are data-independent and typically learn different features or add resilience to the network. Given their data-independence, lanes are amenable for parallel processing. The Multi-lane CapsNet (MLCN) is a proposed reorganization of the Capsule Network which is shown to achieve better accuracy while bringing highly-parallel lanes. However, the efficiency and scalability of MLCN had not been systematically examined. In this work, we study the MLCN network with multiple GPUs finding that it is 2x more efficient than the original CapsNet when using model-parallelism. We introduce the load balancing problem of distributing heterogeneous lanes in homogeneous or heterogeneous accelerators and show that a simple greedy heuristic can be almost 50% faster than a naïve random approach. Further, we show that we can generate MLCN models with heterogeneous lanes with a balanced fit for a given set of devices. We describe a Neural Architectural Search generating MLCN models matching the device's memory that are load balanced. This search discovered models with 18.6% better accuracy for CIFAR-10.


Keywords
Deep learning
Multi-lane
Capsule network

1. Introduction
Several approaches to the distributed model parallelization of Deep Neural Networks (DNN) have concentrated in their depth dimension [9], [17], [2], but DNNs can also be organized in a way to be parallelized along their width dimension [13]. The DNN architecture may be organized into distinct neural network  [6]. This creates separable and resource efficient data-independent paths in the network that can be used to learn different features or add resilience to the network. Examples of neural networks with  are the Google Inception [5], [28] and the Multi-lane Capsule Network (MLCN) [6]. As these  are data-independent they can be (1) processed in parallel and (2) specialized for distinct computational targets (CPUs, GPU, FPGAs, and cloud), as well as resource-constrained mobile and IoT targets, leading to opportunities and challenges. Our recent research focus was on Multi-Lane Capsule Networks (MLCN), which are a separable and resource efficient organization of Capsule Networks (CapsNet) that allows parallel processing while achieving high accuracy at a reduced cost. Table 1 shows results from MLCN in comparison with the baseline CapsNet. With a similar number of parameters, MLCN achieves similar accuracy but with a significant speedup stemming from the  organization. Initial experiments were performed in single GPU environments but, with highly-parallel  it is interesting to explore how MLCN scales with more GPUs. Here we present a first comprehensive study of the scalability and efficiency of MLCN for multi-GPU systems.


Table 1. Comparison between Baseline CapsNet and MLCN.

Network/set	# of lanes	lane's Width	Params.	Train Time (sec./epoch)	Accuracy
CIFAR-10:					
Baseline	-	-	11k	240	66.36%
Mlcn2	4	4	5k	53	69.05%
Mlcn2	32	2	14k	204	75.18%

Fashion-MNIST:				
Baseline	-	-	8k	220	91.30%
Mlcn2	2	4	3.6k	20	91.01%
Mlcn2	8	4	10.6k	92	92.63%
Moreover, the  do not necessarily need to have the same sizes or shapes and may perhaps even learn different features of the given task. This implies that each distinct  may be better suitable for a distinct HW substrate. Further, each  may tolerate different impacts from various optimizations (such as quantization). Thus, given a set of , L, and a set of hardware (HW), H, there is an optimal pair  for  and  and an optimal sequence of  optimizations for each pair  of  and HW.

In this work, we describe and explore this lane-hardware matching problem for homogeneous or heterogeneous accelerator scenarios. We also show that a simple greedy heuristic can be almost 50% faster than a random naïve approach. Then, we show that we can explore generating random MLCN models with heterogeneous lanes that naturally fit the given devices. Generating such models for 64 iterations allowed us to find models with more than 18% better accuracy for CIFAR-10. All this, while maintaining faster performance without extra effort as the generated models are already loaded balanced.

The main contributions of this work are:

•
We present a first comprehensive analysis of the efficiency and scalability of MLCN showing its advantages over the data-parallelism-limited approach of the original CapsNet.

•
We define the load balancing problem of distributing heterogeneous  in heterogeneous hardware.

•
We present a greedy heuristic to solve the lane-hardware match problem showing that it is superior to a naïve approach.

•
We show how to generate random MLCN models that achieve balanced executed in a set of devices and how this can be used to apply Neural Architectural Search (NAS) to find MLCN models with better accuracy for CIFAR-10.

This paper is organized as follows: Section 2 presents the state-of-art in Capsule Networks and DNN parallelization; Section 2.2 describes the Multi-Lane Capsule Network (MLCN) and discusses how it can be parallelized; Section 3 further discusses the heterogeneous distribution problem and presents a heuristic approach to it; Section 4 presents the technique to randomly generate MLCN models that are balanced in a given set of devices and how to use it to make NAS; finally, Sections 5 and 6 show the experimental setup and the experimental results, and Section 7 presents our conclusions.

2. Related work
2.1. Capsule network
Convolutional Neural Network (CNN) is a class of DNN that is commonly used when working with images. CNNs have already achieved state-of-art results in tasks such as image and video recognition, image classification, and medical image analysis. However, these networks have difficulties with location invariance and loss of location information, e.g., one CNN which is able to recognize faces could also mistakenly recognize an image with eyes, mouth, and nose at random positions as a face, not understanding that there is an important spatial relationship between the composing elements. To address this problem, many different new DNN approaches were proposed, including the notion of capsules proposed by Hilton, Krizhevsky, and Wang in 2011 [8].

To encode spatial relationships, Capsule Networks also known as CapsNets, do not work by representing neurons as simple scalars (as in regular CNNs), but as vectors. Later in 2017 an efficient and realistic training algorithm for such networks was proposed [24]. The algorithm, named Dynamic Routing, dynamically chooses activation paths between capsules from one layer to another, calculating the vectors from the next layer based on a mean from dynamically selected vectors from all previous layers.

CapsNet [24] produces a set of Primary Capsules (PCs) by applying two convolutional steps to the original image and splitting it in vectors. Each of these PCs (vectors), identified as 
, is multiplied by a weight matrix 
 and finally, a final set of capsules, the digit capsules, is created using the dynamic routing algorithm. Each of these digit capsule vectors represents one of the classes in the classification problem and the vector's length encodes the probability of the class. The digit capsule can also be used to reconstruct the image like an auto-encoder.

This network with the Dynamic Routing algorithm was shown to have some advantages such as a smaller necessary training set and location invariance and to have some drawbacks such as slower execution and lower accuracy than CNNs. However, since its initial publication, multiple improvements and studies were proposed and the concept has been evolving. Punjabi, Schmid, and Katsaggelos [21] did an evaluation showing the features and differences from CapsNets and regular CNNs, showing that for many features, including invariance, CapsNet has advantages over CNNs, but these advantages only appear when the network is trained using the image reconstruction mechanism (auto-encoder). Shahroudnejad, Mohammadi, and Plataniotis [25] presented an analysis of the explainability of CapsNet, showing that it has properties to help understand and explain its behavior. Jaiswal et al. [10] used the CapsNet in a Generative Adversarial Network (GAN) and showed that it can achieve lower error rates than the simple CNN. Ren and Lu [22] showed that CapsNet can be used for text classification and showed how to adapt the compositional coding mechanism to the CapsNet architecture. Moreover, as the original CapsNet architecture is not deep, some authors proposed solutions to increase its deepness maintaining its properties [27], [29], [4]. All of them show that to increases the deepness of a CapsNet while increasing or maintaining its equivariance property, it is necessary to change and improve the original network and also the routing algorithm. Chen and Liu [4] reduced considerably the number of training parameters when deepening the network.

Regarding architectural improvements, Jia and Huang [11] proposed the use of a hierarchical architecture which uses residual convolutional layers and the position-wise dot product to improve CapsNet over complex data with complex background. Also, Yang et al. [31] proposed the RS-CapsNet, which improves the overall properties of the original CapsNet while reducing to 65% the number of parameters in the model.

Concerning the Dynamic Routing algorithm, Ren [23] and Ding et al. [23] proposed to improve the mechanism. Ren shows that the initialization of the algorithm is important to its final performance and describes how to select better initial values and Ding et al. proposed a new supervised algorithm to be used instead of the dynamic algorithm showing improvements over real-complex data.

Further, CapsNet have also been tested in many different applications. From natural language processing [12], [22] to argumentation of imbalanced images datasets [26]. Jimenez-Sanchez, Albarqouni, and Mateus [14] tested CapsNet for Medical Imaging Data Challenges showing that it can achieve good performance even when having less trainable parameters than the tested counterpart CNNs. Mobiny and Nguyen [18] tested the performance of CapsNet for lung cancer screening and showed that it could outperform CNNs mainly when the training set was small. A similar result was achieved by Kim et al. in traffic speed prediction [15] with CapsNet outperforming traditional CNNs approaches. Mukhometzianov and Carrillo [19] also tested CapsNet with multiple image datasets comparing with CNNs. All of them, argue that CapsNet still requires higher training times compared to other CNNs, showing why it's important to investigate ways to improve its parallelization and scalability.

Finally, many of the advances made in the CapsNet model after its first publication can be found categorized in the survey from Patrick et al. [20].

2.2. Multi-lane CapsNets (MLCN)
In previous work, we introduced a novel organization for the CapsNet named Multi-Lane CapsNet (MLCN) with improved explainability, performance, and parallelization without decreasing accuracy or generalization power [6]. However, beyond just encoding the probability of a class, each vector also contains information to reconstruct the original image, with distinct dimensions of the vector representing different features of the image. With this in mind, we proposed to split the original CapsNet architecture1 (Fig. 1), dividing the PCs into independent sets called . Each of these sets of PCs, a , is responsible for one of the dimensions in the final digit capsules.

Fig. 1
Download : Download high-res image (250KB)
Download : Download full-size image
Fig. 1. MLCN architecture.

The number of PCs per  may vary, as well as the way they are computed. In the original CapsNet, two 2D convolutions are applied to the input image and then reshaped to produce the PCs. More convolutions may be applied, what we call the  of a , or more filters can be used per convolution generating more capsules, what we call the  of a . Further, distinct dimensions of a final digit capsule can be generated by  with different configurations (and thus distinct computational requirements).

There are two key advantages of this organization over the original CapsNet architecture. First, it allows parallelism of the execution, as each set of PCs is constructed independently, improving performance and allowing training and deployment on distributed environments. Second, it improves the explainability of the network by associating different features of the image to each .

Later, Chang and Liu proposed an improvement over MLCN, named MLSCN [3]. This new model improves accuracy while maintaining the parallelism and scalability characteristics of MLCN. As MLSCN work was published concurrently with the development of this work, we did not use MLSCN in your experiments, although given its characteristics we believe the performance results would be similar or possibly the same. Similarly, Canqun et al. [30] proposed the Multi-Scale CapsNet (MS-CapsNet). They introduced a fixed division of the CapsNet network limited to three “” (they did not name or explore the division concept), each with a distinct number of convolutions. Finally, the Path Capsule Networks by Amer and Maul [1] (Path-Capsnet) explore the parallelism of CapsNets by splitting the network such that each path or  is responsible for computing each digitcaps or a primary capsule entirely, unlike the computation of different dimensions/features in MLCN.

2.3. CapsNet parallelization
A DNN can be parallelized in different ways and normally finding the best way for a given network is a complex and hard task. The three most common are data parallelism, model parallelism and pipelining.

The first, data parallelism, splits the data which is going to be computed. It divides the input batch into smaller batches for each compute unit, synchronizing at the end of the batch. Although simple and straightforward, it can only scale by increasing the batch size because dividing too much can result in small computation for each compute unit and too frequent synchronization. And varying the batch size impacts the accuracy, causing a trade-off between accuracy and speedup.

Another possibility is splitting the network operations itself. However, it is not always trivial to find a good place to split the operations. Normally, if two data-dependent operations are split into two computation units, it results in increased communication overhead. Moreover, the implementation details of this kind of network division and communication in currently-available frameworks are not trivial.

Lastly, but not less important, pipelining splits the network into levels which can compute different data at the same time in a pipeline approach. It is normally the approach used in high-performance scenarios.

These are not the only techniques to distribute the training and inference of DNNs and they are not mutually exclusive and can be used together [13]. Related to MLCN, we tested its capability of allowing easy model parallelism and compare it to the common approach that is data parallelism. Of course, for huge MLCN networks pipeline could also be used, but we focus on showing how being able to facilitate the use of model parallelism can bring many advantages over the use of data parallelism alone. This same advantage can be easily extended by adding pipelining per lane, which remains to be explored in future work.

3. Heterogeneous distribution problem
One of the main advantages of having data-independent  is that these  can be deployed separately into multiple accelerators. If we have multiple equal  and multiple equal accelerators, deployment is as basic as dividing the  equally over the HW resources, only being concerned with the communication cost involved. If in other cases we have  with different shapes, characteristics and computational intensity or/and we have multiple accelerators with different characteristics or computational power, deployment becomes more involved. First, because it now involves load balancing the computational intensity of the  and the computational power of the  and, second, because now there is also the chance to apply different optimizations for different pairs of HW and . This scenario is illustrated in Fig. 2, which shows how multiple  can be deployed on different accelerators with different compilation stacks.

Fig. 2
Download : Download high-res image (296KB)
Download : Download full-size image
Fig. 2. Multiple Neural Network lanes can be trained in parallel using multiple HW even in heterogeneous scenarios.

Deciding where to execute each  or what optimizations to apply to each lane/hardware pair is not trivial. In this work, we present an approach to address the first problem using a deployment heuristic. We will address the second problem in future work.

3.1. Execution cost heuristic for MLCN lanes
Finding, statically, the optimal solution to deploy a  given a set of HW resources is a complex task. For example, aspects such as the version of the compiler being used or what other  (and their characteristics) are being executed concurrently on the same HW can have a significant impact on the final performance. These are only two of the many aspects that can affect the performance. However, we observed in our experiments that, at least for MLCN, we do not need to know exactly the accurate final performance to make a good schedule decision. Our experiments have shown that reasonably approximate predictors can provide acceptable results.

We experiment running and taking the average execution time of 10 executions of MLCN  with different width, size, and types, using three different NVIDIA GPUs (K80, P100, and V100). For the same number of parameters, independently of the GPU used, the performance displays a well-behaved pattern. It varies linearly when increasing the depth, quadratically when varying the width, and it was multiplied by a factor when changing the GPU.

Thus, for MLCN  with NVIDIA GPUs and compilers, predicting the performance on a given HW substrate can be approximated by Equation (1), which achieves a 0.901 Pearson correlation with our experimental data.(1)

The 
 in Equation (1) is the speed factor of the GPU being used. It only has any significance when deploying to a heterogeneous set of GPUs and the 
 constant for each GPU can be inferred by simply measuring the execution time of a tiny  in each GPU and normalizing it. This can be done before the execution and it has an insignificant cost in the final execution time. In the case of our experiments, we collect the 
 by executing a 512x512 fully connected network with a small set of data. Normalized by K80, we found the following 
 values for M40, P100 and V100: 3.1, 4.2 and 6.

3.2. Load balancing algorithm
We showed that we can make good execution cost predictions for NVIDIA GPUs and MLCN . However, there is still the problem of how to schedule a set of  with different sizes and widths on a set of GPUs with different speeds. We can model this problem as a numerical set partition with N bins, each bin corresponding to a target GPU. The cost of each  being deployed (inserted into the bin) is equal to the  cost (Equation (1)) multiplied by previous execution speed prediction on host HW via execution of a tiny  (
).

The numerical set partition problem is NP-Hard, but very good results can be achieved using heuristic/approximative algorithms and it can even be solved in pseudo-polynomial time using dynamic programming making it one of “The Easiest Hard Problem” [7], [16]. One of such heuristics that achieved good results and is very simple to implement is the greedy partition which always inserts the remaining  with the largest cost in the emptiest bin. Algorithm 1 shows this greedy algorithm including the pre-execution used to calculate the 
.

Algorithm 1
Download : Download high-res image (71KB)
Download : Download full-size image
Algorithm 1. Greedy scheduling algorithm.

4. MLCN architectural search problem
Previously, we presented the problem of deciding on which device to execute each lane in an MLCN model. In section 3, we showed that we can create a simple equation that describes the performance behavior of MLCN models to guide a load balance scheduling algorithm. It is known that when one can balance the lanes onto the devices, we may achieve better overall performance. Moreover, larger models with heterogeneous lanes were shown to have better accuracy, especially in more complex datasets such as CIFAR-10. Thus, in this section, we explore a slightly more general problem: we still want to balance the load of an MLCN model in a set of devices, but now we do not have this model defined but instead it is built. We built models with heterogeneous lanes that fulled used the device resources, this is used to perform an Neural Architectural Search (NAS) to find better models in accuracy. So, in the end, we can select the best model that both improves the accuracy and uses the devices' resources smartly.

To achieve that, we first introduce a new equation to compute the memory cost of an MLCN given its lane configurations. With this new equation, along with the execution cost equation from the last section, we can predict both memory and flops/time necessary to execute a lane or a MLCN model in a given device. So now the notion of a device is a lane's bucket that has both a memory and flops/time limit. Next we introduce the idea of how to generate random lanes that can respect the memory or computational load (in FLOPS) limit. This mechanism is used to randomly generate lanes to build a MLCN model that fully uses the available set of devices resources. At the end, we have heterogeneous lanes (that we showed are able to achieve better accuracy) and, as the lanes are filled having in mind the device resources, those lanes can be executed on those devices in a balanced way. Finally, we can generate numerous MLCN models and then select the model that gives the best accuracy for a fixed number of epochs.

All these concepts and ideas are illustrated in the diagram from Fig. 3. Note that the buckets are illustrated in equal size, but nothing prevents them to be different in term of flops and memory limits, for instance, in a heterogeneous hardware scenario. Also, notice that we solve the problem from the previous section in such a way that the randomly generated MLCN is kept balanced as we fill the buckets.

Fig. 3
Download : Download high-res image (225KB)
Download : Download full-size image
Fig. 3. NAS Engine mechanism to generated random MLCN models with heterogeneous lanes that naturally fits a given set of devices.

In Section 4.1, we present the memory heuristic and in Section 4.2 we discuss the algorithm to perform this architectural search. Finally, in the results section, we explore how limiting the memory or the flops for an MLCN configuration search or having heterogeneous limits impacts the final accuracy.

4.1. Heuristic memory cost for MLCN lanes
To implement the bucket approach previously described, we need not only to estimate the computational time/resources/FLOPS as described in Section 3.1, but also to estimate the amount of memory necessary to execute the lane. For that, we use the NVIDIA tool nvidia-smi to capture different lane configurations and create a model of memory consumption based on the lane characteristics. It became clear from our experiment that the memory consumption is deterministic and can be approximate by the following equation:(2)

For the lanes that we tested, this equation gave a memory consumption that was always an upper ceiling for the real value and with an average error of less than 256 MiB. We know that this value could have been analytically calculated from the network model layers and configurations, but we want to show that with some experiments we can come up with an equation that is good enough and does not require knowledge of the layers actual functionality. Moreover, this equation provides a fast way to calculate the memory consumption, allowing us quickly test each 
 and 
 that can be used to fit a given memory requirement.

Memory consumption and the execution time have clearly different behaviors because the number of trainable parameters grows differently from the layers output and activation's size. While the first is more related to the execution time, the second is more related to memory consumption.

4.2. Lane configuration search
Given the performance and memory consumption results of the MLCN model, its relation with the lane characteristics, and the fact that in our experiments we had evidence that MLCN models with heterogeneous lanes have higher accuracy than homogeneous ones, we proposed an approach to make a Neural Architectural Search (NAS) of such MLCN models that fit a given set of memory and performance constraints.

Our NAS tries to find a MLCN model with the best possible accuracy or at least a good one that, at the same time, fulled uses all resources available in a balance way. Ensuring that we are testing and generating the largest possible models (as larger models tends to have better accuracy performance) and that can be executed using fully the devices capacities (load balanced execution with higher performance). In other words, we want to find a model M that maximizes the accuracy in a given dataset. Moreover, the lanes 
 that compose the model M are assigned to an accelerator 
 that has a memory limit 
 and maximum load 
. Let 
 be the set of all lanes assigned to be executed in 
, the model M needs to maximize the accuracy while its lanes still fit the devices, i.e.:
 
 

The memory restriction ensures that we are only generating models that can fit the devices and the load limit guarantees that we are using the same load for all devices in a balanced way.

To perform such search respecting the given constraints, we developed a random search approach to implement the NAS engine. First, we develop a generator of random lanes that can fit in a given memory and have a limit flops usage. This is done using the memory and performance equations. Using the lane generator, we then build a random model that fits a set of devices in a balanced way. For this, we select a device that is not yet full and randomly generate a lane that can fit such device, then assign this new lane to the device and the MLCN model being built. This is repeated until all devices became full. Finally, we test the accuracy of this generated model and update the best model found so far, if necessary. This repeated process of generation and test of random generated MLCN networks is used to explore the space of possible MLCNs that fits the given devices to find the best one.

5. Experimental setup
In our experiments, we used machines from Google Cloud. All virtual machines instantiated had 24 vCPUs with 50 GB of RAM and a default network interface. We used different GPU setups, including NVIDIA Tesla M40, K80, P100 and V100 all with CUDA 10.0, Intel MKL-DNN and Tensorflow 1.13.1.

The experimental results did not show sensitivity to the input data set (tested with Fashion-MNIST, CIFAR-10, and others) thus we chose to use the Fashion-MNISTdata set. Execution time was measured by executing 10 Fashion-MNISTepochs, excluding the first, and using the average time for the others. All results had a very small variation. The execution time between epochs consistently had a very similar value. Thus, for simplicity, we present averages.

In this work, we tested three configurations for the CapsNet parallelization, as follows:

•
Original with Data Parallelism (baseline or base): we simply applied the original concept of CapsNet for the Fashion-MNIST dataset parallelized using Keras data parallelism support.

•
MLCN with Data Parallelism (mlcn-data): we used the same approach as in the baseline (Keras data parallelism), but with the MLCN organization.

•
MLCN with Model Parallelism (mlcn-model): we parallelize the execution by executing each  on different GPUs. When using multiple machines, we used the Horovod MPI framework to do handle communication.

For the NAS experiments, we use K80 NVIDIA GPUs and executed the training only until the 10th epoch with batch size 100. The accuracy reported is the geometric average of each generated model trained 4 times. We generated 64 models per tested configuration. The best value is the best average of all iterations and the average is the average of all iterations.

6. Experimental results
6.1. MLCN scalability and performance study
To understand how each approach to the parallelization of CapsNet scales, we studied their performance with 1, 2, 4 and 8 NVIDIA Tesla K80 GPUs.

The graph in Fig. 4 shows the performance comparison between the base (baseline), mlcn-data and mlcn-model configurations. MLCN is faster than the baseline even in a single GPU, as reported earlier. However, it is interesting to notice that the advantage does not increase when scaling to more GPUs with data parallelization, as the speedup difference between mlcn-data and baseline remained constant. This indicates that the reorganization proposed by MLCN does not improve scaling via data parallelism, as expected, but provides benefits for model-parallelism. In this latter case the Mlcn-model has a visible advantage, scaling with higher efficiency and achieving a near 7.18 speedup with 8 GPUs over the single GPU baseline. Thus, MLCN not only is faster than the original CapsNet (baseline) but, because it allows model-parallelism, it scales more efficiently.

Fig. 4
Download : Download high-res image (107KB)
Download : Download full-size image
Fig. 4. Speedup of the three parallelization approaches: baseline with data parallelism (base), MLCN with data parallelism (mlcn-data) and MLCN with model parallelism (mlcn-model). All speedups are relative to the baseline with one GPU. (For interpretation of the colors in the figure(s), the reader is referred to the web version of this article.)

6.1.1. Impact of batch size
The size of the minibatch, or batch size, has a significant impact on the performance of a DNN as a better ratio of computation and communication is available, enabling a more efficient HW use. Also, batch size has a significant impact on data parallelism performance as more data per computation is available to be divided among the GPUs. To study the advantage of MLCN model parallelism over the data parallelism method, we tested both approaches with 100, 150, 300 and 600 batch sizes. The graphs in Figs. 5a and 5b show the speedup versus a single GPU with a 100-sized batch size. For both methods, we observe similar efficiency gain as batch size grows. So, for different batch sizes the relative advantage of MLCN with model parallelism stays the same, as increasing the batch size equally increases the efficiency of data and model parallelism approaches.

Fig. 5
Download : Download high-res image (310KB)
Download : Download full-size image
Fig. 5. MLCN and baseline scalability for 1, 2, 4 and 8 NVIDIA K80 GPUs using Google Cloud VM with 24 vCPUs and 90 GB of RAM.

We also studied the impact of batch size on both baseline and MLCN accuracy, shown in Fig. 6. Increasing batch sizes has the same impact on the accuracy for both cases. The magnitude of this impact is sensitive to the dataset as shown by the differences between Fashion-MNIST and CIFAR-10 results and not to the parallelism model. Thus, as model parallelism has better performance over data parallelism, it has the advantage of scaling better with smaller batch sizes, reducing the pressure to trade accuracy for efficiency.

Fig. 6
Download : Download high-res image (119KB)
Download : Download full-size image
Fig. 6. Validation accuracy impact when increasing the training batch size for the baseline and MLCN in the CIFAR-10 and Fashion-MNIST datasets.

6.1.2. Impact of lane characteristics
The previous results explored the suitability of MLCN and its model parallelization. We also explore how the characteristics of the MLCN  can affect performance and scalability by varying the three main hyperparameters in MLCN : their width, depth and their quantity. The results are shown, respectively, in Figs. 7a, 7b and 7c.

Fig. 7
Download : Download high-res image (578KB)
Download : Download full-size image
Fig. 7. Scalability variance with different lanes configurations.

The width and depth of  has a direct impact on the number of parameters per  and, consequently, the amount of computation per . With more computation per , the efficient use of multiple GPUs becomes advantageous. This is shown in Figs. 7a and 7b as larger lanes increase efficiency. However, increasing the width had a much more significant increase in efficiency, at similar increase in number of parameters. This indicates that, besides the number of parameters, the type of computation affects performance. In the case of MLCN  wider  result in better performance than deeper  with the same number of parameters.

Another interesting point was the fact that increasing the number of lanes did not significantly increase performance, as shown in Fig. 7c. Even though increasing the number of lanes also increases the amount of computation available between batches, there is an overhead of having these computations separable. So, having several  in one GPU is less efficient than having a single extremely large .

6.2. Heterogeneous distribution problem
6.2.1. Heterogeneous lanes and GPUs
One interesting observation about MLCN is that having  with different characteristics, such as  with different sizes and depths, increases the generality of the network. A similar result was reported by Canqun et al. [30] with the MS-CapsNet organization. However, as discussed in Section 3, deploying  in multiple GPUs when the  have different computational footprint can be challenging. To study a proposed heuristic to deploy  with different widths and depths, we tested 4 MLCN networks with 6, 9, 12 and 24 . Each  may have pairs of depth and width values ranging from 1 to 5. As shown in Fig. 8, we obtain a smaller execution time with our proposed heuristic than when naïvely randomly distributing the  between the GPUs. The advantage increases with the number of , showing that, the larger the number of  the harder it is to randomly find a good distribution. Notice that the time accounted for the greedy heuristic includes the (almost negligible) time to run the heuristic.

Fig. 8
Download : Download high-res image (76KB)
Download : Download full-size image
Fig. 8. Average execution time (executed 10 times) of heterogeneous lanes running on four K80 NVIDIA GPUs with a random and a greedy partition of lanes execution distribution. All lanes varying on width and depth.

6.2.2. Heterogeneous lanes with heterogeneous GPU
Besides having heterogeneous  we also tested a scenario with heterogeneous accelerators. Rather than 4 NVIDIA Tesla K80, we deployed four systems each with a different GPU: one M40, one K80, one P100, and one V100. The results are in Fig. 9. For total execution time, there was a significant increase because of network communication between the systems. Moreover, the difference between random deployment and our greedy heuristic becomes larger, showing that for more complex scenarios with many  or heterogeneous HW, carefully computational deployment is key to performance.

Fig. 9
Download : Download high-res image (79KB)
Download : Download full-size image
Fig. 9. Average execution time (executed 10 times) of heterogeneous lanes running on one K80, one P100, one V100, and one M40 NVIDIA GPU in multiple machines communicating using MPI with a random and a greedy partition of lanes execution distribution. All lanes varying on width and depth.

6.3. MLCN architecture search
Next, we tested how the strategy presented in Section 4 can use information about the scalability and performance of the MLCN models to build new ones that fit the accelerators. With the ability to generate random models, we can search for good models that achieve good accuracy for CIFAR-10 while fitting systems with multiples NVIDIA K80. The algorithm behaved well by creating a model that never exceeds the GPU memory limits and generates models that have a very well-fitted load balance. Given this search with the ability to generate models with the presented characteristics, we explored how increasing the memory limit, computational power limit, and the number of accelerators would affect the average and best model found by executing the random search for 64 iterations, 10 epochs per model and minibatch 100 (in our experiments these parameter values showed to obtain the best accuracy performance for most of the models. The hyperparameter search could be done for each model generated but it would require much more execution time to the search). From the plot from Fig. 10, we can observe how accuracy increases when we allow the use of more computational power. We started by using the amount of computation from a baseline model with 4 lanes, width 2 and depth 4. When we generate other models with a similar FLOPS we end up with the average and the best result presented as 1x. Following, we allowed the creation of models 2-fold more computationally intensive, then 4- and 8-fold. We can see that the model's average and best accuracy found by the random search increases for larger networks, but with a tendency to slow after some point. Similarly to what we mentioned earlier: for a given dataset and number of epochs, there seems to be an ideal network size that achieves the best performance.

Fig. 10
Download : Download high-res image (90KB)
Download : Download full-size image
Fig. 10. Average and best model's accuracy for 30 random generated models for each max FLOPs limit: 1x, 2x, 4x, and 8x. 1x starts with a previously given baseline size. All experiments were executed allowing the use of up to 12 Gb of memory and 8 buckets (GPUs).

We then repeated the experiment but fixed the maximum FLOPs that could be used by the models and increased the memory allowance. As both FLOPs and memory consumption are tightly connected, we see a similar behavior: an increase, until a point, of the accuracy, as we increase the allowed networks. This result is presented in Fig. 11.

Fig. 11
Download : Download high-res image (99KB)
Download : Download full-size image
Fig. 11. Average and best model's accuracy for 30 random generated models for each memory limit: 2 Gb, 4 Gb, 8 Gb, and 12 Gb. All experiments were executed allowing the use of up to 8x FLOPS baseline and 8 buckets (GPUs).

The same behavior appears, Fig. 12 when we fix the amount of memory and maximum FLOPs but increase the number of available buckets (the accelerator devices capabilities representation).

Fig. 12
Download : Download high-res image (93KB)
Download : Download full-size image
Fig. 12. Average and best model's accuracy for 30 random generated models using 4, 8, 16 or 32 K-80 GPUs. All experiments were executed allowing the use of up to 8x FLOPS baseline and 12 Gb of memory.

One interesting fact is that, by using our random search we were able to generate models that used the accelerator's resources efficiently such that the best found model had an 18.6% better accuracy than we had previously achieved. So, the results provide evidence that this NAS approach is useful to search for better MLCN models while creating load-balanced well-fitted models. In Fig. 13, we show an example of the best-so-far accuracy result per iteration in the execution of the random search. In this execution, the average model accuracy was 0.62%. It took only 8 iterations to find a significantly better than the average model.

Fig. 13
Download : Download high-res image (91KB)
Download : Download full-size image
Fig. 13. Best Model Accuracy found by Random search thought out its iterations for 4x FLOPS experiment.

7. Conclusion
The Multi-lane CapsNet (MLCN) is a novel organization for the CapsNet network which is shown to achieve better accuracy with more efficient HW utilization. Further, MLCN allows model parallelization by running the  in parallel. In this work, we analyze and measure the advantages of this new parallelization scheme of the CapsNet when compared to the usual data parallelism.

We find that MLCN is faster than the original CapsNet and it scales better with model parallelism being almost 2x more efficient, even with small batch sizes. We also explored the impact of different  configurations on performance and scalability, showing that wider  usually achieve higher HW efficiency.

We found that when parallelizing MLCN with  with different characteristics (or when deploying in machines with different accelerators), load balance is a key factor in reaching good performance. We proposed a greedy algorithm to deploy  in these scenarios and we found that it can be up to 50% more efficient than the naïve random deployment.

Finally, we proposed a Neural Architectural Search (NAS) based on random generation of MLCN models. We show that heterogeneous MLCN models can be generated in such a way that they naturally fit the devices and can be executed load balanced. When we apply this NAS for 64 iterations, we found models that are up to 18% better accuracy in CIFAR-10 than previous MLCN models.