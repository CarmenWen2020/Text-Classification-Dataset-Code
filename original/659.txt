Abstract
Herlihy showed that multiprocessors must support advanced atomic objects, such as compare-and-swap, to be able to solve any arbitrary synchronization task among any number of processes (Herlihy, 1991). Elementary objects such as read-write registers and fetch-and-add are fundamentally limited to at most two processes with respect to solving an arbitrary synchronization task. Later, it was also shown that simulating an advanced atomic object using elementary objects is impossible. However, Ellen et al. observed that the above impossibility assumes computation by synchronization objects instead of synchronization instructions applied on memory locations, which is how the actual multiprocessors compute (Ellen et al., 2016). Building on that observation, we show that two elementary instructions, such as max-write and half-max, can be much better than the advanced compare-and-swap instruction. Concretely, we show the following.
•
[1.] Half-max and max-write instructions are elementary, i.e., have consensus number one.

•
[2.] Half-max and max-write instructions can simulate compare-and-swap instruction in  steps.

•
[3.] For a pipelined butterfly interconnect, concurrent throughput of half-max and max-write instructions exceeds the concurrent throughput of compare-and-swap by a factor  — the number of processes.

•
[4.] The family of instructions max-write-or- are also elementary, where  is a commutative and an associative operation.

•
[5.] It takes  steps to simulate max-write-or-add using compare-and-swap but  steps to simulate compare-and-swap using max-write-or-add and half-max.


Keywords
Wait-free synchronization
Compare-and-swap
Consensus numbers
Half-max
Max-write


1. Introduction
Any multiprocessor chip needs to support some synchronization instructions, such as compare-and-swap or fetch-and-add, to coordinate among several concurrent processes that can take steps asynchronously at different rates. As it is not possible to support every other synchronization instruction on a multiprocessor, the choice of instructions to support is important. Herlihy [10] gave an elegant way to make such a choice based on consensus numbers. The consensus number of an object is defined as the maximum number of processes  among which binary consensus can be solved using any number of instances of the object and read-write registers. In binary consensus, each process is given an input of either  and . Each process must output the same value (agreement) within a finite number of its steps (termination) so that the output value is an input value of some process (validity).

Using objects of consensus number , Herlihy gave a generic and universal construction to construct a linearizable and wait-free implementation of any concurrent data structure or object, such as stacks or queues, shared among  processes. Linearizability implies that although each operation takes several steps to complete, it appears to take effect instantaneously at some point between its invocation and termination. The wait-free property implies that every process completes its operation within a finite number of its steps irrespective of the speed of other processes.

So, a compare-and-swap object, which updates the current value of the object to a given new value if and only if the current value is equal to a given expected value, has an infinite consensus number and is an advanced object to be supported by a multiprocessor. On the other hand, consensus number two objects such as fetch-and-add or fetch-and-multiply are elementary ones. Moreover, as it was later shown in [20] that it is provably impossible to implement an advanced object from any number of reasonable elementary ones, a multiprocessor should support an advanced object.

However, Ellen et al. [5] recently observed that the above classification treats a synchronization instruction as an individual object but in reality the instructions supported by a multiprocessor can be applied on every memory location, without any restriction that forbids the application of more than one instruction to a memory location. Based on the observation, they also give simple examples where a register supporting two elementary instructions can be used to solve binary consensus for any given number of processes in constant number of steps per process. Using Herlihy’s universal construction, it then follows that it is possible to construct any concurrent data structure or object by only using elementary synchronization instructions.

The above possibility relies on the universal construction, which is inefficient both in number of steps taken by a process and the number of shared registers used, and is therefore not used in practical implementations. One may conjecture that the elementary instructions are only good enough for solving binary consensus but are fundamentally limited to produce efficient implementations as can be obtained by widely used advanced instructions like compare-and-swap. In fact, in a followup work by Gelashvili et al. [8], the authors ask a similar question: “The practical question is whether we can really replace a compare-and-swap instruction in concurrent algorithms and data-structures with a combination of weaker instructions.”.

By weaker or elementary instructions, we refer to instructions of consensus number at most two, or ideally one. We define the consensus number of an instruction as the consensus number of an object that supports the instruction and a read operation that returns the state of the object. This models that an instruction can be applied on a memory location along with a read operation that returns the value of the location. It is crucial to add such a read operation to the object when defining consensus number of an instruction as otherwise, one can define instructions that seem to be advanced but still have consensus number one. For instance, consider the compare-and-swap instruction without a return value and an object that supports such an instruction without any read operation. The consensus number of such an object is one because there is no way to read what the object computes. Moreover, we define the read operation to return the complete state of the object as reading a memory location returns its value or all the bits stored at that location. Thus, elementary instructions with consensus number one can be seen as a “write-type” instruction because objects or memory locations that only support read and write operations also have consensus number one. The challenge is to find the limits of elementary instructions with respect to efficiently simulating compare-and-swap, which is primarily used for practical implementations of concurrent objects or data structures and has been proven to yield efficient implementations.

In this paper, we show that it is possible to simulate a compare-and-swap instruction using two elementary instructions and the simulation is efficient in the number of steps taken by each process and the number of shared registers or memory locations used per process. Concretely, we introduce two instructions half-max and max-write of consensus number one each. We show that using read-write registers and registers that support half-max and max-write, we can construct a linearizable and wait-free implementation of a compare-and-swap register so that every compare-and-swap operation takes  steps. The size of the registers required is logarithmic in the maximum length of an execution. If  is the number of processes in the system, then the implementation uses  shared registers to simulate a single compare-and-swap register and  shared registers to simulate  compare-and-swap registers. Thus, any algorithm that uses the compare-and-swap instruction, and takes  steps per process, can be transformed into an algorithm that uses few elementary instructions and also takes  steps per process.

A good thing about these elementary instructions is that they can be much better than the advanced compare-and-swap instruction. If the processes issue these instructions concurrently at a memory interconnect, then we define the throughput as the average number of instructions that are processed in a unit time. We compare the throughput of the elementary instructions half-max and max-write against the throughput of compare-and-swap and show that the throughput of the elementary instructions can be larger by a factor proportional to the number of processes. Moreover, we also consider the problem of simulating the max-write-or-add instruction – an elementary instruction similar to the max-write instruction – using the compare-and-swap instruction. We show that it takes  steps for such a simulation, where  is the number of processes.

2. Related work
One of the most central question in concurrent computing has been to quantify the computing ability of synchronization instructions. Herlihy [10] originally defined the consensus number of an object as the maximum number of processes  that can solve consensus using a single instance of the object and any number of read-write registers. As a consequence of this definition, an object that has higher consensus number or is higher in the Herlihy’s hierarchy cannot be implemented using an object that has a lower consensus number or is lower in the Herlihy’s hierarchy. Jayanti [12] defined robustness of a hierarchy as the property that an object at a higher level in the hierarchy cannot be implemented using any number or combination of objects lower in the hierarchy. He gave an example of an object such that  instances of the object along with read-write registers can solve consensus for  processes. Thus, Herlihy’s hierarchy would not be robust if the consensus number definition is restricted to use only a single object.

A natural fix is to allow any number of instances of the object in the definition of consensus number, which is also the accepted definition and the one that we use [11]. Under this definition, Chandra et al. [4] show that Herlihy’s hierarchy is robust for two objects out of which one is a consensus object and the other one is an arbitrary object. Ruppert [20] showed that Herlihy’s hierarchy is robust for read–modify–write and readable objects, where a readable object returns some part or complete state of the object and a read–modify–write object returns the complete current state of the object and updates it according to a deterministic function in a single atomic step. These objects capture a large class of synchronization instructions but not all. All these results assume computation using objects, where an object supports a synchronization instruction.

Ellen et al. [5] observed that if one relaxes the above assumption and does not treat a set of synchronization instructions as a set of individual objects but as a single object supporting the set of synchronization instructions, then Herlihy’s hierarchy is again not robust. They propose a space based hierarchy in which the computational capability of a set of synchronization instructions is quantified by the minimum amount of space required to solve obstruction free consensus among  processes. Obstruction freedom guarantees that a process returns the output eventually if it is allowed to take steps alone without being obstructed by other processes. A set of synchronization instructions is considered powerful if they require small space to solve obstruction free consensus. Their work has led to some more followup work to understand the capability of a set of synchronization instructions from different perspectives when the instructions are assumed to be supported on the same register.

In [8], the authors give a lock-free implementation of a log data structure by only using x86 instructions of consensus number at most two. They report that the performance achieved was similar to that of a compare-and-swap based implementation. Their log data structure can be used to implement any object, including compare-and-swap, but the progress guarantee is lock-free and does not exclude starvation of a process unlike our wait-free algorithm. The log data structure can be seen as a universal construction and the question remains if specific implementations using compare-and-swap can also be done using elementary instructions without any sacrifice. Also, we do not restrict ourselves to instructions supported on modern architecture as our goal is to find if it is even theoretically possible to efficiently compete with an advanced instruction like compare-and-swap using elementary instructions only, regardless of the task that we choose to do and not just a universal construction. In [17], we observed that a set of low consensus number instructions supported on the same register can help to improve the step complexity of solving the fundamental synchronization task of designing a wait-free queue from  to  for  processes.

In this paper, we look at the capability of a set of elementary instructions supported on the same register with respect to their ability to efficiently simulate an advanced instruction like compare-and-swap. We chose to simulate compare-and-swap not only because of its infinite consensus number but also because it is ubiquitous and has been shown to yield efficient implementations [14], [15], [19]. Our result then implies that a set of elementary instructions can produce equally efficient implementations like compare-and-swap, if not better. In [9], the authors give a blocking implementation of comparison instructions, which includes compare-and-swap, by just using read-write registers and constant number of remote memory references. Their focus is to use read-write registers and hence wait-freedom is impossible to achieve. Overall, there is no prior work that shows that a set of elementary instructions are at least as good as compare-and-swap registers with respect to the number of steps taken for completing any arbitrary synchronization task.

3. An overview of the method
Our method is based on the observation that if several compare-and-swap operations attempt to simultaneously change the value in the register, only one of them succeeds. So, instead of updating the final value of the register for each operation, we first determine the single operation that succeeds and update the final value accordingly. This is achieved by using two consensus number one instructions: max-write and half-max.

The max-write instruction takes two arguments. If the first argument is greater than or equal to the value in the first half of the register, then the first half of the register is replaced with the first argument and the second half is replaced with the second argument. Otherwise, the register is left unchanged. In any case, no value is returned. This instruction helps in keeping a version number along with a value.

The half-max instruction takes a single argument and replaces the first half of the register with that argument if the argument is larger. Otherwise, the register remains unchanged. Again, no value is returned in any case. This instruction is used along with the max-write instruction to determine the single successful compare-and-swap operation out of several concurrent ones. The task of determining the successful compare-and-swap operation can be viewed as a variation of tree-based combining (as in [6], [16] for example). The difference is that we do not use a tree as it would incur  overhead on step complexity. Instead, our method does the combining in constant number of steps as we will see later.

In the following section, we formalize the model and the problem. In Section 5, we give an implementation of the compare-and-swap operation using registers that support the half-max, max-write, read and write operations. In Section 6, we prove its correctness and show that the compare-and-swap operation takes  steps for every process. In Section 7, we argue that the consensus numbers of the max-write and half-max instructions are both one. In Section 8, we give a couple of extensions of the basic algorithm. In Section 9, we analyze the throughput of the half-max, max-write and compare-and-swap instructions under high concurrency. In Section 10, we give a generic family of elementary instructions. The max-write instruction can be viewed as a member of this generic family. In Section 11, we pick an elementary instruction max-write-or-add from the above generic family and show that simulating it requires  steps using compare-and-swap and read-write registers. Finally, we conclude and discuss the results in Section 12.

4. Model
A sequential object is defined by a set of operations that can be performed on the object. Each operation takes zero or more arguments, updates the state or value of the object according to a specified set of rules and optionally returns a value. The value of the object is a sequence of bits, or just an integer.

A register is a sequential object and supports the operations read, write, half-max and max-write. The read() operation returns the current value of the register. The write() operation updates the value of the register to . The half-max() operation replaces the value in the first half of the register, say , with  and does not return any value. The max-write() operation replaces the first half of the register, say , with  and second half of the register with  if and only if . In any case, the operation does not return any value. The register operations are atomic, i.e., if different processes execute them simultaneously, then they execute sequentially in some order. In general, atomicity is implied whenever we use the word operation in the rest of the text.

An implementation of a sequential object is a collection of functions, one for each operation defined by the object. A function specifies a sequence of instructions to be executed when the function is executed. An instruction is an operation on a register or a computation on local variables, i.e., variables exclusive to a process.

A process executes a sequence of instructions. The processes have identifiers . When a process executes a function, it is said to call that function. A schedule is a sequence of process identifiers. Given a schedule , an execution  is the sequence of instructions obtained by replacing each process identifier in the schedule with the next instruction to be executed by the corresponding process.

Given an execution and a function called by a process, the start of the function call is the point in the execution when the first register operation of the function call appears. Similarly, the end of the function call is the point in the execution when the last register operation of the function call appears. A function call  is said to occur before another function call , if the call  ends before the call  starts. Thus, the function calls of an implementation of an object  form a partial order 
 with respect to an execution . An implementation of an object  is linearizable if there is a total order 
 that extends the partial order 
 for any given execution  so that the actual return value of every function call in the order 
 is the same as the return value determined by applying the specification of the object to the order 
. The total order 
 is usually defined by associating a linearization point with each function call, which is a specific point in the execution when the call takes effect. An implementation is wait-free if every function call returns within a finite number of steps of the calling process irrespective of the schedule of the other processes.

Our goal is to develop a wait-free and linearizable implementation of the compare-and-swap register. It supports the read operation and the compare-and-swap operation. The read operation returns the current value of the register. The compare-and-swap operation takes two arguments  and . It updates the value of the register to  if the value in the register is  and leaves it unchanged otherwise. If the value is updated, a true value is returned and false otherwise.

5. Algorithm
Fig. 1 shows the (shared) registers that are used by the algorithm. There are the arrays  and  of size  each. The th entry of the array  consists of two fields: the field  keeps a count of the number of compare-and-swap operations executed by the process , the field  is used to store or announce the second argument of the compare-and-swap operation that the process  is executing. The th entry of the array  consists of the fields  and . The field  is used for storing the return value of the th compare-and-swap operation executed by the process . The register  stores the current value of the compare-and-swap object in the field  along with its version number in the field . The fields ,  and  of the register  respectively store the next version number, the process identifier of the process that executed the latest successful compare-and-swap operation and the count of compare-and-swap operations issued by that process. For all the registers, the individual fields are of equal sizes except for the register . The first half of this register stores the field  where as the second half stores the other two fields,  and .

Algorithm 1 gives an implementation of the compare-and-swap register. To execute the read function, a process simply reads and returns the current value of the object as stored in the register  (Lines 2 and 3). To execute the compare-and-swap function, a process starts by reading the current value of the object (Line 5). If the first argument of the function is not equal to the current value, then it returns false (Lines 6 and 7). If both the arguments are same as the current value, then it can simply return true as the new value is the same as the initial one (Lines 8 and 9).


Download : Download high-res image (237KB)
Download : Download full-size image

Download : Download high-res image (70KB)
Download : Download full-size image
Fig. 1. An overview of data structures used by Algorithm 1.

Otherwise, the process competes with the other processes executing the compare-and-swap function concurrently. First, the process increments its local counter (Line 10). Then, the new value to be written by the process is announced in the respective entry of the array  (Line 11) and the return value of the function is initialized to false by writing to the respective entry in the array  (Line 12). The process starts competing with the other concurrent processes by trying to announce its identifier in  using the max-write operation (Line 13). The competition is finished by writing a version number larger than used by the competing processes (Line 14).

Once the winner of the competing processes is determined, the winner and the value announced by it is read (Lines 15 and 16), the winner is informed that it won (Line 18) after appropriate checks (Line 17) and the current value is updated (Line 19). The value to be returned is then read from the designated entry of array  (Line 20). A closer look at the algorithm reveals that the half-max and max-write operations are only combined on the register . All other registers either only use max-write (and not half-max) or are only read-write registers.

In the following section, we analyze Algorithm 1 and show that it is a linearizable and an  step wait-free implementation of the compare-and-swap object.

6. Analysis
Let us first define some notation. We refer to a field  of a register  by . The term 
 is the value of the field  just after the process  executes Line  during a call. We omit the call identifier from the notation as it will be always clear from the context. Similarly, 
 is the value of a variable , that is local to the process , just after it executes Line  during a call. The term 
 is the value of a field  at the end of an execution.

To prove that our implementation is linearizable, we first need to define the linearization points. The linearization point of the compare-and-swap function executed by a process  is given by Definition 1. There are four main cases. If the process returns from Line 7 or Line 9, then the linearization point is the read operation in Line 5 as such an operation does not change the value of the object (Cases 1 and 2). Otherwise, we look for the execution of Line 19 that wrote the sequence number 
 to the field  for the first time. This is the linearization point of the process  if its compare-and-swap operation was successful, as determined by the value of  (Case 3a). Otherwise, the failed compare-and-swap operations are linearized just after the successful one (Case 3b). The calls that have not taken effect are linearized after all the other linearization points (Case 4b). Fig. 2 gives an illustration of the different cases.

Definition 1

The linearization point of a compare-and-swap call by a process  is defined as follows.

1.
If 
, then the linearization point is the point when  executes Line 5.

2.
If 
, then the linearization point is the point when  executes Line 5.

3.
If 
 and 
, then let  be the point when Line 19 is executed by a process  so that 
 for the first time.

(a)
If 
, then the linearization point is .

(b)
If 
, then the linearization point is just after .

4.
If 
 and 
, then the linearization point is at the end, after all the other linearization points in some order.

Note that we assume in Case 3 that if 
, then there is an execution of Line 19 by a process  with the value 
. So, we first show in the following lemmas that this is indeed true.


Download : Download high-res image (160KB)
Download : Download full-size image
Fig. 2. All possible linearization points of a compare-and-swap() call by a process : The dot labeled ‘start’ is the point in the execution when the call starts. The dot labeled ‘e’ is the end of the execution. The solid line represents the execution. The square close to the ‘start’ is the point in the execution when the process  executes  and reads the value 
. The other square is the earliest point when a max-write instruction by a process  writes 
 to  where 
. The value 
 is read by the process  from the register  before it executes the max-write operation. The value 
 is the value of the  field of  at the end ‘e’. The dots except ‘start’ and ‘e’ (the ones in red) are vertically below the possible linearization points of the call along with the corresponding conditions. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.)

Lemma 1

The value of  is always even.

Proof

We have  at initialization. The modification only happens in Line 19 with an even value.  □

Lemma 2

Whenever  changes, it increases by .

Proof

If  was changed to  by a process , then  must be even (Lines 19 and 17). So, a process  wrote  to  and either 
 or 
 (Lines 13 and 14). As 
 is even by Lemma 1 and  is even too, it must be that 
. As  always increases, we have  before  modifies it. Also  is always even by Lemma 1, so we have  before  modifies it and the value increases by .  □

Lemma 3

The linearization point as given by Definition 1 is well-defined.

Proof

The linearization point as given by Definition 1 clearly exists for all the cases except for Case 3. For Case 3, we only need to show that if 
, then there exists an execution of Line 19 by a process  so that 
. As 
 is even by Lemma 1 and the value of  only increases in steps of  by Lemma 2, it follows from 
 that 
 was written to  at some point.  □

To show that the implementation is linearizable, we need to prove two main statements. First, the linearization point is within the start and the end of the corresponding function call. Second, the value returned by a finished call is the same as defined by the sequence of the linearization points up to the linearization point of the call. In the following two lemmas, we show the first of these statements.

Lemma 4

If the condition 
 is true for a compare-and-swap call by a process , then the value of  is at least 
 at the end of the call.

Proof

We define a set of processes 
. Consider the process  that is the first one to execute Line 17. As the first field of  is always modified by a max operation and process  writes 
 to that field, we have 
. If 
, then 
 and we are done.

So, we only need to check the case when 
. As 
 is even by Lemma 1, so is 
. Moreover, the process 
 as some process(es) (including ) executed Line 13. As 
 always increases whenever modified (Line 11), we have 
. But, if 
, then the process 
 finished even before the process , a contradiction. So, it holds that 
 and the process  executes Line 19.

Now, the execution of Line 19 by the process  either changes the value of  or does not. If it does, then 
 and we are done. Otherwise, someone already changed the value of  to at least 
 because of Lemma 2.  □

Lemma 5

The linearization point as given by Definition 1 is within the corresponding call duration.

Proof

The statement is true for Cases 1 and 2 as the instruction corresponding to the linearization point is executed by the process  itself.

For Case 3, we analyze the case of a finished and an unfinished call separately. Say that the call is unfinished. As 
 and 
 is the value of  at the start of the call, the linearization point as given by Definition 1 is after the call starts. Now, assume that the call is finished. We know from Lemma 4 that the value of  is at least 
 when the call ends. So, the point when Line 19 writes 
 to  is within the call duration.

We know from Lemma 4 that if the call finishes, then we have 
. So, if 
, then the call is unfinished and it is fine to linearize it at the end as done for Case 3b.  □

Now, we need to show that the value returned by the calls is the same as the value determined by the order of linearization points. We show this in the following lemmas.

Lemma 6

Assume that 
 for two distinct processes  and  and that  is even. Then, it implies that 
 and 
.

Proof

Without loss of generality, assume that the process  executes Line 15 before the process  does so. As 
 by assumption, the only way in which the field  can change until the process  executes Line 15, is by a max-write operation on  with  as the first field. This is not possible as  is even and the max-write on  is only executed with an odd value of the first field (Line 13). So, it holds that 
. Similarly, we have 
.  □

Lemma 7

As long as the value of  remains the same, the value of  does not change.

Proof

Say that a process  is the first one to write a value  to . The value written to the field  by the process  is 
. To have a different value of  with  as the value of , another process  must execute Line 19 with 
 but 
. As 
, it follows from Lemma 6 that 
 and 
. As the condition in Line 17 is true for both the processes  and , it then follows that 
. As the field 
 is updated only once for a given value of 
 (Line 11), it holds that 
 and the claim follows. □

Lemma 8

Say that 
 is even and 
 during a call by a process , then 
 for some call by the process .

Proof

As 
, some process  modified  by executing Line 13 or Line 14 with  as the first argument. As  is even and 
 is even by Lemma 1, the process  modified  by executing Line 14. So, it holds that 
. Also, process  executed Line 13 with  as the first field. As 
, the process  also executed Line 13 with  as the first field after the process  did so. So, it holds that 
.  □

Lemma 9

For every even value 
, there is an execution of Line 19 by a process  so that 
 and the first such execution is the linearization point of some call.

Proof

Consider an even value 
. Then, we know from Lemma 2 that  is written to  by an execution of Line 19. Let  be the point of first execution of Line 19 by a process  so that 
. So, it holds for the process 
 that 
 using Lemma 8. As point  is the first time when  is written to the field , it holds that 
. Thus,  is the linearization point of the process  by Definition 1.  □

Lemma 10

The value  is only modified at a Case 3a linearization point.

Proof

Let  be a Case 3a linearization point. Say that the value of  is updated to  at . Let  be the first point in the execution when the value of  is . Using Lemma 9, we conclude that  is either a linearization point (for ) or the initialization point (for ). Using Lemma 7, the value of  is not modified between  and .  □

We want to use the above lemma in an induction argument on the linearization points to show that the values returned by the corresponding calls are correct. First, we introduce some notation for . The term 
 is the value of the abstract compare-and-swap object after the th linearization point. The terms 
 and 
, respectively, are the values of  and  after the th linearization point. These terms refer to the respective values just after the initialization when . For , the term 
 is the expected return value of the call corresponding to the th linearization point. The following two lemmas prove the correctness using induction on the linearization points and checking the different linearization point cases separately.

Lemma 11

After  linearization points, we have 
 except for Case 4b linearization points. For , the 
 values are false for Case 1, true for Case 2, true for Case 3a and false for Case 3b.

Proof

We prove the claim by induction on . For the base case of , the claim is true as  is initialized with the initial value of the compare-and-swap object. Let 
 be the th linearization point for  and say that it corresponds to a call by a process . We have the following cases.

Case 1: Let 
 be the linearization point previous to 
. By induction hypothesis, it holds that 
. By Lemma 10, the value of  does not change until 
. As we have a read operation at 
, it holds that 
. By Definition 1, we know that 
. So, it holds that 
. Thus, it follows from the specification of the compare-and-swap object that 
. Moreover, we have 
 as 
.

Case 2: Again, we let 
 be the linearization point previous to 
. As argued in the previous case, it holds that 
. By Definition 1, we know that 
. So, it holds that 
. Thus, it follows from the object’s specification that 
. Further, we have 
 as 
.

Case 3a: Consider the point 
 when the value 
 was written to  for the first time. As 
 is even by Lemma 1, it follows from Lemma 9 that 
 is a linearization point or the initialization point. Using the definition of Case 3a, 
 is the first point when the value 
 was written to the field . So, we have 
. Thus, it holds that 
 by Lemma 7. Therefore, 
 as 
 by induction hypothesis. Using the definition of Case 3a, it also holds that 
. Thus, we have 
 and 
.

Now, assume that the instruction at 
 was executed by a process . Using the definition of Case 3a, we have 
. As 
 is the first time when the value of  is 
, we conclude that the process  is not finished until 
 by using Lemma 4. As 
, it is true that some process 
 has 
 and that the process executed Line 13 until 
. As 
, the process 
. Moreover, the process  did this during the call corresponding to the linearization point 
 as it follows from Lemma 4 that there is a unique call for any process  given a fixed value of 
. Thus, the process  already executed Line 11 with 
 as the value of the second field. This field has not changed as the call by process  is not finished until 
. So, we have 
 and that 
 as well. Because 
 as shown before, we also have 
.

Case 3b: Let 
 be the first point when the value 
 is written to  (
 is just before the point 
 as defined by Case 3b). Let  and  be the processes that execute the calls corresponding to the points 
 and 
 respectively. By definition of Case 3b, we have 
. As process  wrote 
 to , we have 
 as well. So, we have 
 using Lemma 7. Using definition of Case 3a and Case 3b, respectively, we have 
 and 
. So, we have 
. We have 
 as argued in the previous case, so 
. By induction hypothesis, we have 
. Moreover, there are no operations after 
 and until 
 by definition of Case 3b. So, we have 
 and thus 
. Also, we have 
 as 
.  □

Lemma 12

If the th linearization point for  corresponds to a finished call by a process , then the value returned by the call is 
.

Proof

Say the th linearization point is a Case 1 point. Using its definition, the value returned by the corresponding call is  as the condition in Line 6 holds true. Using Lemma 11, we have 
 as well for Case 1. Next, assume that the th linearization point is a Case 2 point. Then, the value returned by the corresponding call is  as the condition in Line 8 is true by definition. Using Lemma 11, we have 
 as well for Case 2.

Now, consider that the th linearization point is a Case 3a point. Say that the process  executes the operation at the linearization point. As 
 by definition of Case 3a, the process  already executed Line 13 with the first field as 
. So, the process  also initialized  to 
 in Line 12. Moreover, the process  wrote the value 
 to  afterwards using a max-write operation. Thus, the value of  after 
 is . This field is not changed by  until it returns. And, other processes only write  to the field. So, the call returns  which is the same as the value of 
 given by Lemma 11.

Next, consider that the th linearization point is a Case 3b point. Let  be the point when the process  initializes  to a value  during the call (Line 12). Consider a process  that tries to write  to  after  (by executing Line 18). So, it holds that 
 and that 
 is even. Now, we consider three cases depending on the relation between 
 and 
. First, consider that 
. As 
 and 
 is even, we have 
 using Lemma 8. So, we have 
. This cannot happen until  finishes as 
 for the current call by  using the definition of Case 3b. Second, consider that 
. Using the definition of Case 3b, there is a process  so that 
 and 
. As 
 by assumption, we have 
 using Lemma 6 . This contradicts our assumption that 
. Third, consider that 
. As 
 and 
 is even, we have 
 using Lemma 8. So, we have 
. This corresponds to a previous call by the process  as 
 for the current call by . So, it holds that 
 and execution of Line 18 has no effect. Thus, the process  returns  for Case 3b which matches the 
 value given by Lemma 11.

If the th linearization point is a Case 4b point, then we know from Lemma 4 that the call is unfinished and we need not consider it.  □

We can now state the following main theorem about Algorithm 1.

Theorem 1

Algorithm 1 is a wait-free and linearizable implementation of the compare-and-swap register where both the compare-and-swap and read functions take  steps.

Proof

We conclude that the compare-and-swap function as given by Algorithm 1 is linearizable by using Lemma 5, Lemma 12. The read operation is linearized at the point of execution of Line 2. Clearly, this is within the duration of the call. To check the return value, let 
 be the linearization point of the read operation and 
 be the linearization point previous to 
. Then, we have 
 using Lemma 10. So, it holds that 
 using Lemma 11. Moreover, both the compare-and-swap and read functions end after executing  steps and the implementation is wait-free.  □

7. Consensus numbers
In this section, we prove that each of the max-write and the half-max instructions has consensus number one. Note that these are two separate claims. One, that it is impossible to solve consensus for two processes using read-write registers and registers that support the max-write and read operation. Second, that it is impossible to solve consensus for two processes using read-write registers and registers that support the half-max and read operation. Trivially, both operations can solve binary consensus for a single process (itself) by just deciding on the input value. To show that these operations cannot solve consensus for more than one process, we use an indistinguishability argument.

First, we define some terms. A configuration of the system is the value of the local variables of each process and the value of the shared registers. The initial configuration is the input  or  for each process and the initial values of the shared registers. A configuration is called a bivalent configuration if there are two possible executions starting from the configuration so that in one of them all the processes terminate and decide  and in the other all the processes terminate and decide . A configuration is called -valent if in all the possible executions starting from the configuration, the processes terminate and decide . Similarly, a configuration is called -valent if in all the possible executions starting from the configuration, the processes terminate and decide . A configuration is called a univalent configuration if it is either -valent or -valent. A bivalent configuration is called critical if the next step by any process changes it to a univalent configuration. Consider an initial configuration in which there is a process  with the input  and a process  with the input . The process  outputs  if it runs until termination before  takes any step and similarly, the process  outputs  if it runs until termination before  takes any step. So, the initial configuration is bivalent. As the terminating configuration is univalent, a critical configuration is reached assuming that the processes solve wait-free binary consensus.

Assume that the half-max operation can solve consensus between two processes  and . Then, a critical configuration  is reached. Without loss of generality, say that the next step 
 by the process  leads to a -valent configuration 
 and that the next step 
 by the process  leads to a -valent configuration 
. In a simple notation, 
 and 
. We have the following cases.

1.
 and 
 are operations on different registers: The configuration 
 is indistinguishable from the configuration 
. Thus, the process  decides the same value if it runs until termination from the configurations 
 and 
, a contradiction.

2.
 and 
 are operations on the same register and at least one of them is a read operation: Without loss of generality, assume that 
 is a read operation. Then, the configuration 
 is indistinguishable to 
 with respect to  as the read operation by  only changes its local state. Thus, the process  decides the same value if it runs until termination from the configurations 
 and 
, a contradiction.

3.
 and 
 are write operations on the same register: Then, the configuration 
 is indistinguishable from the configuration 
 as 
 overwrites the value written by 
. Thus, the process  will decide the same value if it runs until termination from the configurations 
 and 
, a contradiction.

4.
 and 
 are half-max operations on the same register : Let 
 be the value of the register  in the configuration . Let 
 and 
 be the arguments of the operations 
 and 
 respectively. Without loss of generality, assume that 
. Then, we claim that the value of the register  is identical in the configurations 
 and 
. Indeed, the value of the register  in the configuration 
 is 
 and its value in the configuration 
 is 
 since 
 by assumption. Thus, the process  decides the same value if it runs until termination from these configurations, a contradiction.

So, the critical configuration cannot be reached and the processes  and  cannot solve consensus using the half-max instruction. Thus, its consensus number is one.

We use a similar argument to show that the max-write instruction also has consensus number one. We give the proof in Section 10, where we discuss many other consensus number one instructions that are similar to max-write.

8. Extensions
In this section, we discuss a couple of extensions of the previous algorithm. First, we know that the compare-and-swap instruction returns either true or false depending on whether the value is updated or not. Instead, one may want to return the value of the register prior to the application of the instruction. Let us call this variant read-compare-and-swap.

The presented algorithm can be modified to also accommodate this requirement. If the algorithm returns just after reading the value  from the register  at the start, then we know that the linearization point is the read operation on . So, we can return the value . If the algorithm returns the value  at the end, there are two cases. If  is true, then the compare-and-swap operation had been successful and the value  can be returned as the prior value.

If  is false, then the operation was unsuccessful and we need to return the value that got written to  successfully and led to the failure of the current operation. To do that, we make another read on  to check the current value. However, the value of  might have already been updated several times in the meantime and the current value read from  might be equal to the comparison value . We cannot return this value, since then the operation should be successful and the value returned  should be true. To fix this, we can keep both the current and the previous value of  in the register . We can then return one of these values, whichever is not equal to the comparison argument  of the operation. So, we have the following corollary.

Corollary 1

There is a wait-free and linearizable implementation of the read-compare-and-swap register using elementary instructions so that both the read-compare-and-swap and read functions take  steps.

Second, the algorithm that we presented simulates a single compare-and-set register using  registers that support the half-max, max-write, read and write instructions. If  compare-and-swap registers are to be simulated, then a straightforward approach requires  registers. However, we can improve this if we observe that there is at most one pending operation per process even if  compare-and-swap registers have to be simulated. The counter values in the arrays  and  are incremented for every compare-and-swap operation executed by a process and now give information about the latest pending call per process. Thus, there is no need to allocate these arrays for every compare-and-swap register. Only the registers  and  need to be allocated separately for every compare-and-swap register that is to be simulated. Concretely, to execute an operation (read or compare-and-swap) for the th compare-and-swap register, the process executes Algorithm 1 except that for every operation on the register  or , it executes the same operation on the register 
 or 
 respectively. As the counter value  used in the first half of each entry of array  or  is always increasing, we will be conceptually running  instances of the presented algorithm using  registers. Actually, if one observes closely, the three fields used in the register  are useful only when multiple compare-and-swap registers are implemented. Otherwise, we can use a single counter replacing both  and . So, we also have the following corollary.

Corollary 2

There is a wait-free and linearizable implementation of  compare-and-swap registers using  registers supporting elementary instructions so that both the compare-and-swap and read functions take  steps.

9. Throughput
In this section, we analyze the throughput of compare-and-swap versus half-max and max-write in a highly concurrent setting. We assume that there are  processes in the system that communicate to the memory via an interconnect. We compare the throughput of the following two scenarios.

1.
Each process executes a sequence of  compare-and-swap operations to the same memory location.

2.
Each process executes a sequence of operations so that a read operation alternates with either the max-write or the half-max operation to the same memory location. The number of non-read operations is  for each process.

In the second scenario, we assume that a process executes a read operation after each of the elementary operations as they do not return a value in our case. We call an instruction as compact if the corresponding function satisfies the following: given any sequence  of function calls for , there is a smaller sequence 
 of function calls from  and has the same effect as the sequence , i.e., we obtain identical values on applying  or 
 on any given initial value. Thus, we have the following result.

Lemma 13

Half-max and max-write are compact instructions whereas compare-and-swap is not.

Proof

Consider a sequence  of half-max operations. We consider a smaller sequence 
 so that an operation with the largest argument, say , is in 
. Let the current value of the register be . Thus, applying the sequence  or 
 results in the same value  of the register.

Consider a sequence  of max-write operations. Let  be the last operation in  with the largest value of the first argument. We construct a smaller sequence 
 where  is the last operation with the value  as the first argument. Let the current value of the register be . If , then applying the sequence  or 
 results in the same final value  as  is the largest value of the first argument and  is the last operation in  or 
 with  as the first argument. If , then neither the operations in  or 
 change the value of the register as the first argument of all the operations is strictly smaller than .

Consider a set  of compare-and-swap operations so that all the 2 arguments of the operations are unique values. Assume for contradiction that compare-and-swap is compact. Then, there is a smaller sequence 
 that has the same effect as the sequence . So, there is a compare-and-swap operation that is in the sequence  but not in the sequence 
. Let 
 be the argument of that operation. If the current value of the register is 
, then applying the sequence  results in the final value 
, since all the argument values are unique and consequently, all the operations before and after the one with the argument 
 are unsuccessful. However, applying the sequence 
 does not change the value of the register as none of those operations have first argument as 
. So, the final value remains 
, a contradiction.  □

Say that the processes communicate to the memory via the butterfly interconnect [18]. As we consider concurrent updates to the same memory location, the part of the interconnect connecting the memory location and the processes can be considered as a complete binary tree with processes (leaves) connected to the memory location (root) via intermediate nodes and links. We assume that a batch of instructions — at most one instruction from each process — can enter the butterfly interconnect in each step for updating the memory. It takes one step for an instruction to traverse a link of the interconnect. So, it takes  steps for an instruction to reach the memory location. Instructions can be pipelined while traversing to the memory, i.e., each intermediate node can contain an instruction and they all can traverse the links simultaneously towards the memory (root). We define the concurrent throughput of compare-and-swap as the average number of instructions that are processed and applied to the memory in a single step of the first scenario. Similarly, the concurrent throughput of half-max and max-write is the average number of instructions that are processed and applied to the memory in a single step of the second scenario. The following theorem quantifies the concurrent throughput of the instructions.

Theorem 2

The concurrent throughput of compare-and-swap is 
 
 and that of half-max and max-write is 
 
 for a pipelined butterfly interconnect.

Proof

For compare-and-swap, each process executes a sequence of  operations so a total of  operations are executed. The instructions cannot be combined and reduced to a shorter sequence in the pipeline due to Lemma 13 and so must be applied sequentially at the memory. The first instruction takes  steps to traverse the interconnect. Then, all the instructions are processed sequentially in  steps. So, the throughput is 
 
 
.

Consider the case of half-max and max-write operations. Each process executes  operations including the read operations. We divide each batch of instructions to be processed into three batches so that each batch contains only a single type of instruction, i.e., read, half-max or max-write. These instructions can be combined in the pipeline due to Lemma 13, i.e., if two instructions want to traverse the links and move up to the same intermediate node, then they can be combined and replaced by a single instruction. Thus, the next batch of instructions can enter the interconnect in every step. So, the first batch traverses the interconnect and is reduced to a single instruction in  steps and then, all the  batches are processed by applying a single instruction from each batch. So, it takes a total of  steps and the throughput for the  non-read operations is 
 
 
.  □

If , then the throughput values are approximately 
 
 for the elementary instructions versus  for compare-and-swap, which is lower by a factor proportional to the number of processes.

10. More elementary instructions
In Section 7, we argued that both half-max and max-write instructions have consensus number one. In this section, we consider a generic family of similar instructions that also have consensus number one. Consider the instruction max-write-or-. Say that the current value in the register is 
. If 
, then the new value of the register is 
. Otherwise, the new value of the register is 
, where  is some binary operation that is both commutative and associative. Commutative means that , i.e., the order of operands does not affect the result. Associative means that , i.e., the order of applying operations on a given sequence of operands does not affect the result. Addition or multiplication are a couple of examples of . The instruction does not return a value. In other words, the instruction is like the max-write instruction on the first two fields and also modifies the third field using the commutative operation  if the max-write is “unsuccessful”.

We claim that the instruction max-write-or- has consensus number one, where  is a commutative and associative operation. It is trivial to solve consensus for only one process. Thus, we have to show that it is impossible to solve consensus for more than one process using read-write registers and registers that support the max-write-or- and the read instructions. Consider a system of two processes  and  that want to solve binary consensus using read-write registers and registers that support the max-write-or- and the read instructions. As argued in Section 7, the initial configuration of the system is bivalent and a critical configuration  must be reached. Let 
 be the next step taken by the process  in the configuration  and 
 be the next step taken by the process  in the configuration . We also assume without any loss of generality that the step 
 leads to a -valent configuration 
 and the step 
 leads to a -valent configuration 
. In symbolic notation, we have 
 and 
. We consider the following cases depending on the type of operations 
 and 
.

1.
 and 
 are operations on different registers: Then, we let  take its next step 
 on the configuration 
 and  take its next step on the configuration 
. The resulting configurations 
 and 
 are indistinguishable to either of the processes  or  as 
 and 
 are operations on different registers. Thus,  decides that same value when run until termination from the configurations 
 and 
, contradicting that 
 is -valent and 
 is -valent.

2.
 and 
 are operations on the same register and at least one of them is a read operation: Let us assume without any loss of generality that 
 is a read operation on a register , which is either a read-write register or one that supports the max-write-or-. Now, we let  take its next step 
 on the configuration 
. Regardless of the operation 
, the contents of the register  are identical in the configurations 
 and 
, since 
 is a read operation. Thus, these configurations are indistinguishable to  and it decides the same value if it is run until termination from these configurations, a contradiction.

3.
 and 
 are write operations on the same register: Say that 
 is a write operation on a read-write register . Then, we let  take its next step on the configuration 
. The contents of  are same in both 
 and 
. Registers other than  remain unchanged. So, the configurations 
 and 
 are indistinguishable with respect to both  and . Thus,  decides the same value if run until termination from the configurations 
 and 
, a contradiction.

4.
 and 
 are max-write-or- operations on the same register: Say that 
 is a max-write-or-
 operation on a register  and 
 is a max-write-or-
 operation on the same register . Let us assume without any loss of generality that 
. Let 
 be the value of the register  in the configuration . Then, we have the following cases.

(a)
: Then, we let  take its next step on the configuration 
 and let  take its next step on the configuration 
. We claim that the contents of  are identical in these configurations, i.e., the configurations 
 and 
. As 
 by assumption and 
, we have 
. So, in the configuration 
, both the operations 
 and 
 will not change 
 in  and only apply  on 
. Thus, the final value of  in the configuration 
 is 
. Similarly, the value of  in the configuration 
 is 
. Both these values of  are identical since the operation  is commutative and associative. The values of other registers do not change either. Thus, either of the processes, say , will decide the same value when run until termination from the configurations 
 and 
, a contradiction.

(b)
 and 
: Again, we let  execute 
 on the configuration 
 and let  execute 
 on the configuration 
. We claim that the value of the register  is identical in the configurations 
 and 
. First, consider the configuration 
. As 
 by assumption, the value of the register  in the configuration 
 is 
. Executing 
 on it changes  to 
 as 
. Second, consider the configuration 
. The value of  in the configuration 
 is 
 as 
 by assumption. As 
 by assumption, executing 
 on 
 only applies  on 
 and changes  to 
. Thus, the value of  is identical in the configuration 
 and 
. As the other registers remain unmodified, process  terminates with identical values from both of these configurations, a contradiction.

(c)
 and 
: We let  take its next step on the configuration 
 to yield the configuration 
. The value of the register  in the configuration 
 is 
 since 
. So, its value in the configuration 
 is 
 as 
 by assumption. Since 
, the value of the register  in the configuration 
 is 
. So, register ’s value is identical in the configurations 
 and 
. Thus, the process  decides the same value if it is run until termination from these configurations, a contradiction.

Thus, it is impossible to reach a critical configuration and then terminate with a correct value using read-write registers and max-write-or- registers shared by two or more processes. So, the consensus number of max-write-or- instruction is one.

We can now also conclude that the max-write instruction also has consensus number one. Let the operation  be such that  for all pairs , where  is some constant. Clearly, the operation  is both commutative and associative. Now, every max-write-or- operation performs a max-write on the first two fields. If we can solve consensus among two processes using the max-write instruction, then we can also solve it using the max-write-or- instruction and using only the value of the first two fields, a contradiction. So, the consensus number of the max-write instruction is also one.

11. Step complexity overhead of using compare-and-swap
Consider the max-write-or-add instruction, i.e., the max-write-or- instruction where the operation . Clearly, this instruction can be used with the half-max instruction to simulate the compare-and-swap register in  steps by Theorem 1, since the max-write-or-add operation always performs the max-write operation on the first two fields. However, consider that we have to simulate a max-write-or-add instruction and the available registers are the read-write and the compare-and-swap registers. What is the best step complexity that we can achieve?

In this section, we show that a wait-free simulation of the max-write-or-add instruction using the read-write and the compare-and-swap registers takes at least  steps, where  is the number of processes in the system. We use the lower bound on the step complexity of an implementation for detecting wakeup [7] among processes using the read-write and load-link/store-conditional registers. The load-link/store-conditional registers are advanced registers like the compare-and-swap registers and support the following operations:

1.
LL: Returns the current value of the register.

2.
SC: If successful, the operation writes  to the register and returns true. Otherwise, the operation is unsuccessful and does not change the value of the register and returns false. If executed by a process , the operation is successful if no other process performed a successful SC operation on the register since the latest LL operation on the register by the process . Otherwise, the operation is unsuccessful.

Intuitively, the wakeup problem requires some process to detect the time by which all other processes in the system have taken at least one step (or woke up). Formally, all the processes must terminate in a finite number of steps, returning either 0 or 1. When all the processes terminate, at least one of them returns 1, and every process takes at least one step before any process returns 1. Jayanti et al. [13] showed that it takes at least  steps to solve the wakeup problem using LL/SC and read-write registers. We can use this result to show a lower bound of implementing a max-write-or-add register using LL/SC and read-write registers.

Lemma 14

A wait-free implementation of a max-write-or-add register takes  steps using LL/SC and read-write registers.

Proof

We solve the wakeup problem using a max-write-or-add register in  steps as follows. Initialize a register  with . Each process executes max-write-or-add on the register  and then reads it. If the value returned is , then return 1 else return 0.

Clearly, each max-write-or-add operation adds one to the third field of . Consider the last max-write-or-add operation executed on . Just before that operation, the value of  is . So, no process returns 1 before the last max-write-or-add operation is executed on . Afterwards, the value of  is  and the first process to execute a read later on  returns 1. By that time, all the processes have executed at least one step.

If an  wait-free implementation of a max-write-or-add register is possible using LL/SC and read-write registers, then wait-free wakeup can be solved in  steps using LL/SC and read-write registers, a contradiction.  □

One can easily implement a compare-and-swap register in  steps using LL/SC registers [1]. Thus, we conclude the following.

Theorem 3

A wait-free implementation of a max-write-or-add register takes  steps using compare-and-swap and read-write registers.

Proof

Assume an  implementation was possible. As compare-and-swap can simulated in  steps using LL/SC [1], an  implementation is also possible using LL/SC and read-write registers, which contradicts Lemma 14.  □

12. Conclusion
One issue with Algorithm 1 is that it uses unbounded sequence numbers. Thus, the algorithm only works if the size of the registers is at least logarithmic in the total number of compare-and-swap operations executed. Actually, the growth in sequence numbers can be much slower as out of the two unbounded counter types, one of them counts the total number of compare-and-swap operations executed per process and the other one counts the total number of successful compare-and-swap operations only. We still think that the result helps us to understand the capability of a set of elementary instructions with respect to their ability to efficiently simulate compare-and-swap.

Using our results, we can compare the following two systems: (1) Supports the elementary instructions max-write-or-add, half-max, read and write, and (2) Supports compare-and-swap, read and write. Using Theorem 1, any synchronization problem that can be solved on system (2) in  steps can also be solved by the system (1) in  steps. Since the simulation is wait-free, it can also be used for synchronization problems where wait-freedom is not required. On the other hand, due to Theorem 3, we can conclude that a synchronization task solvable in  time on system (1) might be impossible to solve in  time by using system (2). For example, consider the synchronization task of implementing a shared counter or a max register [2], [3].

There are practical factors other than the step complexity and the number of the shared registers used. For example, the half-max and the max-write operations do not return a value. Thus, they can spend less time in traversing the interconnect and have less latency. In this paper however, we show that a set of two elementary instructions can compare well against the compare-and-swap instruction with respect to the step complexity and also the throughput in a highly concurrent setting. This highlights the capability of a set of elementary instructions and shows another aspect in choosing the best synchronization instructions to support in general.