Abstract
Psycholinguistic research can inform the design of dialogue systems for fault diagnosis. When users provide ambiguous symptom descriptions, dialogue systems can reformulate these descriptions to check the correctness of their interpretation. The present study investigated whether such reformulations should be performed by users or dialogue systems, and how they should be phrased. In a Wizard-of-Oz study, subjects described faults symptoms to a chatbot, which subsequently asked for clarification. Experiment 1 compared the effects of requests for subjects to self-correct their descriptions and reformulations provided by the dialogue system in either common or technical terms. Experiment 2 combined reformulations in common and technical terms with each other and with pictures of fault symptoms. The results revealed that requests for self-correction increased solution times and verbal effort, that common terms decreased solution times but led to errors when seemingly easy reformulations were incorrect, and that technical terms did not mislead subjects into accepting them uncritically. Enrichments with pictures reduced the risk of accepting incorrect reformulations and were considered particularly helpful when combined with common terms. Lexical alignment with dialogue system reformulations was low, but subjects adopted its technical terms most readily when no common terms were provided. Taken together, the results suggest that combining reformulations in everyday language with visual information is most suitable to support grounding.

Previous
Next 
Keywords
Dialogue system

Fault diagnosis

Grounding

Miscommunication

Repair

Reformulation

1. Introduction
In the process and packaging industries, complex interactions result in faults that cannot be foreseen by system designers (Hollnagel, 2012; Müller and Oehm, 2019; Perrow, 1984). Therefore, a fluent cooperation between humans and machines is required for adaptive problem solving (Christoffersen and Woods, 2002; Klein et al., 2004). Such problem solving requires the technical system to rely on information provided by operators, because many important variables cannot be measured. For instance, consider fault diagnosis in the food industry. Fault diagnosis, which is also known as troubleshooting or diagnostic problem solving (e.g., Abele, 2017; Bereiter and Miller, 1989; Patrick et al., 1999), refers to the activity of finding the causes of a process disturbance or defect (e.g., broken chocolate bars). Such causes have to be inferred from a number of symptoms (i.e., observable consequences of the fault cause) and thus presuppose an accurate representation of these symptoms. While the machines are equipped with sensors that serve this purpose and thus can infer some fault causes automatically (Venkatasubramanian et al., 2003), many faults either lead to or result from characteristics of the products and packaging materials that cannot be measured in-line (e.g., crumbliness of cheese) or are highly idiosyncratic (e.g., packaging quality).

Therefore, operators’ verbal symptom descriptions provide valuable inputs for fault diagnosis and can be elicited by dialogue systems (DS). Dialogue systems or conversational agents are computer systems that engage in conversations with human users. However, the interpretation of verbal descriptions is challenging, as language is inherently ambiguous (Levinson, 2016) and people use many different words to describe the same concepts (vocabulary problem, Furnas et al., 1987). At first glance, language seems quite messy (Brennan, 1991; Clark, 1994; Ferreira et al., 2005; Kroch and Hindle, 1982). People use ungrammatical constructions, are imprecise and inconsistent in referring to objects, fail to disambiguate their utterances, use indirect expressions, and rely on knowledge and presuppositions far beyond the current conversation. Therefore, operators and DS need to make sure they are talking about the same things.

The aim of the present study is to experimentally investigate strategies for establishing mutual understanding of symptom descriptions in a DS for fault diagnosis. To provide a theoretical background, the following sections first discuss the principles by which human communication partners establish mutual understanding. We then turn to DS, focusing on systems that adopt dialogue principles from human communication. This is followed by an overview of empirical findings on the consequences of applying the aforementioned communication principles, where we discuss how they affect subjects’ performance and language production. Based on these findings, we derive our research questions. These questions are addressed in two experiments in which human users submit descriptions of fault symptoms to a text-based DS (chatbot), and the DS uses different communication strategies to request clarification by offering reformulations. We assess how these strategies impact performance, communication processes, and subjective ratings of the interaction.

2. Related work
2.1. Dialogue principles in human communication
Strategies for establishing mutual understanding between humans and DS can be derived from the psycholinguistic literature on grounding (for overviews see Brennan et al., 2010; Clark and Brennan, 1991). Despite the ambiguity of language, human dialogue partners understand each other remarkably well. This is because they establish common ground (Clark and Brennan, 1991). They form shared representations of what they are discussing, and then jointly manipulate these representations (Brennan, 1991). This shared knowledge base is continuously updated as mutually understood contents are added and thus become available for subsequent use in the conversation (Clark and Wilkes-Gibbs, 1986). To this end, speakers must convince themselves that addressees understand what they are saying. To let speakers know about their understanding, addressees provide several forms of evidence (Clark and Brennan, 1991; Clark and Schaefer, 1989), ranging from implicit acknowledgments (e.g., continued attention or reacting to requests) to explicit feedback about their understanding of the speaker's message (e.g., ‘okay’ or ‘Pardon…?’). The joint process of adding and adapting the contents of common ground from one dialogue turn to the next has been formalised in the contribution model (Clark and Schaefer, 1989). During the grounding process, miscommunications can occur. There are two ways of dealing with such miscommunications. On the one hand, speakers can try to prevent them, either by selecting references that facilitate understanding in the first place or by adapting these references to the specific needs of their partner. On the other hand, partners can jointly repair miscommunications after they have occurred.

2.1.1. Reference selection
A first way of preventing miscommunications is to select suitable references. Speakers have different possibilities for referring to objects, for instance via proper names (e.g., ‘the blank holder’), descriptions (e.g., ‘the machine part responsible for controlling wrinkle formation in the production of paperboard shapes’), or pronouns (e.g., ‘it’). Speakers often overspecify their referring expression, either by adding more modifiers than needed to identify the referent (Engelhardt et al., 2006) or by referring to objects redundantly with a name and a description (Heller et al., 2012). Overspecification is useful, as the goal of reference selection is that addressees can form a mental image of the object (Arts et al., 2011; Levelt, 1989).

2.1.2. Lexical alignment
The goal of enabling addressees to form a mental image of the object also implies that reference selection needs to be partner-specific. On the one hand, it is shaped by inferences about the addressee's knowledge, both based on global knowledge (e.g., the addressee works in the same plant and thus knows the machines) (Isaacs and Clark, 1987) and on the current situation (e.g., the addressee cannot see a machine part) (Hanna et al., 2003). On the other hand, reference selection is shaped by the ongoing conversation. Speakers adopt the terms their partner has used in the preceding conversation and thereby progressively lexically align with the partner's referring expressions (Garrod and Pickering, 2004; Pickering and Garrod, 2004, 2006). That is, partners cooperatively form conceptual pacts for how to name objects, and then use these conventions in their subsequent interaction (Brennan and Clark, 1996). To align with their partner, speakers even adopt dispreferred or untypical references (Bortfeld and Brennan, 1997; Goudbeek and Krahmer, 2012).

2.1.3. Repair
The second way of dealing with miscommunications is to repair them after they have occurred. Both partners participate in repair activities (Brennan et al., 2010; Clark and Wilkes-Gibbs, 1986), which presupposes that the right evidence is available at the right time (Brennan and Hulteen, 1995). Repair can be initiated by the speaker or the addressee (i.e., self-initiated vs. other-initiated repair) and then the actual repair action be performed by either the speaker or the addressee (i.e., self-repair vs. other-repair). In general, people prefer self-initiated repair to other-initiated repair, and self-repair to other-repair (Schegloff et al., 1977). However, sometimes the problem can only be detected by the addressee, which calls for other-initiated repair. Addressees are more likely to initiate a repair and then let the speaker perform it (i.e., clarification requests) than to directly perform the repair action (i.e., correction) (Colman and Healey, 2011).

Across languages, other-initiated repair conforms to the following basic structure (Dingemanse et al., 2015): there is a problem source, followed by a repair initiation and a repair solution (e.g., A: ‘The blank holder is too tight’, B: ‘What?’, A: ‘The machine part that is responsible for controlling wrinkle formation needs to be loosened’). According to the principle of least collaborative effort (Clark and Wilkes-Gibbs, 1986), the partner who requests a repair should use the most specific repair initiator possible, even when this means more effort for him- or herself. For instance, instead of simply asking ‘what?’, he or she can make a restricted offer (e.g., ‘the blank holder?’), signalling that in principle he or she has understood the utterance but is not sure. Such restricted offers make the trouble source explicit and require much shorter responses (e.g., ‘yes’) (Dingemanse et al., 2015). Two common strategies for restricted offers are expansion and replacement (Clark and Wilkes-Gibbs, 1986). Expansion means that the addressee adds content to the speaker's noun phrase as a request for confirmation, while replacement means that the addressee substitutes the speaker's noun phrase by another one. The latter corresponds to a reformulation. The present study investigates how to perform such reformulations in human–computer interaction. It also contrasts these reformulations with requests for self-correction, meaning that the DS poses an open request by asking the user to reformulate his or her own description.

2.2. Dialogue systems
Principles of human dialogue can serve as a basis for the design of DS, but it is important to find out which principles can be used and how they should be adapted to the context of human computer-interaction. Therefore, the following sections first provide an introduction to DS and then discuss related work on the empirical investigation of dialogue principles for grounding, reference selection, and repair in human–computer interaction.

2.2.1. Types of dialogue systems
Dialogue systems are conversational agents that can interact with their users based on natural language (for an overview see Chen et al., 2017). They can be classified in different ways, some of which are listed below. Considering these variations is important as they influence which principles from human communication are applicable and how they need to be adapted to the context of human–computer interaction. First, DS differ with regard to the use contexts they are designed for. They can broadly be categorised into task-oriented and non-task-oriented systems (Chen et al., 2017). Non-task-oriented systems are conversational agents that interact with humans in open domains: the user can make any contribution in order to get a reasonable response. In recent years, DS such as Amazon Alexa have become increasingly popular for home applications, where they are mainly used for music, search, and IoT device control (Ammari et al., 2019). Task-oriented DS are designed to assist users in accomplishing a specific task goal, and can further be divided according to the tasks they support. For instance, there are systems for navigation (Belvin et al., 2001), customer service (Følstad and Skjuve, 2019), education (e.g., Kerlyl et al., 2007), or fault diagnosis (e.g., Janarthanam and Lemon, 2008; Williams, 2007). Differences in use contexts and tasks affect how grounding should be supported, for instance in terms of the grounding criteria to be selected or the partner-specific adjustments to be accomplished.

Second, DS differ in terms of their computational implementation, which directly affects their language processing capabilities. Traditional systems only understood a limited set of user inputs and relied on canned messages for responding. This considerably restricted their linguistic behaviour. Contemporary DS use increasingly advanced technologies for natural language processing (for an overview see McTear et al., 2016). For instance, generative hierarchical neural network models allow them to produce messages word-by-word (Serban et al., 2016). Such technological advances make DS more flexible and allow dialogues to become more and more similar to human conversation.

Third, DS can be classified according to the modalities they use (e.g., speech, text, gesture, or a combination of different modalities). The number of speech-based DS is constantly increasing, and considerable research has been conducted to inform their design and study their impacts on human–computer interaction (for an overview see Clark et al., 2019). However, the processing of speech is prone to recognition errors and poses a number of other challenges. Therefore, speech-based DS have not replaced text-based systems, and currently chatbots experience growing popularity (Brandtzaeg and Følstad, 2017). Chatbots are text-based digital assistants that enable people to converse with a machine in a dialogic fashion by using natural language (Dale, 2016). They greatly vary in their implementation, and different chatbots are suitable for different purposes (Følstad et al., 2018). From a user perspective, the main advantage of chatbots is that they allow users to receive relevant information in a timely and efficient manner (Brandtzaeg and Følstad, 2017). However, a factor reducing their acceptance is their high rate of miscommunications (Følstad and Taylor, 2019). When chatbots do not understand user inputs, they usually either provide an inappropriate response or simply inform the user that they cannot understand the input. To mitigate these problems, repair strategies have been suggested, such as making uncertainty known or providing several alternatives (Følstad and Taylor, 2019). However, up to now it is unclear how exactly such repair should be designed and how it influences communicative processes. Therefore, the design of DS in general and chatbots in particular can greatly benefit from psycholinguistic knowledge about conversation and grounding.

2.2.2. Dialogue systems based on principles of grounding
The utility of knowledge about human dialogues in human–computer interaction has been recognised for decades (Brennan, 1988, 1991; Brennan and Hulteen, 1995; Cahn and Brennan, 1999), which has resulted in a number of conceptual models and implementations of DS that focus on common ground. To this end, the contribution model (Clark and Schaefer, 1989) has been formalised and extended to enable its use in DS (Cahn and Brennan, 1999). However, excessive explicit grounding can be tedious, for instance when the system repeats user inputs or asks for acknowledgement too often (e.g., user: ‘Loosen the blank holder’, DS: ‘I heard you say loosen the blank holder’). Therefore, Brennan and Hulteen (1995) have proposed a strategy for how DS can ask for clarification in an adaptive and context-dependent manner. They differentiate between seven states a DS can be in. For instance, when the DS is interpreting (state 4), this means that it has understood the user's utterance but cannot map its interpretation onto a command. Depending on its current state, the DS will ask for clarification in different ways (e.g., more or less specifically).

Several other authors have built on psycholinguistic models to develop DS that are capable of continuously evaluating and updating common ground with their human partner (e.g., Moore, 1989; Paek and Horvitz, 1999; Pérez-Quiñones and Sibert, 1996; Roque and Traum, 2008; Wolters et al., 2009). For instance, Roque and Traum (2008) have developed a DS that tracks the extent to which material has been grounded in the previous conversation. Based on this graded representation of common ground, the DS selects its utterances using different types of evidence of the user's understanding (e.g., whether the DS has merely submitted material or the user has also acknowledged it, repeated it back, or even used it in a subsequent utterance).

This applied work on grounding between humans and DS has led to some very promising implementations. However, it provides little information about the detailed mechanisms of grounding between humans and DS. This is because in order to be usable, DS need to be adaptive and allow for dialogues to unfold depending on the user's contributions, instead of only varying one specific aspect at a time in a controlled setting. Accordingly, most experimental investigations of actual DS have merely asked how users evaluate the interaction, instead of studying interaction mechanisms. For instance, Roque and Traum (2009) performed a user study in which subjects interacted with their DS and later rated how much they felt the DS had understood them, had put effort into understanding them, and had given appropriate responses. Conversely, most applied studies do not ask how a specific dialogue principle used by a DS (e.g., particular type of request) affects user behaviour. This is not a flaw of the respective studies but a logical consequence of studying naturalistic dialogues. They simply are not controllable. Therefore, to learn about the mechanisms of human–computer dialogues we have to turn to more basic experimental research.

2.3. Consequences of reference selection, lexical alignment, and repair
The effects of applying human dialogue principles in DS has been investigated in numerous studies (e.g., Bohus and Rudnicky, 2005; Branigan et al., 2011; Brennan, 1991, 1996; Brennan, 1998; Brennan and Ohaeri, 1994; Campana et al., 2011; Paraboni et al., 2007; Sheeder and Balogh, 2003; Skantze, 2005; Zoltan-Ford, 1991). As the study of grounding between humans and DS requires close experimental control, most studies have used simulated DS, with human experimenters acting as a DS in Wizard-of-Oz paradigms. The following sections summarise the results most relevant to the present study, focusing on three aspects of grounding: reference selection, lexical alignment, and repair.

2.3.1. Reference selection
With regard to reference selection, it has been found that overspecification can be beneficial, just like in human communication. For instance, DS that are more verbose in their examples for user inputs improve performance (Sheeder and Balogh, 2003): in an open prompt call-routing application, a DS primed user responses with system examples, either using keywords or complex sentences (e.g., ‘You can ask me about “minutes used”, “automatic payments” and “calling plans”’ vs. ‘For example, you can say things like “I want to activate my phone” and “I want to change my voicemail password”’). Performance benefitted from the latter type of description. Similarly, long and logically redundant descriptions of pictures in a text improved performance and were preferred in risky situations (Paraboni et al., 2007).

2.3.2. Lexical alignment
Reference selection is a product of the previous conversation, and therefore the tendency to align with a partner can override individual preferences (Branigan et al., 2011; Goudbeek and Krahmer, 2012). Accordingly, conversation with DS can benefit from tracking how users have referred to objects, enabling the DS to use the same verbal expressions (Brennan, 1991). Although the potentials of DS aligning with users have been emphasised repeatedly, no research on the application and effects of such strategies is available. However, the opposite direction of alignment also is highly relevant for human–computer interaction. If a DS could get users to align with its references, it would be able to constrain and predict user inputs (Brennan, 1996; Koulouri et al., 2016), which would mitigate the vocabulary problem (Furnas et al., 1987). There is ample evidence that users align with DS just as much or even more than they do with other humans (Branigan et al., 2011; Branigan et al., 2003; Brennan, 1991, 1996). Users name objects in the same way a DS has named them even when the DS uses dispreferred or untypical names. A likely explanation is that users try to avoid errors, being unsure about the linguistic capabilities of DS (Branigan et al., 2010).

DS can deliberately make users align with their expressions (Brennan, 1996): in a series of Wizard-of-Oz experiments, a DS sometimes responded with different terms than those suggested by the user. The overall rate of alignment was 58%, which is comparable to alignment with human partners (Brennan and Clark, 1996). Critically, the study varied whether corrections were embedded or exposed. For embedded corrections, the DS incorporated its reformulation in its next utterance (e.g., user: ‘Is the thing holding the wrinkles set correctly?’, DS: ‘The blank holder is set correctly.’), while for exposed corrections it used an extra turn to ask for clarification (e.g., user: ‘Is the thing holding the wrinkles set correctly?’, DS: ‘By thing holding the wrinkles, do you mean blank holder?’, user: ‘Yes.’, DS: ‘The blank holder is set correctly.’). Users were more likely to adopt DS terms after exposed corrections (94 vs. 37% for text interfaces). Still, while users adopt some terms, they rarely mirror DS messages completely (Zoltan-Ford, 1991). Therefore, when it is important that users adopt particular terms, DS should use exposed corrections and re-iterate what kind of vocabulary they can understand, instead of merely relying on alignment (Brennan, 1996; Zoltan-Ford, 1991).

2.3.3. Repair
Empirical research on the consequences of different repair strategies in human–computer interaction is quite limited. Perhaps somewhat counterintuitively, it has been suggested that whenever possible, repair should be avoided (Bohus and Rudnicky, 2005; Skantze, 2005). This is because error recovery performance is better when DS do not make non-understandings explicit but use implicit strategies such as moving on, providing help and explanations, or telling users how they can respond (Bohus and Rudnicky, 2005). These strategies make users change the semantic concept of their message, which leads to higher recovery rates than repeating or rephrasing. Thus, instead of trying to repair, it can work better to use alternative dialogue plans.

With regard to chatbots and their challenge of interpreting user inputs, a study in the context of customer service empirically investigated the impacts of two repair strategies on performance and dialogue processes: making the uncertainty of interpretations explicit and providing several alternative solutions (Følstad and Taylor, 2019). It was found that although these strategies reduced the number of inappropriate responses by the DS (i.e., false positives), they did not improve dialogues with regard to dialogue length, conclusiveness, or helpfulness. However, a closer look at the implemented strategies reveals that the DS did not actually repair misunderstandings but merely indicated that it was unsure about the interpretation but offered several alternative solutions regardless. The limited scope of improvements might suggest that it is worthwhile to put more effort into repairing the misunderstanding first before going on with the dialogue.

2.4. Open questions
Previous research suggests that principles from human dialogues can support grounding with DS in general, and alignment, clarification or repair in particular. However, little is known about the strategies DS should use to ask for clarification or replace user utterances. For instance, Brennan (1996) reported that exposed corrections increase alignment, but by what kind of terms should user utterances be replaced? This is important as the content of a replacements is likely to affect how users react to them. In the processing and packaging industries that provide the context for the present study, complex machinery is supervised by operators with low qualification (Mason et al., 1994; Müller and Oehm, 2019). Accordingly, we need to ask whether the language used by DS should be technically precise or easily understandable. If operators do not understand replacements suggested by a DS, this might lead them to uncritically accept the replacements, leading to misunderstandings if the DS referred to the wrong concept. In the Human Factors literature, ample research shows that humans tend to uncritically accept the suggestions of decision support systems, which is known as automation bias (for an overview see Parasuraman and Manzey, 2010). However, up to now there is no literature integrating the psycholinguistic research on grounding in DS and the Human Factors literature on (mis-)using decision support systems. While the former typically asks how dialogue principles affect users’ verbal utterances, the latter focuses on task performance in terms of solution times and accuracy. The aim of the present study is to build a bridge between dialogue processes and performance effects, integrating two research traditions that so far have only co-existed in parallel in order to enhance our understanding of the interaction between humans and DS.

3. Present study
3.1. Aims and contents of the study
We investigated how DS for industrial fault diagnosis should support the formation of common ground by reformulating users’ verbal descriptions of fault symptoms. Note that our aim was not to develop a DS and then evaluate it in a setting that mirrors its actual use context. Instead, the scientific background is applied cognitive psychology, which can be classified as use-inspired basic research (Pasteur's quadrant, Stokes, 1997). The questions are derived from real-world problems, but we address them by isolating particular aspects and that can be studied in controlled lab experiments. This is necessary to investigate the mechanisms of dialogue between humans and DS.

In the present experiments, subjects verbally described faulty products of packaging machines to a DS. Following these descriptions, the DS asked for clarification in one of several ways. It either asked subjects to reformulate their own description or provided reformulations in common terms, technical terms, combinations thereof, or enrichments of verbal reformulations with pictures of fault symptoms. More conceptually, we investigated strategies for other-initiated repair via replacements (or exposed corrections), and varied who was responsible for these replacements. While requests for self-correction are open requests that require subjects to make the replacement (i.e., self-repair), reformulations by the DS are restricted offers for other-repair that only require short responses such as ‘yes’ and thereby reduce collaborative effort. The effects of reformulations were studied in standard trials in which the DS correctly reformulated the fault symptom and in catch trials in which the reformulation was incorrect. We investigated the effects of reformulations on performance (Experiment 1 and 2) and on the alignment of subjects’ subsequent descriptions with the reformulations used by the DS (Experiment 2).

3.2. Research questions
Two general research questions were addressed. First, when a DS asks for clarification by using either other-correction or self-correction, how does this affect solution times, dialogue turns, error rates, subjects’ subsequent utterances, and their ratings of the dialogues? Second, what are the consequences of different forms of other-corrections? More specifically, Experiment 1 investigated the following questions:

(1)
Do requests for other-corrections lead to more efficient performance and higher subjective ratings of the dialogues than requests for self-corrections?

(2)
Does using common terms in the reformulation requests of the DS result in shorter solution times and fewer dialogue turns, fewer errors, and better dialogue ratings than the use of technical terms?

(3)
Does the use of technical terms lead subjects to accept them uncritically and thus commit more errors when the terms do not match the current situation?

Experiment 2 focused on different forms of other-correction and investigated the effects of combining them with each other and with pictures of fault symptoms. It investigated the following questions:

(1)
Does the combination of reformulations with pictures prevent an increase in error rates when the (common or technical) reformulation does not match the current situation?

(2)
Does the combination with pictures result in higher subjective ratings? Does it eliminate a possible dispreference of technical terms as they get more understandable when pictures are added?

(3)
How do reformulations including common terms, technical terms, or both affect subjects’ verbal alignment with the terms used by the DS when later referring to the same fault symptoms again?

4. Experiment 1
Using a Wizard-of-Oz paradigm, Experiment 1 investigated the effects of three types of reformulations of symptom descriptions: reformulations provided by the DS in either common or technical terms, and requests for subjects to reformulate their own descriptions. Reformulations in common terms restate the subject's description in simple everyday language. Presumably, their main advantage is that they are easy to understand and their interpretation does not presuppose knowledge about the machine or process. Such reformulations are likely to be beneficial in domains such as food processing, where operator qualification tends to be low. However, when describing faults in common terms rather than using their proper names, descriptions can get quite long. Moreover, they have no pre-defined meaning. Similar common terms can be used to describe different faults. In contrast, reformulations in technical terms refer to faults by their proper names and have complementary benefits and costs. On the one hand, they are succinct and highly specific. On the other hand, if subjects do not know the fault's name, such reformulations can be hard to understand. Finally, requests for self-correction allow subjects to use the words they see fit. However, their main disadvantage is that they require additional verbal effort.

Accordingly, we hypothesised that when the DS presents reformulations in common terms, solution times will be shorter and subjects will need the DS to offer fewer reformulations as they can already understand its first reformulation. Moreover, as they are easy to understand, common terms were expected to support accurate performance in trials where reformulations are incorrect. For reformulations in technical terms, subjects have two options when being unsure about their meaning. First, they can simply accept the reformulation, which should lead to high error rates when reformulations are incorrect. However, in a previous study we have found that during fault diagnosis subjects are not likely to uncritically accept suggestions provided by an assistance system (Müller et al., 2019; Müller et al., 2020). Therefore, the second option is for subjects to frequently ask for clarification of the technical terms, and thus require more dialogue turns. In either case, the use of technical terms was expected to be costlier than common terms. Finally, in line with previous research (Bohus and Rudnicky, 2005) we expected requests for self-correction to be least beneficial. Although addressees are more likely to ask for clarification than to correct speaker utterances (Colman and Healey, 2011), unspecific requests for self-correction do not comply with the principle of least collaborative effort (Clark and Wilkes-Gibbs, 1986) which requires partners to formulate repair requests as specifically as possible (Dingemanse et al., 2015). Therefore, self-corrections were expected to slow down performance, increase dialogue turns, and be rated least favourably.

4.1. Methods
4.1.1. Subjects
Eighteen students of the Technische Universität Dresden (12 female) in the age range of 18–27 years (M = 21.4, SD = 2.9) participated in the experiment in exchange for course credit or a payment of 5€ per hour. All procedures followed the ethical principles of the Declaration of Helsinki. Subjects were informed about their rights to anonymity, to withdraw from the experiment without any negative consequences, to have their data removed on request, as well as about the procedures of data handling.

4.1.2. Apparatus and stimuli
The faulty product items to be described were available as physical objects, one of which was handed to subjects before each trial. They were products of three packaging machines: a drawing machine for paperboard shapes, a thermoforming machine for plastic yoghurt cups, and a thermoforming machine for plastic cheese packages sealed with a plastic lid (see Fig. 1, top). For each product type, one correctly produced reference item and eight faulty items were used. All faults were reflected in an incorrect product geometry (e.g., holes, ruptures, wrinkles, incomplete forming, open seals), which is a very common problem in a variety of packaging processes (Müller and Oehm, 2019).

Fig. 1
Download : Download high-res image (638KB)
Download : Download full-size image
Fig. 1. Examples for faulty products and the DS interface. The upper part of the figure presents a paperboard shape (top left), a yoghurt cup (top middle), and a cheese package (top right). The lower part presents an extract from a dialogue (bottom) between the DS (left, orange boxes) and the subject (right, brownish boxes). English translations are given in italics and were not present during the experiment. Note that the dialogue extract in the lower part does not refer to the three shapes in the upper part. (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.)

The DS was a chat application programmed in LiveCode (LiveCode Ltd., 2001). Note that this system was not an actual DS capable of interpreting human utterances. Instead, the system displayed the subject's symptom description to an experimenter in another room so that she could select appropriate replies. Subjects typed text messages and the DS also replied with text messages that the experimenter selected by double-clicking on the respective line in a text file, which transmitted the selected line to the subject. These messages were presented in German and their specific contents are described in the Procedure section. In the subject's view of the chat system, messages by the DS were presented in orange boxes at the left side of a message window, while subjects’ own messages appeared in brownish boxes at the right side (see Fig. 1, bottom). Below the message window there was a text box in which subjects could type their messages and send them by pressing a ‘send’ button to the right of the text box, marked with an arrow. Moreover, ‘yes’ and ‘no’ buttons allowed subjects to accept or reject the reformulations provided by the DS.

4.1.3. Experimental conditions and measures
An overview of the experimental conditions and measures is provided in Table 1. The conditions (i.e., independent variables) that were varied in Experiment 1 were dialogue condition and trial type. First, the factor dialogue condition varied how the DS asked for clarification. Three dialogue conditions were used. For reformulations in common terms, the DS asked a question starting with the words ‘So you mean…?’ and then added an easily understandable description of the fault in everyday language (e.g., ‘So you mean that the upper edge is higher at one side and has a hole in it?’). The average length of reformulations in common terms was 13.3 words (SD = 4.4, range = 7–16). For reformulations in technical terms, the DS asked a question starting with the words ‘So you mean…?’ and then added the technical name of the fault (e.g., ‘So you mean there is a local edge rupture?’). Technical terms were hard to understand without previous knowledge (e.g., earing, incomplete sealing edge, perforation of the lateral surface) and consisted of 1–3 content words plus the additional words required to form a grammatical sentence. Taken together, the average length of reformulations in technical terms was 7.8 words (SD = 0.9, range = 6–10). For self-corrections, the DS asked ‘What do you mean?’ and subjects had to provide a second description.


Table 1. Overview of conditions (independent variables) and measures (dependent variables).

Variable	Definition	Levels or units
IV	Dialogue condition	DS method of asking for clarification	Common terms, technical terms,
self-correction
Trial type	Correctness or incorrectness of DS reformulation	Standard trials,
catch trials
DV	Solution time	Time needed to complete a conversation	Seconds
Number of reformulations	DS reformulations needed to complete a conversation	Number
Error rate	Conversations that do not reach a solution	Percent
Ratings of dialogue conditions	How helpful, precise, cumbersome, and self-explanatory subjects rated the dialogues	5-point Likert scale
Note: IV = independent variable, DV = dependent variable.

Second, the factor trial type varied whether the reformulation provided by the DS was correct or incorrect. In standard trials, the reformulation was correct (i.e., matched the fault that subjects had just described), while in catch trials the reformulation was incorrect (i.e., corresponded to another fault of the same machine).

To investigate the effects of these conditions, four measures (i.e., dependent variables) were compared between conditions. First, solution time denotes how long it takes to complete a conversation. Second, the number of reformulations denotes how often the DS needed to ask for clarification in order to complete a conversation. Third, error rates were compared to investigate how many conversations did not reach a solution within the available dialogue turns. Fourth, subjects provided ratings of the dialogue conditions on a customised four item questionnaire. The following items were used: “How helpful was the dialogue system?”, ”How precise was the dialogue system?”, “How cumbersome was the progression of the dialogue?”, “How self-explanatory was the progression of the dialogue?” Ratings were provided on 5-point Likert scales with the poles “not at all” and “very much”.

4.1.4. Procedure
Upon arriving in the lab, participants provided informed consent and demographic data before receiving an instruction about the experimental procedure. The experiment consisted of three blocks corresponding to the three dialogue conditions. One product type was used for each dialogue condition, and the assignment of product types to dialogue conditions was counterbalanced across subjects. Block order was also counterbalanced across subjects. In each block, eight trials corresponding to eight faults were presented in random order. Six of them (75%) were standard trials and two (25%) were catch trials, with their position being determined randomly.

The events within a trial are presented in Fig. 2. Each trial started with the DS asking ‘What fault can you see in the product?’. Subjects typed their description, and subsequently the DS performed a reformulation according to one of three dialogue conditions. Unbeknownst to subjects, all reformulations were canned messages that were selected and sent by an experimenter. For each trial, this experimenter had two reformulations, a solution, and the message ‘We will continue with something else’ available for selection. Following the subject's initial description, the experimenter selected the first reformulation, or immediately proceeded to the solution if the subject's description already matched the first reformulation. This was done to avoid awkward exchanges such as the subject saying ‘The shape has a hole at the bottom’ and the DS replying ‘So you mean that the shape has a hole at the bottom?’. Additionally, the experimenter could send ‘I beg your pardon?’ if subjects’ descriptions did not describe any specific fault (e.g., ‘The product is not okay’).

Fig. 2
Download : Download high-res image (328KB)
Download : Download full-size image
Fig. 2. Events within a trial in Experiment 1. In the second line (first reformulation), green corresponds to standard trials and red corresponds to catch trials. In the fourth line, green indicates that all second reformulations were correct. Likewise, green arrows indicate correct responses and red, dashed arrows indicate incorrect responses (see text for details). (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.)

Subjects were required to judge the reformulation. They could do so by pressing the ‘yes’ or ‘no’ buttons, which made the word ‘yes’ or ‘no’ appear in the text box. They also were free to include additional text (e.g., explain their judgement), or only write text without pressing ‘yes’ or ‘no’ (e.g., reformulate their previous utterance). If subjects pressed ‘yes’ in standard trials, the trial was classified as correct and a solution was provided (e.g., ‘The solution for the problem probably is: increase the punch force to 20,000 N’). If subjects pressed ‘yes’ in catch trials, the trial was classified as an error and ended with the DS saying ‘We will continue with something else’. If subjects pressed ‘no’ in standard trials or in catch trials or only added text without explicitly providing a confirming or disconfirming answer, the DS proceeded with a second reformulation. This reformulation was always correct and presented in common terms in all dialogue conditions. Again, subjects were required to judge the reformulation in the same way as before. If they pressed ‘yes’, the trial was classified as correct and the solution was provided, while if they pressed ‘no’ or only added text without confirming or disconfirming the reformulation, the trial was classified as an error and the DS replied ‘We will continue with something else’. In both cases, the next trial followed with a delay of one second. After each block, subjects rated the dialogues on four scales. A post-experimental interview was conducted to ask about subjects’ general impression of the experimental conditions and any difficulties they encountered while performing the task. In total, the experiment took about one hour.

4.2. Results
To provide a first overview of the communication process and how it was affected by the strategy pursued by the DS, Fig. 3 represents the events within a trial for each dialogue condition. An inspection of the figure provides two main insights. First, a higher percentage of trials was already terminated after the first reformulation with common terms than with technical terms, while with self-correction the vast majority of trials required all three turns. Second, the percentage of errors was higher with common terms than with technical terms, particularly those errors that occurred in response to the first reformulation. With self-correction, errors did not occur before the third turn, which follows logically from the fact that in this condition subjects did not have to make a decision until then.

Fig. 3
Download : Download high-res image (715KB)
Download : Download full-size image
Fig. 3. Events within a trial for each dialogue condition. The vertical axis represents the succession of events and the horizontal axis represents the percentage of trials in which an event occurred. Contributions by the DS and the subject are presented in dark and light grey colour, respectively. Below each contribution by the subject, green bars indicate how many trials were ended successfully after this turn and red bars indicate how many were terminated unsuccessfully. (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.)

For all experimental conditions, normal distribution was assessed with the Kolmogorov–Smirnov test and sphericity was assessed with the Mauchly test. Although normal distribution was not given for most dependent variables and sphericity was not given for some of them, ANOVAs were still used as they are considered robust against violations of these assumptions (Blanca et al., 2018; Schmider et al., 2010). To statistically analyse solution times, number of reformulations, and error rates, 3 (dialogue condition: common terms, technical terms, self-correction) × 2 (trial type: standard trial, catch trial) repeated measures ANOVAs were computed. All pairwise comparisons were performed with Bonferroni correction. In case of non-sphericity, the Greenhouse–Geisser correction was applied and the degrees of freedom were corrected accordingly.

4.2.1. Solution times
The Kolmogorov–Smirnov test revealed deviations from normal distribution for common terms in standard trials and for technical terms in catch trials. The ANOVA revealed a main effect of dialogue condition on mean solution times, F(1.517,25.782) = 18.928, p < .001, ηp² = 0.527, but no main effect of trial type, F(1,17) = 1.515, p = .235, ηp² = 0.082, and no interaction, F(2,34) = 0.497, p = .613, ηp² = 0.028 (see Fig. 4A). Solutions were faster with reformulations in common terms (M = 111 s, SD = 38) than with self-corrections (M = 165 s, SD = 53), p < .001, and faster with technical terms (M = 127 s, SD = 39) than with self-corrections, p = .004, while common terms and technical terms did not differ significantly, p = .057.

Fig. 4
Download : Download high-res image (235KB)
Download : Download full-size image
Fig. 4. Results of Experiment 1 as a function of dialogue condition and trial type. (A) Mean solution time, (B) mean number of reformulations per trial, and (C) mean error rates. CT = common terms, TT = technical terms, SC = self-correction. Error bars represent standard errors of the mean.

4.2.2. Number of reformulations
To investigate how dialogue conditions affected dialogue length, we analysed the average number of reformulations performed by the DS in one trial. If this number is close to one, this indicates that the DS's first reformulation already was perceived to be sufficiently understandable by subjects so that they accepted it (in standard trials) or rejected it (in catch trials) without any need for further clarification. The Kolmogorov–Smirnov test revealed deviations from normal distribution in all experimental conditions. The ANOVA revealed a main effect of dialogue condition, F(2,34) = 15.045, p < .001, ηp² = 0.469, but no main effect of trial type, F(1,17) < 0.001, p = .983, ηp² < 0.001, and no interaction, F(2,34) = 0.981, p = .358, ηp² = 0.055 (see Fig. 4B). Fewer reformulations were needed with common terms (M = 1.10, SD = 0.42) than with self-corrections (M = 1.71, SD = 0.41), p < .001, and fewer with technical terms (M = 1.37, SD = 0.48) than with self-corrections, p = .018, while common terms and technical terms did not differ significantly, p = .116. However, as can be seen from the error bars in Fig. 4B, this result can be attributed to the large variance in catch trials. When comparing the dialogue conditions only for standard trials, there were significant differences between common terms and technical terms, p = .002, between common terms and self-corrections, p < .001, as well as between technical terms and self-corrections, p < .001. Thus, technical terms also increased dialogue length relative to common terms.

4.2.3. Error rates
The Kolmogorov–Smirnov test revealed deviations from normal distribution in all experimental conditions. The ANOVA revealed significant main effects of dialogue condition on mean error rate, F(2,34) = 4.969, p = .013, ηp² = 0.226, and trial type, F(1,17) = 20.025, p < .001, ηp² = 0.541, as well as an interaction of both factors, F(2,34) = 5.276, p = .010, ηp² = 0.237 (see Fig. 4C). Errors were less frequent in standard trials (M = 4.9%, SD = 9.0) than in catch trials (M = 22.2%, SD = 27.3). However, an unexpected finding was that reformulations in common terms (M = 22.2%, SD = 21.1) yielded higher error rates than reformulations in technical terms (M = 11.6%, SD = 18.8), or self-corrections (M = 6.9%, SD = 14.6), although only the difference to self-corrections was significant, p = .015. Moreover, the interaction indicated that this difference was limited to catch trials, p = .005, while in standard trials error rates were comparable for all three dialogue conditions, all ps > .9. For reformulations in common terms, a performance decrement of 33.3% more errors in catch trials than in standard trials was observed, p < .001, while performance did not significantly differ between standard and catch trials for reformulations in technical terms, p = .063, or for self-corrections, p = .484.

4.2.4. Stimulus-specific effects
In the last step, we descriptively analysed to what degree the results presented above were specific to particular stimuli. To this end, we plotted the three dependent variables solution time, number of turns, and error rates for each stimulus (see Fig. 5). Overall, the figure shows that the pattern of results found in the ANOVAs was fairly consistent: Solution times were longer with self-corrections than in the two reformulation conditions, reformulations occurred more often with technical terms than with common terms as well as more often with self-corrections than in the two reformulation conditions, and error rates were similarly low for all dialogue conditions in standard trials while being highest with common terms in catch trials.

Fig. 5
Download : Download high-res image (427KB)
Download : Download full-size image
Fig. 5. Results of Experiment 1 for each stimulus as a function of dialogue condition and trial type. (A) Mean solution time, (B) mean number of reformulations, and (C) total error rate. CT = common terms, TT = technical terms, SC = self-correction, Yo = yoghurt, Ch = cheese, Pa = paperboard. Error bars represent standard errors of the mean.

However, Fig. 5 also reveals that the latter result varied between stimuli: Three of the six stimuli used in catch trials produced higher error rates for common terms than technical terms. This difference was particularly high for one stimulus (Pa7), which yielded 83.3% errors with common terms and 0% with technical terms. Also note that for this stimulus every single subject sought additional evidence for mutual understanding with technical terms instead of accepting the first reformulation. The common term used for this stimulus provided an analogy, stating that the edge of the shape looked “wavy”. Taken together, the descriptive analysis indicated that when interpreting the results, it should be considered that the particular stimulus material can play an important role.

4.2.5. Rating of the dialogues
The Kolmogorov–Smirnov test revealed deviations from normal distribution in all experimental conditions. Ratings were compared using one-way repeated measures ANOVAs with the within-subjects factor dialogue condition (common terms, technical terms, self-correction). An overview of the results is provided in Table 2. Subjects considered self-corrections less helpful, p = .010, and less self-explanatory, p = .24, than reformulations in common terms, and they considered them more cumbersome than common terms, p < .001, and technical terms, p = .033.


Table 2. Mean subjective ratings on a 5-point scale and standard deviations (in parentheses) of the dialogue conditions in Experiment 1.

CT	TT	SC	F(2,34)	p	ηp²
Helpful	3.8 (0.9)	3.7 (0.7)	3.0 (1.0)	6.491	.004	0.276
Precise	3.7 (1.0)	3.7 (1.0)	3.2 (0.9)	2.053	.144	0.108
Cumbersome	2.4 (0.8)	2.7 (0.8)	3.6 (1.0)	11.295	< .001	0.399
Self-explanatory	4.2 (0.6)	3.8 (1.0)	3.7 (0.8)	3.400	.045	0.167
Note: CT = common terms, TT = technical terms, SC = self-correction.

4.3. Discussion
The results of Experiment 1 revealed that reformulations in common terms allowed for efficient performance, as reflected in short solution times and low numbers of reformulations required to complete a trial. Trials with common terms were not significantly faster and more efficient than trials with technical terms, although this can partly be attributed to the high variance in catch trials. When restricting the comparison between common and technical terms to standard trials, common terms required fewer reformulations, indicating that less verbal effort was needed to establish mutual understanding. Instead, technical terms were not accepted uncritically but led subjects to engage in higher verbal effort to understand the reformulation and maintain a high level of accuracy. This result is in line with previous research (Müller et al., 2019; Müller et al., 2020).

As expected, requests for self-correction slowed down solutions and required more dialogue turns, although they resulted in accurate performance. However, the latter result is not surprising, because with self-corrections subjects could not possibly experience catch trials, as they performed the first reformulation by themselves. Accordingly, standard and catch trials were indistinguishable to them. Subjective ratings mirrored the performance findings, also indicating difficulties with self-corrections. They were rated as less helpful and self-explanatory than reformulations with common terms, and more cumbersome than both reformulation conditions. At first glance, the unfavourable outcomes for self-corrections seem at odds with psycholinguistic research showing that human speakers prefer self-repair and addressees are more likely to ask for clarification instead of correcting the speaker (Colman and Healey, 2011). Conversely, our subjects clearly did not appreciate self-corrections. This discrepancy could result from at least two sources. First, people may like self-repair in human communication because addressees transparently display implicit evidence of their understanding (e.g., by looking puzzled), in contrast to DS that usually fail to provide such evidence. In consequence, it is difficult to know whether and when a DS has trouble interpreting an utterance, and people inevitably have to rely on other-/DS-initiated repair. Second, requests for self-correction were open and unspecific, requiring subjects to completely redesign their description. Such open requests violate the principle of least collaborative effort (Clark and Wilkes-Gibbs, 1986; Dingemanse et al., 2015), and it is possible that subjects would have liked self-corrections more if the DS had pointed out the trouble source more specifically when asking for self-corrections.

The most unexpected finding of Experiment 1 was that in catch trials common terms produced the highest error rates. Catch trials are situations in which a seemingly simple reformulation by the DS is incorrect. Common terms are not very specific, and presumably in everyday language subjects do not clearly differentiate between similar terms (e.g., hole and slash) or use analogies that only vaguely describe the symptom (e.g., wavy). Consequently, they accept reformulations that might be judged as fitting the symptom in everyday use but are defined differently in the DS. However, the results also reveal that this outcome depended on the characteristic of specific stimuli — a point that will be addressed in the General Discussion. Taken together, the results indicate that reformulations in technical terms are less problematic than expected, while common terms also have their problems. Therefore, the question for Experiment 2 was how to make use of the benefits of both conditions while avoiding their costs.

5. Experiment 2
Experiment 1 revealed that common terms can be misleading when DS use them in a highly symptom-specific way, while subjects do not differentiate as clearly between similar terms in everyday language. Therefore, a way to disambiguate common terms is to enrich them with visual information about the fault symptom. Visual information can support grounding by constraining referential domains, restricting the possible meanings of a verbal expression (Brennan, 2004; Clark and Krych, 2004). Consequently, visual information allows for linguistic shortcuts and simpler language (Gergle et al., 2013). Conversational partners can use brief, underspecified expressions as the relevant objects can be seen and thus do not need to be described verbally (Clark and Krych, 2004). This suggests that even when reformulations in common terms are ambiguous, pictures of fault symptoms can mitigate the resulting problems, providing the information that is missing in the verbal description. Moreover, visual information should not only be able to disambiguate common terms but also make technical terms easier to understand.

The second question of Experiment 2 was whether and how subjects align with DS reformulations. Such alignment would result in a restriction of subjects’ utterances to terms a DS can interpret. Therefore, Experiment 2 asked which of the dialogue conditions were most effective in encouraging alignment. Different outcomes are conceivable. On the one hand, common terms are more familiar, which might result in a higher likelihood of adopting them. On the other hand, technical terms are shorter, which might make it easier to learn and reuse them later in the dialogue. Indeed, previous research indicates that it is easier to align with a computer's brief utterances than with its complete sentences, both in terms of utterance length and vocabulary (Zoltan-Ford, 1991): subjects interacting with a DS that generated brief utterances produced 40% more conforming messages than those interacting with a DS that generated complete sentences. Also, adjustment to the DS message style (brief vs. complete sentences) was independent of whether the vocabulary was familiar or unfamiliar, and other studies have found that people align with a partner's referring expressions even when they are dispreferred or untypical (Goudbeek and Krahmer, 2012). Together, these results suggest that alignment might actually be higher with technical terms. Moreover, it is interesting to study how the presence of visual information affects alignment. In a Wizard-of-Oz experiment investigating alignment with a robot's terms (Koulouri et al., 2016), pictures of the robot's workspace led subjects to match the robot's utterances less and use more new words instead. Conversely, without pictures subjects had to rely on alignment more strongly to ensure successful communication.

To investigate these issues, we used three ways of combining reformulations: enriching either common or technical terms with pictures of the fault symptom, or combining technical and common terms. First, we hypothesised that all three conditions would facilitate judgments of whether a reformulation corresponds to the present fault, leading to lower error rates in catch trials than in Experiment 1. Second, in line with previous research on the effectiveness of visual information, we expected pictures to be particularly helpful, leading to low or absent increases in error rates in catch trials relative to standard trials in both conditions including pictures. Third, we expected differences between dialogue conditions in subjects’ alignment with the DS. As technical terms are more succinct, reformulations including technical terms should result in higher alignment – presumably, it is easier to remember and repeat a single word or phrase than a complete sentence description. This was investigated by having subjects describe the same faults a second time after having read the reformulation, and analysing whether they would use the terms used by the DS.

5.1. Methods
5.1.1. Subjects
Eighteen students of the Technische Universität Dresden (14 female) in the age range of 19–45 years (M = 29.1, SD = 8.9) participated in the experiment in exchange for course credit or a payment of 5€ per hour. The performance of two subjects indicated that they had not understood the instructions and thus their datasets were excluded and replaced by two new subjects. All procedures followed the ethical principles of the Declaration of Helsinki.

5.1.2. Apparatus and stimuli
The same faulty shapes, common terms, and technical terms were used as in Experiment 1. In contrast to Experiment 1, the DS selected its replies automatically and no second experimenter was present. Thus, the selection of messages was identical for all subjects and independent of their verbal descriptions. Otherwise, the DS was similar to that used in Experiment 1, except for the following changes. To judge the DS reformulation, subjects were required to press one of three buttons (‘yes’, ‘no’, or ‘uncertain’, replacing the text box below the message window when the reformulation was provided). This button press immediately advanced the trial, without subjects having the opportunity to write additional text. Moreover, in two dialogue conditions the DS provided a picture of the faulty shape along with its verbal reformulation. This picture was presented to the right of the message window (13.4 × 8.7 cm). Pictures were idealised greyish drawings of the faulty shapes with the fault area marked in red (see Fig. 6).

Fig. 6
Download : Download high-res image (239KB)
Download : Download full-size image
Fig. 6. Example pictures of the faults, corresponding to the faults presented in Fig. 1.

5.1.3. Experimental conditions and measures
An overview of the experimental conditions and measures is provided in Table 3. The conditions (i.e., independent variables) that were varied in Experiment 2 were dialogue condition and trial type. First, the factor dialogue condition varied how the DS asked for clarification. Three dialogue conditions were used. For descriptions in common terms & pictures, a reformulation in common terms was accompanied by a picture illustrating the fault. For reformulations in technical terms & pictures, the DS presented a reformulation in technical terms along with a picture. For reformulations in technical terms & common terms, the DS first provided a reformulation in technical terms, which was followed by a description in common terms after a short delay of about one second. This second description restated the technical term and provided an explanation (e.g., ‘So you mean that there is a local edge rupture?’ & ‘Local edge rupture: The upper edge is higher at one side and has a hole in it’). Note that the dialogue conditions do not change only one variable of interest, neither in a specific sense (e.g., presence of pictures, presence of technical terms) nor in an abstract sense (e.g., familiarity, precision). Instead, they provide different combinations of reformulation components that are characterised by distinctive features, in order to investigate the effects of these combinations.


Table 3. Overview of conditions (independent variables) and measures (dependent variables).

Variable	Definition	Levels or units
IV	Dialogue condition	DS method of asking for clarification	Common terms & pictures, technical terms & pictures, common terms & technical terms
Trial type	Correctness or incorrectness of DS reformulation	Standard trial,
catch trial
DV	Error rate	Conversations that do not reach a solution	Percent
Alignment trialwise	Overlap between DS reformulation and subsequent subject utterance for a specific fault	Number of content words
Alignment trialwise corrected	Overlap for a specific fault, corrected for the length of DS reformulations	Number of content words divided by word number of DS reformulation
Alignment overall	Subjects’ overall use of DS technical terms across trials	Number of DS technical terms
Ratings of dialogue conditions	How helpful and understandable reformulations have been, whether it has been easy to compare them to subjects’ own descriptions, and how much subjects have learned	5-point Likert scale
Note: IV = independent variable, DV = dependent variable.

Second, the factor trial type varied whether the reformulations provided by the DS were correct or incorrect. In standard trials, the reformulation was correct (i.e., matched the fault that subjects had just described), while in catch trials, both elements of the reformulation (e.g., technical terms and picture) corresponded to a fault different from the one that subjects had been given.

To investigate the effects of these conditions, three measures (i.e., dependent variables) were compared between conditions. First, error rates denote how many conversations do not reach a solution. Second, alignment assessed the degree to which subjects adopted the terms used by the DS, and thus denotes the number of overlapping words between the DS reformulation and the subject's subsequent description. Three forms of alignment were compared between dialogue conditions: (a) overlap of content words, (b) overlap of content words corrected for the length of DS reformulations, (c) overall frequency of using DS technical terms. While the first two forms of alignment include all content words and are restricted to specific trials, the third form considers DS technical terms that were later used by subjects, irrespective of the trial in which they occurred. Details about the analyses are provided in the respective parts of the Results section. Third, subjects rated the reformulations on a customised four item questionnaire. The following items were used: “How much did the reformulations help you during fault diagnosis?”, “How easily could you understand the reformulations?”, “How easily could you compare the reformulations to your own description?”, “How much did you learn during the dialogue (e.g., technical terms, names of components)?” Ratings were provided on 5-point Likert scales with the poles “not at all” and “very much”.

5.1.4. Procedure
Upon arriving in the lab, participants provided informed consent and demographic data before receiving an instruction about the experimental procedure. The experiment consisted of two phases. In the first phase, subjects described the faulty shapes and judged the DS's reformulations of these descriptions. The number, identity, and ordering of blocks and trials was identical to Experiment 1. There were eight blocks, each consisting of six standard trials and two catch trials. Just like in Experiment 1, a trial started with the DS asking ‘What fault can you see in the product?’, subjects typed a description into their text box, and the DS performed a reformulation in one of three ways, corresponding to three dialogue conditions. Following the DS reformulation, subjects could not type new text, as the text box was replaced by three buttons from which they had to select (‘yes’, ‘no’, or ‘uncertain’) to judge the reformulation. If this response was correct (i.e., subjects pressed ‘yes’ in standard trials or ‘no’ in catch trials), the DS provided the solution. If the response was incorrect (i.e., subjects pressed ‘no’ in standard trials, ‘yes’ in catch trials or ‘uncertain’), the DS responded ‘We will continue with something else’ and proceeded to the next trial. After each block, subjects rated the reformulations on four scales.

In the second phase, subjects were shown the same faulty shapes again, in the same order as in the first phase, and were asked to provide a description for each. Only shapes from standard trials were used, resulting in 18 trials (3 dialogue conditions × 6 shapes). The DS started a trial by asking ‘What fault can you see in the product?’, and subjects had to type their description and press the ‘send’ button. Regardless of description content, the DS replied ‘We will continue with something else’ and proceeded to the next trial. A post-experimental interview asked about subjects’ general impression of the experimental conditions and any difficulties in performing the task. Overall, the experiment took about one hour.

5.2. Results
Solution times and the number of turns were not analysed, as the dialogue procedure was fixed, so that solution times were mainly affected by typing speed and utterance length and turn numbers were identical for all dialogue conditions. Therefore, in terms of performance we only analysed mean error rates, and instead focused our analyses on subjects’ verbal descriptions. These analyses are described in the respective sections. All pairwise comparisons were performed with Bonferroni correction. In case the Mauchly test for sphericity was significant, the Greenhouse–Geisser correction was applied and the degrees of freedom were corrected accordingly.

5.2.1. Error rates
The Kolmogorov–Smirnov test revealed deviations from normal distribution in all experimental conditions. To analyse mean error rates, a 3 (dialogue condition: common terms & pictures, technical terms & pictures, technical terms & common terms) × 2 (trial type: standard trial, catch trial) repeated measures ANOVA was computed. It revealed a main effect of trial type, F(1,17) = 4.884, p = .041, ηp² = 0.223, but no effect of dialogue condition, F(1.503,25.543) = 1.793, p = .192, ηp² = 0.095, and no interaction, F(2,34) = 1.628, p = .211, ηp² = 0.087 (see Fig. 7A). Error rates were lower in standard trials (M = 3.7%, SD = 7.8) than in catch trials (M = 13.0%, SD = 25.28). Although there was no significant interaction, this performance drop in catch trials was only significant in the technical terms & common terms condition, p = .028, but not with common terms & pictures, p = .500, or technical terms & pictures, p = .384. An analysis of the ‘uncertain’ judgments yielded a main effect of trial type, F(1,17) = 6.398, p = .022, ηp² = 0.273, but no effect of dialogue condition, F(2,34) = 0.736, p = .486, ηp² = 0.041, and no interaction, F(2,34) = 0.025, p = .976, ηp² = 0.001. While subjects more often indicated that they were uncertain in catch trials (M = 17.6%, SD = 26.2) than in standard trials (M = 7.7%, SD = 12.6), this did not differ between the three dialogue conditions.

Fig. 7
Download : Download high-res image (212KB)
Download : Download full-size image
Fig. 7. Results of Experiment 2. (A) Mean error rates, (B) mean number of overlapping words per description, and (C) mean overall number of technical terms used by subjects in their second description. CT & Pic = common terms & pictures, TT & Pic = technical terms & pictures, TT & CT = technical terms & common terms. Error bars represent standard errors of the mean.

5.2.2. Alignment with the dialogue system
Alignment denotes the overlap between subjects’ description in phase 2 and the preceding reformulation of the same fault by the DS in phase 1. It was computed as the number of content words (or their word stems) that were present in both utterances. To minimise subjectivity in the rating of what counts as a relevant word, all content words were included, even when they were not specific to the fault description (e.g., ‘product’, ‘lid’), but non-content words that did not describe any features of the product or fault were not considered (e.g., ‘the’, ‘and’, ‘looks like’). Overlapping words were determined via string matching, and given the simplicity of the task (i.e., describing a specific feature of a faulty shape), producing a non-overlapping utterance usually means that subjects used a synonym. A first inspection of this overlap reveals that alignment was quite low: for their second descriptions, subjects only adopted an average of 1.1 terms from the reformulation provided by the DS. For comparison, with an average of 2.2 words from their own first description, they used twice as many of their own words than of the words presented by the DS.

To test whether the amount of alignment differed between the three dialogue conditions, the mean number of overlapping words was subjected to a one-way repeated measures ANOVA with the within-subjects factor dialogue condition (common terms & pictures, technical terms & pictures, technical terms & common terms). First, we computed this alignment as the mean number of overlapping words per description. The Kolmogorov–Smirnov test revealed deviations from normal distribution for technical terms & common terms. The ANOVA revealed a main effect of dialogue condition, F(2,34) = 18.050, p < .001, ηp² = 0.515 (see Fig. 7B). It indicated that the overlap was lower with technical terms & pictures (M = 0.58 words, SD = 0.65) than with common terms & pictures (M = 1.30 words, SD = 0.66), p < .001, and lower with technical terms & pictures than with technical terms & common terms (M = 1.50 words, SD = 0.55), p < .001, while the two dialogue conditions including common terms did not differ, p = .910. Thus, at first glance it seems that alignment is higher when the DS uses common terms.

However, when interpreting this result, it needs to be considered that those DS reformulations that included only technical terms (M = 7.88 words, SD = 0.88) were much shorter than reformulations in common terms (M = 13.29 words, SD = 4.44) or combinations of technical and common terms (M = 20.03 words, SD = 5.01). Therefore, the calculation of alignment should account for these differences in word counts, because otherwise there is more potential for alignment with reformulations using common terms due to their sheer length. Accordingly, the number of overlapping words between subjects’ second descriptions and their corresponding DS reformulations was divided by the total number of words included in the DS reformulation. When correcting for word number in this way, the Kolmogorov–Smirnov test indicated that the data in all experimental conditions were normally distributed. In the ANOVA, the effect of dialogue condition disappeared completely, F(2,34) = 0.205, p = .816, ηp² = 0.012, indicating no differences in trialwise alignment between the three dialogue conditions.

However, a closer inspection of the descriptions revealed that it is too short-sighted to restrict the alignment analysis to a comparison of each specific subject description in phase 2 with the corresponding DS reformulation that preceded it. In many cases, subjects did in fact adopt the technical terms used by the DS, but this usage was not restricted to particular trials but was reflected in a more general usage of technical terms. Therefore, we compared subjects’ overall frequency of using DS technical terms in phase 2, irrespective of the trial in which they were used. The Kolmogorov–Smirnov test revealed deviations from normal distribution in all experimental conditions. The ANOVA revealed a main effect of dialogue condition, F(2,34) = 4.371, p = .020, ηp² = 0.205 (see Fig. 7C), indicating that technical terms were used more often when the DS provided technical terms & pictures (M = 2.39 words, SD = 2.61) than when it provided common terms & pictures (M = 0.44 words, SD = 2.30) or common terms & technical terms (M = 0.72 words, SD = 1.96). However, despite the overall main effect, the pairwise comparisons did not reach the significance level: There were no significant differences between common terms & pictures and technical terms & pictures, p = .065, between common terms & pictures and common terms & technical terms, p > .9, or between technical terms & pictures and common terms & technical terms, p = .184. Moreover, it needs to be noted that the overall frequency of using technical terms was quite low. Even in the condition with technical terms & pictures, subjects only used technical terms 2.4 times per experiment on average, although they had the opportunity to use them in six trials.

5.2.3. Rating of the dialogues
The Kolmogorov–Smirnov test revealed deviations from normal distribution in all experimental conditions. The mean ratings were compared using one-way repeated measures ANOVAs with the within-subjects factor dialogue condition (common terms & pictures, technical terms & pictures, technical terms & common terms). An overview of the results is provided in Table 4. Subjects considered reformulations in common terms & pictures more helpful, p < .001, and easier to understand, p = .033, than reformulations in technical terms & common terms. They also found it easier to compare their own descriptions to reformulations with common terms & pictures than with technical terms & pictures, p = .010, or common terms & technical terms, p < .001. Moreover, subjects perceived no difference in how much they had learned in the three conditions, all ps > .9. Finally, when asked about their favourite dialogue condition, 14 subjects (78.8%) preferred common terms & pictures, three subjects (16.7%) preferred technical terms & pictures, and only one subject (5.6%) preferred technical terms & common terms.


Table 4. Mean subjective ratings on a 5-point scale and standard deviations (in parentheses) of the dialogue conditions in Experiment 2.

CT & Pic	TT & Pic	TT & CT	F(2,34)	p	ηp²
Helpful	4.6 (0.6)	4.0 (0.9)	3.4 (1.0)	9.659	<.001	0.362
Understandable	4.1 (0.8)	3.5 (1.0)	3.4 (0.8)	3.986	.028	0.190
Comparable	4.2 (0.7)	3.3 (1.0)	2.9 (1.0)	10.865	<.001	0.390
Learning	2.5 (0.9)	2.4 (0.9)	2.6 (1.0)	0.143	.867	0.008
Note: CT = common terms, TT = technical terms, Pic = pictures.

5.3. Discussion
Experiment 2 combined common and technical terms, or combined either of them with pictures of the fault symptom, in order to investigate which of these combinations would best support performance and lead to the highest alignment with the reformulations by the DS. Errors were committed in 13% of the catch trials, which is markedly less than the 22% observed in Experiment 1. Still, it is interesting that subjects still committed so many errors, given the high amount of information they received. However, a closer look at the data revealed that catch trials only deteriorated performance with common terms & technical terms but not when pictures were presented. This is in line with previous demonstrations of the beneficial effects of visual information on cooperative performance and grounding (Brennan, 2004; Clark and Krych, 2004; Gergle et al., 2013). It also is in line with subjects’ preferences: only one subject indicated to have preferred reformulations in technical terms & common terms, while all other subjects preferred a dialogue condition including pictures.

The second question of Experiment 2 was whether subjects would adopt the terms used by the DS when re-referring to fault symptoms. Interestingly, such alignment was quite limited. Subjects only adopted about one content word per description from the DS reformulations—only half as much as they adopted from their own first descriptions. Why was alignment so low? A first reason might be that the DS required written instead of spoken dialogues. However, this explanation is unlikely as previous research has shown that alignment is similar for text- and speech-based interaction (Brennan, 1996; Zoltan-Ford, 1991).

Second, the opportunity to re-use the DS terms was far removed from the time when subjects had read the reformulations. In between these two events, subjects had to name all other 23 products, and there were between 54 and 144 intervening turns (i.e., remainder of phase 1 and start of phase 2). This amount of activity between initial and subsequent naming instances is much higher than in most other studies, which have investigated alignment between only two or very few turns (Branigan et al., 2011; Brennan, 1996; Goudbeek and Krahmer, 2012). Indeed, previous research has shown that alignment with a DS is highly reduced when the subject's turn is temporally delayed from the reference by the DS and when several intervening references to other objects are present (Brennan, 1996). In the present experiment, the long delay was necessary, because asking subjects to re-name the symptoms immediately after each block would have been likely to alert them to the fact that re-using (and therefore memorising) the DS terms was required. Presumably, this would have prompted a strategy change in subsequent blocks.

A third reason might be that subjects had previously named the fault symptom in their own initial utterance. It is conceivable that having generated a precedent makes people less likely to abandon it and go with the DS terms instead. On the one hand, this assumption is corroborated by the finding that subjects’ second descriptions were much more similar to their own first descriptions than to the DS reformulations (overlap of 2.2 vs. 1.1 words, respectively). On the other hand, previous research has revealed that people abandon their own previous terms in order to align with a DS (Brennan, 1996), and even use the computer's terms when they are dispreferred or untypical and subjects had already named the object in their own terms before (Branigan et al., 2011). These findings make it unlikely that the mere presence of a precedent eliminates alignment.

Fourth, the limited alignment might have resulted from the fact that the technical terms were quite difficult, which was also reflected in the subjective ratings: the majority of subjects (78.8%) preferred the combination of common terms & pictures, and far less subjects preferred technical terms & pictures (16.7%). Moreover, common terms & pictures were rated to be more helpful and understandable than the condition without pictures, while technical terms & pictures were not. This suggests that even with pictures technical terms were hard to use. Still, the difficulty of technical terms is unlikely to be the only reason for limited alignment, because when corrected for differences in word numbers, alignment did not differ between the three dialogue conditions, and thus was similarly low for common terms. A related reason for the low alignment could be that subjects’ descriptions and DS reformulations were not single words but entire sentences or phrases. This contrasts with most previous studies, where alignment has usually been studied for single words only. For instance, Brennan (1996) investigated whether subjects replace references such as ‘college’ by related words such as ‘school’ when the DS prompts them to do so. In fact, it has been reported that people are more likely to align with brief DS utterances than with complete sentences (Zoltan-Ford, 1991). In the present study, all DS reformulations consisted of several words, making alignment less likely.

Finally, alignment might have been low simply because it was unnecessary. The degree of alignment with DS is likely to depend on whether people believe that the conversation will benefit from it (cf. Branigan et al., 2010), and people align most strongly if they expect a DS to be inflexible or low in its linguistic capabilities (Branigan et al., 2010). That is, they align to avoid errors. In Experiment 2, this was unnecessary as the second description was provided in the rather non-interactive phase 2, where subjects merely re-described the faults but the DS did not provide any feedback or further requests. On the other hand, alignment with DS has been observed in situations where DS had already demonstrated to be perfectly capable of interpreting the user's terms (Brennan, 1996), so error avoidance is unlikely to be a necessary condition for alignment. Taken together, several factors might have contributed to the limited alignment, and future research is needed to empirically tease them apart.

A particularly interesting result was that alignment depended on the presence of additional information. Pictures did not reduce alignment, which contrasts with a previous study (Koulouri et al., 2016). However, alignment with technical terms depended on the presence of common terms. When alignment with technical terms was not analysed for particular reformulations but for dialogue conditions as a whole, it was highest when the DS did not include common terms. Presumably, common terms distracted subjects’ attention away from the technical terms, providing alternative and easier ways of describing faults. However, it should be noted that despite the significant main effect of dialogue condition and although alignment with technical terms & pictures was 3.4 times higher than with technical terms & common terms, the pairwise comparison did not reach significance. Therefore, this issue requires further research.

More generally, the present results raise questions about the cognitive mechanisms underlying alignment. It is an interesting finding that alignment was so weak, although the study was embedded in a work domain in which subjects were no experts, whereas the DS knew the domain-specific terms. This is different from most previous studies (e.g., Branigan et al., 2010; Brennan, 1996) in which subjects were well aware of the fact that their own terms were just as correct as alternative terms used by the partner or DS (e.g., couch vs. sofa, school vs. college). Therefore, one might have expected more instead of less alignment in the present study. This raises the question whether high attention and memory demands reduce alignment. Aligning with the DS was cognitively challenging in the present study, where reformulations were either long, complicated or both, and the second naming opportunity was far removed from the reformulation by the DS. It is possible that under such conditions with high demands on attention and memory people are less inclined to use the partner's terms when alternative ways of referring are available. This interpretation also is supported by the finding that subjects’ usage of technical terms was higher when only these terms were available (i.e., in the technical terms & pictures condition) than when less demanding ways of referring were presented (i.e., in the combined common & technical term condition).

Another question about the mechanisms of alignment concerns the degree of automaticity. There has been an ongoing debate as to whether alignment reflects passive priming (Garrod and Pickering, 2004; Pickering and Garrod, 2004) or an active, conversational strategy to jointly construct common ground (Brennan et al., 2010; Clark and Brennan, 1991). In the present study, aligning with the DS provided no benefits for the dialogue, which already was over by then and the DS always moved on to the next trial, regardless of what subjects wrote. This speaks against alignment being used as a conversational strategy to establish common ground. However, it needs to be noted that alignment is just one way of interpreting the adoption of DS terms. Given that we used objects that subjects had probably never named before, the alignment we observed might simply reflect that they learned how to name unknown objects. That is, one needs to be careful not to put too much interpretation into the concepts used to describe empirical findings. A more straightforward demonstration of alignment would be that people change their references with changing partners (Brennan and Clark, 1996). It will be an interesting question for future research whether people express such partner-specificity when conversing with DS.

6. General discussion
In two experiments, we investigated how DS should reformulate user descriptions of fault symptoms to establish mutual understanding. Asking subjects to self-correct their descriptions led to inferior performance and subjective ratings, while reformulations in common terms speeded up performance but posed a risk for errors when they were incorrect in catch trials. Contrary to the hypotheses, reformulations in technical terms were not clearly inferior to common terms, neither in terms of performance nor in subjective ratings. Being shorter than reformulations in common terms, they might have the potential to encourage alignment more effectively. However, in the present study alignment was low overall, although the adoption of technical terms was increased by presenting them without additional common terms. Pictures of fault symptoms prevented errors in catch trials, and were considered particularly helpful and understandable when combined with reformulations in common terms. Taken together, combining common terms with symptom pictures is a promising strategy for DS to reformulate user descriptions and ask for clarification. This conclusion is in line with previous findings that verbose and partly redundant contributions by a DS can be helpful (Paraboni et al., 2007; Sheeder and Balogh, 2003).

6.1. Reformulations to provide evidence for grounding
The reformulations used in the present study are a means of providing evidence in the process of establishing common ground between humans and DS. For one, they provided evidence that the DS had not understood subjects’ descriptions well enough. At the same time, they were a means for the DS to seek evidence from subjects on whether it had interpreted their description correctly. Such evidence-seeking is often neglected in human–computer interaction (cf. Brennan and Hulteen, 1995). Thus, should DS always seek evidence as explicitly as it was done in the present study? The answer clearly is no.

In the present study, non-acceptance of subjects’ descriptions was signalled by the DS providing explicit negative evidence or exposed corrections (Brennan, 1996). In this way, reformulations caused high verbal effort as they required additional turns. In practical applications, such cumbersome exchanges should be reserved for situations in which it is important that partners understand each other on a very detailed level (i.e., there is a high grounding criterion, Clark and Wilkes-Gibbs, 1986) or in which the costs of misunderstandings are high (Brennan and Hulteen, 1995). The grounding criterion for particular types of utterances depends on the domain (Roque and Traum, 2008). Symptom descriptions are very critical in industrial fault diagnosis. Therefore, if excessive verbal effort is to be reduced, this should not be done by lowering the grounding criterion but by using other data for disambiguation. When information about machine settings and sensor data is available or when the DS is part of a conversational case-based reasoning (CCBR) system (Aha et al., 2001, 2006), it could use the available data or cases to determine possible meanings of user descriptions. For instance, when the blank holder force of a drawing machine is high, the utterance ‘shape is torn apart’ is more likely to describe a local edge rupture than a fracture at the bottom. Thus, the DS would not need to request all evidence for its correct understanding from the user him- or herself but could derive parts this evidence from the context. In this way, the referential domain would be constrained by external data, reducing the need for verbal effort (Brennan, 2004; Clark and Krych, 2004). Similarly, narrowing down the referential domain by goal-specific constraints and task relevance has been reported to allow for referential underspecification (Brown-Schmidt and Tanenhaus, 2008). Using these potentials in DS requires high level domain models (Cahn and Brennan, 1999) and thus requires a close interdisciplinary cooperation between engineers, computer scientists, psycholinguists, and psychologists.

6.2. The role of visual information
Pictures of fault symptoms had positive impacts on performance and subjective ratings, which becomes most evident when comparing the error rates in catch trials between experiments. In Experiment 1, common terms led to high error rates in catch trials, but this no longer was the case when pictures were added in Experiment 2. In fact, errors decreased to almost one third, from 22.2% in Experiment 1 to 8.3% in Experiment 2. This benefit of multimodality is a key finding of the present study and should be considered when making design decisions for DS.

It also is in line with findings from three research areas. First, Human Factors research on interface design has emphasised the benefits of distributing information across modalities to support the control of dynamic systems (Wickens, 2008). Second, research on instructional design and multimedia learning (Mayer, 2001) has produced an abundance of studies suggesting that learning materials should combine pictures and text (for an overview see Butcher, 2014). Such multimodal instruction supports learners in forming a mental model and facilitates object identification in complex tasks (Gellevij et al., 2002). Third, research on the role of visual information in communication has shown that people can more easily establish common ground when seeing the same objects as the partner (Brennan, 2004; Clark and Krych, 2004; Gergle et al., 2013).

A difference between these three areas concerns the role of redundancy, or the question whether it is beneficial to present the very same information in different ways. In the context of interface design, redundancy is usually found to be beneficial, even when the same modality is used, for instance when combining digital and analogue speed indicators that both provide visual information (Francois et al., 2017). This is because redundant presentation ensures that the information is processed even when one signal cannot get through, for instance as a consequence of distraction. In instructional design, redundancy is not always beneficial but can even impair learning, for instance when learners have to unnecessarily split their attention between pictures and text (Kalyuga et al., 1999). In human communication, redundancy is usually avoided as speakers adapt their referring expressions to the visual context. Communication partners typically resort to largely underspecified and often deictic expressions (e.g., ‘this one’) when visual information is available (Clark and Krych, 2004). This differentiates human communication from the present study, where the visual and verbal information was highly redundant, aiming to convey the exact same information.

However, in DS similar strategies of sharing the task between the visual and verbal modality is conceivable and could reduce users’ effort of processing DS reformulations (e.g., ‘So you mean the shape looks like this?’). However, a risk associated with pictures of fault symptoms is that they are very specific and addressees might over-interpret this specificity. This might create an effect similar to unwanted implicatures which can occur when people or DS overspecify their referring expressions (Campana et al., 2011; Grice, 1975). In the worst case, users might even reject the reformulation by the DS, assuming that the symptom should look exactly like in the picture. This is problematic in the food processing industry where fault symptoms can be highly idiosyncratic. In the present study, we tried to mitigate this risk by using abstract drawings instead of photographs. Still, future studies should investigate how users deal with pictures as parts of DS reformulations, depending on how closely these pictures match current symptoms. Presumably, the risk of overinterpretation can also be mitigated by reformulations in common terms, reflecting another form of task-sharing between the visual and verbal modality: while pictures can disambiguate the symptom, verbal descriptions can highlight the relevant aspects for comparison and the intended level of detail. For instance, it can clarify whether a DS providing a picture intends to ask about ‘holes at the bottom’ or about ‘three small holes at the bottom in a triangular pattern of around 1 cm’. Again, these considerations emphasize how important it is to consider domain characteristics in the design of DS (e.g., variability of fault symptoms), instead of merely relying on heuristics derived from research on human communication (e.g., humans tend to verbally over- or underspecify, so DS should do the same).

6.3. Alternatives to other-repair
The reformulations in the present study are a form of other-initiated other-repair at position 2 (P2OIOR, Colman and Healey, 2011): complete replacements of speaker descriptions by an addressee who contributes an alternative description. However, there are interesting alternatives to this type of repair actions. First, the DS could generate its reformulations together with the user in a more interactive manner. Human dialogue partners typically use mixed initiative, in line with the principle of mutual responsibility (Clark and Wilkes-Gibbs, 1986) which states that both partners take responsibility for the conversation's success, not just the one who initiated the conversation (Brennan et al., 2010). For instance, partners take responsibility by completing each other's utterances. Similarly, DS can expand users’ descriptions instead of replacing them (Clark and Wilkes-Gibbs, 1986), for instance by aligning with the user's description in the next turn (Brennan, 1988) and then elaborating it to obtain further information (e.g., user: ‘The shape has holes at the bottom’, DS: ‘Holes at the bottom that cover the entire area?’). A benefit of this approach is that DS appear more cooperative when they reply in the same lexical and syntactic forms that the user had selected (Brennan, 1988).

A second alternative is that DS could select alternative dialogue plans instead of reformulating users’ descriptions when not understanding them. Research on human dialogues and human–computer interaction suggests that DS should avoid signalling non-understanding and asking for repetition or rephrasing (Bohus and Rudnicky, 2005; Skantze, 2005). Instead, they could ask a qualitatively different question to confirm the same hypothesis, leading to better understanding in the subsequent turn. One benefit of alternative questions asked by DS is that they constrain the referential domain for the user's response to something the DS can interpret. However, DS would need to rely on semantic domain models to infer what alternative questions they can ask to confirm the same hypothesis (Skantze, 2005). For instance, in the food processing industry some fault symptoms are correlated. If symptom A is present, then symptom B is very likely to be present as well. Thus, if the DS did not understand the description of symptom A, it could ask about symptom B to infer the same fault. Actually, alternative dialogue plans already are part of many CCBR systems, which provide several alternative questions for users to select from (Aha et al., 2001; McSherry, 2001, 2002).

Finally, it should be investigated how clarification can be achieved when DS make themselves more understandable. Dialogue partners try to infer each other's communicative intent by using information from different sources, including prior knowledge about the addressee and information gained from the previous conversation (Brennan et al., 2010). Conversely, when interacting with a DS, people typically know little about the DS, about its knowledge, its communicative capabilities, and its intentions (Branigan et al., 2010). To change this, DS could explain why they are asking particular questions. Consider the following scenario. The user describes a fault symptom in the packaging process of chocolate bars, saying ‘The chocolate is deformed.’ The DS generates two possible interpretations: (1) the coating is uneven, indicating problems in the chocolate production process, or (2) the chocolate bar has been squished when entering the packaging machine. In addition to providing a reformulation, the DS could add ‘I am asking because it could be that the chocolate has collided with a machine part.’ This would enable the user to immediately reject the hypothesis, without the DS needing to infer this indirectly from the elaborated symptom description. The use of explanations in CCBR systems (McSherry, 2005, 2011) usually is limited to explaining how user inputs would affect case selection but ignore domain-specific knowledge about fault causes. Current research in our lab investigates the potentials of using explanations in a DS to make its reasoning about fault causes more transparent (Blunk et al., 2019).

6.4. Generalising to other dialogue systems and to the real world
A first aspect of generalisability concerns other types of DS. The present study used a simulated chatbot that conversed with its users via text messages. Would the same results be obtained with a speech-based DS? Different communication media and modalities have different grounding costs, which directly affects the way they are used (Clark and Brennan, 1991). Several studies have examined whether people converse differently when using text or speech to interact with a DS. A first relevant finding is that the two modalities do not differ in all respects, and for instance no differences have been found in the degree of lexical alignment (Brennan, 1996; Zoltan-Ford, 1991). With regard to the present results, this suggests that the adoption of technical terms may not be higher with a speech-based DS.

However, in other variables plenty of differences between text and speech were observed. First, the input provided by users differs between modalities. In written conversation, messages are shorter and include less content, requiring DS need to make more requests to obtain the necessary information, while in spoken conversation people provide more of this information spontaneously (Zoltan-Ford, 1991; Zoltan et al., 1982). A possible consequence could be that with speech-based DS people's initial descriptions and their responses to DS reformulations might contain more information that the DS can interpret. While this is unlikely to affect the difference between common terms and technical terms, it might mitigate the problems associated with self-correction requests. On the other hand, spoken conversations contain more non-standard input, such as grammatically incorrect constructions and false starts (Kroch and Hindle, 1982). Thus, users’ descriptions might be harder to interpret and thus repair might be needed more often. This would increase the need for efficient reformulation strategies.

A second type of difference between modalities concerns the effort people put into clarification (Cohen, 1984): In spoken conversation, people use more fine-grained communication, being more likely to ask their partner to disambiguate the referent before moving on. In written conversation, such cross-checking is almost absent. Therefore, it is conceivable that the problems we observed with common terms in the catch trials of Experiment 1 (i.e., faster acceptance at the cost of more errors) would vanish with speech-based DS, where people might more readily cross-check mutual understanding with the DS before moving on. Also, in spoken conversation people use more hedging (e.g., ‘I guess’, ‘it might be’), expressing uncertainty and marking their contributions as provisional (Brennan and Ohaeri, 1999). DS could pick this up and offer reformulations when inferring that users are not sure how to name an object. The other way around, DS might themselves use hedging to indicate uncertainty and invite user intervention.

Taken together, many of the questions posed in the present study also are relevant in the context of speech-based DS, but the outcomes are likely to differ in some respects. Therefore, it will be an interesting challenge for future research to investigate whether and how reformulation and repair strategies should differ between DS using different modalities.

Another type of generalisability concerns generalisation to the real world. Most studies on the application of dialogue principles in DS, including the present study, were performed with student subjects, and thus it is important to ask whether the findings actually apply to industrial production contexts. Although operators in the processing and packaging industry have low qualification (Mason et al., 1994; Müller and Oehm, 2019), they are experienced in operating their specific machine. Thus, one might expect that technical terms are no problem for them as they are highly familiar with these terms. However, research from our lab suggests otherwise. In a project aimed at developing a CCBR system for the processing industry (Rahm et al., 2018), we performed interviews and observations with more than 20 operators and technicians in a chocolate production plant. Despite their work experience of 3 to 23 years, their references to fault symptoms and machine components were all but standardised or technically correct. For instance, operators used the word rods (“Stäbchen”) for chocolate bars, eyes (“Augen”) for sensors, and green meadow (“grüne Wiese”) for the downholder (a machine component). A particularly interesting observation was that their references were quite unspecific, resulting in the same problem associated with common terms in Experiment 1. For instance, operators used the word breakage (“Bruch”) for products that have a broken edge, are broken midway through, or are destroyed completely. As the specific type of breakage is highly indicative of the fault cause, more specific and discriminative references would be desirable for a DS to infer possible fault causes. Thus, although the present experiments were performed with student subjects, their results will be helpful in designing DS that are used by operators in real production contexts.

6.5. Limitations of the present study
Several limitations of the present study should be highlighted. One set of limitations concerns external validity. First, our simulated DS did not mirror an actual DS that could be used in practice in its current form. The communication context was quite minimalistic, with dialogues only consisting of a few turns and the system using a small set of canned messages. The ability of the DS to adapt to subjects was low in Experiment 1 and completely absent in Experiment 2. These design choices were based on the goal of the present study to examine a particular aspect of human–computer interaction in a controlled manner, instead of observing natural user behaviour in a realistic setting. Second, we did not realise a complete fault diagnosis dialogue, and thus the present study cannot tell whether the reformulation strategies would ultimately affect diagnostic outcomes (e.g., percentage of correct diagnoses). Third, the present experiments consisted of one-shot interactions instead of investigating the effects of extensive experience with a DS. Upon repeated use, specific conceptual pacts would be likely to emerge (i.e., naming conventions for particular product parts or symptoms) and more general aspects of subjects’ symptom descriptions would be likely to change over time (e.g., message length and similarity). Therefore, it would be interesting to study the effects of reformulations over several days or weeks.

A complementary set of limitations concerns internal validity. Although the present dialogues were minimalistic compared to real DS, they were quite uncontrolled compared to most psychological experiments. First, in Experiment 1 some trials did not present any reformulations, as subjects’ first descriptions already led to a successful outcome, namely when they contained all relevant keywords. This resulted in an unequal number of reformulations experienced by different subjects. Second, the stimulus material was not controlled, for instance concerning the difficulty of describing the shapes or concerning the length, complexity, familiarity, and descriptive power of the reformulations. The analysis of stimulus-specific effects showed that this indeed affected the results. For instance, the difference between common terms and technical terms was stronger for some faults than others. We intentionally refrained from controlling the stimulus material as this study was intended to be a first exploratory step in investigating reformulation effects. Thus, we considered a high variety to be beneficial in order to gain a rich set of insights into subjects’ verbal behaviour. It seemed counterproductive to only use stimuli that were likely to elicit a particular response, for instance by pre-selecting common terms that are highly misleading and then showing that they are misleading. The limitation of low stimulus control at the same time can be regarded as an advantage as it illustrates an important point: For some object/reformulation combinations, it can be hard to establish mutual understanding with a DS, especially when there is no fixed definition (e.g., when using analogies that potentially match different objects). The investigation of stimulus-specific effects was not a goal of the present study, which is why we neither designed the experiment in a way that is suitable for such an investigation nor applied statistical procedures that are capable of modelling such effects. However, future studies should build on the present findings in more controlled experiments and quantify stimulus-specific effects by using advanced statistical procedures such as linear mixed models. In this way, it will be possible to determine what sub-types of reformulations are particularly prone to misunderstandings.

A third limitation concerns the measures and analyses applied in the present study. For instance, a non-standardised questionnaire was used to assess subjective ratings. This was done with the intention to ask the specific questions we wanted to ask, but it puts the interpretation at risk, as the reliability and validity of customised questionnaires are uncertain and usually low. Additional limitations arise from the alignment analysis used in Experiment 2. This analysis differs from several studies that defined alignment as the overlap between simple object naming in turns that closely followed each other (see Discussion of Experiment 2). It also is subject to a number of potential confounds. For instance, the delay between the DS reformulation and the subject's turn to rename the faulty shape was not only long but also highly variable, depending on the position of the block and the trial within the block. Moreover, alignment most likely was affected by features of the stimuli such as variations in the length, complexity, or familiarity of the reformulations. These factors certainly limit the comparability to other studies and put the interpretation of the alignment results at risk. At the same time, they reflect some of the minimal requirements for a transfer to realistic contexts. If alignment does not even survive delays of less than an hour and differences in the low-level features of reformulations from similar object categories (i.e., geometrically distorted packages), it is hard to imagine how it could be an answer to the vocabulary problem, as suggested in previous studies (Brennan, 1996; Koulouri et al., 2016). That is, only when the tendency to align is sufficiently stable, it can make users adopt the terms used by a DS.

Taken together, the present study reflects a compromise between internal and external validity, which necessarily sacrifices parts of both. Therefore, future work should be conducted in both directions: studying the variability of natural dialogues with DS in realistic fault diagnosis settings, and conducting more controlled lab experiments that minimise variation and confounds.

7. Conclusion
The omnipresent risk of miscommunications between humans and DS calls for an application of psycholinguistic knowledge about human dialogues. To this end, the present study investigated how DS can reformulate user descriptions of fault symptoms to exchange evidence and establish common ground. Reformulations that use people's everyday language and combine verbal with visual information about fault symptoms were found to be particularly useful. At the same time, they were associated with the problem of low specificity, posing a range of new questions to be investigated in future studies. To advance theories and applications of human–machine cooperation in interactive fault diagnosis, a much closer integration of knowledge from disciplines as diverse as engineering, psycholinguistics, psychology, and computer science will be needed.