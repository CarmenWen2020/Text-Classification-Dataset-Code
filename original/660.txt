Abstract
We design distributed algorithms to compute approximate solutions for several related graph optimization problems. All our algorithms have round complexity being logarithmic in the number of nodes of the underlying graph and in particular independent of the graph diameter. By using a primal–dual approach, we develop a -approximation algorithm for computing the coreness values of the nodes in the underlying graph, as well as a -approximation algorithm for the min–max edge orientation problem, where the goal is to orient the edges so as to minimize the maximum weighted in-degree. We provide lower bounds showing that the aforementioned algorithms are tight both in terms of the approximation guarantee and the round complexity. Additionally, motivated by the fact that the densest subset problem has an inherent dependency on the diameter of the graph, we study a weaker version that does not suffer from the same limitation. Finally, we conduct experiments on large real-world graphs to evaluate the effectiveness of our algorithms.

Keywords
Distributed algorithms
Coreness
Round complexity

1. Introduction
The -core decomposition and algorithms for finding densest subgraphs have proved to be a valuable tool in graph mining and data analysis, with applications encompassing sociology, bioinformatics as well as graph visualization.

Definition 1.1

Coreness Value [30]
The coreness value of a node  in a (weighted) graph  is defined as the largest number  such that  belongs to a subgraph of  with minimum (weighted) degree equal to . We denote the coreness value of  by .

The goal of -core decomposition is to compute the coreness value of every node in a given graph. Intuitively, nodes belonging to well-connected communities tend to have high coreness values. Besides, several definitions of density have been proposed in the literature. In our work, we focus on the average degree density which is defined as the ratio between the number of edges and the number of nodes in a graph [9]. One of the appealing properties of such a definition is that a densest subgraph (in terms of average degree density) can be computed in polynomial time. Such an optimization problem is often referred to as the densest subset problem. Recently, the diminishingly-dense decomposition has been introduced and studied [12], [23], [31], which elegantly merges the concepts of density and -core decomposition. Such a decomposition assigns to each node a real number, which we refer to as maximal density value. We also study the min–max edge orientation problem [32] which turns out to be related to the previous problems. The goal is to assign an orientation to every edge such that the maximum weighted in-degree is minimized.

In our work, we develop distributed algorithms for approximating coreness values and local density values, as well as to find an approximate solution for the min–max edge orientation problem. Moreover, we study a weaker version of the densest subset problem.

We envision the following applications for our work. Our distributed algorithms can be executed by agents in a social network or a P2P network, for example, so as to collect relevant statistics of the agents with respect to the underlying graph. Users in a social network with large coreness value are known to have “good spreading” properties in epidemiological studies [24]. Therefore, the coreness value (or an approximation of it) can be leveraged to maximize the spreading of a diffusion protocol. Communities in a social network consist of set of users sharing similar interests, such as hiking, traveling or photography. The density of a given subgraph can be used to measure how likely the corresponding users belong to a same community [34]. Our work allows to approximate such a metric in a distributed fashion. Our distributed algorithms can also be used in distributed graph processing systems [27] to process very large graphs not fitting into the main memory of one single machine.

We are given an undirected edge-weighted graph . We consider the classical  model, in which every node can directly communicate only with its neighbors in synchronous rounds. Moreover, we assume each node knows (an upper bound on) the number  of nodes. The hop-diameter  (or diameter) of a graph  is at most  if every pair of nodes in  can be connected with a path consisting of at most  edges. We use the convention that the approximation ratio  is at least 1.1 There is often a tradeoff between the number of rounds and the approximation guarantees of a distributed algorithm. Fig. 1.1 shows that approximating the coreness values or the min–max edge orientation problem within a factor strictly less than  requires  communication rounds. Similarly, for a node to be aware of whether it is included in an approximate densest subgraph,  communication rounds are required. This follows from the fact that such a node has to verify whether a subgraph with higher density (possibly many hops away) is included in the graph or not. Hence, it is natural to investigate the following question: Can we devise distributed approximation algorithms for the aforementioned problems, while requiring a number of communication rounds independent of the diameter?

Although some of the problems discussed above have been studied in a distributed environment, there is no work focusing on breaking the diameter barrier, to the best of our knowledge. All our algorithms require a number of communication rounds logarithmic in , while we provide tight lower bounds expressing the tradeoff between communication rounds and approximation ratio.


Download : Download high-res image (141KB)
Download : Download full-size image
Fig. 1.1. Example graphs showing that the we cannot beat -approximation for coreness values and min–max edge orientation problem unless the number of rounds is at least . All graphs have unit edge weights, and the nodes are labeled with their coreness values. The coreness of  is  in (a), but is  in (b) and (c). The arrows in (b) and (c) indicate an optimal orientation, where the maximum in-degree is ; any other orientation of edges incident on  will result in a maximum in-degree of at least . It takes  rounds for node  to distinguish between the different graphs.

1.1. Our results and contribution
Coreness values. Just like almost every work on these related problems [5], [7], [9], [13], [14], [16], [21], [28], [33], we consider an elimination procedure that repeatedly “peels off” nodes with small (weighted) degrees. In particular, we follow the interpretation by Montresor et al. [28]. Specifically, given a threshold value , in each round, nodes with degree less than  in the remaining graph are removed. Using a compact representation, we can imagine that the elimination procedure for all possible thresholds is run in parallel, where after each iteration, each node  just remembers the largest threshold 
 (which we call the surviving number), for which it still survives.

It is known that after each iteration, the surviving number of each node  is at least its coreness ; moreover, after  rounds, the surviving number will reach the coreness value.2 However, to the best of our knowledge, so far there is no approximation analysis for the process in terms of the number of iterations.

Densest subset. It is interesting that a variant of the elimination process was considered by Bahmani et al. [5] to give a -approximation for the densest subset streaming model. In each iteration or pass, the threshold is chosen to be  times the density of the current subset of surviving nodes. Then, the process terminates in 
 iterations, and the subgraph from one of the iterations gives a -densest subset. This inspires us that density can provide the right tool to design and analyze distributed approximation algorithms for coreness values. However, an immediate issue is that for every node to know the (approximate) density of the current subgraph, we already need  rounds.

Local notion of density. We observe that it is not necessary to use the global density of the (sub)graph, because in some sense, coreness value measures the local density of a node. Recently, Tatti and Gionis [31] considered a so-called locally-dense graph decomposition, which has been further studied by Danisch et al. [12], who defined a quantity known as maximal density for each node. This decomposition has actually been considered in passing by Khuller and Saha [23] in the context of finding densest subsets with large sizes. Intuitively, the decomposition works by repeatedly peeling off maximal densest subsets as follows. Given a weighted graph , the maximal densest subset  forms the first layer, which induces a subgraph with density 
≔
 
 (here  denotes the sum of weight of edges connecting nodes in ); every node  has maximal density equal to 
, even though no special importance is given to this value in [23]. Next, remove the nodes in  to form a quotient graph 
, where an edge between  and 
 
 becomes a self-loop at  in 
. Then, the procedure is recursively applied to 
 until all nodes are removed. Equipped with this notion of maximal density, we can adapt previous analysis to achieve the following result.

Theorem 1.2 Gracefully Degrading Approximation Ratio for Coreness Values

After running the compact elimination procedure in the  model for  rounds, the surviving number at each node gives a 
 
-approximation to its coreness value (and maximal density).

In particular, if every node knows (an upper bound on) the number of nodes  and would like to achieve -approximation, then 
 rounds are sufficient.

Matching lower bound. For , by considering a -ary tree, we show in Section 3 that achieving -approximation in computing coreness values or maximal densities requires 
 
 rounds.

Min–max edge orientation problem. The centralized version of the problem was proposed by Venkateswaran [32] and has applications in telecommunication network design; it is known that the special case of unit edge weights can be solved in polynomial time. The connection of the problem with densest subsets has been explored in subsequent works [3], [4].

To the best of our knowledge, the only distributed algorithm for this problem with round complexity independent of the diameter is for unweighted graphs. Specifically, Barenboim and Elkin [6] actually studied a stronger problem, where the goal is to partition the edges of a graph into a minimum number of forests. Instead of density 
 
, a similar notion of arboricity 
 
 is considered. In hindsight, it is not surprising that they also used a variant of the distributed elimination procedure to approximate the forest-decomposition problem in the  model. Indeed, they showed that if the maximum arboricity is known by every node, then 
 
 rounds are sufficient to achieve -approximation. As remarked above, if every node needs to know the (approximate) maximum arboricity, then  rounds are required. A careful study of their algorithm reveals that the first phase [6, Algorithm 3] serves a similar purpose as computing the surviving numbers as in our Theorem 1.2, after which they run the second phase as if the maximum arboricity is known. This degrades the quality of the solution, and only -approximation is achieved.

Primal–dual approach. By observing that the LP relaxation of the min–max edge orientation problem is the dual of the densest subset LP (see Section 2), we have the intuition that a procedure for approximating the maximal densities should also give information about the dual problem, without using a second phase. Indeed, we augment the distributed elimination procedure by maintaining an auxiliary subset 
 for every node  (which represents its in-neighbors) to give the same approximation ratio for the edge orientation problem. In addition, a very careful invariant analysis in Lemma 3.11 is performed to make sure that every edge is taken care of by at least one of its end-points.

Theorem 1.3 Gracefully Degrading Approximation Ratio for Min–Max Edge Orientation Problem

After running the augmented elimination procedure (Algorithm 2) in the  model for  rounds, the auxiliary subsets 
 gives a 
 
-approximation to the min–max edge orientation problem.

In particular, if every node knows (an upper bound on)  and would like to achieve -approximation, then 
 rounds are sufficient.

Densest subset problem. As argued above, for every node to be aware of whether it should be included in an approximate densest subset, it takes  rounds. Indeed, Sarma et al. [29] gave a distributed algorithm that gives -approximation with 
 rounds.

We consider a weaker version of the problem (in Definition 4.1). Instead of just producing one subset, the distributed algorithm will return a collection 
 of disjoint subsets such that for each , each node will be aware of which subset (if any) it belongs to; moreover, there exists some  such that 
 is an approximate densest subset. Using the procedure in Theorem 1.2 as a subroutine, we have the following.

Theorem 1.4 Distributed (Weak) Densest Subset Problem

For any  and , there exists a distributed algorithm that gives a -approximation to the (weak) densest subset problem in Definition 4.1 on any graph with  nodes using 
 rounds.

1.2. Related work
In addition to the most relevant works that have already been compared with our work, we also describe other related works.

Coreness values. After Seidman [30] first proposed -core decomposition, Batagelj and Zaversnik [7] presented a (centralized)  algorithm to compute a -core decomposition. This notion has been extended to weighted graphs [10], [15] and directed graphs [14]. The distributed setting has been studied by Montresor et al. [28], and was further extended to dynamic graphs by Aridhi et al. [2]. The distributed algorithms have been adapted to (centralized) I/O efficient algorithms [10], [33] that were proposed to handle large graphs that cannot fit into memory.

Min–max edge orientation problem. Since Venkateswaran [32] introduced the problem, (centralized) polynomial-time algorithms are known to give optimal solutions for unweighted graphs  [4], [25], [32]. However, a series of works [3], [4] showed that the weighted version is NP-hard even when all edge weights belong to the set , where  is any fixed integer greater than ; on the other hand, for integer edge weights, 
 
-approximation can be achieved, where  is the maximum weight. Gillet and Hanusse [16] considered the more general asynchronous distributed model with faults. However, their algorithm has round complexity depending on the graph diameter, and achieves -approximation.

The problem can be also be seen as a special case of a load-balancing task, where each node is a machine and each edge is a job to be assigned to one of its incident machines. From this perspective, minimizing the maximum in-degree is equivalent to minimizing the makespan. Czygrinow et al. [11] considered minimizing a slightly different objective that is the sum of the squares of the loads of the machines. In contrast, they gave a distributed -approximation algorithm that runs in 
 rounds, where  is the maximum degree.

The Min–Max Edge Orientation Problem shares some similarities with the vertex cover with hard capacities problem [18].

Densest subsets. The densest subset problem [17] has been extensively studied and extended to directed graphs [22]. In the centralized setting, it can be solved in polynomial time by an algorithm based on maximum flow [17], linear programming [9] or a recent approach based on convex optimization [12]. Charikar also proposed in [9] a simple greedy approach based on -core decomposition that produces a -approximation.

As mentioned above, even though Khuller and Saha [23] have implicitly considered locally-dense graph decompositions, this notion has not been fully studied until recently by Tatti and Gionis [31] and Danisch et al. [12]. One of its surprising applications is its usage in the computation of the (non-linear) Laplacian operator of a hypergraph [8].

There are also interesting connections between load balancing and the densest subset problem [1], [19], [20].

2. Preliminaries
Distributed model. The input is an edge-weighted undirected graph , where  and the edge weights 
 are non-negative. We consider the classical  model, where each node in  is a CPU with a unique identifier that is aware of only its incident edges (and their weights) and its neighbors; each node knows the total number  of nodes (or an upper bound). In addition to the  model, our protocols satisfy the following.

•
Synchronous rounds and polynomial-time computation. In each round, a node can send a message to its neighbors in . Moreover, after receiving messages from its neighbors, a node can perform polynomial-time computation in each round.

•
Broadcast model. We consider protocols in which a node sends the same message to (a subset of) its neighbors in each round.

•
Message content and size. In our protocols, we assume that each message contains the identity of the sender, and typically, the content of a message is a constant number of real numbers. In most useful applications, each edge weight is an integer whose size is polynomial in . In this case, every number sent in a message in our protocols can be represented by  bits, which means the requirements of the Congest model are also satisfied. Alternatively, with arbitrary edge weights, by restricting the set of numbers to appropriate powers of , we can also restrict the message size.

Graph terminology. In most cases, each edge  is interpreted as a 2-subset of , although in the analysis we sometimes consider a self-loop, which is a singleton.

For each node , the set of neighbors of  is 
≔, and the weighted degree of  is 
≔
, i.e., the sum of the weights of the edges that contain . When there is no ambiguity, the subscript  can be omitted.

Density. For a non-empty , we denote ≔, and its density is 
≔
 
, where  denotes the sum of edge weights. A (non-empty) subset  in  is a densest subset if and only if  has maximum density among all subsets in . The following fact is standard.

Fact 2.1

The maximal densest subgraph of  is unique and contains all densest subgraphs of .

Local density. While a densest subset gives the densest region in a graph , there are other notions that measure the local density around each node . The first notion is coreness 
, which is mentioned in Section 1. Another notion is maximal density, which is defined in terms of quotient graph and diminishingly-dense decomposition as follows.

Definition 2.2

Quotient Graph [12]
Given a weighted undirected graph  and a subset , the quotient graph of  with respect to  is a graph ≔
, where 
≔ and 
≔
. (Intuitively, every edge  not contained in  contributes towards 
: an edge  is retained, while an edge connecting  and  becomes a self-loop.) Moreover, for 
, 
≔
.

Definition 2.3

Diminishingly-Dense Decomposition and Maximal Density [12]
Given a weighted undirected graph , define the diminishingly-dense decomposition  of  as the sequence 
 as follows.

Initially we set 
≔ and 
≔. For , if 
, the decomposition is fully defined. Otherwise, define 
≔
 and let 
 be the maximal densest subset in 
. Then, define 
≔
. Finally, for each node , we say that the maximal density of  is 
≔
 if 
. For simplicity, we use  to denote 
 if the context is self-evident.

Fact 2.4 Strictly Diminishing Densities

In Definition 2.3, the sequence 
 is strictly decreasing in .

Distributed approximation of local density. Our goal is to design distributed protocols such that at the end, each node  outputs some number , which is an approximation of its coreness  or maximal density . Observe that there are example graphs such that computing  or  exactly needs  rounds. The main result of this paper is that there are protocols with  rounds that give -approximation for both  and  for each node . To be precise, we use the following convention to describe approximation ratio.

Definition 2.5 Approximation Ratio

Given a non-negative real number 
 and , another number  is a -approximation for , if . In general, a function 
 is a -approximation for 
, if for all , .

Min–max edge orientation problem. The input is an edge-weighted undirected graph . The goal is to give an orientation to each edge such that the maximum weighted in-degree of a node is minimized. Equivalently, we wish to determine an assignment  such that each edge  is assigned to one of its end-points, and 
 is minimized.

Distributed setting. After the distributed algorithm terminates, each node  will have computed some subset 
 of its neighbors, which represents the set of incident edges that are assigned to it. Observe that it is sufficient to enforce the condition that for every edge , we have 
 or 
; by means of one more round of communication, we can resolve any conflict (i.e. an edge being assigned to both its end-points).

LP relaxation and relationship with densest subset. The following is an LP relaxation of the min–max edge orientation problem together with its dual. In the primal LP, for each  and node , 
 is the portion of the weight 
 of edge  that is assigned to . As observed in [12], the dual LP is exactly the densest subset LP by Charikar [9]. 
 

Approximation analysis. As we shall see, a by-product of our distributed algorithm is that it gives an assignment of edges to their incident nodes. If 
 is the maximum density of a subset, and the sum of the edge weights assigned to every node is at most 
, then weak duality implies that the assignment is a -approximation to the min–max edge orientation problem.

3. Distributed approximation algorithms for coreness values (and min–max edge orientation)
In this section, we give a distributed protocol that approximates both the coreness value and maximal density for each node in a given graph.

3.1. Warmup: Single threshold
The warmup protocol is based on a well-known idea of iteratively eliminating vertices with small degree [5], [7], [9], [14], [16], [21], [28], [33], which can be easily implemented in the distributed model. Here we consider a protocol that is parameterized by some universal threshold . In each round, each node with weighted degree less than  in the subgraph induced by surviving nodes is marked to be removed at the end of the round. It is clear that after running the protocol for  rounds, all surviving nodes have coreness at least  [14]. It is shown in [21] that running the protocol for various thresholds and  rounds can be used to give -approximation for the densest subgraph problem. We show that the analysis can be adapted to approximate both coreness values and maximal density.

We describe the elimination procedure in the distributed model in Algorithm 1. Each node  keeps a state 
 which records whether the node is present () or removed ().


Download : Download high-res image (138KB)
Download : Download full-size image
3.2. Parallel execution with multiple thresholds
Parallel execution. Observe that the elimination procedure for different threshold values can be executed in parallel, but this can cause a large message size. Hence, for the purpose of analysis, we imagine that the protocol is executed for all possible thresholds in parallel and concentrate on the round complexity. Later in Section 3.3, we show how parallel execution for all threshold values can be performed compactly. Depending on this parallel thought experiment, we define the surviving number as follows.

Definition 3.1 Surviving Number

Given a weighted undirected graph , the surviving number of a node  after  rounds, denoted as 
, is defined as the maximum  such that  survives after  rounds of the elimination procedure using threshold value . When the context is clear, we omit  or  from the notation and simply use .

It is known that after running the procedure for  rounds, 
 gives the exact coreness value [28]. Our goal is to show that to get a constant approximation, it suffices to use  rounds, independent of the diameter of the graph. In particular, we shall prove that for , if we set ≔
, then 
 is a -approximation for both the coreness value  and maximal density .

Lemma 3.2 Lower Bound on Surviving Number

For any node  and any positive integer , 
.

Proof

For any node  with coreness , by the definition of coreness, suppose  is a subset such that  and 
 for all . Then, for , all members of  will survive the elimination procedure with threshold , no matter how many rounds of elimination are executed. In particular,  will survive. Hence, 
 for all positive integers . ■

Lemma 3.3 Upper Bound on Surviving Number

For any positive integer  and any node , we have 
 
. In other words, to achieve an approximation guarantee of , it suffices to set 
 
; in particular, for , the special case ≔
 gives 
.

Proof

The proof approach has appeared in several previous works [5], [9], [21]. For completeness, we adapt the proof from [21, Lemma 3.1].

Fix some . Consider the diminishingly-dense decomposition of . Let 
 and 
 be defined as in Definition 2.3 such that 
, which is the maximal densest subset in 
. Then, 
. Observe that 
 
 is a quotient graph, which means that any edge connecting 
 to 
 
 becomes a self-loop in 
, which implies that for all 
, for all , 
.

Fix any 
 
. It suffices to show that after applying  rounds of elimination with threshold  to the graph 
, no node in 
 (which includes ) can survive.

Define 
≔
; for , let 
 be the set of nodes that survive after round . Suppose for some , both 
 and 
 are non-empty. Then, since 
 is a densest subset in 
, we have 
 
 
 
 
. Note that we use the inequality 
 
, because 
 could contain self-loops. Hence, we have 
 
. Therefore, if 
 is non-empty, we have 
 
, which is a contradiction. So 
 is empty, which means that no node in 
 survives after round , as required. ■

Lemma 3.4 Relating Maximal Density and Coreness Value

For any , .

Proof

Consider the diminishingly-dense decomposition of . Let 
 be defined as in Definition 2.3. Recall that there is a unique  such that 
, and 
 is the maximal densest subset in 
.

We prove by induction on  that 
 for all 
, which is slightly stronger than the required statement. From this, it follows that 
 for all 
.

Base case. Suppose 
. Since 
 is a densest subset in 
, we must have 
, as required. Otherwise, the subset 
 would have higher density than 
 in 
.

Induction hypothesis. Suppose for some , for all 
, 
.

Inductive step. Consider 
. Note that 
. For 
, since 
 is a quotient graph, it follows that 
, because any edge connecting 
 to 
 will become a self-loop in 
. Similar to the base case, because 
 is a densest subset in 
, we have 
, as required. For 
, we have 
, where the second inequality follows from the induction hypothesis. By Fact 2.4, the last term satisfies 
, thereby completing the inductive step and the proof of the lemma. ■

Theorem 3.5

Given  and , let ≔
 
. Then, for all , we have 
.

In particular, for any , we can set  and ≔
.

Proof

This follows directly from Lemma 3.2, Lemma 3.3, Lemma 3.4. ■

Corollary 3.6 Relating Coreness Value and Maximal Density

For each node , .

3.3. Compact parallel execution
Observe that it is inefficient (or infeasible) to naively execute the elimination procedure for all threshold values in parallel. Instead, at any moment, a node  just needs to remember the maximum threshold value  for which it still survives in the corresponding elimination procedure. Hence, in each round, a node  just needs to send its current value  to all its neighbors. Indeed, this observation is made by Montresor et al. [28] to produce a compact algorithm, which we restate in Algorithm 2. However, they run the algorithm till the exact coreness value for each node is achieved, while we have already shown that for any , 
 
 rounds are sufficient to obtain -approximation for both the coreness value and maximal density.

Message size. If every edge has integer weight that is polynomial in , then each message has size  bits. For general edge weights, to further optimize for the message size, we can restrict the sent numbers from some set , thereby achieving 
 bits per message. After the receiving the current numbers from its neighbors, the node calls the subroutine  to modify its own number, and round it down to the next number in . For instance, one can set  to include appropriate powers of , for some small . We use the convention  to denote the case when  includes all real numbers.

Keeping auxiliary information for the min–max edge orientation problem. One can augment the elimination procedure to approximate the min–max edge orientation problem. In this case, we consider . For each node , in addition to the current surviving number 
, node  also maintains a subset 
 of its neighbors for the min–max edge orientation problem. Intuitively, 
 contains the neighbors  such that edge  should be oriented towards  because  has a higher surviving number than . The subroutine  in Algorithm 3 is also augmented to return a subset , which is only needed in the last round; however, in the description, we still maintain 
 after every iteration for the purpose of analysis.

For technical reasons, we need to assume that for each node ,  is stateful; in particular, each node  remembers the surviving numbers of its neighbors in all past iterations, which will be used by  in the current iteration. Specifically, the following invariants are preserved.

Definition 3.7 Maintained Invariants

For the special case , the following invariants are defined on the variables in Algorithm 2.

•
For each node , 
.

•
For each edge , we have 
 or 
.



Download : Download high-res image (233KB)
Download : Download full-size image

Download : Download high-res image (264KB)
Download : Download full-size image
Remark 3.8

Note that the running time of  is  due to sorting. When the graph is unweighted, similar to the original implementation in [28], the running time can be reduced to  with the help of a counter array of size .

Fact 3.9

At the end of round , each node  has 
.

Corollary 3.10

By setting  to include appropriate powers of  and setting ≔
, Algorithm 2 produces 
 for each node  such that 
 
 
.

Lemma 3.11 Invariants are Maintained

For the case , after the end of every round of Algorithm 2, the invariants in Definition 3.7 are maintained.

Proof

For the first invariant, consider some node . By construction, when it calls  in Algorithm 3, the pair 
 returned satisfies 
.

We next consider the second invariant, which is satisfied initially. Suppose  is the first iteration after which the second invariant fails for some edge , i.e., after the iteration , the auxiliary subsets are such that 
 and 
, where we use the superscript  to denote the states of the variables at the end of iteration .

Same surviving numbers in consecutive iterations. We next show that 
 implies that 
. For the execution of  in the th iteration, suppose  is the smallest index reached in the for loop in line 5 of Algorithm 3. For contradiction’s sake, assume that 
, where 
 and  are the local variables in line 10. If 
, then we have 
, which implies that 
; if 
, then we have 
, which also implies that 
.

The same argument gives us 
. However, since the surviving numbers are monotonically decreasing with the iterations, we must have ≔
.

Vertex-induced surviving number. We next show that for the local variables 
 and  defined above, we actually must have 
. Otherwise, we have 
, which means 
 and 
. However, we have 
, which gives a contradiction. Therefore, we have 
.

Reaching contradiction. Since  is the first iteration such that the second invariant is violated, without loss of generality, we assume that 
. Since the first invariant holds, we must have 
. Observe that it is crucial that , because we need the upper bound  in the inequality, and a looser upper bound will not work.

Suppose in the th iteration, in the subroutine , the index of  among  is 
 after sorting. We next show that any node not in 
 must appear before  in this order. This is enough to get the contradiction, because we must have 
, which means  should have been included in 
.

We next consider what happens in  during -st iteration; suppose 
 is the smallest index reached in the for loop in line 5. Similar to before, consider the local variables 
 and 
 in line 10, where we use the widehat notation to distinguish from the local variables defined in the th iteration.

The first case is 
, which implies that 
 and 
. We know that the surviving number 
 of  can drop to 
. However, because of our stable sorting rule, in iteration ’s ,  must appear after 
 and any nodes not in 
.

The second case is 
, which implies that 
. This means that any node not in 
 must have surviving numbers strictly less than  during the sorting of the th iteration. Hence, all these nodes must appear before . This completes the proof. ■

Corollary 3.12 Approximation for Min–Max Edge Orientation Problem

For , after running Algorithm 2 for ≔
 
 rounds, the subsets 
 maintained by the nodes give a -approximation for the min–max edge orientation problem as defined in Section 2.

Proof

We prove in Lemma 3.11 that both invariants in Definition 3.7 hold. In particular, the second invariant implies that the subsets 
 form a feasible solution, i.e., for each edge , we have 
 or 
.

We next prove the approximation ratio. Let 
 be the maximum density of a subset in the given graph . The first invariant implies that for each node , 
, which, by Lemma 3.3, is at most . Finally, since the maximal density of every node is at most 
, we have 
.

By weak LP duality as mentioned in Section 2, 
 is a lower bound on the optimal value of the min–max edge orientation problem. Therefore, -approximation is achieved, as required. ■

The next lemma shows that the running times of our distributed algorithms are asymptotically tight, while the approximation factor cannot be improved without introducing a linear dependency on the diameter of the underlying graph.

Lemma 3.13 Lower Bound on the Running time

For , any distributed algorithm computing the coreness values or the maximal densities with an approximation ratio strictly smaller than  requires 
 
 communication rounds, where  is the number of nodes in the underlying graph ( sufficiently large). Moreover, any distributed algorithm for -core decomposition or min–max edge orientation with an approximation ratio strictly smaller than  would require  communication rounds.

Proof

Without loss of generality, we assume that  is an integer. We construct a graph  as follows. Start with a vertex  as a root, and construct a complete -ary tree with at least  leaves. Let  be the number of nodes in the tree, while let ≔
 
 be the depth of the tree. Since  is a tree, 
. We next construct another graph 
 obtained by planting a clique on the leaves of . Since every node in 
 has degree at least , we have 
. Any distributed algorithm for -core decomposition with an approximation ratio  must allow the root  to distinguish between  and 
. Hence, at least  rounds are needed. Also note that 
 while 
, so the same argument applies for the maximal densities.

Finally, Fig. 1.1 shows that any distributed algorithm for -core decomposition or min–max edge orientation with an approximation ratio strictly smaller than  requires  communication rounds. ■

4. Distributed algorithm for approximate densest subset problem
In Section 3, we gave a distributed algorithm that approximates the coreness value and the maximal density of each node, which also approximately solves the min–max edge orientation problem in the meantime. We next consider a distributed algorithm to return an approximate densest subset. This would mean that at the end, each node should know whether it is contained in the approximate solution. However, if we restrict the round complexity to be independent of the hop-diameter of the graph, then it is impossible for a node to know whether there is some other much denser subset that is many hops away. Therefore, we define the following notion of distributed approximation of the densest subset problem.

Definition 4.1 Distributed Approximation Algorithm for (Weak) Densest Subset Problem

For , a distributed algorithm achieves -approximation for the densest subset problem, if after the algorithm terminates, there exists a collection of disjoint subsets 
 of nodes such that the following hold:

•
For each , every node in 
 knows that it belongs to 
 (and also the density of 
). To be specific, each 
 will have some vertex 
 as its leader, and every node in 
 knows the identity of the leader 
 (but might not know who else is in 
).

•
There exists some  such that the density 
 
, where 
 is the maximum density of a subset in the input graph.

Suppose we fix the approximation ratio . In view of Lemma 3.3, we set ≔
 
. The distributed algorithm consists of several phases.

Phase 1: Approximating the maximal density. In this phase, Algorithm 2 is run for  rounds. After that, each node  knows some number 
, which is a -approximation of its maximal density.

Phase 2: Building BFS trees. In this phase, each node will try to identify a leader  who is the node within  hops with the largest value 
. Moreover, for each potential leader , a breadth-first-search (BFS) tree rooted at  with depth at most  is constructed in Algorithm 7. To resolve ties among nodes with the same 
 value, we assume that there is a global ordering  on  that is known by every node. This induces a total ordering on 
 defined by 
 iff (i) 
, or (ii) 
 and .


Download : Download high-res image (311KB)
Download : Download full-size image
Fact 4.2

Suppose node  has the maximum value 
 and is also the maximum under the ordering . Then, Algorithm 7 correctly constructs the BFS tree rooted at  that includes all nodes within  hops from  in the original input graph .

Remark 4.3

Observe that if , then  is the root of some BFS tree. Moreover, every node  in this BFS tree has 
.

Phase 3: Elimination procedure within each BFS tree. In this phase, each node  communicates only with its  and , and ignores all other nodes. Within each BFS tree, all nodes have a common 
. The elimination procedure in Algorithm 1 is run with threshold value 
 from the leader. However, in the augmented Algorithm 5, each node remembers its weighted degree for every iteration, which is later used to determine an approximate densest subset. Moreover, we optimize the algorithm such that a node does not participate in the protocol once it is eliminated.

After this phase, each node  computes two arrays 
 and 
. For each , 
 indicates whether  survives after iteration ; if 
, then 
 gives the corresponding weighted degree. Lemma 4.4 gives some ideas on how these results can help to find an approximate densest subset.


Download : Download high-res image (266KB)
Download : Download full-size image
Lemma 4.4 Surviving Nodes in Some Iteration Give Approximate Densest Subset

Let , and ≔
 
. Suppose the augmented elimination procedure in Algorithm 5 is run for  rounds on the depth- BFS tree rooted at some node  with threshold value 
 (which is the value returned by running Algorithm 1 on the original graph  for  rounds). Suppose in Algorithm 5, for , 
 is the set of surviving nodes at the end of round . Then, there exists some  such that the density of 
 is at least 
 
.

Proof

The proof uses the same idea as in Lemma 3.3 (which is inspired from [5], [9], [21]).

For , the density 
 
 
. Therefore, 
 
 
. Observe that since threshold 
 is used, the node  still survives after round , implying that 
. In addition, 
. Hence, there exists some  such that 
 
 
 
, where the last inequality follows because 
 
. ■

Phase 4: Aggregation and finding approximate densest subset. In view of Lemma 4.4, one should compute 
, where the density satisfies 
 
 and the summation is over all nodes in the BFS tree. The details are given in Algorithm 6.


Download : Download high-res image (407KB)
Download : Download full-size image
Optimizing message size. During aggregation, a node  sends its length- aggregated arrays 
 to its parent. Since the depth of the BFS tree is at most , the aggregation part takes  rounds, but the size of each message contains  words. To reduce the message size, the entries of the arrays can be sent to the parent in a pipelined fashion. For instance, one entry from each of the two arrays are sent per message to the parent, and the number of rounds increases by .

Corollary 4.5 Correctness

Algorithm 6 gives -approximation to the distributed densest subset problem as in Definition 4.1.

Proof

This follows from Fact 4.2 and Lemma 4.4. ■

5. Experiments
5.1. Experimental setup
In this section, we evaluate the empirical approximation ratio for -core decomposition and min–max edge orientation produced by the compact elimination procedure (Algorithm 2) on real-world graphs. Since we are interested in the empirical approximation ratio, it suffices to simulate the distributed algorithms on a personal computer.

We consider several large real-world graphs available at [26]. Statistics of the datasets we use are summarized in Table 1. We assume that each edge has unit weight. The set  of threshold values is implicitly the set of integers.


Table 1. Specifications of the datasets analyzed.

Dataset		
Amazon	334 863	925 872
DBLP	317 080	1 049 866
YouTube	1 134 890	2 987 624
WikiTalk	2 394 385	5 021 410
LiveJournal	3 997 962	34 681 189
5.2. -core decomposition
We compare the following algorithms:

•
Our Algorithm. The elimination procedure in Algorithm 2. As observed in [28], the estimations given by this algorithm converge to exact coreness values.

•
Frank–Wolfe Based Algorithm. This was proposed by [12], which can also be performed in the distributed setting. However, the estimations converge to maximal densities instead. Moreover, in [12], convergence is analyzed theoretically in terms of additive error, rather than (multiplicative) approximation ratio.

Approximation ratio. For each algorithm, we use  to denote the current estimation for each node. We use  to denote either the coreness value  or the maximal density . For our algorithm, we always have , but this might not be the case for the Frank–Wolfe based algorithm. Hence, we define the approximation ratio as ≔
 
 
.

The following quantities are plotted against the number  of iterations:

•
Theoretical guarantee of our algorithm. As given in Theorem 3.5, the theoretical ratio is 
 
.

•
Average approximation ratio: 
 
.

•
Maximum approximation ratio: 
.

Results. Empirical results are illustrated in Fig. 5.1, Fig. 5.2, Fig. 5.3, Fig. 5.4, Fig. 5.5. It can be seen that our algorithm converges to , while the Frank–Wolfe based algorithm converges to . However, both algorithms can achieve approximation ratios close to 2 quite quickly, compared to the theoretical guarantee of our algorithm. This is interesting, because the approximation ratio of the Frank–Wolfe based algorithm was not analyzed in [12].

When we study the distribution of  in the graphs (see Table 2 and Fig. 5.6), we see that the coreness value and the maximal density are quite close to each other for most nodes. Therefore, although maximum approximation ratio for  in our algorithm and that for  in the Frank–Wolfe based algorithm are (slightly above)  as shown in Fig. 5.1, Fig. 5.2, Fig. 5.3, Fig. 5.4, Fig. 5.5, only a tiny fraction of nodes fall into that worst case.


Download : Download high-res image (441KB)
Download : Download full-size image
Fig. 5.1. Comparison of rate of convergence between two algorithms, Amazon.


Download : Download high-res image (447KB)
Download : Download full-size image
Fig. 5.2. Comparison of rate of convergence between two algorithms, DBLP.


Download : Download high-res image (452KB)
Download : Download full-size image
Fig. 5.3. Comparison of rate of convergence between two algorithms, YouTube.


Download : Download high-res image (448KB)
Download : Download full-size image
Fig. 5.4. Comparison of rate of convergence between two algorithms, WikiTalk.


Download : Download high-res image (449KB)
Download : Download full-size image
Fig. 5.5. Comparison of rate of convergence between two algorithms, LiveJournal.


Table 2. Distribution of the ratio of the coreness value  to the maximal density  over all nodes. For example, in dataset Amazon, for 51.6% of the nodes, ; for 34.8% of the nodes, .

Dataset					
Amazon	51.6%	34.8%	10.6%	2.6%	0.4%
DBLP	54.0%	25.4%	10.9%	6.6%	3.1%
YouTube	94.1%	4.4%	1.1%	0.3%	0.0%
WikiTalk	99.6%	0.2%	0.0%	0.0%	0.2%
LiveJournal	94.1%	3.9%	1.2%	0.6%	0.2%
5.3. Min–max edge orientation
We evaluate the quality of the approximate solution given by the compact elimination procedure for the min–max edge orientation problem. To this end, for each graph we compute the optimal value , i.e., the least possible maximum in-degree over all nodes using a (centralized) polynomial-time algorithm.3 Then, we execute Algorithm 2 to obtain gradually improving approximate solutions. Recall that for a node , 
 indicates the incident edges that should be oriented towards , so we define the approximation ratio as ≔
 
 
. We plot 
 against the number of iterations.

However, the final approximation ratio achieved varies between datasets. Indeed, it is not difficult to see that for unweighted graphs,  defined above will converge to 
 
. This fact is reconfirmed by Table 3, where we compare the maximum coreness value over all nodes and the optimal value for the min–max edge orientation problem.

6. Conclusion and future directions
We have shown that the well-known elimination procedure, when implemented in a distributed setting, provides a constant approximation to both the coreness values and maximal densities. The asymptotically round complexity is tight and independent of the diameter. Moreover, by augmenting the elimination procedure, the min–max edge orientation problem can also be approximated with the same theoretical ratio. Finally, the procedure is further augmented to approximately solve a weaker version of the densest subgraph problem.

Empirical results on real-world graphs show that the approximation ratio often converges to  much quicker than what the worst-case analysis suggests. Are there any special properties that can explain this phenomenon? Moreover, the theoretical lower bound on round complexity applies to the worst case approximation ratio over all nodes. Can one improve the theoretical upper or lower bounds on the round complexity when average approximation ratio over all nodes is considered?