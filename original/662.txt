Abstract
The robots using visual simultaneous localization and mapping (SLAM) system are generally experiencing excessive power consumption and suffer from depletion of battery energy during the course of working. The intensive computation necessary to complete complicated tasks is overwhelming for inexpensive mobile robots with limited on-board resources. To address this problem, a novel task offloading strategy combined with a new dense point cloud map construction method is proposed in this paper, which is firstly used for the improvement of the system especially in indoor scenes. First, we develop a novel strategy to remotely offload computation-intensive tasks to cloud center so that the tasks that could not originally be achieved locally on the resource-limited robot systems become possible. Second, a modified iterative closest point algorithm (ICP), named fitness score hierarchical ICP algorithm (FS-HICP), is developed to accelerate point cloud registration. The correctness, efficiency, and scalability of the proposed strategy are evaluated with both theoretical analysis and experimental simulations. The results show that the proposed method can effectively reduce the energy consumption while increase the computation capability and speed of the multi-robot visual SLAM system, especially in indoor environment.

Previous
Next 
Keywords
Energy consumption

Multi-robot system

Computing offloading

SLAM system

ICP algorithm

1. Introduction
The aim of simultaneous localization and mapping (SLAM) technology is to provide good estimation of both the robot pose and the map in unknown environments, where an accurate map is unavailable [16]. With the rapid development of onboard hardware, software and machine learning technology, increasing research interests have been witnessed on using SLAM technology to solve various complicated tasks in daily life, e.g., fire rescue, geological exploration, underwater search and archeological excavation. In order to adapt to practical engineering applications, one of the most notable research interests is three-dimensional SLAM. At present laser radar assisted camera is always used as signal inputs in 3D-SLAM solutions and commercially available products, such as drones. However, due to the high cost, fast power consumption and low precision of laser radar, the application of the laser radar based 3D-SLAM technique is hindered to be widely used in the future.

3D visual SLAM, which uses depth cameras as the only sensor inputs, is considered to be beneficial in scenarios where the requirement of cost, energy and weight of the system is strict. Due to various advanced cameras, 3D visual SLAM has attracted more and more attention and opens up a whole new range of possibilities in robot autonomous navigation field, e.g., virtual reality (VR), augmented reality (AR), and of course, autonomous driving technology. However, 3D visual SLAM still faces the challenges of extensive amount of computation and communication tasks where powerful CPU and other on-board resources are required. It turns out that the cost of a single robot is too high to be applied to the actual scenario. Thus, a new performing model or strategy is of great necessity in the 3D visual SLAM process.

To overcome the aforementioned problems, more research works have been conducted to incorporate mobile multi-robot system into SLAM, so that the complexity of a task can be shared by a group of small, often less expensive mobile robots. Generally speaking, mobile robots are limited in size and power, which hinders them from carrying powerful computation and storage unit. Consequently, it might not be fair to ask them to perform extensive computation locally. In order to get rid of this bondage, the “cloud robotics” proposed by Dr. Kuffner [13] in 2010 provides us with a new way to handle complex robot tasks. In cooperation with the cloud platform and data center, robotic network can improve its ability by a large margin. As for applying cloud robotics to solve multi-robot visual SLAM problems, there are still some challenges to be tackled.

We summarize the main problems faced by the multi-robot 3D visual SLAM into the following three aspects.

(1)
The main task of 3D visual SLAM system is image processing, which requires intensive calculation. However, for most inexpensive robotic embedded devices (e.g., Raspberry Pi), it can hardly be handled in real time  [5], [35], [37]. (Also, some recent hardware like NVIDIA Jetson Nano provides us with powerful GPU to process the images. But the price of NVIDIA Jetson Nano is about two times of Raspberry Pi with similar hardware [1]. On the other hand, the battery consumption is always proportional to the circuit complexity. The strategy proposed in the paper is also expected to be applicable in such embedded devices.) Moreover, some high robustness visual SLAM systems, such as ORB-SLAM [25] and RTAP-MAP [15], can only be calculated by powerful computer or even a server. Therefore, how to apply these visual SLAM systems to robotic network needs to be investigated.

(2)
In order to solve the problem of excessive calculations, some researchers have proposed using cloud robotics to offload tasks to cloud. However, images are the only data input for visual SLAM system. The memory space occupied by images is very large, which may cause certain problems while transmitting, such as communication loss and unexpected latency. For example, Wi-Fi connections are not invariably usable and reliable [29]. Moreover, security issues such as hacker interception tend to occur when using images as the only input. Hence, a safe and efficient offloading strategy is required.

(3)
The state of art SLAM systems which can be used in robot embedded devices always have latency and cannot make sure the robustness of the system. After a long running, robots always lost themselves and cannot be restarted in a short time. In the condition of absence of any priori artificial calibration, how to achieve real-time calculation as well as ensure system robustness also need to be studied.

According to the above analysis, the main problems to solve are the insufficient on-board computing power and limited network transmission bandwidth, which cause multi-robot visual SLAM system very hard to achieve high robustness and real-time calculation. Therefore, we will tackle the aforementioned challenges with a two-step strategy. In the first step, we elaborate on the partial computing offloading method. Second, we propose a novel iterative closest point algorithm to reduce the computational complexity of multi-robot visual SLAM system.

The main contributions of our work can be summarized as follows.

(1)
A novel partial offloading strategy: It is built to balance the calculation of local robots and cloud. To take full advantage of local computing and cloud computing, a strategy to offload most of the computation into the powerful cloud while reducing the amount of data which needs to be transmitted is designed. An efficient algorithm is proposed to find the best offloading point of the system. By this way, incomplete information is offloaded to the cloud in which case even if a hacker intercepts the network, it is hard for them to steal original and complete image information. In addition, this method greatly reduces the energy consumption as well as decreases the processing time.

(2)
Improvement of point cloud map registration algorithm: The other main tasks of SLAM is map building, which is especially important for multi-robot SLAM system. The robots work independently in the environment and the maps built by each of them need to be matched. In order to reduce the complexity of the whole SLAM system, we improved classic registration algorithm, iterative closest point (ICP) algorithm, by proposing a novel fitness score hierarchical iterative closest point (FS-HICP) algorithm. The improved algorithm reduces the time costs and energy consumption.

(3)
Construction of dense and semi-dense point cloud map: There are two kinds of map: the dense and semi-dense point cloud map, which are a set of data points in space and reflects the points on the external surfaces of objects [32]. The global dense map provides abundant visual and geometric information and is able to represent the environment for localization, path planning, virtual reality and so on [19]. On the other hand, a semi-dense point cloud constructs a sparse map containing limited information which describe partial features of the environment (general shape etc.) in application such as obstacle avoidance and path planning [26], [40]. In order to describe whether the scene is accessible, a semi-dense octree map is also constructed. And the two kinds of maps have different pros and cons so that they have different applications.

We also conduct extensive simulation and experimental evaluations to demonstrate the efficiency of the methods. Our partial offloading strategy and the FS-HICP algorithm can greatly reduce the whole system running time and the energy consumption.

The rest of this paper is organized as follows. Section 2 presents a brief survey of the related works. Section 3 presents the system model and then introduces the formulation of the partial offloading problem and multi-robot registration model. Section 4 discusses the details of the methods. Section 5 gives the evaluation results. Finally, Section 6 concludes the paper and presents our proposed future work.

2. Related works
Visual SLAM, as an effective and economical way to help robot localization and mapping in an unknown environment, has reached substantial robustness and accuracy in the centimeter range for single robot applications [7], [27]. ORB-SLAM2 [27] system proposed by Raul Mur-Artal et al. which uses three threads to work simultaneously, greatly improves the efficiency of the algorithm. And multi-robot systems have been becoming popular in numerous scenarios, ranging from search and rescue applications to digitization of archeological sites. Some researchers [10], [12], [33], [34] aim to improve the multi-robot SLAM network, for example [33] uses an information fusion technology, in which the information processed by the single robot is collected and optimized by the base station. Another example [10] is based on the multi-robot altitude map positioning of the unknown initial relative pose. Other researchers focus on improving the speed and efficiency of the whole system, for instance, MTM (map transformation matrix) is a multi-robot fast SLAM proposed by HeonCheol Lee [18], which only calibrates the particle of the nearest point. Of course, there are a lot of successful multi-robot robot models [6], [11], [17], [30], such as five-legged robot [30], using SLAM technology to achieve different kinds of tasks in unknown environment.

Because the application environment of multi-robot SLAM is relatively complex, in order to solve the calculation problem, some successful frameworks based on cloud robot architecture have been proposed. For instance, DaVinci [2] provides the scalability and parallelism advantages of cloud computing implemented in the FastSLAM algorithm, which is a software framework that provides the scalability and parallelism advantages of cloud computing implemented in the FastSLAM algorithm. Rapyuta [9], an open source Platform-as-a-Service (PaaS) framework designed specifically for robotic applications such as SLAM, can help robots to offload heavy computation by providing secured customizable computing environments in the cloud. All these frameworks have verified the feasibility and effectiveness of using “cloud  robot” synergy in solving SLAM problems.

The performance of robots depends not only on their motion mechanism, but also on their familiarity with the environment. At present, the two-dimensional map creation technology has been well developed and the three-dimensional map creation technology still has great potential in the enhancement of efficiency and reduction of cost. A tailoring and scaling iterative closest point (TsICP) algorithm is proposed by Liang Ma [21], which considers the problem of merging grid maps with different resolutions. Authors in [38] introduce a global optimal solution to Euclidean registration in 3D. The work in [23] modifies the well-known iterative closest point (ICP) algorithm by introducing the concept of deletion mask. Except using traditional method, Rebro University’s Magnusson team proposed a 3D NDT point cloud registration method [22], which has little correlation with the initial value. Since then, many algorithms have been improved based on NDT algorithm. In terms of application, Kinect Fusion [28] proposed by Imperial College and Microsoft Research Institute in 2011, takes advantage of the point–plane method to realize point cloud map registration, which opened a prelude of real-time 3D reconstruction with RGB-D camera.

To date, most successful local SLAM systems have the problem of too much calculation. Thus, the “cloud robotic” technology is proposed to solve this problem. However, they mainly focus on the multi-robot network structure, network transmission path, data compression and so on, without considering combining cloud computing and local computing [3], [31], [36], [39]. Different from most works mentioned above, in this paper, a novel strategy is used to choose a offloading point to reduce the energy consuming by offloading part of the calculation to cloud. In addition, this paper proposes an improved ICP algorithm realizing multi point cloud map registration, with the objective to reduce the time of iterations as well as improve convergence accuracy.

3. System model and problem formulation
In this section, the total system framework is presented firstly. Then the model of multi-robot partial computing offloading is proposed. Finally, the multi-robot dense point cloud map model and its calculation method is introduced.

3.1. System framework
In this paper, it is assumed that mobile robots work independently to execute tasks of SLAM in an unknown indoor scenario. In the SLAM system, the two main tasks are localization and mapping. Localization which includes tracking thread, localmapping thread and loopclosing thread is mainly meant to calculate the trajectory and keyframe of the robot in an unknown environment. Mapping is mainly used to construct a global map of the scene based on local maps got by each robot. In the system model, the keyframes and trajectory got by localization of each robots are used by the mapping process.

During the localization process, robots may lose themselves unexpectedly, especially when running for an extended period of time because the indoor environment is usually complicated. If the complexity of the algorithm for better robustness and accuracy is increased, it will lead to too much computation. On this condition if all the data (captured images) is offloaded to the cloud to compute, the transmission will be too slow due to large amount of data. Meanwhile, during the mapping process, the partial 3-D dense map built by each robot takes up too much memory. In order to unify them into the global coordinate, very large amount of calculation is needed, which will cost a lot of time and energy. Therefore, the partial computing offloading model and improved registration of point cloud map model are used to solve the problems mentioned above. The most suitable point is investigated to choose to offload parts of the tasks which require extensive calculation, to cloud in order to reduce the total energy consumption of the localization process. Furthermore, a novel point cloud registration algorithm is utilized to reduce the time of dense map building in the mapping process.

It is assumed that there are several heterogeneous robots 
 which build local maps from different starting points. Each robot gets images  independently. During the localization process, the most suitable offloading point is chosen to offload some tasks to cloud for further calculating. The keyframe and trajectory obtained by each robot will be used to build partial dense point cloud map, which can be expressed as 
. Finally, during the mapping process, the partial maps match one by one to get a global coordinate with our novel point cloud registration algorithm. Fig. 1 presents the offloading strategy based on the RGBD SLAM whose basic concepts (items) are introduced in [26]. In general, RGBD SLAM based on the feature point method is divided into two steps: front-end and back-end. At the front-end, the classic approaches include extracting key points such as ORB, calculating feature descriptions and then find their depth values from the camera. Then according to the two sets of matching points, the ICP method etc. [24] is used to calculate the pose change between frames to obtain the camera’s inter-frame motion. Then the calculated results are passed to the back-end as initial values. The back end uses graph optimization to optimize them.

The whole system energy consumption of our framework is noted as 
, the energy consumption of localization process as 
 while 
 represents the energy consumption of mapping process. Hence, the 
 can be expressed in (1). (1)


Download : Download high-res image (282KB)
Download : Download full-size image
Fig. 1. Overview of the system.

The basic viewpoint in our problem is that the smaller the value of 
 which can be seen in (2), the better the system works. So the problem can be divided into two separate parts: to get the smaller localization part and mapping part as far as possible. (2)

The next two parts will introduce the models to get the minimum value of 
 and 
, respectively.

3.2. Partial computing offloading model
In a nutshell, partial computing offloading or the so-called cloud robotic system [8] can be summarized as encapsulating partial high-density computing tasks that would otherwise be performed by local robot nodes into a central processor or cloud to execute, by which way the energy consumption of robot can be reduced and the network efficiency will be improved. It is shown in Fig. 2. As in Fig. 1, the visual SLAM system processes the images one by one. During the processing of one image there are several tasks to carry out in sequence. Thus, some potential offloading points at the end of each step exist during the execution.

For each robot in the system has the same status level, we can calculate the minimum value of each 
 in order to get the minimum value of the 
. In the partial offloading model proposed, some parts of the tasks are calculated by local devices while the others are calculated by cloud. It is supposed that 
 represents the energy consumed by one of the local embedded device while 
 represents the energy consumed by cloud. Transmission consuming is expressed by 
. The calculation of 
 is given in (4). (4)

Problem description: With 
 as the energy consumption of each robot, our aim is to choose a suitable offloading point, which can make each 
 reach its minimum value, at which time the 
 can reach its minimum value as much as possible.

3.3. Registration of point cloud map model
If every frame of the point cloud is merged into the map, the capacity of the local map will be too large, which will reduce the real-time performance of the system. As the pose of adjacent frames changes very little, it is proposed to use key frames to build the point could map.

It is supposed that multi-robot 
 moves in a room. The partial dense point cloud maps of the room built by these robots can be expressed as 
. These maps need fusing one by one and unifying to a global coordinate and finally a complete global point cloud map of the whole room is built as shown in Fig. 3.

The energy consumption of mapping process is expressed by 
. In this paper, three-dimensional global dense map is built in the cloud. If the power of cloud is 
 and the time used by mapping processing is 
, the 
 can be expressed as (5). (5)


Download : Download high-res image (224KB)
Download : Download full-size image
Fig. 3. Multi-robot dense map registration model.

Problem description: Our aim is to propose a novel point cloud registration method in order to make the value of 
 and 
 as small as possible.

4. Method description
The objective of this section is to introduce our strategy to find the minimum value of 
 stated in Section 3, and then introduce an improved point cloud map registration algorithm, Fitness Score Hierarchical Iteration Closest Point(FS-HICP) algorithm, in order to reduce the value of 
.

4.1. Offloading point selection
Generally speaking, mobile robots use embedded devices as processing unit. It is supposed that the power of the local embedded devices, in watts, is 
 and constant, while 
 represents the power of cloud server. 
 and 
 are used to denote the data quantity to be calculated by local devices and cloud. 
 and 
, in GB per second, represent the data processing speed of the local devices and the cloud, respectively. The visual SLAM problem includes much of image process and the images could be calculated by the pixel quantity or the pieces of image (uniform pixel per image). To approximate the relationship between the energy consumed and data quantity, a linear model is adopted assuming that the processing power is constant. Calculations of 
 and 
 mentioned in Section 3 are shown in (6), (7). (6)
 
 (7)
 

It is assumed that 
 is the power for sending and receiving data. 
 represents the time consuming of the transmission processing. The calculation of 
 is shown in (8). (8)

So the 
 can be summarized as in (9). (9)
 
 

Suppose  is the offloading point. The time consumed by tasks which have been executed divided by the time consumed by total tasks as the value of , which can be seen in (10). This definition is essential considering the experimental measurability of the quantities. In addition, the definition of  can also be reflected in the relationship of 
 and 
 as Eq. (11) shows. But the data amount processed is difficult to obtain while the measurement of time is more accessible. The value of  is between 0 and 1. When , all the tasks are computed by cloud and when , all the tasks are computed by local embedded devices. Thus, 
 and 
 can be rewritten as is shown in Eq. (11). (10)
 
 (11) 
 

Because the transmission time is related to the amount of data that needs to be transferred at the offloading point, 
 is a function of  as shown in (12). (12)

Thus, we substitute (11), (12) into (9) to get (13). (13)
 
 

From (13), the value of  determines the value of 
.

4.2. FS-HICP algorithm
Suppose 
 and 
 are the two point cloud data sets needed to be matched and we simply mark them as  and . First, we search each point 
 in the point set  for its nearest point 
 in the point set  as the corresponding point. Then the corresponding set of 
 is set to 
 to solve a European transformation ,  in order to satisfy (14). (14)

The centroid position  and  of the two sets of points is calculated and so are the coordinates of each point after removing the centroid, shown in (15). (15)

Then the rotation matrix based on the following optimization formula is calculated (16). (16)
 

Finally, the  is derived according to , as shown in (17). (17)

The transformation matrix is a fourth-order matrix containing translation() and rotation(), which can be expressed as (18). (18) 
 

The classical ICP algorithm (point–point algorithm) proposed by Besl and McKay [4] can be described as  is unchangeable while  rotates and translates iteratively to get as close as possible to the . In the improvements of the classic algorithm, the most famous one is the point–plane method used by Kinect Fusion [28]. However, both of them cannot achieve registration quickly and the iteration times and the overall time costs are too much  [20], [28].

In this paper, the hierarchical iterative closest point algorithm based on fitness score (FS-HICP algorithm) is proposed. There are three main improvements in this novel algorithm.

(1)
The stop condition according to the change of fitness score is carefully revised.

(2)
The idea of layered filtering is used to accelerate iteration.

(3)
In the iterative process, the maximum response distance will be adjusted according to the matching quality so that the algorithm can converge in a relatively short time.

The flow of the FS-HICP algorithm is shown in Fig. 4.

5. Experimental results
This section presents the evaluation results of the improved methods through extensive simulation and experimental studies. The results include two parts: computing offloading and point cloud map fusion.

The embedded device used on the multi-robot system in this paper is Raspberry Pi 3 and data is transferred by Wi-Fi network. The RGB-D image acquisition device is Kinect 2.0 and the master node used in the simulation experiment is a desktop computer with 16 G memory and 4-core 3.3 GHz CPU. The ROS system is used for network transmission. In the simulation experiments, the data sets used are RGB-D data sets from TUM (https://vision.in.tum.de/data/datasets/rgbd-dataset/download). In the simulation experiments of FS-HICP, there are four robots 
 with the TUM. And in the practical application part, two robotic cars equipped with Kinect are utilized in laboratory and corridor scenes respectively. The programming code is based on C++.

5.1. Computing offloading
In general, the offloading point  cannot be directly determined by the theoretical model employed. In our experiment parts, we will choose a suitable offloading point  in order to make 
 get its minimum value. When we know the best offloading point, the steps of partial computing offloading model can be seen as follow:

(1)
Get frames from Kinect camera.

(2)
Based on the value of , some tasks are calculated on local embedded devices.

(3)
Store the results calculated by local embedded devices in a YAML file.

(4)
Send the message to cloud by ROS system.

(5)
Calculate the other part of the SLAM on cloud using the data uploaded by local devices.

The specific implementation process is shown in Fig. 5.

In this section, it is assumed that the total calculation is 1G. The values of the parameters in the partial computing offloading model are shown in Table 2 [14]. So the formula (13) can be written as in (19). (19)
 
 


Table 1. Important notations.

Notation	Description
Power of local devices
Power of cloud
Power of network transmission
Data processing speed of local devices
Data processing speed of cloud robotics
Calculation of local devices
Calculation of cloud
Total calculation amount
Transmission time
Offloading point(Independent variable)
(19) can be simplified to (20). (20)


Table 2. Parameter values.

Parameters	Description	Value
Power of local devices	0.9 W
Power of cloud	0.3 W
Power of network transmission	1.3 W
Data processing speed of local devices	1.2 G/s
Data processing speed of cloud	3.3 G/s
Total calculation amount	1 G
During the localization process, there are several potential offloading points. As for processing 10 images, of which the pixel is 640 × 480, the average time consumed by each part can be seen in Table 3. Based on (10), we can calculate the value of . Then according to our experiment, the minimum data needed to be offloaded at each possible offloading point and the memory occupied by these data can be described in Table 4. Suppose the network transmission rate is 10 Mbps, the value of 
 is shown in Fig. 6.

Based on (19), the curve of the 
 can be seen in Fig. 7. From the curve we can clearly see, the best offloading point is . That is to say, extractor, descriptor and compute stereo are calculated in the local device and other parts are offloaded to the cloud.


Table 3. Time consumption of each part in the localization processing.

Items	Extractor	Descriptor	Compute stereo	Feature match	PnP	Others	Total
Time (s)	0.0154	0.0124	0.0062	0.0236	0.0022	0.011	0.0702

Table 4. Data to be offloaded at each possible offloading point.

Data	Memory footprint (M)
0	RGBimageDepth image	6.3
0.22	RGBimageDepth image	6.3
0.396	Featuresdepth image	3.8
0.484	Featuresdepth value	2.9
0.818	Featuresdepth valueRotation matrix	3.7
0.852	Featuresdepth valueRotation matrix  undistort	4.0
Featuresdepth valueRotation matrix  undistortkeyframe	4.2

Table 5. Memory size occupation of different files.

1	2	3	4	5	6	7	8	9	10
YAML (kB)	290.6	290.6	289.4	290.6	288.9	291.2	290.7	290.1	290.8	290.5
RGB image (kB)	506.5	498	498.8	497	501.3	500.2	507.7	495.7	484.7	489.8
Depth image (kB)	125.5	124.9	123.7	123.5	123.5	122.3	121.9	121.2	120.7	120.9
RGBDepth images (kB)	632	622.9	622.5	620.5	624.8	622.5	629.6	616.9	605.4	610.7
Now we have got the best value of , and then we design our experiments from the following three aspects.

5.1.1. Memory comparison
In this experiment, the key information extracted in each image, which records the values of feature points, depths, descriptors, and time stamps, is stored in a YAML file. Comparing the original RGB image  Depth image with the YAML file including the feature points, the memory occupation can be seen in Fig. 8a.

As is shown in Table 5, the frames caught by Kinect camera include RGB images and Depth images, which take up twice as much memory as YAML file.

5.1.2. Transmission rate comparison
This paper assumed the bandwidth is 10Mbps. If there are 1000 frames needed to process, the time used by offloading all the images to the cloud (total offloading) and offloading the YAML file after extracting the feature point to the cloud (partial offloading) is compared in Fig. 8b. From the figure we can see that the transmission time is about 200 s using our partial offloading method, which is much less than total offloading.

5.1.3. Time of localization process comparison
Since the tasks of feature point extraction are calculated by the embedded device, the speed of the localization processing is significantly improved, as shown in Fig. 9. The reduced time cost demonstrates the effectiveness and advantages of offloading strategy compared with the total offloading method.

To further prove the efficiency of the offloading approach, system average delay is shown in Table 6 (for example there are 1000 frames). It can be seen that if we offload all the tasks to cloud the average sum time would be 578 s. However, if we offloaded partial tasks  to the cloud, the average total time cost is 327 s. Actually, the average running time of the system is 282 s, nearly one-half of the time of total offloading.


Download : Download high-res image (368KB)
Download : Download full-size image
Fig. 9. Running time comparison of the localization processing.


Table 6. Running time comparison of the system.

Robot	Trans	Cloud	Sum	Total
Total offloading (s)	0	498	80	578	578
partial offloading (s)	50	232	45	327	282

Table 7. Pseudo code of the FS-HICP algorithm.



5.2. FS-HICP algorithm
The specific execution steps of the algorithm are as follows:

(1)
Delete the error point and useless point. In this paper, a point with a depth value of 0 or a depth value greater than 7000 is considered to be an invalid point and it will be rejected. Then the points are filtered and the voxel filter size is 0.1.

(2)
Match two point cloud map. In the process the algorithm is iterated twice internally.

(3)
Determine whether the fitness score is less than a given threshold (50%). If not, the source is updated and return to the second step and continue iteration.

(4)
Filter with a voxel filter size of 0.05.

(5)
Match two point cloud map. In the process the algorithm is iterated twice internally.

(6)
Determine whether the fitness score reduces. If not, the source is updated with a certain probability (30%) and return to the fifth step to continue searching the iteration direction. If the fitness score does not reduce more than three times or the algorithm reaches the convergence precision, the iteration is stopped. If the fitness score decreases, reduce the minimum iteration distance between two points, update the source and return the fifth step to continue the iteration.

The pseudo code of FS-HICP algorithm is shown in Table 7.

In this experiment, there are four robots 
. And the local dense maps obtained by each robot can be expressed as 
. The point cloud maps obtained by different robots are shown in Fig. 10.

We design five experiments based on the  scene mentioned above to test the efficiency of the FS-HICP algorithm proposed in this paper.

5.2.1. Point cloud voxel filtering
Since the partial room map obtained by each robot is very large, the first step of the algorithm is the point cloud map filtering. The number of the points in the map has dropped dramatically after filtering. The running time comparison of the ICP algorithm before and after filtering is shown in Table 8. It can be seen that after filtering, the running time of registration be greatly reduced.


Table 8. Running time comparison of the mapping process.

1	2	3	4	5	6	7	8	9	10
Time of classical ICP (ms)	521 269	524 367	523 680	519 856	523 154	521 236	527 189	537 834	521 238	519 807
Time of FS-HICP (ms)	4540	4569	4493	4526	4498	4446	4487	4533	4496	4510
5.2.2. FS-HICP algorithm time comparison
In the indoor scenario discussed in this paper, two ICP algorithm strategies were investigated: the basic point–point ICP algorithm that comes from the point cloud library (PCL) and the point–plane ICP algorithm used by Kinect fusion. The time taken by the three algorithms is shown in Fig. 11. It was found that due to the large number of feature points in the indoor scene, point–plane method will generate a lot of mistakes. The convergence speed of FS-HICP method is much faster than the other two algorithms in four different situations.

5.2.3. Convergence speed comparison
This section analyzed two typical indoor scenes. In one case, 
 and 
 registration, there are fewer data points, but the overlapping parts are concentrated and the density of point clouds in the overlapping part is larger. In the other case of 
 and 
 registration, there are more data points, including features of ground and walls, which have relatively fewer feature points including desktops, chairs and so on, in which the overlapping parts are scattered. Hence, it is difficult to achieve a right match in this case and the iteration time is more than in the first case. Fig. 12a shows that in relatively simple scene, FS-HICP algorithm can quickly reach convergence. In relatively complex scene, as shown in Fig. 12b, the point–plane algorithm cannot get a satisfactory result, and the convergence rate of point–point algorithm is slower than FS-HICP algorithm.

5.2.4. Error rate comparison
This section presents the error rate of the algorithm as a function of iteration times. Firstly, the standard rotation and translation (transformation matrix)  were given and then transformation matrix 
 was obtained after the iteration. The variance of 
 and  was used to represent the error rate of the algorithm.

The standard transformation matrix  is given in (21). (21)
 

The transformation matrix 
 obtained after the iteration of the ICP algorithm is given in (22). (22)
 

The variance between the two matrices is used as the criterion for error rate evaluation. It is expressed by 
, which can be seen in (23). (23)

Fig. 13 is the experimental result of error rate. We tested three different cases (
 and 
 registration; 
 and 
 registration; 
 and 
 registration). In each case, the three different algorithms iterated 30, 40, and 60 times separately. We can see, FS-HICP can get the same or even lower error rate than point–point algorithm. The registration processing of FP-HICP algorithm can be seen in Fig. 14.

5.2.5. Map building
The final global indoor point cloud map is shown in Fig. 15a. The map constructed by this method may be used for real-time navigation of robots. However, the point cloud maps occupy a lot of space and display too much unnecessary information. Additionally, it cannot reflect the transparency of space and is unable to process moving objects and update maps in time. In this paper, the global point cloud maps were converted into octree maps for storage as shown in Fig. 15b. The memory space occupied by octree map is 4.9M while the memory space occupied by point cloud map is 11.4M. It turns out that octree map can save more space.

5.3. Practical application
In order to prove the validity of the whole system, in this section, we use two robotic cars equipped with Kinect cameras as experimental platforms and the two cars move independently in the scene.

5.3.1. Indoor scene
Our laboratory is used as an indoor scene. In this scenario, the number of feature points is large and the environment is relatively complicated. The motion path of the two robots can be seen in Fig. 16a. The blue boxes in the figure represent the keyframes and the green lines are the motion path of the robots. Global keypoints are the red points in the figure while local keypoints are represented by black points. It is assumed that the local map got by 
 is 
 and the local map obtained by 
 is 
. The registration of 
 and 
 can be seen in Fig. 16b and the point cloud map after splicing is shown in Fig. 16c. Since the frames got by Kinect cameras have distortion error, which are not as accurate as the frames in TUM data set, we can see there is a little dislocation in Fig. 16c, but it does not affect the overall map.

5.3.2. Corridor scene
We used corridor out of our lab as another scene where the number of feature points is small and the environment is relatively simple. The motion path of two robots can be seen in Fig. 17a. It is assumed the local map obtained by 
 is 
 and the local map obtained by 
 is 
. The registration of 
 and 
 can be seen in Fig. 17b. Furthermore the point cloud map after splicing is shown in Fig. 17c.

5.3.3. Performance of the whole system
We tested the whole system performance based on the experiments mentioned in the two sections above. The energy consumed by robot 
 is 
 while 
 represents the energy consumed by robot 
. 
 can be calculated by Eq. (3). Since the pixel of image obtained by our Kinect is 1280 × 960, the memory occupied by images are much larger than YAML file obtained by our partial offloading method. As for the original total offloading system, most of the energy is consumed on the network transmission. From Table 9 we can see that the energy consumption and time consumed by our algorithm are nearly one quarter of the original algorithm. No matter in indoor scene or in corridor scene, our method can greatly reduce the system energy consumption and time cost.

6. Conclusion and future work
Firstly, this paper proposes a multi-robot visual SLAM partial computing offloading strategy where the best offloading point is given to reduce the energy consumption and time cost of the whole visual SLAM system so that tasks that could not be achieved on the robot embedded device become possible. Secondly, an improved point cloud map registration algorithm (FS-HICP algorithm) suitable for indoor scene is proposed, which improves the convergence speed dramatically. Finally, we build the global dense point cloud map and semi-dense octree map. In some special environments, such as fire, underwater, underground situations etc., the method proposed in this paper can have wider application.

In the future, we will continue our research on multi-robot visual SLAM, aiming at solving the problem of communications between multiple robots, realizing multi-robot path planning, using the method of deep learning to achieve semantic SLAM and so on.


Table 9. Performance of the whole system.

 (J)	
 (J)	
 (J)	
 (J)	
 (s)
Indoor	Our system
(partial offloadingFS-HICP)	
221.2	537.2	3.15	540.35	291.45
316
Original system
(total offloadingICP)	
1019.2	2475.2	6.93	2482.13	1176.8
1456
Corridor	Our system
(partial offloadingFS-HICP)	
95.3	249.3	4.35	253.65	154.1
154
Original system
(total offloadingICP)	
436.8	1164.8	10.95	1175.75	610.8