Abstract
Machine learning and artificial intelligence (AI) applications often rely on performing many small matrix operations—in particular general matrix–matrix multiplication (GEMM). These operations are usually performed in a reduced precision, such as the 16-bit floating-point format (i.e., half precision or FP16). The GEMM operation is also very important for dense linear algebra algorithms, and half-precision GEMM operations can be used in mixed-precision linear solvers. Therefore, high-performance batched GEMM operations in reduced precision are significantly important, not only for deep learning frameworks, but also for scientific applications that rely on batched linear algebra, such as tensor contractions and sparse direct solvers.

This paper presents optimized batched GEMM kernels for graphics processing units (GPUs) in FP16 arithmetic. The paper addresses both real and complex half-precision computations on the GPU. The proposed design takes advantage of the Tensor Core technology that was recently introduced in CUDA-enabled GPUs. With eight tuning parameters introduced in the design, the developed kernels have a high degree of flexibility that overcomes the limitations imposed by the hardware and software (in the form of discrete configurations for the Tensor Core APIs). For real FP16 arithmetic, performance speedups are observed against cuBLAS for sizes up to 128, and range between  and . For the complex FP16 GEMM kernel, the speedups are between  and  thanks to a design that uses the standard interleaved matrix layout, in contrast with the planar layout required by the vendor’s solution. The paper also discusses special optimizations for extremely small matrices, where even higher performance gains are achievable.

Previous
Keywords
Matrix multiplication

Half precision

Batch linear algebra

GPU computing

1. Introduction
High-performance linear algebra libraries enable scientific applications to run efficiently on massively parallel architectures. Considering dense linear algebra software, high performance is usually achievable through algorithmic designs that express as many computational stages as possible in terms of compute-bound routines in general, and matrix multiplication (GEMM) in particular. The GEMM kernel is not only embarrassingly parallel; it also has a relatively high arithmetic intensity [30], which is defined as the ratio between the amount of floating-point operations (FLOPs) and the number of bytes transferred to/from the main memory. These two properties make the GEMM kernel extremely important for numerous computational domains. According to the standard interface of Basic Linear Algebra Subprograms (BLAS), a standard GEMM operation updates a matrix , where . Both  and  are scalars. The total number of FLOPs in a GEMM operation is equal to . The amount of bytes transferred is equal to , where  is the number of bytes required to represent a floating-point number in a specific precision. We consider real and complex half-precision arithmetic in this paper. For real FP16 arithmetic (HGEMM),  is equal to , while for the Half-Complex GEMM (i.e., HCGEMM),  is equal to . The name HCGEMM does not follow the standard Basic Linear Algebra Subprograms (BLAS) notation, which assigns a single letter prefix to denote the precision. In fact, there is no consensus to date for naming linear algebra operations in half-complex precision. The term HCGEMM is used only within this paper, and may not be used in the final API of the released software.

The GEMM kernel is also important in many scientific domains other than dense linear algebra, especially where the computational workload can be broken down into a large number of small matrix operations. The workload is often called a “batched workload”, and dedicated routines have been optimized for such workloads (e.g., batched GEMMs). It turns out that the batched GEMM kernel is almost as important as the regular non-batched GEMM, since it has been featured in many applications, such as sparse direct solvers [32], and tensor contractions [1]. Some hardware vendors now provide optimized batched GEMM kernels, such as the Intel Math Kernel Library (MKL) library1 and the NVIDIA cuBLAS library.2 The latter provides an optimized batched HGEMM kernel as well. However, for the complex case, the vendor advises the use of “split-complex” computations, which assumes that the real and the imaginary parts of the matrix are separated in a “planar layout”.

The artificial intelligence (AI) and machine learning revolution has created a huge demand for high-performance half-precision arithmetic (-bit floating-point format), since most AI applications do not necessarily require the accuracy of single or double precisions [10]. In terms of memory bandwidth and footprint, half precision theoretically offers a natural  improvement over single precision, and  over double precision. With respect to floating-point operations, the improvement factors over single and double precision depends on the architectural properties of the hardware. In this paper, we target NVIDIA’s Volta GPUs, which offer native FP16 arithmetic. The theoretical peak performance for “general” FP16 computation is about 31.25 tera floating-point operations per second (teraFLOP/s). This is double the peak single precision performance of 15.6 teraFLOP/s, and four times the peak double precision performance of 7.8 teraFLOP/s. However, Volta GPUs offer even more performance for special operations in FP16 arithmetic. These operations are mostly variations of matrix multiplication, and are hardware-accelerated using special units called Tensor Cores (TCs). For such special operations, the performance of a single GPU can reach up to  teraFLOP/s. Although the Tensor Cores units first appeared in Volta GPUs, NVIDIA’s first GPUs to ever support half precision were the Pascal GPUs. Half precision on NVIDIA GPUs implements the “binary16” format which is defined by the IEEE-754 standard [14]. As shown in Fig. 1, half precision uses one bit for the sign, five bits for the exponent, and ten bits for the fraction. However, the format assumes an implicit leading bit that is set to one unless the exponent is all zeros. This gives the FP16 format a total of eleven bits for the fraction, which accounts for an accuracy of about three decimal digits.

This paper investigates the use of the Tensor Cores to provide a general-purpose batched matrix multiplication in FP16 arithmetic. The paper extends the previous effort by the authors [4] by considering half-complex computation on the GPU, which is not natively supported to the best of our knowledge. While half-complex precision may be of limited use in the machine learning domain, it is of significant value in linear algebra, especially with respect to mixed-precision solvers. The paper addresses some challenges in using the Tensor Cores programmatically in a GPU kernel, such as discrete sizes and restricted thread configurations. The kernel design is expressed in terms of “building blocks”, which are developed as device routines that perform specific tasks in the kernel. Some of these building blocks are shared across half and half-complex precisions, but some other functions had to be explicitly developed for each precision. An important goal was to provide highly flexible kernels that can withstand potential future changes to the Tensor Core technology. The developed kernels have at least eight tuning parameters that control different aspects of the kernel. An extensive tuning process has been applied to the developed kernels, with respect to typical use cases for batched GEMM operations. We also investigate the benefit of using Tensor Cores for extremely small problems, where full utilization of the Tensor Core units is not possible. While the vendor routine is very optimized for relatively large sizes, we observe that the batched HGEMM kernel outperforms cuBLAS for sizes with speedups that range between and . We also show that the batched HCGEMM kernel outperforms the vendor solution (which uses split complex computation) by improvement factors between and . This work is part of the open-source MAGMA library [6].3


Download : Download high-res image (69KB)
Download : Download full-size image
Fig. 1. Half precision format according to the IEEE-754 standard.

Below are the main highlights of the paper:

1.
The use of FP16 arithmetic is proven to be useful for numerical linear algebra, and so important kernels like batched matrix multiplications must be optimized. This kernel is the focus of the paper.

2.
The use of Tensor Cores is restricted by some limitations that are imposed by the programming model.

3.
The proposed kernel design builds a flexible abstraction layer over the tensor cores. Such a layer hides the aforementioned restrictions.

4.
The final kernel design has tuning parameters. A comprehensive tuning sweep is conducted to record the best performance at different sizes.

5.
For half-complex computations, we theoretically prove that the interleaved layout is better for performance than planar layouts (split-complex).

6.
The performance gains for batch GEMM in half precision are between and for sizes up to .

7.
The performance gains for batch GEMM in half-complex precision are between and for sizes up to .

8.
For tiny matrices, we show that the use of Tensor Cores might be questionable due to the sub-optimal utilization of the Tensor Core units.

2. Related work
Matrix multiplication is an embarrassingly parallel operation with a relatively high operational intensity. These two properties enable GEMM operations to run asymptotically at % of the GPU’s peak performance. This good match between the properties of GEMM and GPUs being throughput-oriented processors has led to the emergence and success of GPU-accelerated dense linear algebra. Research efforts date back almost a decade, when GPUs started to have programmable shared memories (i.e., user-controlled caches). This enabled researchers to develop the first compute-bound GEMM on GPUs [29]. Since then, the GEMM kernel has been subject to continuous improvements, like register and shared-memory blocking and prefetching [23]. Such developments sparked many efforts in providing fast high-level dense linear solvers on GPUs, such as the MAGMA library [27], ViennaCL [25], and Chameleon [5]. Performance portability of GEMM was achieved through performance-critical tuning parameters that control different properties of the GEMM design [18], [21]. Following the publicly available developments from the research community, the GPU vendor started providing highly optimized GEMM implementations that are written in a low-level language [26] in order to overcome some limitations imposed by the compiler and the hardware scheduler. Similarly, assembly implementations [9], [19] are available today in the cuBLAS library, with the ability to achieve a performance that is very close to the GPU theoretical peak. Similar to the libraries mentioned above, the vendor also provides a library called cuSOLVER4 for high-level dense linear algebra algorithms.

All the aforementioned efforts address the problem of one GEMM operation that is relatively large enough to provide sufficient parallel work for the GPU. The recent application-driven interest in batched linear solvers have encouraged vendors and library developers to design dedicated routines that can address a large number of small matrix problems. Algorithmically, a batched GEMM is still a very important operation, since it remains the performance key to higher-level algorithms such as the batched one-sided factorizations [12]. However, the importance of the batched GEMM goes beyond the boundaries of dense linear algebra to affect other scientific domains, such as sparse direct solvers [32], tensor contractions [1], [15], and machine learning [8]. The challenges in optimizing batched GEMM are different from the regular GEMM kernel. As the problem sizes are relatively smaller, the GEMM operations are no longer compute-bound, and more attention should be paid to optimizing the memory traffic. Automatic performance tuning is even more important in batched routines, since it has been found that the performance is more sensitive to tuning parameters in small matrix problems [2].

Batched GEMM operations are crucial to machine learning applications in particular. For example, convolutional neural networks (CNNs) are a very popular class of deep neural networks (DNNs). They were initially implemented using custom dense kernels, as originally done in Caffe [16] and other libraries, such as tensor convolutions and activation functions. These custom kernels were developed locally per package. And since they dominate the training time for CNNs, re-optimizations had to be done whenever the underlying architecture changed. This is why research efforts, such as cuDNN [8], MagmaDNN [24], and others, focused on providing optimized primitives for deep learning, similar to the way BLAS provides optimized primitives to LAPACK algorithms. The most important operation in CNNs is batched spatial convolution, which can be cast into batched matrix multiplication [7], [8]. In addition, the work done in [20] uses batched GEMMs of very small sizes (3 × 3) to implement fast CNN algorithms based on minimal filtering algorithms [31]. On another front, the batched GEMM operations in machine learning are not necessarily required to have the accuracy of single or double precisions. In fact, it has been shown that lower precisions are enough for training deep neural networks [10]. Furthermore, the need for extreme computational power in DNNs arises from their hyperparameter tuning—a process of training multiple DNNs to empirically find the best network in various applications [28]. With the popularity of GPUs in large-scale AI applications, the latest architectures from NVIDIA, namely Volta and Turing, are equipped with Tensor Cores, which provide hardware acceleration for matrix-multiply-accumulate operations. The cuBLAS library provides high-level APIs for GEMM and batched GEMM in half precision (i.e., HGEMM and batched HGEMM, respectively). There are also low-level APIs that can be used to program the Tensor Cores inside a GPU kernel. While the high-level APIs have been used to accelerate mixed-precision iterative refinement dense linear solvers [11], [13], this is the first effort, to the best of the authors’ knowledge, to programmatically use the Tensor Cores in an open-source and general-purpose batched GEMM routine that is competitive with the vendor optimized library.

3. The FP16 tensor cores in GPUs
The CUDA Toolkit is one of the first programming models to provide half-precision (i.e., FP16) arithmetic. Early support was added in late 2015 for selected embedded GPU models that are based on the Maxwell architecture. The FP16 arithmetic has become mainstream in CUDA-enabled GPUs since the Pascal architecture. In general, half precision has a dynamic range that is significantly smaller than single or double precisions. Incorporating such a reduced precision was mainly motivated by the disruptive emergence of machine learning applications.

The Volta and Turing architectures introduce hardware acceleration for matrix multiplication in FP16. The hardware acceleration units are called Tensor Cores. They can deliver a theoretical peak performance that is up to faster than the peak FP32 performance. As an example, each Volta V100 GPU has Tensor Cores, evenly distributed across multiprocessors. Each Tensor Core possesses a mixed-precision 4 × 4 × 4 matrix processing array which performs the operation , where , , and are 4 × 4 matrices. The inputs and must be represented in FP16 format, while and can be represented in FP16 or in FP32 formats. It is also possible that and point to the same matrix.

The vendor library (cuBLAS) provides various optimized routines, mostly GEMMs, that can take advantage of the Tensor Core acceleration by setting the proper flag. As an example, the routine cublasHgemmBatched implements the batched GEMM operation for real FP16 arithmetic. All matrices are assumed to have the same dimensions. Considering complex FP16 computations, there is no native support for half-complex precisions yet in the library. In fact, the only way to use cuBLAS is to use a “planar layout” for the matrices, where the real and the imaginary parts of the matrices are separated. The planar layout enables an easy solution using existing cuBLAS routines, but it lacks an important performance advantage, which will be discussed later in the paper. The same concept of split-complex computation applies to the cuBLASLt library,5 as well as the open-source CUTLASS library.6

Taking advantage of the Tensor Cores in a custom kernel is possible through the use of low-level APIs that are provided by the programming model as well. As shown in Fig. 2, Tensor Cores deal with input and output data through opaque data structures called fragments. Each fragment is used to store one matrix. Fragments can be loaded from shared memory or from global memory using the load_matrix_sync() API. A similar API is available for storing the contents of an output fragment into the shared/global memory of the GPU. The mma_sync() API is used to perform the multiplication. The user is responsible for declaring the fragments as required, and calling the APIs in the correct sequence.

The programming model imposes some restrictions to the programming of the Tensor Cores. First, the GEMM dimensions (, , ), which also control the size of the fragments, are limited to three discrete combinations, namely (, , ), (, , ), and (, , ). Second, the operations of load, store, and multiply must be performed by one full warp (32 threads). Finally, the load/store APIs require that the leading dimension of the corresponding matrix be multiple of -bytes. As an example, a standard GEMM operation of size (, , ) requires three load_matrix_sync() calls (for , , and ), one mma_sync() call, and then a final store_matrix_sync() call to write the result. The latest CUDA version to date (10.1) provides direct access to the Tensor Cores through an instruction called mma.sync. The instruction allows one warp to perform four independent GEMM operations of size (, , ). However, using the explicit instruction may lead to long-term compatibility issues as new architectures are released. This is why we decide not to consider using parallel thread execution (PTX) instructions, and focus only on the device-level CUDA APIs.


Download : Download high-res image (143KB)
Download : Download full-size image
Fig. 2. Programmability of the Tensor Core units.

4. General design outlines
This section describes some general concepts about the design of the batched HGEMM and the batched HCGEMM kernels. The dimensions (, , ) of the GEMM operation are assumed to be unified across the batch. According to the CUDA programming model, a GPU kernel is, in general, a three-dimensional grid of three-dimensional thread blocks (TBs). The number of GEMM operations in the batch is referred to as batchCount. For real half-precision computation, the CUDA data type __half is used. For half-complex computations, we use the __half2 vector type. The low -bits represent the real part, while the high -bits represent the imaginary part.

4.1. Grid design
The MAGMA library uses a common grid design for all of its batched kernels [2], [3]. The output matrices are subdivided into smaller blocks that can fit into a fast memory level (i.e., registers or shared memory). Such blocks can be square or rectangular, with their sizes denoted as (BLK_M BLK_N). The first two dimensions of the grid are used to denote a two-dimensional “subgrid” for each output matrix in the batch. The third grid dimension is used for batching across the problems, which yields a three-dimensional grid configuration of (, , batchCount). Each subgrid has a unique batchid (the dimension of the grid) and takes care of a single GEMM operation. Similarly, the input matrices and are subdivided into smaller blocks of sizes (BLK_M BLK_K) and (BLK_K BLK_N), respectively. Within every subgrid, each TB is responsible for computing a block of the output matrix by reading a block row of and a block column of . The block rows/columns are read in steps of BLK_K. At each iteration of the main loop, a TB multiplies a block of (BLK_M BLK_K) with a block of (BLK_K BLK_N). Since we are using the Tensor Core units, the accumulations of the partial results take place in the fragments rather than regular register buffers or shared memory. Fig. 3 illustrates the TB organization of the kernel.

The following sections describe the main design aspects of the kernel, which leverages some design concepts from existing kernels [2] while modifying or generalizing them to take advantage of the Tensor Cores.

5. Detailed thread block design
5.1. Abstracting tensor cores
As mentioned before, the use of the Tensor Cores programmatically must follow some constraints that are required by the device-level APIs. Our goal in this paper is to design GPU kernels with an abstraction layer over the Tensor Cores. The abstraction layer takes care of the device-API constraints and provides a general-purpose use of the cores. The three main constraints for using Tensor Cores are:

1.
Tensor Cores can be used only with three discrete combinations of blocking sizes

2.
Device-level APIs must be called by a single warp

3.
Loading/storing fragments must be from/to memory spaces with specific leading dimensions (multiples of bytes).

The abstraction layer proposed in this paper addresses each one of these constraints. Eventually, the GPU kernels will be able to use arbitrarily large blocking sizes, using any number of warps, with no constraints on the leading dimension of the matrices.

The developed solution must support arbitrary leading dimensions for , , and in the global memory of the GPU. A straightforward solution is to read the matrices into shared-memory buffers, rather than reading them directly into the Tensor Core fragments. The shared-memory buffers are allocated with leading dimensions that abide by the -byte rule.

5.2. Double-Sided Recursive Blocking (DSRB)
This technique allows GPU kernels to use arbitrarily large blocking sizes (BLK_M, BLK_N, BLK_K) that are not necessarily restricted to the Tensor Core sizes (TC_M, TC_N, TC_K). Recursive blocking is a well-known technique that has been used for years in previous GEMM designs [23]. It transfers each block of , , and to/from the global memory using a two-dimensional thread configuration DIM_X DIM_Y. These blocks are subdivided into smaller DIM_X DIM_Y tiles that can be stored in shared memory or in the register file. The same DIM_X DIM_Y subdivision is used for computing the partial products of these blocks. We call this technique single-sided recursive blocking (SSRB). Such a technique is not applicable to Tensor Cores, which have discrete blocking sizes for computation. This is why we propose a generalization over SSRB, which we call double-sided recursive blocking (DSRB). As shown in Fig. 4, it simply decouples the way blocks are read/written from the way they are passed to/from the Tensor Core units. During the read/write of a block from/to the global memory, a matrix block is always subdivided into DIM_X DIM_Y tiles. During the computation of a partial product, however, the loaded blocks are subdivided using the TC sizes (TC_M, TC_N, TC_K). Thread regrouping is also used to select the best configuration at each stage of the kernel. As an example, assume a single warp configuration in a kernel where all blocking sizes are equal to . When loading a 16 × 16 block of data, it is much better to use a 16 × 2 or 8 × 4 configuration rather than the default 32 × 1 one. This is why we allow a single warp to reorganize itself into a DIM_X DIM_Y configuration when reading and writing blocks of data. During the computation, the regular 32 × 1 configuration is used.

5.2.1. How does DSRB improve memory traffic?
The DSRB technique generalizes BLK_M, BLK_N, and BLK_K so that they are not bound to the TC sizes. Generic block sizes help improve the memory traffic required by the GEMM kernel. To illustrate this point further, we simplify our analysis by assuming that BLK_M and BLK_N fully divide and , respectively. According to our grid configuration in Section 4.1, the proposed kernels would require TBs. Theoretically, an ideal implementation would read , , and from the global memory exactly once, and write once. The proposed kernels perform an ideal memory traffic for , since each TB takes care of one block of , and so it reads and writes such a block exactly once. However, since the parallel work is distributed across many independent TBs, there have to be redundant memory loads for and . The redundant loads are significantly affected by BLK_M and BLK_N. The total memory loads for and per TB are given by (), since it has to read an entire block row of and a block column of . The total memory traffic (for and ) for the whole kernel is given by . We can now calculate the improvement (reduction) in memory traffic by comparing the previous formula against the case when (BLK_M, BLK_N, BLK_K) (TC_M, TC_N, TC_K), which eventually yields:

Fig. 5 shows the relative reduction in memory traffic for blocking sizes up to . As an example, using (BLK_M, BLK_N) (, ) can theoretically reduce the memory traffic by a factor of when (TC_M, TC_N) (, ), and by a factor of when (TC_M, TC_N) (, ). Such a reduction usually leads to a better performance, since most of the memory requests are fulfilled from the main memory due to the relatively small caches in GPUs (compared to CPUs). However, this theoretical analysis assumes infinite resources available for each TB. Using too many resources per TB could actually worsen other aspects of the kernel, such as the occupancy and the register pressure. Therefore, a tuning process is usually required to search for the best blocking sizes on a specific GPU.

5.3. Two-stage loading of input data
Instead of loading the blocks of and directly into the shared-memory buffers, we use a two-stage process where the data is first read in register buffers, then offloaded to the shared memory. The use of double buffers would allow us incorporate a prefetching mechanism into the register file while the data are being processed through the buffers. Two device-level functions have been developed for this purpose. The first one reads a block of data from the global memory into the register file.


Download : Download high-res image (135KB)
Download : Download full-size image
The function is templated for type, block sizes (BLK_R BLK_C), as well as the thread configuration (DIM_X DIM_Y). These parameters are known at compile time, which enables fully unrolled loops, and avoids register spilling into the local memory. The fetch<T>() function returns either the required element, or zero if the passed address is out-of-bound. This function is one of the shared building blocks across half and half-complex kernels.

The second device function offloads the content of a register buffer into a shared-memory space. A similar structure of two nested loops is used.


Download : Download high-res image (116KB)
Download : Download full-size image
Unlike the previous device function, the store_reg2smem function cannot be used as it is for half-complex buffers. Since the TCs support only real arithmetic, we need to split the contents of the register file into two separate memory spaces; one for the real part, and the other for the imaginary part. The function below shows the corresponding stroe_reg2smem_complex function, which uses low-level intrinsics for splitting the values.

Download : Download high-res image (148KB)
Download : Download full-size image
5.4. Multi-warp configuration
All TC device functions must be invoked using one warp. However, this does not necessarily mean that the TB configuration should be restricted to a single warp. In fact, it is sometimes beneficial to use multiple warps per TB, especially when the amount of work per TB is relatively large, or when a warp is stalled. We generalize our thread configuration to support any number of warps. As mentioned before, threads reorganize themselves into a DIM_X DIM_Y configuration during memory operations. During computation, however, we must reorganize the threads in a configuration in order to use the TC. Note that the parameter space for DIM_X and DIM_Y is now much bigger, which serves the design flexibility. As an example, four warps can be used in many configurations, such as 8 × 16, 16 × 8, 32 × 4, 64 × 2, etc. When a partial product is being computed, the block accumulator of is subdivided into many sub-blocks of size TC_M TC_N. Warps loop over these sub-blocks in a round-robin manner. For each sub-block, the respective warp loops over the corresponding sub-block row of and the sub-block column of , sends them in chunks to the Tensor Cores, and keeps accumulating the results in its respective fragment. Fig. 6 shows the workload distribution for two different configuration on a block accumulator that has sub-blocks.

The use of multiple warps enables controlling the amount of work per warp, especially for large blocks of data. Reading in large blocks is usually required to achieve a high memory bandwidth and to increase data reuse. But since TC multiplications must be performed by a single warp, large blocks of data could mean too much work for one warp, and it becomes better to involve more warps in computation. To better quantify this concept, we performed an experiment on a batched HGEMM on square 64 × 64 matrices. For simplicity, we fix BLK_M BLK_N BLK_K , and TC_M TC_N TC_K . We also show two possible thread configurations, one with DIM_X , and the other with DIM_X . Fig. 7 shows that increasing the number of warps per TB leads to significant performance gains when the blocks are relatively large (while fixing all other parameters). However, the performance drops if too many warps are used per TB. One reason is the occupancy, which impacts the number of live TBs that can be scheduled by the runtime on the same multiprocessor. Another reason is that some warps may be idle during the compute phase. This appears in Fig. 7 when the number of warps is set to . With blocking sizes set to and Tensor Core sizes set to , we have a 4 × 4 sub-block organization, which means that warps out of the are not assigned to any computational workload.


Download : Download high-res image (182KB)
Download : Download full-size image
Fig. 6. Multi-warp configuration with round-robin assignment for the output block.

5.5. Performing the multiplication using the tensor cores
After loading the input data blocks in the shared-memory buffers, another device function (tc_multiply()) is invoked to perform the TC multiplication using the round-robin style illustrated before. The function is heavily templated with a number of constants that are known at compile time, such as TC_BLOCKS, NWARPS, and NFRAG. The parameter TC_BLOCKS refers to the total number of multiplications a thread block performs, while NWARPS and NFRAG refer to the number of warps and the number of accumulator fragments per warp, respectively. The pseudocode below distributes the TC_BLOCKS multiplications across warps. At each iteration of the outer loop, every warp independently calculates the coordinates of the sub-block it should compute. It then proceeds to the innermost loop, where the corresponding sub-block row of is multiplied by the corresponding sub-block column of using the TC APIs. The code has a cleanup section that handles the situation when NWARPS does not fully divide TC_BLOCKS.


Download : Download high-res image (268KB)
Download : Download full-size image
The tc_multiply() function does not work for half-complex arithmetic, and a dedicated function is required to do a split-complex multiplication on the TB level, which we call tc_multiply_complex(). The function accepts two shared-memory pointers (real and imaginary) for each of the and blocks. The innermost loop performs four multiplications instead of one. The partial results are accumulated in separate output fragments.


Download : Download high-res image (348KB)
Download : Download full-size image
5.6. Post processing
Recall that the GEMM operation is defined as . So far, all of the different computational stages serve for computing the product. The scaling operations by and are performed at a post-processing stage. For the batched HGEMM kernel, the post-processing stage loads the respective block of into a register buffer and scales it by . In order to save memory traffic, this step takes place only if is a non-zero. The product resides in shared memory. It is scaled by and added to the register buffer of before finally writing it to the global memory. As for the half-complex case (batched HCGEMM), recall that the tc_multiply_complex() function has the real and imaginary parts of the product separated. The product is therefore merged first in the shared memory before any scaling takes place. The rest of the post-processing step is similar to the batched HGEMM kernel.

5.7. The main kernel structure
The pseudocode below shows the main loop of the kernel. At each iteration, a pair of data blocks – from and – is loaded from global memory to shared memory (in two stages). The actual dimensions of these blocks (am, an, bm, bn) are computed at the beginning of the iteration so that the read_global2reg() function accounts for partial blocks by means of zero-padding if required. Synchronization is required to make sure all data are visible to all warps before proceeding to the multiplication subroutine tc_multiply(). The multiplication takes place simultaneously while reading a new pair for data blocks. Another synchronization point is required to make sure all warps are done with the currently loaded data, and that it is safe to overwrite the contents of the shared memory.


Download : Download high-res image (213KB)
Download : Download full-size image
5.8. Tuning Parameters
The developed kernels are written using CUDA C++ templates, with eight main tuning parameters. These parameters are:

•
The configuration sizes of the Tensor Cores (TC_M, TC_N, TC_K). These sizes are discrete configurations that are enforced by the programming model.

•
The blocking sizes for , , and (BLK_M, BLK_N, BLK_K). These sizes control the amount of data reuse by improving the memory traffic, as explained in Section 5.2.1. They also influence the shared memory requirement for the kernel.

•
The thread configuration for reading and writing data blocks (DIM_X, DIM_Y). These parameter control the number of threads processing the data blocks of , , and . They also influence the register pressure per thread. Assuming that the blocking sizes (BLK_M, BLK_N, BLK_K) are not changed, increasing the number of threads leads to fewer registers per thread.

The tuning parameters must satisfy a number of conditions in order for the kernel to perform correctly. The Tensor Core dimensions TC_x must fully divide BLK_x, where x {M, N, K}. Each of DIM_X and DIM_Y must fully divide every blocking size BLK_x. The product DIM_X DIM_Y must also be multiple of , in order to have full warps. We chose to run a comprehensive brute-force tuning sweep for all eligible kernel instances. Our reasoning behind that decision was to get the best performance for small sizes, where the performance is more sensitive to the tuning parameters than for large sizes [2].

6. Half-complex batched GEMM: Planar vs. interleaved layouts
In order to perform half-complex computations using the cuBLAS library, a user must use “split-complex” computation. The real and the imaginary parts of , , and must be stored separately in a planar layout. Considering dense linear algebra algorithms, planar layouts do not help achieve good performance, especially for relatively small sizes. First, all the existing linear algebra numerical libraries assume an interleaved layout, meaning that the real and the imaginary parts of each element are contiguous in memory. It is not practical to rewrite entire algorithms using split-complex computations to make use of the TCs. Second, while it is relatively easy to develop the GEMM kernel in planar layouts using the existing GEMM kernels, other linear algebra components might not be as straightforward. Examples are triangular solve and the pivoting stage in the LU factorization. For such operations, it is not possible to use the existing triangular solve or pivoting kernels, which leads to new developments at the BLAS level. Therefore, it is more convenient to use the standard interleaved layout. Third, complex compute-bound kernels normally reach the peak performance of the underlying hardware earlier than their counterparts that use real arithmetic. This is because complex kernels have more operational intensity than real arithmetic kernels. The operational intensity of an operation is the ratio between the number of FLOPs and the number of bytes transferred for that operation. As an example, the real FP16 scalar operation () costs 2 FLOPs (1 addition and 1 multiplication), which results in FLOP/byte ratio. Using complex FP16 arithmetic, the same operation would cost 8 FLOPs. The product costs 6 FLOPs (4 multiplications and 2 additions), and the update of costs 2 more additions. This results in FLOP/byte ratio (double the operational intensity of the real scalar operation). Such a difference in the operational intensity leads to a “slowly growing” theoretical peak performance (i.e. roofline) for the HCGEMM kernel, thus requiring relatively large sizes to approach the hardware peak performance. In other words, the planar layout makes the HCGEMM kernel operate at the same roofline as the real arithmetic HGEMM kernel, since it calls HGEMM four times.

We theoretically investigate the half-complex GEMM operation based on the Roofline model [30]. Our focus is on batched square multiplications (), as well as batched rank-k updates (, is a relatively small constant). The former use case demonstrates a test for the peak performance of the kernel, while the latter is an important use case in batched linear algebra, especially in the batched LU factorization.

6.1. Roofline for HCGEMM (standard interleaved layout)
For simplicity of our analysis, we can safely ignore the scaling by and . According to the LAPACK Working Note #41,7 a standard GEMM operation () involves () additions, and () multiplications. This can be explained as follows. The output matrix has elements. Each element is computed as , which accounts for multiplications and additions. Therefore, the HCGEMM-interleaved operation requires () additions and () multiplications. In order to estimate the number of FLOPs, we recall that one multiplication of complex numbers requires 6 FLOPs and one addition requires 2 flops. The total number of flops is, therefore, ().

In order to derive a roofline (i.e. a performance upper-bound), we derive the ideal (minimum) amount of memory traffic for the operation. The ideal amount of data required for HCGEMM is

1.
One read and one write for , which equals ()

2.
One read for ()

3.
One read for ()

6.2. Roofline for HCGEMM (planar layout)
We now estimate the roofline for the HCGEMM-planar operation similar to Section 6.1. For a split-complex computation, the operation can be done via four calls to the standard HGEMM operation. Assuming that we have two separate output matrices (real) and (imaginary), the four HGEMM calls are:

1.
First call:

2.
Second call:

3.
Third call:

4.
Fourth call:

Each call involves () additions and () multiplications. Since these calls perform real half-precision arithmetic, each addition/multiplication accounts for one FLOPs. Therefore, one call performs () FLOPs, leading to a total of () FLOPs. From an operation count point of view, these four calls have the same complexity as HCGEMM-interleaved. However, the collective memory traffic for these calls is different. Each call to HGEMM involves () traffic for , an () traffic for , and an () traffic for . The memory traffic for one HGEMM call is -bytes, leading to an aggregate traffic of . Similarly, we can write (2)

To summarize, HCGEMM-interleaved has double the operational intensity of HCGEMM-planar. For a square multiplication (), the operational intensity is simplified to for HCGEMM-planar, and to for HCGEMM-interleaved. The roofline model [30] estimates the performance upper bound as operational intensity the peak memory bandwidth. Using a GPU STREAM benchmark on the Tesla V100 GPU, we got up to 847 GB/s of peak memory bandwidth. Fig. 8 shows the performance upper bounds for HCGEMM-planar and HCGEMM interleaved for square matrices. Both graphs grow linearly with the matrix size until they hit the GPU peak performance. However, due to the increased operational intensity, HCGEMM-interleaved reaches the peak earlier than HCGEMM-planar. This is very crucial for relatively small sizes. As an example, HCGEMM-planar is bandwidth-limited at size , while HCGEMM-interleaved becomes compute bound. For large sizes (larger than in Fig. 8), we may not see a difference between the two kernels (if properly optimized), since HCGEMM-planar will be able to saturate the GPU anyway.

Another useful test case for GEMM is the rank-k updates. This is a very important use case in dense linear algebra. Fig. 9 shows a similar roofline analysis for updating a square matrix (), with a rank-16 update (). As per the roofline model, this is a use case that always remains bandwidth-limited on the V100 GPU, regardless of the size. The operational intensity of HCGEMM-planar will be given by , while the HCGEMM-interleaved has its own at . This means that HCGEMM-interleaved is theoretically faster than HCGEMM-planar, no matter how large and are.

7. Performance results
This section shows the performance results of the proposed MAGMA kernels against the equivalent implementations by the vendor library (cuBLAS).

7.1. Experimental setup
The performance tests are conducted on a system equipped with a two-socket CPU (Intel Xeon E5-2650 v3 @ 2.30 GHz), with cores per socket, and a Tesla V100-PCIe GPU. The GPU has GB of memory, and streaming multiprocessors, which are clocked at 1.38 GHz. We use CUDA Toolkit 10.1 for the compilation of the MAGMA kernels, as well as for testing cuBLAS. The tests are done for relatively small square sizes and for relatively small rank-16 updates.

7.2. Performance of the batched-HGEMM kernel
Fig. 10 shows the performance of the tuned MAGMA kernel against cuBLAS for the batched HGEMM operation. Performance speedups are observed for the MAGMA kernel for sizes up to on square sizes and for sizes up to for the rank-16 updates. This behavior shows the importance of auto-tuning. For small problem sizes, we notice that the performance of a given kernel is sensitive to tuning parameters in the sense that more kernel instances are required for relatively small problems. This is unlike the situation for larger sizes, where usually one or two kernel instances can deliver the best performance.

Fig. 11 summarizes the performance speedup for every size between and . On average, the speedup numbers range between and , except for few spikes or drops. The spikes/drops in speedup are mainly due to the cuBLAS performance behavior, which seem to have periodic drops for some sizes , and some other performance spikes after that.


Download : Download high-res image (466KB)
Download : Download full-size image
Fig. 10. Performance of the batched HGEMM kernel against cuBLAS. Results are for square sizes up to , with batchCount , on a Tesla V100-PCIe GPU.

Recall that the cuBLAS kernel is written in a low-level language to utilize some optimization techniques that are not available in CUDA C or PTX instructions. And so, its asymptotic performance is faster than MAGMA by factors greater than for large matrices. However, the significant cuBLAS advantage is observable only for sizes that are multiples of . As an example, for a batch of square problems of size , the cuBLAS kernel is faster than MAGMA (68.8 teraFLOP/s for cuBLAS vs. 29.5 teraFLOP/s for MAGMA). However, the same batch for sizes of 2100 × 2100 sees a significant drop for cuBLAS vs. a slight one for MAGMA. In fact, MAGMA has a slight advantage in this case (26.5 teraFLOP/s for cuBLAS vs. 27.4 teraFLOP/s for MAGMA). In general, large sizes that are not multiples of witness competitive performance numbers from both libraries.


Download : Download high-res image (259KB)
Download : Download full-size image
Fig. 11. Performance speedup of the batched HGEMM kernel against cuBLAS. Results are for sizes up to , with batchCount , on a Tesla V100-PCIe GPU.

7.3. Performance of the batched-HCGEMM kernel
Fig. 12 shows the performance of the batched HCGEMM kernels, while Fig. 13 shows the respective relative speedups. Recall that the cuBLAS library uses a planar layout where the real and the imaginary parts of the matrices are stored in separate memory spaces. However, the reported results do not include the timing for separating and merging these components. We only compare the execution time of the computational kernels with no overheads. As per the theoretical analysis in Section 6, there is an advantage to using interleaved layouts over planar layouts. The increased operational intensity in the former gives a performance advantage for the MAGMA kernel on a wider range of sizes. In fact, the speedup numbers reported in Fig. 13 are much more significant than those reported in Fig. 11. The reported speedups for the MAGMA batched HCGEMM kernel are in the range between and .

7.4. Impact of batch size on performance
The performance of batched kernels often depends on the batch size. So far, we have tested the batched kernels on relatively large batches, which saturate the GPU with enough parallel work. Such experiments are conducted to test how much of sustained peak performance is achievable by the kernels. However, applications may not necessarily use large batches, and the typical sizes vary from one application to another. This is why we show the performance of the batched HGEMM/HCGEMM kernels on relatively small batches that may not necessarily occupy all of the GPU resources.

Fig. 14, Fig. 15 show the performance for two different batch sizes ( and ), for the batched HGEMM and HCGEMM kernels, respectively. We highlight square sizes only, since the behavior for rank-k updates is the same, and hence was omitted to avoid redundancy.


Download : Download high-res image (490KB)
Download : Download full-size image
Fig. 12. Performance of the batched HCGEMM kernel (interleaved layout) against cuBLAS (planar layout). Results are for square sizes up to , with batchCount , on a Tesla V100-PCIe GPU.


Download : Download high-res image (266KB)
Download : Download full-size image
Fig. 13. Performance speedup of the batched HCGEMM kernel (interleaved layout) against cuBLAS (planar layout). Results are for sizes up to , with batchCount , on a Tesla V100-PCIe GPU.


Download : Download high-res image (298KB)
Download : Download full-size image
Fig. 14. The impact of batch size on the batched HGEMM performance. Results are for sizes up to , with batchCount , on a Tesla V100-PCIe GPU.

As expected, a small batch leads to a slower performance, since less parallel work is available for the GPU. However, the relative performances between MAGMA and cuBLAS in both figures are very similar to the asymptotic behaviors observed in Figs. 10 and 12. As an example, the behavior of the batched HGEMM kernel in Fig. 14 shows two similar pairs of graphs. Increasing batchCount from to results in shifting up the performance graphs without a significant change in the relative speedups. The exact same behavior is observable in Fig. 15 for the batched HCGEMM kernel. However, since the MAGMA HCGEMM kernel has a better arithmetic intensity, we can observe that the MAGMA performance on operations is sometimes equivalent to the cuBLAS performance on operations. Such a behavior emphasizes the advantage of the interleaved layouts for dense matrix computations, especially for batch workloads.

8. Optimization for extremely small matrices
The Tensor Core APIs have discrete configurations (TC_M, TC_N, TC_K). For batched GEMM problems with sizes smaller than these configurations, the TC utilization is below %, and depending on the problem size, the use of the TCs might be questionable. This section focuses on performance optimization for small sizes that cannot fully occupy the TCs when using the vendor-provided APIs. Without loss of generality, we are looking into small square matrices whose dimensions are . This range of sizes has been subject to many research efforts recently, due to its popularity in many applications [1], [17].

8.1. Multiple GEMMs in one tensor core multiplication
Recall that the permissible sizes for the TC multiplications are (, , ), (, , ), and (, , ). A single TC multiplication can therefore perform FLOPs using any of these combinations. Considering a square multiplication of size , the operation count is FLOPs, which is less than 1.5% of a full TC multiplication. A previous proposition by the authors [4] showed how to improve the utilization by performing multiple GEMM operations in a single call to the TC APIs. An example is shown in Fig. 16, which improves the utilization for a (, , ) GEMM from 1.5% to about %. While this is a improvement, the TC utilization is still very low. Fig. 17 shows the maximum achievable utilization for the TCs when performing square multiplications using the (, , ) configuration. The utilization in this figure is computed as . The figure also shows the peak half-precision performance (as a percentage) without using the TC multiplications. Theoretically, a sufficiently optimized kernel that does not use the TC APIs can outperform the TC kernels for small sizes, .

The low utilization percentages shown in Fig. 17 raise questions about using TCs for tiny matrices, and whether conventional methods (i.e., without Tensor Cores) can perform the multiplications more efficiently. In this regard, we refer to a kernel developed for very small matrices specifically [22]. The kernel addresses very small square multiplications of sizes up to . The main design idea is to use an thread configuration for each GEMM, such that each thread is responsible for a single element in the output matrix. The code is fully unrolled for every size using C++ templates. In this paper, we use a similar kernel design that supports both half and half-complex arithmetic. We call this kernel magma-small.


Download : Download high-res image (106KB)
Download : Download full-size image
Fig. 16. Improving Tensor Core utilization by assigning multiple GEMMs at a time. Example for square matrices of size , with all Tensor Core sizes set to .


Download : Download high-res image (247KB)
Download : Download full-size image
Fig. 17. The percentage of available Tensor Core compute power for square multiplications of small matrices.

We tested the performance of the magma-small kernel against the general MAGMA kernel as well as against cuBLAS, the results of which are summarized in Figs. 18 and 19 for the batched HGEMM and HCGEMM kernels, respectively. The generic MAGMA kernel is the best performing kernel for sizes and up. For sizes smaller than , the magma-small kernel is the best solution, though no Tensor Cores are used. This is an interesting observation that nicely aligns with Fig. 17. Suboptimal TC utilization does not give access to the full teraFLOP/s of compute power. Depending on the utilization (which is impacted by the problem size), the available compute power through the TCs may be lower than the available compute power without them. The threshold of % shown in Fig. 17 is the ratio between the full FP16 compute power in both situations. A utilization below this threshold may give the advantage to a kernel that does not use the TCs. This is realized in Fig. 18, Fig. 19, where the magma-small kernel outperforms any kernel that uses the TCs as long as the problem size is below . For sizes larger than that, the use of TCs begins to pay off. For the batched HGEMM kernel, the observed speedups against cuBLAS are between and . For the batched HCGEMM, the use of the interleaved layout adds an extra advantage for the MAGMA kernels, which score speedups up to .


Download : Download high-res image (289KB)
Download : Download full-size image
Fig. 18. Performance of the batched HGEMM kernels. Results are for small square sizes up to , with batchCount k, on a Tesla V100-PCIe GPU.

9. Conclusion and future work
This paper introduced optimized batched matrix multiplication kernels using FP16 arithmetic on GPUs. The developed kernels address both half and half-complex precisions, and take advantage of the Tensor Core accelerator units in NVIDIA GPUs. The kernels share a common abstraction layer that encapsulates several constraints when calling the vendor-supplied APIs for programming the TC units. For half-precision matrices (batched HGEMM), the developed kernel outperforms cuBLAS for sizes up to , with speedups ranging between and . For sizes larger than , the developed kernel is still very competitive with cuBLAS—except for sizes multiple of , where cuBLAS has a clear advantage. For half-complex matrices, the paper shows that the standard interleaved layout provides a better solution than using planar layouts. This is mainly due to the increased operational intensity. The developed batched GEMM for complex matrices (batched HCGEMM) is between and faster than the cuBLAS solution using planar layouts. The paper also discusses special optimizations for extremely small problems, where the use of the TC APIs is questionable. The overall solution for tiny matrices can be up to / faster than cuBLAS for the batched HGEMM and HCGEMM kernels, respectively.

The development of optimized matrix multiplication kernels is usually the most important step in developing higher-level dense linear algebra algorithms. The developed kernels can be used in reduced precision factorizations as well as mixed-precision solvers for linear systems of equations. The role of auto-tuning is critical in maintaining performance portability across different architectures. Some applications also require batches to have problems of different sizes. All of these directions are promising for future work based on the developed kernels.

