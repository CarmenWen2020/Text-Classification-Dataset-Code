Abstract
Clustering, which explores the visualization and distribution of data, has recently been studied widely. Although the existing clustering algorithms can well detect arbitrary shape clusters, most of them face the limitation that they cluster points on the basis of two physical metrics, distance and density, but ignore the orientation relationship of data distribution. Beside, they have a difficulty of selecting suitable parameters, which are important inputs of the clustering algorithms. In this paper, we firstly introduce a new physical metric, namely direction. Then, based on this new metric, we propose an adaptive direction-based clustering algorithm, namely ADC, which can automatically calculate appropriate parameters. Finally, we develop a parallel ADC algorithm based on multi-processors to improve the performance of the ADC algorithm. Compared with other clustering algorithms, experimental results demonstrate that the proposed algorithms are more general and can get much better clustering results. In addition, the parallel ADC algorithm has the best scalability over large data sets.

Previous
Next 
Keywords
Adaptive

Clustering

Direction-based

Parallel

1. Introduction
Clustering is an indispensable basic method, has found huge applications like data analysis [22], [29]. So far, various clustering algorithms have been proposed, including partitioning clustering algorithms [21], [24], density-based clustering algorithms [8], [23], [27], spectral clustering algorithms [2], [32], hierarchical clustering algorithms [5], [18].

DBSCAN [8] is a well-known density-based clustering method, which randomly selects a point  from a data set, and determines whether  is a center point based on two parameters  and  [8]. If  is a center point, DBSCAN finds all reachable points of  to form a cluster. Otherwise, DBSCAN chooses another point. Another representative density-based clustering algorithm is the DPC algorithm [25]. DPC constructs a decision tree to find cluster centers by local density and distance. But both of these algorithms only consider distance and density, while they ignore distribution relationship between points. In addition, neither DBSCAN nor DPC can achieve good results on unevenly distributed data sets and it is difficult for them to choose suitable parameters [8], [20], [25], we illustrate this problem by using an example shown in Fig. 1(d). In Fig. 1(a), we only get one cluster which color is red. However, if we change the parameters of DBSCAN, DBSCAN returns two clusters while the yellow and blue points are considered noisy data in Fig. 1(b). If cutoff distance  [25] of DPC is not suitable, then clustering results will be inaccurate. In Fig. 1(c), when  is set to 5%, DPC returns only one cluster which color is red. If we set  to 1% in Fig. 1(d), we can get three correct clusters with different colors.

To address this concern, we propose new and effective clustering algorithms, including three key points: a direction-based clustering algorithm, a method for automatically obtaining parameters, parallelization of clustering algorithms. A direction-based clustering algorithm is proposed, which can be used to cluster points by the distribution relationship between points. On this basis, an adaptive calculation algorithm is proposed to automatically obtaining parameters. Finally, a parallel method based on these algorithms is proposed to process large-scale data sets. The contributions of this article are described as follows.


Download : Download high-res image (997KB)
Download : Download full-size image
Fig. 1. DBSCAN and DPC with different parameters.

•
To address the problem that it is difficult to dig out the orientation relationship between points based on density and distance, a direction-based measurement method is proposed, which achieves more accurate clustering results.

•
Direction is determined by the distribution of each point’s  nearest neighbors. In order to solve the difficulty in choosing parameter , we propose an adaptive method based on natural neighbors, which can obtain the parameter  automatically.

•
To adapt to process large-scale data sets, we implement parallelization of these algorithms. Finally, a parallel adaptive direction-based clustering algorithm is proposed.

•
Experimental results show that compared with DBSCAN and DPC, our algorithms can achieve better effect in clustering accuracy.

The remainder of the paper is organized as follows. Section 2 reviews the related work. Section 3 presents related concepts. In Section 4, the ADC algorithm and the APC algorithm are introduced. The parallelization method is introduced in Section 5. Section 6 analyzes experimental results and evaluates algorithms performance. The conclusion is summarized in Section 7.

2. Related work
Clustering algorithms, because of their efficient and unsupervised nature, are widely used in various applications [10], [24], [29].Partition-based methods (e.g., K-Means and K-Medoids) [21], [24] have low time complexity and are easy to implement, but they are not suitable for irregular data sets and are sensitive to noisy data. Hierarchical methods (e.g., BIRCH, CURE, and ROCK) [5], [18] have few restrictions, can discover hierarchical relationships between clusters and do not need to pre-define a number of clusters, but they have high time complexity. Density-based methods (e.g., DBSCAN) [8] can find arbitrary clusters and noise points. But different parameters will cause great differences in clustering results for these algorithms.

Currently, there are a lot of researches on density-based clustering algorithms, mainly including two aspects, one is the improvement of traditional clustering algorithms, and the other is to propose new density-based clustering algorithms [8], [9], [25]. Groups of Density-Peak-based Clustering (DPC) algorithms were proposed in [25]. The selection of clustering centers by DPC is that it has a high local density and a large distance from other high-density points [25]. The algorithm mainly highlights the guidance method for selecting cluster centers. DPC can process arbitrary data and achieve low computational complexity [25]. But its disadvantages are that the setting and selection of cutoff distance are done manually, which directly affect the quality of clustering [7] and have limited clustering effect on multiple domain density maximums data sets [4].

DBSCAN [8] does not require users to pre-define a number of clusters and can divide clusters with complex shapes. However, DBSCAN requires two parameters,  and .  refers to the radius of neighbors of a given object and  is a neighbor threshold of the object’s  neighborhood [8]. And DBSCAN is sensitive to the two parameters.

Aiming at the improvement of clustering algorithms, many researchers have also focused on parallel methods. Gou et al. [13] constructed a sparse spectral clustering framework based on the parallel computation of MATLAB. Ghaffari et al. [11] proposed a parallel algorithm for density-based network clustering. Jinet et al. [16] combined spectral clustering with MapReduce. Huo et al. [14] proposed an efficient parallel spectral clustering algorithm on multi-core processors in the Julia language. Sevillano et al. [26] introduced hierarchical consensus architectures, an inherently parallel approach based on the divide-and-conquer strategy for computationally efficient consensus clustering.


Download : Download high-res image (129KB)
Download : Download full-size image
Fig. 2. Illustration of two concepts.

Compared with existing clustering algorithms, the proposed direction-based method in this work can effectively process data of various shapes by combining the directional information of data distribution. To reduce the selection of parameters in this direction-based method, an adaptive method is introduced to get parameters. Moreover, this method is feasible and practical in actual big data applications.

3. Related concepts
Definition 1 Neighbor Vector

Each neighbor of a point corresponds to a unit vector called a neighbor vector whose direction points from the point to the neighbor and whose length is 1. So the dot product of two neighbor vectors is the cosine of their angle.

Definition 2 Receiving Direction

A direction, denoted as where the neighbors have the densest distribution, is called receiving direction. In Fig. 2(a), the upper right neighbors of point  are densely distributed, so the receiving direction of  is shown by the black arrow. It can be computed as (1)
 

In Eq. (1),  represents the set of all vectors, 
 is a neighbor points set of ,  is receiving direction of . If  has  neighbors, denoted by 
, 
, …, 
, the corresponding neighbor vectors are 
, 
, …, 
 
, and the calculation of receiving direction is as follows.

1.
For  in 
, calculate the dot product of  and the remaining  neighbor vectors. If 
 is greater than or equal to 0.5, the value of 
 is accumulated. Finally, the value is recorded as 
.

2.
Find the maximum value in , the corresponding vector is the receiving vector of , and the direction of this vector is the receiving direction of .

Definition 3 Cluster Label

Cluster label is used to indicate the cluster that each point belongs to in a data set.

Definition 4 Maximum Deviation Angle

An angle is set to determine the acceptable range based on the receiving direction. This angle is regarded as maximum deviation angle. Only in this range, all neighbors’ cluster labels are reliable.

In Fig. 2(b), neighbors of  are , , , , ,  and  and the receiving direction is shown by the black arrow. When the maximum deviation angle is 30°, , , ,  and  are in the receiving range. If the maximum deviation is 45°, then all the neighbors of  are in the receiving range.

Definition 5 Sender

If a neighbor is in the receiving range, it means that the neighbor can send the cluster label to , we regard this neighbor is a sender of . In Fig. 2(b), all neighbors are senders of  if then maximum deviation angle is 45°.

Definition 6 Receiver

If a neighbor is in the receiving range,  can get the cluster label from this neighbor, we regard  is a receiver of this neighbor. In Fig. 2(b),  is a receiver of all neighbors if maximum deviation angle is 45°.

4. An adaptive direction-based clustering(ADC) algorithm
4.1. The direction-based clustering algorithm
For a given point, its neighbors may be distributed in all directions. If a point with unevenly distributed neighbors, it can clearly identify there is a relatively dense area with more neighbors and a few neighbors in other sparse areas. For boundary points, this phenomenon is even more obvious. An example is shown in Fig. 3. In this data set, there are five clusters while others are noise points. The points in clusters are densely distributed, and the points outside clusters are sparsely distributed [30].

Thus, for these boundary points, there must be a dense side where points are in the clusters and a sparse side with noisy points. So how can these boundary points determine the received cluster labels are credible? Based on common sense, we think clustering is propagated according to the cluster label of each point and a point with its densest neighbor points should belong to a same cluster. Therefore, cluster labels should be discarded from the sparsely distributed side, and cluster labels should be received from the densely distributed side.

The DBC algorithm requires two parameters. The first parameter is , which is used to determine the range of the neighbors. Then the receiving direction of each point can be calculated according to  nearest neighbors. The second parameter is maximum deviation angle, which is used to determine acceptable range .

In Section 4.2, we will introduce an adaptive method to get the parameter . And the determination of maximum deviation angle still requires multiple adjustments. Generally, the selection of maximum deviation angle depends on the distribution of points. If a data set is sparse, the maximum deviation angle should be as large as possible to find suitable neighbor nodes. If it is a dense data set, the value of maximum deviation angle should be small to avoid classifying multiple clusters into one cluster. But the maximum deviation angle should between in .

In Algorithm 1, when initial work is completed, we first select the point with most receivers as the starting point, and assign it a cluster label  to indicate that it belongs to the -th cluster from line 4 to line 6. In lines 7 to 8, we initialize a queue  and enqueue the starting point. On line 9, it determines whether queue is empty. If it is empty, it means that a cluster has been discovered. If it is not empty, it means that a cluster has not been completely discovered, and the cluster label of this cluster needs to continue to propagate. From lines 10 to 21, We first get a starting point  from the queue , and get  of  according to the . If the cluster label of  is , the cluster label  is sent to the  of  and we enqueue the . Then the  spread again as new senders until there are no new senders. When the process is over, a new cluster is discovered. On line 25, we calculate all the points that are not labeled. If all the points have been labeled, then the clustering process ends and clustering results are obtained.

In the process of discovering each cluster, at most  data points are added to the list  according to the DBC algorithm, and the number of potential receivers of data points in  does not exceed , so the DBC algorithm takes  at most. And the space complexity is .

4.2. The adaptive parameter calculation algorithm
To determine receiving direction, we first need to get neighbors of points, that is,  nearest neighbors. In general, the value of  depends on the distribution of data sets, if  is large, which can deal with noise well, but may merge multiple clusters into one cluster. A small value of  will cause a cluster to be divided into multiple. Therefore, determining parameter  depends on the empirical knowledge of the researcher and a lot of experiments. To solve this problem, we introduce an adaptive parameter calculation(APC) algorithm to compute parameter .

The APC algorithm is designed based on the concept of natural neighbors proposed in [6], [34]. If 
 considers 
 to be a neighbor and 
 also considers 
 to be a neighbor, then 
 is called a natural neighbor of 
. For a data set , taking  = 1,2,3, …, . In this search process, if  increases to , there is at least another data point that is a neighbor to a point in , then the current search state is called original natural stable state(NaSS) [34]. NaSS can be formulated by (2)
 
 is the th neighbor of . The parameter  increases from 1 to , where  is named Natural Neighbor Eigenvalue(NaNE) [34] and  is 2 ¡. In the case of no outliers, its normal value is 6 or 7. As the dimension increases, the value of  becomes larger, but usually can stay within 30 [34].


Download : Download high-res image (223KB)
Download : Download full-size image
Algorithm 2 shows the entire process in detail. On line 1, we initialize some variables.  is the record of neighbor number of each point.  is a list used to store neighbors of each point.  is a list used to store inverse neighbors of each point. If 
 considers 
 to be a neighbor, then 
 is an inverse neighbor to 
. On line 2, we construct a KD-tree for data set . From lines 4 to 8, we compute th nearest neighbor of each point by the KD-tree. Then we update corresponding lists , , . On line 10, we compute the number of points in  that are not considered as neighbors. If the number does not change, then we update . If the value of  meets the conditions [6], the loop process ends from line 11 to line 16. In lines 17 to 21, in order to find the appropriate number of neighbors, we will continue to expand the neighbor range, that is, increasing the value of . On line 23, when the entire calculation process is complete, APC returns the set of neighbors for each point.

In Algorithm 2, firstly in order to quickly find  nearest neighbor nodes, a KD-tree is constructed. Then th nearest neighbor is found for each point by the KD-tree, the initial value of  is 1, if the total number of points is , time complexity is . When the clustering process is complete, it costs . In addition, for some noise points, other points do not consider them as neighbor nodes, so these noise points have no natural neighbors. To a certain extent, this algorithm can filter noise well.

5. A parallel adaptive direction-based clustering(PADC) algorithm
The objective of the PADC algorithm is to combine the search process of the adaptive method and speed up the clustering process of the ADC algorithm. The clustering process contains two steps. One is to calculate receiving direction of each point in a data set according to neighbor points, and the other is to determine the receivers of each point according to receiving direction. In this paper, we adopt a multi-processors parallel method to complete the creation of parallel tasks and coordinate the parallel tasks.

After obtaining the  nearest neighbors by Algorithm 2, the receiving direction of each point in a data set is calculated according to Eq. (1). It is independent to find receiving direction of each point. Therefore, this task can be divided into multiple subtasks to be performed simultaneously. In addition, after each point determines receiving direction, the neighbor acceptable range can be determined according to the maximum deviation angle. This process does not need to wait for other points, but only needs to obtain its own receiving direction. Finally, when the above task is completed, the receivers of each point can be obtained, and then we use Algorithm 1 for clustering. The PADC algorithm is shown in Algorithm 3.


Download : Download high-res image (176KB)
Download : Download full-size image
On line 1, we calculate the  nearest neighbors for each point by Algorithm 2 and the results are stored in a list . Then we need to complete a task, which gets the direction and receivers of each point. On line 2, we divide this task to multiple subtasks based on , which is the number of CPU processors. For each subtask, we calculate the receiving direction  of each point based on  on line 4. Then, we determine the receivers  for each point based on  on line 5. On line 7, when the multiple subtasks are all completed, we use Algorithm 1 for clustering. On line 8, we return the final clustering results.

The first step of PADC is to find  nearest neighbors for each point by Algorithm 2, so time complexity is  and space complexity is . The second step of the algorithm is to calculate receiving direction of each point in parallel, the time complexity is 
, where  is the number of CPU processors. Since the receiving direction of each point needs to be stored, the storage space is . The third step of PADC is to determine receivers of each point. Its time complexity is , and the required storage space is at most . The fourth step of PADC is to cluster the data set. In Algorithm 1, its time complexity is primarily determined by the number of points added to queue . So the fourth step takes  time at most and the space complexity is .

6. Experiments
Experiments are conducted on Intel(R) processors(TM) i7-8700 CPU, 16 GB main memory. Through experiments, we compare ADC with DBSCAN and DPC that are implemented by Scikitlearn from two aspects of clustering effect and algorithm performance. Then we give the performance analysis results of the ADC algorithm. Two groups of data sets including synthetic and real world data sets are used in the experiments. The data sets are shown in Table 1, Table 2. The PADC algorithm is implemented using multiple-processors parallel programming [11] by a standard Python library multiprocessing. The multiprocessing is a package that supports spawning processes using an API similar to the threading module. The multiprocessing package offers both local and remote concurrency, effectively side-stepping the Global Interpreter Lock by using subprocesses instead of threads. Due to this, the multiprocessing module allows the programmer to fully leverage multiple processors on a given machine. The Pool class represents a pool of worker processes. It has methods which allows tasks to be offloaded to the worker processes in a few different ways.


Table 1. Synthetic data sets.

Data sets	Attributes	Clusters	Samples
Flame [10]	2	2	240
Jain [15]	2	2	373
Aggregation [12]	2	7	788
Spiral [3]	2	3	312
t4.8k [17]	2	6	8000
t8.8k [19]	2	8	8000
t7.10k [19]	2	8	10,000
6.1. Clustering effect on synthetic data sets
In experiment 1, there are seven artificial data sets, including Flame, Jain, Aggregation,Spiral, t4.8k, t8.8k, t7.10k. We divide the experimental data sets into two groups. The first group includes Jain, Flame, Spiral, Aggregation with overlapping regions, different shapes and great density difference. The other includes t4.8k, t8.8k, t7.10k, which are large-scale data sets with a lot of noise. In Fig. 4(l), ADC achieves better clustering results than DBSCAN and DPC. For data sets with large difference, like Jain, both DBSACN and DPC cannot achieve good results. And DBSACN is not suitable for data sets with different shapes, like Aggregation.

In these small data sets, we have seen the problems of DBSCAN and DPC. In large data sets, their problems will be displayed in a clearer way. To prove this, we use three large data sets t8.8k, t4.8k and t7.10k. In Fig. 5(i), we can find that DPC has the worst clustering effect, while ADC has the best clustering effect. For these three data sets, DPC cannot obtain a correct clustering result, DBSCAN algorithm obtains two correct clustering results, and ADC obtains three correct clustering results. Therefore, ADC has a stronger ability to discover clusters of arbitrary shapes and large-scale data sets than DBSCAN and DPC.

6.2. Clustering effect on real data sets
We test these algorithms using real world data sets in this section. The real world data sets are from UCI Machine Learning Repository [1]. The clustering results are shown in Fig. 6(l). For these real world data sets, no algorithm can get the best clustering results on all data sets. In the Banknote DBSCAN cannot achieve correct clustering results. And in the Wine DPC cannot achieve correct clustering results. For Iris data set all algorithms cannot achieve correct clustering results.

6.3. Performance evaluation
6.3.1. Clustering performance evaluation
To evaluate the clustering performance of these algorithms, we use an indicator Clustering Accuracy(CA) to evaluate the clustering algorithm. CA measures the ratio of the correctly clustered instances to the pre-defined class labels. Let  be the data set in this experiment,  be the set of clusters detected by the corresponding algorithm, and  be the set of pre-defined class labels. CA is defined as: (3)
 
where 
 is the data points in the th cluster, 
 is the pre-defined class labels of the data points in 
, and  is the number of . 
 is the number of data points that have the majority label in 
. The experimental results of the clustering accuracy comparison are given in Table 3.

In Table 3, ADC achieves better accuracy on both synthetic and real-world data sets than DBSACN and DPC. ADC achieves a high average CA of 0.9983 in synthetic data sets. The average accuracies of DPC and DBSCAN are lower than that of ADC. In the real world data sets, ADC has higher CA than that of the other algorithms, the lowest accuracy is 0.8752 and the highest accuracy is 0.9606. It illustrates that ADC achieves higher clustering accuracy over DBSCAN and DPC.

6.3.2. Algorithm performance evaluation
For different size data sets, the number of processors increases from 1 to 6, and the performance of the PADC algorithm is evaluated by observing the average time it takes to execute the algorithm. The case with the best performance is selected, and the speedup of the PADC algorithm in different cases is tested. The results of PADC performance evaluation experiments are shown in Fig. 7(b).

In Fig. 7(b), when the number of processors is set to 1, the execution time of kdd-train (125973 nodes) is 8645.52s, the execution time of kdd-train-50percent (62986 nodes) is 4283.69s, and the execution time of kdd-train-20percent (25194 nodes) is 1453.11s. As the number of processors increases from 1 to 6, the performance of the clustering process is significantly improved. The average execution time of kdd-train decreased from 8645.52s to 1446.11s. The average execution time of kdd-train-20percent decreased from 4283.69s to 905.82s. The average execution time of kdd-train-50percent decreased from 1453.11s to 486.66s.


Table 3. Clustering accuracy.

Data set	ADC	DBSCAN	DPC
Aggregation	0.9973	0.9898	0.9987
Jain	1.0	0.9276	0.9222
Flame	0.9958	0.65	1.0
Spiral	1.0	1.0	1.0
Banknote	0.9555	0.9346	0.9803
Wine	0.9606	0.8845	0.8932
Wdbc	0.8752	0.8646	0.8576
Iris	0.9466	0.9066	0.9533
In Fig. 7(a), obviously, the advantages of parallel optimization are greater in the case of large-scale data sets than in the case of small-scale data sets. When the size of the data set increases or the number of compute nodes increases to a limited range, the benefits become more obvious.

7. Conclusion
This paper presented clustering algorithms, where we regard direction as the main physical metric rather than density and distance. To solve the problem of parameter selection, an adaptive parameter acquisition method based on natural neighbors was proposed. In addition, in order to adapt to data of different scales and acceptable computing time, we introduced parallel techniques on multi-processors to improve the clustering performance. In comparison with state-of-the-art clustering algorithms, DBSCAN and DPC, whether for artificial or real world data sets, the proposed PADC algorithm requires fewer parameters and achieves best clustering results.

The next work is to apply the proposed algorithms to a simulated industrial control system to perform cluster analysis on abnormal traffic in the network. And trying more parallel ways, such as Spark, Hadoop, to further research issues of big data clustering analysis based on our work [31], [33].