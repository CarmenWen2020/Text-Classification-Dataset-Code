Abstract
In the big data era, large amounts of data are under generation and accumulation in various industries. However, users usually feel hindered by the data quality issues when extracting values from the big data. Thus, data quality issues are gaining more and more attention from data quality management analysts. Cutting-edge solutions like data ETL, data cleaning, and data quality monitoring systems have many deficiencies in capability and efficiency, making it difficult to cope with complicated situations on big data. These problems inspire us to build SparkDQ, a generic distributed data quality management model and framework that provides a series of data quality detection and repair interfaces. Users can quickly build custom tasks of data quality computing for various needs by utilizing these interfaces. In addition, SparkDQ implements a set of algorithms that in a parallel manner with optimizations. These algorithms aim at various data quality goals. We also propose several system-level optimizations, including the job-level optimization with multi-task execution scheduling and the data-level optimization with data state caching. The experimental evaluation shows that the proposed distributed algorithms in SparkDQ run up to 12 times faster compared to the corresponding stand-alone serial and multi-thread algorithms. Compared with the cutting-edge distributed data quality solution Apache Griffin, SparkDQ has more features, and its execution time is only around half of Apache Griffin on average. SparkDQ achieves near-linear data and node scalability.

Keywords
Parallel data quality algorithms
Distributed system
Data quality management system
Multi-tasks scheduling
Big data

1. Introduction
Data quality problems are prevalent and critical. More and more organizations are becoming aware of the impact big data has on their way of progress. However, there is one alarming problem - if their data are not accurate or consistent, it can lead to a stumble when making decisions [17]. Data quality management is a set of practices to maintain high information quality [24]. Technically, it is all about finding out dirty data (i.e., detection) and trying to clean them up (i.e., repair) [5]. For example, when we discover duplicated records of the same account, we may keep one of them and delete the others. Data quality management has become a fundamental part of the entire life cycle of big data processing in data warehouses.

As to data quality management, a versatile and scalable system is in demand in real-world scenarios. However, existing solutions, like extract-transform-load (ETL) systems [25], data cleaning systems [3], and data monitoring systems [30], still have drawbacks in functionality and processing performance.

Data ETL systems help process and analyze data to some degree, but they are not specialized solutions to data quality management. On one side, they do not usually provide adequate functions regarding data quality management for the users. For example, Kettle [2] could detect faults by throwing errors at the user, but the user must fix it manually, since Kettle does not have a repair function to figure out the actual problem. On the other side, the stand-alone database-based architecture would be pretty slow. Take Kettle [2] as an example again. Typically, its query speed is up to 5,000 rows per second. At this speed, it may run for hours to handle millions of lines of data.

Data cleaning systems are specialized data quality management solutions. Each of them works well in a particular type of data quality problem-solving. Nevertheless, they usually focus on a single model or interface. For example, NADEEF [4]'s core copes with multiple data quality rules holistically. Also, they do not provide a way to chain up different models. Therefore, they are not very expressive when handling complicated data quality workloads, such as the complex pre-processing stages of machine learning tasks.

Data quality monitoring systems [23], [31] can present comprehensive dimensions of basic data quality problems. They can detect data quality problems automatically, but generally, they cannot describe complex data quality problems like functional dependency conflict, entity redundancy, etc. Most of them cannot fix detected problems by themselves, either.

What is worse, we cannot just integrate current data quality management systems into one, because current systems vary in both usages and interfaces. Different systems have different definitions for the same data quality problem. For example, HoloClean [21] and BigDansing [15] are both solutions for detecting and repairing data integrity. However, they have divergence about what are the integrity constraints. In BigDansing [15], integrity constraints can be expressed as a functional dependency, a denial constraint, or a user-defined function. However, HoloClean [21] defines integrity constraints as denial constraints only. Therefore, it is almost impossible to integrate such different systems.

Detecting and repairing data quality problems in big data is more challenging. Firstly, their execution times surged due to the explosion of data size. Secondly, many tasks may fail on big data occasions because stand-alone systems are short of storage resources.

Generally speaking, there exist some significant challenges to build an efficient and generic data quality management system:

1.
The system interfaces should be flexible and expressive enough to allow the construction of various data quality detection and repair tasks with ease.

2.
The system needs to cover a wide range of usage scenarios, including data detection and data repair.

3.
The system has to process a large amount of data in a reasonable time.

In the face of the challenges, we propose SparkDQ, a distributed generic data quality management system that supports large-scale data quality detection and repair. In this work, we make the following contributions:

1.
We first propose a generic big data quality management model and programming framework built on top of Apache Spark. It allows users to develop their data quality tasks easily and quickly.

2.
We design a set of distributed data quality management algorithms in SparkDQ. Some of them are basic, like mode detection algorithm, mutual information detection algorithm, and iForest model-based outlier detection algorithm. The others are relatively complex, like the priority-based multi-CFD data detection and repair algorithm, the semantic- and block-based entity detection and extraction algorithm, and the naive Bayesian model-based missing value filling algorithm.

3.
We propose two system-level optimization strategies in SparkDQ, including the job-level optimization with multi-task execution scheduling and the data-level optimization with data state caching. The two optimizations can improve efficiency when dealing with multiple data quality issues like multiple approximate quantile detection.

4.
We implement a prototype of SparkDQ1 and evaluate its performance with extensive experiments. The experimental results indicate that the proposed distributed data quality management algorithms are up to 12 times faster than the stand-alone serial and multi-thread algorithms. The proposed system-level optimization can reduce the execution time by up to 56%. Compared with the cutting-edge data quality system Apache Griffin, SparkDQ supports more features like outlier detection, CFD-based detection, and repair. For different data quality tasks, the execution time of SparkDQ is only about half of Apache Griffin on average. Moreover, SparkDQ achieves near-linear data and node scalability.

We organize the rest of the paper as follows. Section 2 introduces related work of data quality management. Section 3 presents the proposed distributed data quality management algorithms and system-level optimizations. Section 4 describes the system design of SparkDQ. Section 5 presents the performance evaluation results. Finally, we conclude the paper and discuss the future work in Section 6.

2. Related work
Data ETL system: Data extract-transform-load (ETL) systems extract the scattered data from various sources, transform them into a unified format, and load them to the destination. ETL systems are not for data quality problems in particular, but they can solve such problems to some degree. SQL-based ETL system is relatively easy to build. However, subject to the limitations of the SQL engine, it is time-consuming and unable to solve complex data quality issues. Furthermore, writing complex SQL instructions requires the user's expertise in SQL programming. Various specialized ETL engines emerged to overcome these limits. Systems like Kettle [2], Informatica [32], Talend [1], and DataX [33], provide complex transformations towards data warehouses. However, they are not user friendly in data quality management due to the difficulty of learning the system configuration and usage. Oracle GoldenGate [9] provides both fault and disaster tolerance towards the database. Nevertheless, it falls short in supporting distributed data transformation at a large scale, and it is not extendable to non-database problems. DataPipeline [34] focuses on fast and changing data fusion and exchange. It supports massive data processing based on streaming data models. However, since streaming data models struggle to express such issues, they are not good at solving complex data quality problems.

Data cleaning system: Unlike data ETL systems, data cleaning systems are specialized solutions to data quality issues. Constraint-based systems (like NADEEF [4], LLUNATIC [8], and BigDansing [15]) detect data conflicts through pairwise comparison of data with all pre-defined integrity constraints and obtain repair decisions based on heuristic methods. Rule-based systems [13] match the input data to existing data or repositories to figure out dirty data and recommend proper solutions to solve these data quality problems. Typical rules include editing rules [7], fixing rules [26], sherlock rules [14], and knowledge base [10]. Statistic-based systems like HoloClean [21], ERACER [20], and SCARE [29], train probability models based on statistic properties of the incoming data. They train their probability models by treating attributes as random variables in dirty data. Systems with human-computer integration are another type. Users are required to participate in the data cleaning process continuously and help make decisions. GDR [28] uses conditional functional dependence [6] (CFD) to generate possible cleaning methods. Whether to run the generated methods is decided by the user. FALCON [11] interacts with the user to repair data quality problems. CrowdER [27] generates possible redundant data pairs for the user to diagnose. Data cleaning systems can solve data quality issues. However, they usually concentrate on one single model or interface only. Also, typically they cannot bind different models. Therefore, they are not very expressive to handle complex data quality workloads, like the pre-processing stages of machine learning tasks.

Data monitoring system: To enhance accuracy and reliability in data quality detecting, systems like Data Quality Center [12] and Apache Griffin [31] have emerged. These systems allow users to customize data quality monitoring tasks and get corresponding problem alerts, showing the found dirty data. However, data monitoring systems do not provide a way to repair these dirty data.

Summary: We summarize the features of related systems in Table 1, where SparkDQ is our work to be introduced. The existing systems vary in both usages and interfaces. Different systems also have different definitions for the same data quality problem, making it almost impossible to integrate such different systems. Therefore, it calls for a unified model and framework for data quality management of big data. The framework includes three layers: programming interfaces, distributed data quality management algorithms, and system-level optimization for multi-task scenarios.


Table 1. Features for data quality management systems.

Solution	ETL System	Cleaning System	Monitoring System	SparkDQ
Kettle [2]	Datapipeline [34]	NADEEF [4]	BigDansing [15]	HoloClean [21]	CrowdER [27]	Griffin [31]
Data Detection
Missing	×	✓	✓	✓	✓	×	✓	✓
Redundancy	×	✓	×	×	×	✓	✓	✓
Rule-based	×	✓	×	×	✓	×	✓	✓
Integrity-based	×	×	✓	✓	✓	×	×	✓
Outlier	×	×	×	×	×	×	×	✓
Data Repair
Fill the missing	✓	✓	✓	✓	✓	×	×	✓
Deduplication	✓	✓	×	×	×	✓	×	✓
Rule-based	✓	✓	×	×	✓	×	×	✓
Integrity-based	×	×	✓	✓	✓	×	×	✓
Supporting Data Sources
Multiple files	✓	✓	✓	×	×	×	✓	✓
Big data support	✓	✓	×	✓	×	×	✓	✓
3. Data quality programming framework and algorithms
3.1. Programming framework
3.1.1. Programming interfaces
This layer presents necessary high-level programming interfaces for managing single and multiple data quality problems. It provides users with both imperative and declarative interfaces for different programming models. Imperative interfaces are used for single-task scenarios. They call the related functions, execute directly, and return the results immediately. In contrast, declarative interfaces are used for multiple-task scenarios. They define all tasks before launching the function calls and chain up the execution process.

SparkDQ provides a number of APIs for different types of data quality detection and repair operators. Specifically, SparkDQ provides the following interfaces in Table 2, Table 3. Users can use these interfaces via writing a trivial Spark application.


Table 2. Detection interfaces provided by SparkDQ.

Type	Interface	Parameters	Implication
Integrity	is_complete	col, where	check if a row is complete
has_completness	col, assert, where	check the completeness of a row

Uniqueness	has_uniqueness	col, assert	check if a row is unique
has_distinctness	col, assert	check if a row is distinct
has_histogram_values	col, assert, binning_udf, max_bins	check the histogram distribution of a row
has_approx_distinct	col, assert, where	check approximate distinct values
has_entities	cols, assert, index	check similar entities

Consistency	is_positive	col, assert, where	check if the value is positive
is_contained_in	col, vals, assert, where	check if the value is in given range
satisfies_exp	exp, assert, where	check if the value satisfies the expression
has_data_type	col, type, assert, where	check possible data types
matches_pattern	col, pat, assert, where	check if the value matched the pattern
satisfies_fd	cfd, assert, index	check if the value satisfies the dependency

Validity	has_size	col, assert, where	check the number of rows
has_approx_quantile	col, quantile, assert, where	check the approximate quantile of two columns
has_mutual_info	col1, col2, assert	check the mutual information of two columns
has_outliers	cols, model, params, assert, where, index	check if has outliers

Table 3. Repair interfaces provided by SparkDQ.

Type	Interface	Parameters	Implication
Filling	fill_value	value	fill the given value
fill_previous	-	fill the previous value
fill_next	-	fill the next value
fill_automatic	model	fill according to a probability model

Dropping	drop_null	col, where	drop rows with missing values
drop_duplicates	cols, keep	removes redundant rows
extract_entities	cols	extract similar entities
filter_by_exp	exp, where	filter by expression
drop_by_exp	exp, where	drop by expression
drop_outliers	cols, model, params	drop rows with outliers

Replacing	replace_by_pattern	col, pat, way, value	replace by pattern
replace_by_sub_pattern	col, sub_pat, value	replace by sub-pattern
extract_by_sub_pattern	col, sub_pat, where	extract by sub-pattern
replace_outliers	cols, model, params, way, value	replace the outliers according to a pattern
repair_by_fd	cfds, priorities, index	repair the data according to functional dependency
3.1.2. Distributed data quality management algorithms
There are four types of data quality problems, each of which violates data integrity, uniqueness, consistency, or validity. The data quality detection operator detects the problems mentioned above.

Data integrity requires that every record has a value of each attribute. For example, in Table 4, Record #2 violates data integrity. Data uniqueness requires that all the values of one attribute differ if that data column has a unique attribute. For example, in Table 4, Record #3 and #4 violate data uniqueness since they have the same ID. Data consistency requires that the table meets all data dependencies. Record #5 violates data consistency in Table 4 since London is not a city in China. Data validity requires that all values of the data are valid. In Table 4, Record #6 violates data validity since age should not be lower than 0.


Table 4. Different types of data quality problems: an example.

Record No.	ID	Age	Country	City
1	1	20	China	Nanjing
2	2		China	Beijing
3	3	23	UK	London
4	3	13	USA	New York
5	4	24	China	London
6	5	-1	Japan	Tokyo
Also, there are three types of operators (i.e., filling, filtering, or replacing) to fix data quality problems. Filling operation is conducted by generating valid values to the data found lost by the detect operator. Filtering operation is done by directly delete records with wrong attributes to evade data quality problems. Replacing operation modifies false data records to a correct status that meets all data quality rules.

SparkDQ utilizes distributed computing techniques to detect the four types of data quality problems mentioned above. Then it repairs data quality problems by the three types of repairing operators. SparkDQ parallelizes the data quality management operators by developing optimized distributed data quality management algorithms. Section 3.2 shows some algorithm examples.

3.1.3. System-level optimization for complex jobs
In SparkDQ, we handle data quality problems as a job. The job is chained up by one or more tasks. Each task in the job solves one data quality problem. Therefore, complex jobs in SparkDQ involve multiple data quality problems. SparkDQ provides two system-level optimization strategies to handle multiple data quality tasks simultaneously: semantic-based multitask scheduling optimization at the job level and the data state caching strategy at the data level. See details in Section 3.3.

3.2. Distributed data quality management algorithms
In this subsection, we propose three typical distributed algorithms for data quality management, namely the mode detection algorithm, mutual information detection algorithm, and iForest model-based outlier detection algorithm. After that, we explore how to parallelize three existing complex data quality management approaches for distributed systems: the priority-based multi-CFD management algorithm for data detection and repair, the semantic- and block-based entity detection and extraction algorithm, and the naive Bayesian model-based missing value filling algorithm.

Some algorithms in this subsection have similar implementations. However, our algorithms are different in many ways. Generally, existing claims of distributed data quality management algorithms have limitations on their heterogeneity in both parallelization models and programming interfaces. Unlike designing one specific algorithm, the goal of SparkDQ is to provide a distributed algorithm framework. SparkDQ is different because it is a unified, DataFrame-based distributed algorithm framework for data quality management, sharing the same parallelization optimization strategies and the same programming model.

Regarding the limitations of existing distributed data quality management algorithms, there exist similar implementations of distributed data quality management algorithms. However, they are not perfect solutions to generic, efficient data quality management because of two limitations. On the one hand, existing claims of distributed data quality management algorithms mainly consider the algorithm itself only. The parallelization models they used may vary. As a result, significant difficulties may emerge when applying different parallelization models. On the other hand, these implementations are heterogeneous, having many differences in their interfaces. It may involve more I/O operations when applying different detection rules over various computing engines.

SparkDQ is different from these existing solutions because of unified parallelization optimization strategies as well as a generic programming model. Actually, the six algorithms we mentioned in the paper are merely examples of data quality management algorithms in our framework. Adopting a unified framework has many benefits. First of all, it is easier to integrate and plug in different algorithms with the same framework. Secondly, using a unified framework is more efficient because we have unified data formats and computing engines. Thus, some ETL operations like data transformation between different data formats can be reduced. Thirdly, it is easier for users to build data quality management applications under the framework.

3.2.1. Distributed mode detection algorithm
Background: The mode detection algorithm aims to find the statistical mode [35] of a series of data. It is designed over the objective function , whose goal is to find the mode of a specific field c, that is to say, the most common number in the field.

Workflow: The computation tasks are generated and distributed to partitions in a Spark DataFrame, a distributed data structure with multiple partitions in different computing nodes. In each partition, the mapped cache is initialized, then all the data in the partition is scanned to add up data counts in that partition. After that, the algorithm merges the map caches of each partition to find out the global distribution. The data item with the highest frequency is selected and returned as the mode. Fig. 2 shows an example of the workflow of the proposed distributed algorithm.

Fig. 2
Download : Download high-res image (205KB)
Download : Download full-size image
Fig. 2. Workflow of distributed mode detection algorithm.

In this example, we want to find out the most common age of several students from three classes. Our objective function is . According to their class number, the records of all students are distributed among different partitions of the Spark DataFrame. In each partition, the algorithm initializes its data cache, goes through the rows of distributed data c, and stores the number of every age as a key-value pair in the data cache. The algorithm merges the key-value pairs in the data cache of different partitions and picks the maximum value from these pairs to return.

Similar to this algorithm, objective function-based approaches can be modified to a distributed version in the manner mentioned above.

3.2.2. Distributed mutual information detection algorithm
Background: Mutual information [22] is a common way to detect data dependency in data quality management. Consider a person information table with columns birth country and birth city. If the birth city of a person is Nanjing, his/her birth country must be China, because the two columns have a dependency that every person born in Nanjing must be born in China. If some entries violate this dependency, the mutual information detection algorithm can find it out by measuring the interdependence between these variables.

Concepts: In the mutual information detection algorithm, the interdependence of two columns X and Y is calculated by Equation (1). The equation requires the joint probability distribution of X and Y  and their respective marginal probability distributions  and . On the one hand,  represents the probability that the X and Y columns share the same value, which can be calculated by dividing the number of occurrences of each value by the total number of rows. On the other hand,  and  need other statistics for the X and Y columns, respectively.(1) 
  
 
 

Mutual information detection is a typical transformation-based data quality management approach. This kind of approach can be similarly converted to distributed algorithms.

3.2.3. Distributed IForest model-based outlier detection algorithm
Background: The iForest model [18] detects outliers purely based on the concept of defined isolation instead of traditional distance or density measure.

To build an isolated forest, we must first perform parallel sampling on the data set to obtain each tree's sample points. Given a DataFrame X with features, the algorithm generates sample indices randomly. According to row indices, it uses a distributed flat-map operator provided in Spark to group indices into row trees. We count the number of samples for each tree and broadcast the aggregated value to all computing nodes. After broadcasting, the algorithm goes through every sample in X, gets the number of copies in these trees, and replicates sample data to all trees, whose frequency is saved as points. The algorithm notes each tree's tree ID and points as a paired resilient distributed data set (RDD); then, it outputs the reduced paired RDD Y.

After parallel sampling, each isolated tree in the forest is constructed in parallel. For the input sample point X, let Q be a list of attributes of X. The algorithm selects a non-constant attribute  at random, then generates a random split point p within the range of q. The input point is divided by p. Finally, the middle node is returned as an iTree (a tree structure with isolation). If X is not dividable or there is no non-constant attribute in Q, then it returns the leaf node X.

To improve the efficiency of anomaly data detection, the broadcast mechanism is used to send the isolated forests to each computing node, and the average depth of each sample is calculated in parallel. The path length is calculated in a binary search manner. It checks whether the node is a leaf node. If not, it compares the index of the node with a preset split value. If it is smaller, the algorithm returns the path length of the left child plus 1. Otherwise, it returns the path of the right child plus 1.

3.2.4. Distributed priority-based multi-CFD management
Background: Traditional functional dependency data quality management algorithm [6] is based on a serial processing method, which increases the overhead exponentially in distributed scenarios. To reduce the overhead, we propose a priority-based solution that processes multiple function dependencies simultaneously.

Concepts: Conditional Functional Dependency [6] (CFD) is a two-tuple , where two dependency property sets X and Y are defined on the relation model R. When two records have the same attribute X, their values of attribute Y must be the same.  is a pattern table containing all the attributes in X and Y. It is composed of a series of pattern tuples that specify the value conditions. Each attribute can be a specific value or an underscore (_). An underscore indicates that there is no constraint.

Fig. 6 shows an example with two pattern tuples and two conflict situations. The left-hand side (LHS) has two attributes, A and B, while the right-hand side (RHS) has the attribute C. The pattern table contains two pattern tuples:  means that when attribute A takes value a, attribute C should be c to satisfy functional dependency.  means that when attribute B has value b, the functional dependency is satisfied. Two kinds of conflicts may occur, namely constant conflict and variable conflict. In this example, record 's attribute C is inconsistent with the constraint value of C in tuple , causing a constant conflict (dotted circles). The values of attribute C of records  and  are different under tuple , leading to a variable conflict (dashed circles).

Fig. 6
Download : Download high-res image (74KB)
Download : Download full-size image
Fig. 6. An CFD example.

One constant conflict (CC) affects one record only, which means a traversal of the table could finish the job. However, to handle variable conflicts (VC), grouping values by attributes of LHS is required before detecting and fixing. Therefore, traditional methods detect and fix both constant and variable conflicts separately. In fact, two conflict types will not happen in the same record, because they require different values of attribute C in the conflicting tuple (constant for CC and underscore for VC). This motivates us to design a distributed algorithm to detect and fix two types of conflicts simultaneously.

Workflow: Fig. 7 shows the workflow of the algorithm. Input data are stored as a distributed Spark DataFrame. For data detection, the algorithm detects constant conflicts and gathers records into groups in each partition separately. It then detects variable conflicts from each group. For data repair, the CFD consistency is checked first. The algorithm conducts the CC repair and record grouping later. In each group, a repair plan is generated based on the value of the maximum probability on the RHS. Finally, the partitions are merged. The plan is summarized and determined according to the priority of each CFD. As to possible new conflicts after the repair, another round will be executed until no conflict can be found, or the round number has reached the preset threshold.

Fig. 7
Download : Download high-res image (144KB)
Download : Download full-size image
Fig. 7. Workflow of distributed priority-based multi-CFD management.

3.2.5. Distributed semantic- and block-based entity management
Background: Real-world data often contain many overlapping and similar semantic information. As a solution, entity matching appears. It recognizes the same entity by comparing the similarity between records, whose complexity is up to , leading to huge overhead in big data scenarios. The data block method [16] is a proper solution to reduce overhead, but such a method requires the user's expertise and pre-trained model to determine how a block is constructed. To solve these problems, we propose an entity detection and extraction algorithm based on semantic information and block technology. It uses the semantic information of the data itself and adopts a structure-independent block method, which greatly reduces the number of record comparisons.

Data slicing is a key part that includes four stages: attribute division, entry slicing, block cleaning and filtering, and generating meta-blocks. To generate a meta-block, computing the threshold is done first. Since the cost of constructing a global block graph is too high, each block with the profile information contained is broadcast to each node, and a local block graph is constructed for each profile.

The threshold is computed in a distributed way. First, it initializes the local weight, entropy, and neighbor information. Then, it traverses each profile on the partition and calculates the weight and other local information. Finally, the threshold is calculated based on the local information. Multiple profiles in the same partition can reuse the local information space.

The calculated results, called the thresholds, are used to prune the computation. The threshold information of each profile is broadcast, and the average of the neighbor thresholds is used as the pruning threshold. All edges smaller than that are deleted. Entity matching for candidate profiles is based on similarity. We provide several commonly-used similarity computing algorithms, such as algorithms based on word fragments, like Cosine and Jaccard Index, and algorithms based on edit distance, like Levenshtein and Jaro-Winkler. With the transitivity of the matching relationship, a matching graph is constructed, and its connected components are calculated to obtain clusters of similar entities.

3.2.6. Distributed naive Bayesian model-based missing value filling
Background: We propose a distributed missing value filling algorithm based on the naive Bayesian model [19] that uses probability and statistical information of the data to realize automatic missing value filling.

Concepts: The naive Bayesian model is a classification model based on probability statistics. Considering the problem of missing data, it uses non-missing records in the data to calculate the probability of attribute values. Equation (2) reveals the relationship of probabilities between attributes to be filled and related attributes. Considering that  is some value of the missing attribute and X is the value of relevant attributes, the  that makes  the largest is the target value. We know that  is a constant, and  can be hypothesized as a constant since the prior probability of each value is unknown. Therefore,  is the only variable under our hypothesis. For multiple dependent properties, Equation (3) can be used to compute.(2)
 
(3)

The parameter statistics module contains three stages. First, public parameters such as related attributes and possible values are broadcast to each computing node. Second, it traverses the data set and uses a unified dependency information format to count the missing and non-missing target attributes in parallel, respectively. Third, the grouping is performed according to each value of the target attribute to calculate the value probability of the relevant attribute in parallel.

In probability computation, the algorithm first traverses all the groups, and saves the IDs of all missing and non-missing records for each related attribute and its value. Then, it calculates the probability of each relevant attribute value in parallel. Finally, it groups and summarizes all probability information based on the record IDs.

As to summarizing, it first multiplies the probabilities of the relevant attributes under the same target attribute value to obtain the probability of the target attribute. Then, the algorithm takes the value with the largest probability as the filling value. It selects the one with the most related attribute and the highest probability. Finally, the algorithm broadcasts the collected filling information to each computing node.

3.3. System-level optimizations for complex jobs
In the previous section, we have discussed several examples of distributed data quality management algorithms. In these examples, each algorithm deals with one specific data quality problem. As for more complex scenarios, say, given a dataset of multiple data quality problems, we have also proposed two system-level optimizations, namely the semantic-based multi-task scheduling strategy at the job level, and the data state caching optimization at the data level.

In complex processing, generally, we have different types of tasks in a job. Shared-scanning tasks only need to go through the whole data set once, like the mode detection task. Group computing tasks require dividing the input data into blocks. Then, each block is processed independently. The entity detection task is a typical group computing task. Other tasks containing complicated data transformation or model implementation are sophisticated tasks. For example, the mutual information detection task is a sophisticated one.

Instead of running them one by one in a sequence manner, SparkDQ's optimizations for complex processing aim to parallelize the independent tasks of the same type. As a result, SparkDQ can process the chained tasks in a job to holistically and efficiently.

3.3.1. Semantic-based multi-task scheduling strategy
We optimize the multi-task scheduling strategy with the knowledge of the computing task characteristics and the task combining techniques in data quality management.

Data quality detection: The execution time of multiple data quality detection tasks can be quite different because of the various task complexity. Based on the classification mentioned above, given a job, SparkDQ runs similar procedures of the same type of tasks in parallel.

For the shared scanning tasks, SparkDQ uses a shared cache to handle the operations of all tasks simultaneously. Specifically, we can divide detection tasks into multiple partitions. Each partition is computed in parallel. Finally, the system can aggregate result sets to get the metrics of the data set. Algorithm 1 shows how the algorithm handles detection tasks that share scanning. Firstly, it combines all tasks of the same type and generates task indices. Then, it gets all the aggregation functions that the current task needs to execute. Since the model may have more than one aggregation function, the offset of each task aggregation function needs to be recorded. Next, the calculation of all aggregation functions is executed in parallel at one time. Through the shared cache method, a similar offset calculation method is used to determine the starting position of each cache area. Finally, all calculation results are summarized, and metrics are constructed for each original task according to the offset information and the index of the task.

Algorithm 1
Download : Download high-res image (104KB)
Download : Download full-size image
Algorithm 1. Multi-task scheduling for shared scanning detection.

Regarding the group computing tasks, if the tasks ask for division based on the same column, such procedures could be done in parallel if later procedures of these tasks contain aggregation. Fig. 10 illustrates the workflow of data detection tasks that need group processing. First, the algorithm divides the tasks based on grouping columns and processes a set of tasks in the same column each time. Then, tasks are further divided into sharing and non-sharing types according to whether to continue to execute the aggregation function or not. We put sharing tasks before the non-sharing ones. For sharing tasks, it generates grouped statistical data, including related columns and their count columns; then, it calculates aggregation functions in parallel according to the workflow of Algorithm 1. If there is a non-sharing task, the grouped statistical data is cached, and a serialized task stream is constructed according to the pipeline mode given in Section 4.2. Finally, the algorithm summarizes the two types of processing results.

Fig. 10
Download : Download high-res image (289KB)
Download : Download full-size image
Fig. 10. Workflow of distributed multi-task scheduling for group processing detection.

Sophisticated tasks, however, have little related logic, so they have to be executed one by one. Nevertheless, SparkDQ is somehow more efficient because it removes unnecessary I/O operations.

Data repair: Similar to data quality detection tasks, the repair tasks can be classified into three categories according to the computational characteristics, including filtering tasks, replacing tasks, and sophisticated repair tasks. Since filtering tasks helps to reduce the data size and replacing tasks only takes one pass of the data, we usually execute the filtering tasks and replacing tasks first, and run the complex tasks at last to save the entire task waiting time.

For filtering tasks, SparkDQ combines the filter conditions of each task directly with logic & to construct a new filter rule, and uses the method to filter records in each partition in parallel. Note that the deletion operators are the opposite of the filter ones logically. Thus, the deletion operators must be transformed into filter operators at first.

For replacing tasks, the process of combining conditions to original values in SparkDQ is shown in Fig. 11. First, it groups replacing tasks according to related columns. Second, it combines replacing tasks in the same column each time and prepares the replacement values for all replacing tasks in advance. That is to say, it runs the scanning and sharing tasks in parallel to obtain specific values. At this time, each replacing task contains a fitting condition and a corresponding replacing value. Then, it combines each fitting condition and replacing value into a binary tree structure. Finally, the fitting conditions of each column are uniformly processed in parallel, using a  statement.

Fig. 11
Download : Download high-res image (182KB)
Download : Download full-size image
Fig. 11. Combining conditions to original values.

In particular, a special type of sub-pattern replacement exists, in which the condition is not based on the overall value of the column. Instead, it judges the matching sub-string when the value is scanned. Therefore, the combination of such tasks fits a nested structure, and the combination is done from inside to outside.

For the sophisticated ones, few relationships could be found as well. Therefore, SparkDQ runs them serially.

3.3.2. Data state caching optimization
Data quality management tasks involve the calculation of massive intermediate data. We propose a data state cache optimization mechanism that supports both memory and HDFS storage. Thus, it can achieve execution efficiency and track data state changes by saving data states persistently.

Data state caching is used by SparkDQ, depending on whether or not the tasks have common objectives or not. If they are only replicas for the same task, SparkDQ combines the parameters of each task into one cache. Then, SparkDQ constructs a new task to execute. If they are not the same task but tasks of the same type, SparkDQ can cache the computation results in the middle for sharing. If the tasks are of different types, we can still reuse data states if some of them have some computation steps in common. An example of caching in multiple detection processes is illustrated in Fig. 12.

Fig. 12
Download : Download high-res image (57KB)
Download : Download full-size image
Fig. 12. Data caching in different detection processes.

4. System implementation
4.1. System implementation overview
•
Application programming interfaces. This layer provides high-level interfaces of the programming models, as discussed in Section 3.1.

•
Data quality management model. This layer contains models of data quality detection and repair. It also allows connection between models by pipeline-based multitasking. Section 4.2 introduces more details of it.

•
Implementation and optimization. This layer contains Spark-based DataFrame, RDD from Spark, alongside built-in distributed data processing algorithms and complex optimization models.

•
Distributed computation. The computation is implemented as Spark applications in a master-slave distributed computing cluster.

4.2. Workflow of data quality management model
Detection model workflow: An data detection process is usually run before solving data quality problems. The data detection workflow includes four stages: data loading, parameter checking, target computation, and feedback. These four stages are connected by standardized operators. It receives data source and data quality targets set by the user, and gives an output of detection results, in the form of metrics, to the user.

Repair model workflow: The procedure of data repair is similar to the one in data detection. It includes four stages as well: data loading, parameter checking, data transformation, and data storing. The input of the repair model is the same as the detection model, while the output is the transformed data to be written to the destination. For unexpected consequences that may occur during the writing process, a mechanism for throwing and handling exceptions is necessary.

Pipeline-based multi-task model workflow: Since real-world dirty data sets may contain multiple data quality issues. Motivated by the design of ML Pipeline [36], a pipeline-based multi-task model is introduced to connect the above-mentioned modules. For the data detection tasks that share the same input, the execution of one task does not interfere with the execution of the other tasks. Therefore, the data detection model shares the data load for those tasks. They are connected and executed in the pipeline way. For the repair model, transformed data sets will affect the input of the latter model. As a consequence, the input of the latter model should be the output of the previous model in a connection, and models are executed in a pipeline way. As to unexpected consequences that may occur, exceptions will be thrown and handled once they appeared.

SparkDQ is based on the Spark engine, but it is much more than translating the algorithms to Spark's interface. SparkDQ parallelizes each API, including operators and algorithms, with a batch of sophisticated Spark parallel computing APIs. What's more, to run the jobs efficiently, SparkDQ also has job-level optimization with the semantic-based multi-task scheduling strategy and the data-level optimization with data state caching. It can also perform auxiliary functions such as parameters validity checking.

4.3. Programming in SparkDQ
Fig. 14 gives an example of programming in SparkDQ. This job contains three tasks. It checks the integrity of the job type column “WorkClass”, range of working hours per week column “HoursPerWeek”, and approximate distinct values of column “Age”. In the program, we first read data into DataFrames; then, we use different operators to express different data quality management requirements. From the user's perspective, he could chain up these operators and execute them by adding a “run” operator. From the computer's perspective, SparkDQ can understand the operation to do by reading these operators. It will attempt to optimize the execution procedure of tasks to improve efficiency.

Fig. 14
Download : Download high-res image (94KB)
Download : Download full-size image
Fig. 14. An example SparkDQ application.

5. Evaluation
In this section, we evaluate and analyze the performance of SparkDQ with extensive experiments.

5.1. Experiment setup
Hardware and software environment. Experiments in this paper are conducted in a cluster with five nodes, consisting of one master node and four worker nodes. Each machine has 24 logical CPU cores. The specifications of the hardware and software of each machine are listed in Table 5. Apache Spark is run on Yarn, where 80 GB of memory and 80 virtual CPU cores are deployed in total. To make good use of the cluster resources, we set the number of Spark Executor instances to 16, and each Executor allocates 4 GB of memory and four virtual CPU cores.


Table 5. Equipment of machines for experiments.

Hardware	CPU	2× Intel E5-2620 Xeon @ 2.10 GHz
Memory	32 GB
Disk	2 TB SAS
File System	Ext4
Network Bandwidth	10 Gbps

Software	Operating System	CentOS Linux release 7.1.1503 (Core)
JVM Version	1.8.0
Python Version	3.6.3
Hadoop Version	2.7.4
Spark Version	2.2.0
Dataset. In the experiments, the data set are generated from the real-world census data set Adult [37]. The Adult data set has 48,842 records and 15 attributes, including numeric and enumerated types. There are specific dependencies between attributes, which can be used to express various data quality problems. Based on the original table, we generate data sets with 11 different scales of records from 400,000 up to 10,000,000.

Experiment method. We organized the experiments into four stages:

1.
In SparkDQ, different algorithms focus on different data quality problems. Therefore, for different algorithms, we designed different types of dirty data according to the definition of each data quality problem that the algorithm tackles. Generally speaking, we set the proportion of dirty data to 5%. Each of such records posed one data quality problem. By running the proposed data quality models in SparkDQ, we have verified the correctness of our algorithms, shown in Section 5.2.

2.
We use the generated data set for execution performance comparison. For distributed algorithms, we measure and compare the throughput of distributed and stand-alone algorithms (see Section 5.3); for multitask scenario optimizations, we compare the execution time with and without the optimizations (see Section 5.4).

3.
Further, we compare SparkDQ with the cutting-edge distributed data quality management system (see Section 5.5).

4.
Finally, we evaluate the data and node scalability of SparkDQ (see Section 5.6).

5.2. Evaluation of detection capability
For the measurement of the effectiveness of the data quality detection, we proposed the detection accuracy metric in our experiments. Detection accuracy is the ratio of problems detected to all problems in a set of records. It makes sense to determine if a data quality management system is capable of detecting data quality problems.

Mode detection algorithm. To prove the correctness of the distributed version, we compared the stand-alone and the distributed algorithm on big data situations. We collected all of the return values by these algorithms, comparing their values. The result shows that all return values are the same as the stand-alone serial algorithm. Therefore, we can see that the mode detection algorithm in SparkDQ has a detection accuracy of 100%.

Mutual Information Detection Algorithm. Similarly, We checked if there are any differences between the mutual information calculated of different versions of the algorithms, shown in Table 6. Despite the possible round-off error caused by the computation over float numbers, the mutual information detection algorithm in SparkDQ is proven to be 100% correct.


Table 6. Accuracy of the mutual information detection algorithm.

Number of Records	Returned value of mutual information
Stand-alone serial algorithm	Distributed algorithm
1,000,000	0.0041386	0.0041386
2,000,000	0.0041386	0.0041386
3,000,000	0.0041386	0.0041386
4,000,000	0.0041386	0.0041386
5,000,000	N/A (Execution Failed)	0.0041386
IForest Model-based Outlier Detection Algorithm. In this algorithm, we counted the proportion of the outliers found in the input dataset. We have mentioned that 5% of our data records are dirty. Therefore, we could calculate detection accuracy by simply dividing the two proportions. The execution results are in Table 7. The numbers in these cells present the proportion of the outliers found to the number of records in the dataset. IForest model is a machine learning model that mainly focuses on traits, losing some detection accuracy. However, it is as accurate as of the stand-alone serial version. The results prove that our algorithms can detect over 90% of existing data quality problems in general.


Table 7. Accuracy of the iForest model-based outlier detection algorithm.

Number of Records	Proportion of records detected as outliers to the number of records
Stand-alone serial algorithm	Distributed algorithm
1,000,000	4.55%	4.55%
2,000,000	4.66%	4.66%
3,000,000	4.88%	4.88%
4,000,000	4.87%	4.87%
5,000,000	4.78%	4.78%
Priority-based Multi-CFD Management Algorithm. By inspecting the number of records with violations, we can figure out how many data quality issues have the algorithm found. We have checked the number of violated records by downloading the .csv file before and after computation. The results are in Table 8. The results prove that all functional dependencies are fulfilled after detecting and repairing violations in SparkDQ.


Table 8. Accuracy of the multi-CFD management algorithm.

Number of Records	Number of records which violates CFDs
Before running the algorithm	After running the algorithm
1,000,000	105,020	0
2,000,000	210,040	0
3,000,000	315,060	0
4,000,000	420,080	0
5,000,000	525,100	0
Semantic- and Block-based Entity Management Algorithm. To test the functionality of this detection algorithm, we run an example program. In this program, we use rows ‘workClass’ and ‘Occupation’. Firstly, we set the size of records to 400,000. We have calculated the input data records' distribution over the value of the two rows and checked the returned DataFrame. We can see three pairs of matched entities: (Federal-gov, Armed-forces), (Private, Priv-house-serv), and (Never-worked, null). Our algorithm can detect the three matched entity pairs. They also returned all records with one of these pairs, as well as the pair it has. We have figured out the statistics as Table 9. When the data size increases, it shows 100% detection accuracy in SparkDQ.


Table 9. Accuracy of the semantic- and block-based entity management algorithm.

Number of Records	Entity pairs & related records found
Entity pairs	Entity pairs found	Related records	Related records found
1,000,000	3	3	5200	5200
2,000,000	3	3	10400	10400
3,000,000	3	3	15600	15600
4,000,000	3	3	20800	20800
5,000,000	3	3	26000	26000
Naive Bayesian Model-based Missing Value Filling Algorithm. We have compared the dataset before and after transformation, noticing the algorithm has filled all blanks in the dirty data, proving that it can detect data quality problem and fix it in SparkDQ.

5.3. Performance of distributed data quality management algorithms
Mode detection algorithm. From Fig. 15(a), we can find that the proposed distributed algorithm is not efficient when the data size is relatively small, because Spark has its overhead in task scheduling and resource management when running on a cluster. However, as data size increases, the benefits brought by distributed computing outweigh the system overhead. The distributed algorithm gets around ten times faster than the serial version, and six times faster than the multithread version. Besides, stand-alone versions failed to execute in big data scenarios, whose throughput decreases to zero in the figure. However, the distributed version can still work well and fast.

Mutual Information Detection Algorithm. Fig. 15(b) illustrates the experiment results of different versions of mutual information detection algorithms. The multi-threading version is much faster than the serial version because mutual information detection is CPU-intensive. The proposed distributed version has a similar performance as the multithread version when the data size is relatively small. When it comes to massive data, frequent thread switching and memory operations slow down the multithread version. In contrast, our distributed solution provides outstanding performance, which is 6.5× faster than the serial version and is 3× faster than the multithread version when processing 10 million records.

IForest Model-based Outlier Detection Algorithm. The performance of three versions of the iForest outlier detection algorithm is shown in Fig. 15(c). The proposed algorithm shows the best performance due to handling the forest construction in a distributed way. As bottleneck arrives for the multithread version on massive data sets, the proposed distributed version shows an obvious advantage over the two stand-alone versions.

Priority-based Multi-CFD Management Algorithm. In this experiment, we evaluate the performance of two CFDs over five tuples, as is shown in Fig. 17. The performance results are shown in Fig. 16(a). Note that this algorithm starts to fail on a stand-alone machine when the number of records reaches only 1.6 million. This is because of the enormous intermediate overhead generated during grouping. The distributed version takes much time to transmit such data. Thus, its performance advantage is not huge. However, the distributed version still runs well for different sizes of data inputs.

Fig. 17
Download : Download high-res image (66KB)
Download : Download full-size image
Fig. 17. CFDs and tuples to be tested.

Semantic- and Block-based Entity Management Algorithm. In this experiment, we take complex entity extraction as an example to evaluate the performance of the three counterparts. We select the work-related attributes in the data set for comparison, including job type, occupation, and net income. Fig. 16(b) illustrates that the distributed version is robust to massive data input, and has a greater acceleration when the data size increases. The distributed execution lightens the burden of a single machine, making it less likely to fail with massive data input.

Naive Bayesian Model-based Missing Value Filling Algorithm. For the lack of job type attributes in the data set, we fill the missing values according to their occupation. The execution time of the three versions of the missing value filling algorithm is measured. Fig. 16(c) compares the throughput of two algorithms. Still, the proposed algorithm is robust to a huge number of input records because of its distributed policy.

5.4. Effectiveness of system-level optimizations
Multi-task Scheduling Optimization. To evaluate the effects of the scheduling strategy, we design three sets of tasks to test the performance. The three sets of tasks are function-based tasks, group tasks, and filter tasks. These tasks are performed in the form of Python applications.

For function-based tasks, we design three objective function-based tasks for data detection, including checking application type integrity, checking the range of working hours per week, and checking different approximate distinct ages. As for group tasks, we design three group tasks for dividing records by different values of capital gain and loss, distinctness, and mutual information between the two values. Using SparkDQ, it is easy to express them in a Python application. As for Filter tasks, we design three filter tasks, including removing records without job type attribute, and filtering records whose net income has a mode of ≤ and age is over 20.

The performance results of the three tests are shown in Fig. 18. Compared with the non-optimized version, the algorithm brings an improvement of 30% to 45% in execution time for objective function-based jobs (Fig. 18(a)), 20% to 39% in group-based jobs (Fig. 18(b)), and 9% to 25% in filter jobs (Fig. 18(c)). The optimization reduces a lot of repetitive data scans, leading to decreased execution time.

Fig. 18
Download : Download high-res image (431KB)
Download : Download full-size image
Fig. 18. Performance of system-level optimizations.

Data State Caching Optimization. The task in this experiment queries the three quartiles of age data attributes. The execution time is shown in Fig. 18(d). Without the data state caching optimization, the task needs to perform a scan on all the partitions for each quantile, and calculate the approximate quantile. By using the data state caching optimization, scanning is only needed once and for all. The data state is cached as an object named QuantilesSummary on the driver. The evaluation results show that the query overhead is much smaller when scanning the data set only once, as the execution time is only 46% to 56% to the original version.

5.5. Performance comparison with Apache Griffin
This subsection compares the performance of SparkDQ and the widely-used big data quality management system Apache Griffin [31] from two aspects: the single data quality detection task and the multiple data quality detection tasks. The single data quality detection task includes the integrity of attribute WorkClass, different values of attributes Race and NativeCountry, the max value for attribute CapitalLoss, or constraint HoursPerWeek >20. For multiple detection tasks, the systems need to handle all of the four problems above, and detect unique values for an attribute. To fairly compare Griffin with SparkDQ, we ran Griffin on Spark and uses the latest stable version, 0.5.0. Fig. 19 shows the experiment results.

Fig. 19
Download : Download high-res image (499KB)
Download : Download full-size image
Fig. 19. Performance comparison with Apache Griffin.

From Fig. 19, we can find that SparkDQ runs faster than Apache Griffin in integrity detection (Fig. 19(a)) and expression-based detection tasks (Fig. 19(d)). It is because that Griffin transforms detection tasks into SQL statements, running different counts serially. However, SparkDQ combined these computations, which makes it runs faster. For the different value detection task (Fig. 19(b)) and the maximum value detection task (Fig. 19(c)), SparkDQ runs the group by statement and the max function, which is similar to Apache Griffin. Therefore, these two systems have close execution times, not showing significant throughput differences. As to multiple tasks, SparkDQ also shows an obvious advantage due to the system-level optimizations (Fig. 19(e)).

In addition, Griffin does not support complex detection models such as outlier detection and CFD-based detection. It cannot repair detected data quality problems either. In contrast, SparkDQ supports complex models and data quality repair, with more features and better performance.

5.6. Scalability performance
Data scalability: The aforementioned evaluation already indicates that SparkDQ has near-linear data scalability. To consolidate that, we made another set of experiments, testing the execution time for six different tasks, including both short and long tasks. The results are shown in Fig. 20 to a log scale, directly revealing the near-linear data scalability of SparkDQ.

Fig. 20
Download : Download high-res image (172KB)
Download : Download full-size image
Fig. 20. Data scalability (to log scale).

Node scalability: We further evaluate the node scalability of SparkDQ. We have designed a job that goes through six data quality detection and repair tasks to evaluate the node scalability performance. These tasks are all about running through the proposed six algorithms: the mode detection algorithm, the mutual information detection algorithm, the iForest model-based outlier detection algorithm, the priority-based multi-CFD management algorithm, the semantic- and block-based entity detection and extraction algorithm, and the naive Bayesian model-based missing value filling algorithm. The job is run on clusters of different node sizes, from 2 to 8, with an input of 5,000,000 records. This complex job is a representative example that can be used for node scalability performance evaluation. We record their execution times, draw Fig. 21, and assert that SparkDQ achieves the near-linear node scalability.

Fig. 21
Download : Download high-res image (22KB)
Download : Download full-size image
Fig. 21. Node scalability.

6. Conclusion and future work
In today's big data management systems, data quality management becomes vital due to its importance in data mining. In response to big data quality management's urgent needs, in this paper, we propose SparkDQ, a generic big data quality management model and framework with a lot of built-in distributed data detection and repair algorithms and system-level optimizations. SparkDQ provides a series of flexible and convenient data quality detection and repair operators that allow users to customize data detection and repair logic for specific needs in terms of ease of use and flexibility. All the proposed data quality operators are run in a distributed way on Apache Spark. Users only need to write a small amount of Python code to use them. Moreover, SparkDQ supports various data sources and achieves near-linear scalability.

In the future, we plan to focus on the following two aspects. One aspect is to discover and integrate more distributed data quality management algorithms into SparkDQ. The other aspect is to study how to tune the parameter setting (like the degree of parallelism) for specific needs and data types automatically.