Abstract
The scalability of high-performance, parallel iterative applications is directly affected by how well they use the available computing resources. These applications are subject to load imbalance due to the nature and dynamics of their computations. It is common that high performance systems employ periodic load balancing to tackle this issue. Dynamic load balancing algorithms redistribute the application’s workload using heuristics to circumvent the NP-hard complexity of the problem However, scheduling heuristics must be fast to avoid hindering application performance when distributing the workload on large and distributed environments. In this work, we present a technique for low overhead, high quality scheduling decisions for parallel iterative applications. The technique relies on combined application workload information paired with distributed scheduling algorithms. An initial distributed step among scheduling agents group application tasks in packs of similar load to minimize messages among them. This information is used by our scheduling algorithm, PackStealLB, for its distributed-memory work stealing heuristic. Experimental results showed that PackStealLB is able to improve the performance of a molecular dynamics benchmark by up to 41%, outperforming other scheduling algorithms in most scenarios over almost one thousand cores.

Previous
Next 
Keywords
Distributed load balancing

Workload discretization

Work stealing

1. Introduction
Scientific and industrial applications commonly make use of High Performance Computing (HPC) resources to meet their needs related to size (e.g., to treat terabytes or petabytes of research data), performance (e.g., to understand the spread of an epidemic as soon as possible), or deadlines (e.g., to predict the weather before the day comes). Even though HPC applications may have different characteristics, they all have to be properly scheduled on the available HPC resources to achieve their objectives. As these platforms grow in scale, so does the risk of wasting their costly resources due to load imbalance [1], [2].

Load imbalance emerges when applications’ tasks are waiting for some other tasks to complete, making resources go idle. This problem can be caused, among others, by tasks with different workloads, tasks whose workloads dynamically evolve during execution, or by having computing resources take on more tasks to recover from a node failure. A natural solution to load imbalance is to periodically redistribute the tasks over the computing resources. This Periodic Load Balancing (LB) approach is seen in applications such as the Gordon Bell award-winning application NAMD [3], and it can be applied on applications based on parallel iterative methods.

The problem to decide an optimal mapping of tasks to computing resources is considered to be of NP-Hard complexity [4], [5]. Moreover, applications with periodic load balancing must employ scheduling heuristics to dynamically redistribute its workload. In this study, we showcase how commonly-used heuristics can fail to achieve good results when operating in larger scales (i.e., larger numbers of tasks or computing resources) due to issues in their scheduling decisions or due to their overhead (Section 6). Without scalable periodic load balancing solutions, the scalability of HPC applications will be compromised, leading to longer execution times and wasted computing resources.

In this work, we propose a scalable load balancing approach by combining distributed load balancing and the discretization of application workload. Our approach reduces scheduling overhead by applying the following design: (i) execute scheduling steps in a distributed fashion; (ii) perform workload discretization to simplify decisions; (iii) group local tasks to avoid numerous fine-grained migrations and (iv) minimize the messages between scheduling actors. The discretization technique, called packing, extends a previous technique [6] with notions related to -Nash Equilibrium. We also present PackStealLB, a new distributed load balancing algorithm that uses packing and inherits ideas related to constrained [7], [8] and randomized [9] Work Stealing (WS) heuristics. We carried out an experimental evaluation of PackStealLB with LeanMD, a molecular dynamics benchmark based on NAMD [10], on 20 compute nodes (960 cores) of the supercomputer Joliot-Curie SKL1. Our results show the benefits of PackStealLB in terms of total LB overhead, reduced number of task migrations, preservation of the original locality between tasks, and reduced total application execution times. Overall, our main contributions are:

1.
A method for workload discretization named packing that extends previous work [6];

2.
A novel distributed, WS-based periodic load balancing algorithm called PackStealLB;

3.
The implementation of PackStealLB in Charm++, a state-of-the-art runtime system that supports load balancing in distributed memory scenarios, and an experimental evaluation comparing it to other state-of-the-art algorithms [6], [11], [12].

The remainder of this work is divided as follows. Section 2 discusses related work in periodic LB and WS. Section 3 presents the distributed model considered in this work and its notations. Section 4 presents our packing model and load balancing algorithm. Section 5 discusses their implementation details in Charm++. Section 6 presents our performance evaluation and results. Finally, Section 7 concludes this paper.

2. Related work
State-of-the-art WS and LB techniques differ in scope when used in novel decentralized load balancing solutions. WS heuristics are regarded as distributed load balancing mechanisms for task-parallel applications. The simpler WS scheme is to have an executor to steal some workload from another when the former is idle. WS schedulers may also be classified as distributed receiver-initiated (or pull-based) load balancing schemes. In other words, underloaded processing elements request load from overloaded ones. LBs, on the other hand, are periodically invoked to remap the system workload. LBs exist on different flavors regarding topology (e.g. centralized or distributed) and are commonly implemented as sender-initiated (or push-based) schemes. Periodic LBs target a different application set, when compared to WSs, such as Bulk Synchronous Parallel (BSP) [13], iterative, and other data-parallel applications. This is especially efficient when using persistence-based techniques, which allows the scheduler to more accurately predict the load of the tasks it is moving.

Both LB and WS approaches have their advantages. Notably, WS is often applied in task-parallel Runtime Systems (RTSs) [14], and shared memory scenarios [15], [16] (even though it is distributed in nature), although it has been used for highly unpredictable applications in distributed memory as well [17]. LBs, on the other hand, have been widely applied to both shared and distributed memory scenarios, but due to their periodic nature, they are often applied to applications that follow the principle of persistence [11], [18] (i.e., applications with dynamic workload that tends to change slowly over time). Although they have been traditionally implemented as centralized schedulers, LB performance points towards distributed approaches [6], [12], and with limited strategies proposed in this field [19], WS emerges as a source of inspiration for novel LB policies. This is specially interesting considering the convergence properties of WS in large-scale scenarios [20]. In this section, we discuss recent work in periodic LBs and WS, providing a solid base for our novel approach.

2.1. Periodic load balancing
We divide LBs into two categories based on their behavior: global and diffusive [21]. The global approach uses centralized or hierarchical schedulers in order to balance load. This approach aggregates relevant system information, allowing precise understanding of the application state [22] with a potentially high overhead, especially in large scale overdecomposed applications. Hierarchical algorithms usually try to divide the underlying system in different locality levels, and schedule tasks on each level with a different strategy to reduce overheads [23].

Other relevant hierarchical algorithms are those based on graph and hypergraph partitioning. Zoltan [22] uses multi-level partitioning to parallelize and schedule work. This approach can also consider past work to reschedule tasks, reducing the decision time. Meanwhile, Scotch [24] and Metis [25], [26] use classical graph partitioning techniques such as Dual Recursive Bipartitioning and k-way coarsening to map tasks. In a similar fashion, Weighted-Hop and Max-Congestion [27] use topology information in order to enhance other classical graph partitioning algorithms when scheduling tasks.

The diffusive approach, on the other hand, follows the classic greedy algorithms principle: optimize locally to optimize globally. These strategies try to solve the imbalance issue from a much narrower scope by using local information only [28]. The DistributedLB (or Grapevine) [12] scheduler uses probabilistic transfer of load and high levels of parallelism to achieve a balanced state of the system, scaling much better than global load balancing strategies. Although these strategies have shown high scalability, they are still scarce in the state of the art [19].

The preservation of locality is a common secondary objective in scheduling as it affects the performance of large-scale applications [29]. In this sense, strategies that take system topology into account are a promising trend in global scheduling to preserve locality [30]. For instance, NuCo [31] is a load balancer that considers the different latencies at Non-Uniform Memory Access (NUMA) and network levels. Likewise, TreeMatch [32] does topology mapping by taking the different levels in machine topologies into consideration. Our approach at this moment differs from these works by trying to be platform-agnostic. In this case, locality is preserved by keeping tasks bundled together when migrating. This avoids adding explicit information about the machine topology to the distributed scheduling agents. Nevertheless, we see the addition of topology-awareness to our algorithms as future work.

On a different approach, PackDropLB [6] attempts to preserve the affinity among tasks by grouping them in resources prior to migration but completely disregards the network topology. This kind of workload grouping has been used in shared-memory scenarios as well with BinLPT in OpenMP loop scheduling [33], or in job scheduling with packing-based placement to reduce fragmentation on 3D-Torus HPC systems [34].

2.2. Work stealing schedulers
WS schedulers are inherently distributed. In WS, independent scheduling agents manage resources in a parallel system, and may take roles of: (i) thieves, which attempt to dynamically remap work to underloaded (or idle) resources, trying to manage workload so tasks are constantly available to be computed; or (ii) victims, which are targets chosen by thieves to have their tasks stolen. WS schedulers are commonly applied to dynamic and imbalanced applications [9], [35] that cannot afford a stable work decomposition, but may be applied to any parallel application decomposable as a Direct Acyclic Graph (DAG). This way, applications decomposed in models like fork/join [36], general task parallelism, and parallel loops in shared memory [37] have also benefited from WS.

Following the topology-aware approach, WS strategies have been able to greatly increase application performance. The Feudal Work Stealing [7] approach shares system information as tasks are stolen and attempts to select victims both in local and remote work groups, which increases task locality (e.g., a scheduler that manages a subgroup of cores may attempt to migrate work within its own subgroup of cores or from remote cores to it). CLAWS [8] is a contention- and locality-aware work stealing runtime for NUMA architectures, which takes care of task migration, reducing remote memory accesses. In a similar fashion, ADWS [38] uses localized hierarchical stealing to compensate imbalance in task-parallel applications.

On a different approach, DistWS [15] uses application task affinity instead of topology-awareness by selecting tasks that are more favorable for migration (i.e., have less data to be copied) on steal attempts in distributed shared-memory machines. Retentive Work Stealing [11] tries to apply the benefits of Persistence-based Load Balancing into a WS model in a distributed memory MPI environment. This approach uses a persistence model in iterative applications, in which, instead of rescheduling all of the workload every iteration, resources keep a list of processed tasks that is used as a seed for the next iteration, improving the balance as the application is executed, and performing work stealing when appropriate.

2.3. Discussion
As distributed schedulers rise as solutions in the periodic load balancing domain, the use of WS heuristics is attractive due to their well documented past and known convergence times [20]. However, distributed strategies must be adapted to achieve harmony with balancing load, locality, and quickly computing a new mapping. We aimed to achieve this with our packing scheme with PackDropLB, and we believe strategies such as Feudal WS [7] and Randomized WS have much to give in the periodic scheduling scenario (if correctly adapted). In the next sections, we present how we aim to achieve this harmony by first explaining our distributed scheduling model, and then explaining the packing model, PackDropLB, and the new algorithm PackStealLB.

3. Scheduling model and notation
A parallel application may be described as a set  of  tasks :. The load of a task  is denoted by . For the sake of simplicity, we extend the notion of load to sets of tasks. We also assume that the load of an empty set is equal to 0; both are shown in Eq. (1) where  represents an arbitrary set of tasks. (1)We also consider a set  of  identical machines : , and , resembling overdecomposed parallel applications. Each machine in this notation represents a core in the HPC platform. Additionally, for each machine , a subset of tasks  is assigned to .

Each machine  has a unique scheduling agent  that makes load balancing decisions. Its local view is composed of: (i) a set of tasks  assigned to it; and (ii) an indexed communication table of , containing all machine identifiers. The remaining information used in decision making has to be entirely derived from this local view. All communication between agents is lossless.

The objective of load balancing is to minimize application makespan. The best way to do it is to distribute load evenly across machines. Eq. (2) describes the load of a machine  and the lower bound of the makespan . (2)

Since optimal scheduling of parallel machines is NP-Hard, achieving a process mapping that yields  to every core is rather unrealistic. In this situation, we focus on assigning tasks to machines so that the makespan approximates . We give this value a relaxation , which should be based on the imbalance characteristics of each application. In other words, given a parameter , we want to find a schedule such that the load  of each machine  is less than the relaxed makespan lower bound. This objective is reflected in Eq. (3). (3)

Observe that if  is large enough (e.g., greater than the load of the largest task), a schedule can be greedily computed in a centralized fashion. In this work, we want to assign tasks to machines in a distributed fashion, so that the makespan approximates .

We use the game-theoretic idea of achieving an -Nash Equilibrium [39], a concept widely applied to distributed algorithms. Achieving -Nash Equilibrium means that no agent in the system (in this case, our schedulers) profits from taking actions that modify the state of the system: given any two machines , .

In this paper, we target parallel applications with non-uniform and non-preemptable tasks2 that leverage persistence-based load balancing algorithms (usually implemented in asynchronous runtime systems) to improve load balancing. The application is paused during rescheduling time, meaning that LB time is considered an overhead for the application. Thus, a load balancing strategy must run quickly to actually diminish application makespan. We also consider that each local scheduling agent executes the same LB algorithm from beginning to end with no interruptions. New messages execute in receiving order as their predecessors are processed. Every communication is asynchronous, so messages expect no answers.

4. PackStealLB: A work stealing-based load balancer
In this section, we present the details of our workload discretization method and load balancer PackStealLB. First, we discuss the characteristics and limitations of a previous algorithm (PackDropLB). We then present our method for workload discretization that can be applied to any distributed LB to improve scheduling time and to preserve some of the original task locality. Finally, we detail our new distributed WS-based LB that makes use of the proposed workload discretization (packing) model and overcomes the aforementioned limitations of previous approaches.

4.1. Limitations of previous efforts
PackDropLB is a distributed algorithm that migrates packs of tasks from overloaded to underloaded machines introduced in a previous work [6]. Overall, PackDropLB executes the following steps in each LB call. First, agents exchange messages to compute the average load of the system. Then, agents are divided into two groups based on the load of their corresponding machines. Overloaded agents create packs of tasks based on a fixed threshold. Underloaded agents, on the other hand, start a Gossip Protocol [41] to propagate machine load information. After a global barrier synchronization, overloaded agents send packs of tasks to randomly chosen underloaded targets, which in turn can reject packs if they become overloaded. Rejected packs can be sent again by overloaded agents to other random targets. This process is repeated up to a fixed number of times or until a stop criterion is reached.

PackDropLB was our first effort on improving the scalability of state-of-the-art LBs for iterative overdecomposed parallel applications. Although it has shown promising results compared to other global and diffusive LBs, it has the following limitations: (i) information propagation (Gossip Protocol [41]) and task migrations are carried out in two separate steps, which are in turn synchronized with a global barrier; (ii) several messages sent by overloaded agents containing packs of tasks may be discarded by underloaded target agents, causing the former to waste time to find new possible target agents; (iii) inferior performance than other diffusive LBs in some applications and/or platforms [6]. Overall, issues (i) and (ii) may have a significant impact on LB overhead and scalability, especially in large-scale platforms. The latter limitation is usually the outcome of issues (i) and (ii).

4.2. Application workload discretization (packing model)
Load balancing scenarios may be described as either discrete or continuous [42]. The discrete case describes uniform non-preemptable tasks, while the continuous case describes non-uniform preemptable tasks.

Our packing model aims to improve the scheduling process by approximating our load balancing scenario to the discrete scenario. The main objectives of packing are: (i) to reduce the scheduling time by making it simpler to decide if tasks will be migrated or not (as the groups of tasks have all the same approximate load), and by reducing the number of messages exchanged between agents (as multiple tasks are suggested for migration in the same message); (ii) to preserve some of the original locality of the application (assuming that the original mapping already grouped together communicating tasks in the same machine).

Recall that given a fixed parameter , we want to find a schedule such that the load  of each machine  is less than . Taking this into account, scheduling agent  takes a subset of tasks  in  so that the condition in Eq. (4) can be satisfied. (4)

This subset  corresponds to the set of tasks the agent  intends to migrate. Moreover,  is subdivided into disjoint sets of tasks called packs that have almost the same load  with . Since we want to make all migrating workload discrete, and our tasks are non-preemptable, each agent  must aggregate them into several packs so that each pack  respects Eq. (5) according to a fixed parameter  with . (5)

As our focus lies on the discretization of the overloading tasks (i.e., those that make a machine overloaded), we use a greedy approximation algorithm to solve the bin packing problem of assigning tasks to packs for further migration. We slightly adapt the bin packing problem to our context. Unlike bin packing, agent  also selects a subset of tasks that will insert into the packs. In our adaptation, we consider that the load of each pack cannot be greater than . The tasks are considered in an arbitrary order. If a task fits inside the currently considered pack  (of load ), then the task is placed inside it, and the set  is also updated. Otherwise, the current pack is closed: this means that we cannot insert another task. Observe that since  is greater than the largest task, .

The decision process to the creation of a new pack of tasks is as follows: if , then a new pack is opened and the current task is placed inside it. The process then repeats the previous steps. At the end of it, all packs respect the condition in Eq. (5) and . Observe that we also have . Indeed, it is enough to consider the last pack generated called :  and .

To sum it up, if agent  has a load (strictly) greater than , then it computes a set  of tasks and a set of packs so that the conditions in Eqs. (4), (5) are satisfied. These conditions also imply that the number of the packs generated by our algorithm in  is less than .

4.3. Load Balancing (LB)
PackStealLB is a new distributed LB that is motivated by feudal and randomized WS [7]. The feudal aspect comes in its information propagation, while the randomized aspect in its victim selection. As in PackDropLB [6], PackStealLB also migrates packs of tasks to reduce the scheduling time and to preserve task locality.

4.3.1. Description of the algorithm
PackStealLB progressively gathers the state of the system (the load of each machine ) at the same time it performs its decisions. This is possible due to the attachment of the sender local load information to the messages targeting its peers. Now, we describe how system information is broadcast during message exchanging.

Consider that when the algorithm begins, the agent  is not aware of the current load of the other machines. However, each time agent  sends a message, it also includes its load information  with the message. If agent  has already attained information on the load of any of its peers, this information may also be passed with every message it sends in a piggybacking fashion.


Download : Download high-res image (340KB)
Download : Download full-size image
PackStealLB is described in Algorithm 1, following a simple and standardized notation for distributed algorithms [43]. It is split into two main parts. The first part (INIT on lines 1–11) describes the initial calculations and role determination (victim or thief). The initial flow begins by issuing a reduction in order to assess the average system load (line 3). Then, it calculates the number of steals  for thief agent  using Eq. (6). (6)

Pack load  is fixed so that  and is defined for experimentation by Eq. (8) in Section 5.

After that, agents take roles of either victims or thieves depending of their load (stated as Victim case and Thief case): agent  is a victim if its load () is greater than . Agent  is a thief if its load is less than . Observe that thief agents have  and victims have .

After determining their role, victims will assemble their packs and send HINT messages in order to warn a potential thief. A potential thief is determined by choosing one machine in  with the lowest known load (line 7) uniformly at random. This choice is performed using only the local information of the victims (i.e., victim  does not know the load of the machines of  that have not yet communicated with it, and thus, victim  does not consider these machines).

When an agent  receives a HINT (lines 12–14), it stores relevant information about its peers (especially their loads). Additionally, if agent  is a victim, it will send a new HINT to its known most probable thief. This informs agents of each others’ states incrementally, which assists future stealing attempts.

Thief  will attempt  STEALs to target machines  (line 9), where  is a possible victim for . First, it sends a STEAL message containing the number of attempts  previously done ( at the beginning). At each time an agent receives this message,  is increased by 1. If the agent is not a victim, then the STEAL message is forwarded. The  information is used to determine if the victim selection will be constrained or randomized.

The standard choice is the constrained selection, which picks uniformly at random one of the  most loaded machines in , as determined in Eq. (7). Since each agent  can perform this selection using its local information, it cannot know the load of all machines. For sake of simplicity, it has no estimation of it, and we assume that we do not consider these machines or we can assume that their load is equal to . We denoted by  the set of the  most loaded machines among  for agent . Formally, (7) As shown in line 21, a configurable threshold value () is used to check if constrained selection is not working very well. Once  surpasses , agent  will then use randomized victim selection, simply choosing a possible machine index uniformly at random.

When receiving a STEAL message from machine , a victim  will send the load contained in the first element of their list of packs,  (lines 17–19). Additionally, agent  must register that the tasks in  will migrate to  (line 20), as the runtime system or the scheduler must perform these migrations. Meanwhile, if the receiving agent is not a victim, it will forward the steal to another possible victim. Lines 21 and 22 portray the two distinct aforementioned victim selection behaviors (randomized and informed selection, respectively).

Finally, once agent  receives a TASKS message, meaning agent  is a thief and it has received new tasks,  will add the received pack to its received tasks list (), which is used to update the current load of  (line 25).

Asynchronous distributed algorithms often need synchronization mechanisms to perform certain operations. Although PackStealLB does not require a barrier to separate different steps of the algorithm, it needs one to coordinate the end of execution and assess global information. The ending is performed via quiescence detection [44], meaning that when no agent has messages in its queue, they have been synchronized.

4.3.2. Convergence and complexity
Assume that at some step, a machine  is a victim: . By the definition of 
 
 corresponding to a mean, there exists a machine , having its load less than . Thus, machine  is a thief and it should be sending a STEALmessage or waiting for an answer to a STEALmessage. So, we can define the following property:

Property 1

When no more messages are sent, each machine  has load .

Now, we compute the complexity in terms of messages. The migration process boils down to three parts: (1) the initialization phase (2) the search for a victim initiated by a thief, and (3) the migration of tasks from the victim to the thief.

First, the initialization phase is mainly devoted to computing  using a reduction. The cost in complexity in terms of messages is  [45]. Moreover, we count the number of HINT messages. At the beginning, an agent  knows only its own load and can deduce if it is a victim. If this is the case, it sends a HINT message to a machine  adding its own knowledge (its load and its belief of the load of others) and so on, until the message reaches a thief. So, a victim  cannot receive a HINT message initiated by a victim  twice. Thus, a HINT message initiated by a victim  can be forwarded at the worst case  times. Since the number of victims is at most , we can deduce that

Property 2

During the entire process, there are at most  HINT messages.

Second, we count the total number of communications corresponding to task migrations. To perform this, we will focus on agent  having  at the beginning. It implies that agent  is a victim. So, agent  creates a set of packs so that the conditions in Eqs. (4), (5) are satisfied. During that, only tasks in  migrate. Thus during this process, its load is greater than  (from Eq. (4)). So, we have:

Property 3

Let  be an agent. If , then its load is greater than . Moreover, agent  does not become a thief during the algorithm.

To be precise, a victim  creates at most 
 
 packs at the beginning. Thus, there is at most  migration communication corresponding to ’s tasks. In total, an upper bound of migration communication is 
 

Since , we obtain the following property:

Property 4

During the entire process, there are at most 
 
 communications devoted to task migrations.

Observe that this bound is tight: we can consider the case where only one machine has all the tasks.

Third, we count the total number of communications corresponding to the search for a victim initiated by a thief. This number mainly depends on the knowledge of each machine.

At the beginning, a thief  knows only its own load. Thus, it randomly sends a STEALmessage to a machine  adding its own knowledge (its load and its belief of the load of others) and so on, until the message reaches a victim. When a thief  receives an answer to this message, the message contains the route of STEALmessages. Thus, thief  can deduce that all machines except the sender of the message are not victims, but thieves. Property 3 implies that these machines cannot become a victim. Thus, when thief  knows that agent  is not a victim, then thief  has no incentive to send a STEALmessage to machine . Moreover, when thief  sends the next STEALmessage, this message also contains the information about the fact that agent  is a thief. So, if this message is forwarded, then it is not forwarded to .

Property 5

During the entire process, given the fact that agent  is not a victim, then agent  receives only once the STEALmessage initiated by agent .

Since the number of not victim agents is upper bounded by , we have:

Property 6

During the entire process, there are at most  STEALmessages corresponding to a retransmission of a STEALmessage.

Observe that a migration of tasks is triggered by a thief. So the total number of messages corresponding to an initial STEALmessage is equal to the total number of messages devoted to task migrations. So we can conclude that:

Property 7

During the entire process, there are at most 
 
 messages.

5. Implementation
We implemented PackStealLB in Charm++ [46] using its distributed load balancing framework.3 ,4 Charm++ is one of the most receptive runtime systems for new LB strategies, especially in distributed memory systems [47], which allows us to pair it up with applications already existing in the environment. Charm++’s load balancing framework commonly follows a sequence of steps as it: (1) pauses the application, (2) organizes execution statistics for the schedulers, (3) computes a new schedule, (4) migrates tasks, and then (5) resumes application execution. This framework is applied to NAMD [3] with success, for instance. Nevertheless, periodic load balancing can also be done in parallel to the application’s execution in general.

In Charm++, the workload is decomposed in independent and migratable virtual processors named chares, usually following a geometric decomposition scheme. Charm++ is a message-driven, asynchronous RTS, meaning that work is issued when chares (our tasks) receive messages. Load balancers are also implemented as chares, meaning that they benefit from Charm++’s native synchronization mechanisms to perform the reduction operation and the quiescence detection (CkReduction and CkStartQD). A chare’s load usually represents its execution time since the last load balancing call (as measured by the runtime system), while a core’s load is the sum of its chares’ loads with any other background times spent on activities such as communication.

In Section 4, we used some variables whose values must be set beforehand. Ideally,  is the best value for packing load, since it will mitigate more of the algorithm complexity. However, the larger the packs, the larger is their potential gap between  and the ideal . So, smaller packs tend to be tighter, which leads to higher quality in load balancing.

We propose to calculate  as a fraction of , using a  factor, such that the maximum overall imbalance will be at most . Additionally, we use a  coarsening factor to make the pack size  smaller, as described in Eq. (8). (8)

Also detailed in Eq. (8) is the  relaxation factor, which is used to define pack size in Section 4.2, Eq. (5).

The imbalance tolerance  value was determined as 0.05, meaning that we only consider that a given core is balanced when its load is in an interval of 5% to , which is plausible in this scenario and is used by other schedulers in Charm++. Meanwhile, the pack narrowing factor  is fixed to 0.4 as a middle ground between optimizing the balance and accelerating the algorithm.  and  are defined in Eq. (9). (9)

The seeded neighborhood of PackStealLB (initial known  in Algorithm 1) of a given scheduling agent  was predetermined as being its right-hand neighbor, which is given by  for . Charm++ attribute indices to each computational resource. In a nutshell, every core in a processing node is assigned an integer in ascending order. This process is repeated for every processing node so that every core has an unique id. Indeed, in homogeneous clusters, it is possible to use the indices to determine if a given core is local or remote. For instance, if each cluster node has  cores, the seed may be  for 
 
, which is an in-node neighbor of . This way, we would first attempt local neighbors in  during the constrained WS phase, and global machines in the randomized one.

Finally, we set the victim selection strategy threshold to
 
. The value is based on an empirical study carried out on the platform used to assess the performance of PackStealLB (Section 6).

6. Performance evaluation
We performed an experimental evaluation of our workload discretization model and PackStealLB to evaluate the scalability of the algorithms when handling a molecular dynamics benchmark on a supercomputer [48]. This evaluation includes a comparison of PackStealLB to LBs from the state of the art (listed in Table 1), and with a Baseline execution using the Charm++’s DummyLB, which captures statistics but performs no actual load balancing. In this section, we first provide an overview of the metrics, statistical methods, experimental environment, and details of the molecular dynamics benchmark. This is followed by an analysis of the results obtained in our experiments.


Table 1. Brief description of other LBs used in the experiments.

LB	Type	Short description
GreedyLB	Global	Optimizes load distribution, not communication. Assigns tasks to cores using a Longest Processing Time (LPT) first policy [49].
RefineLB	Global	Attempts to minimize the number of migrations. Migrates tasks from overloaded to underloaded resources only [11].
DistributedLB	Diffusive	Push-based strategy that uses probabilistic transfer of load to choose task receivers [12]. Gathers system information with a gossip protocol.
PackDropLB	Diffusive	Push-based strategy that migrates packs of tasks from overloaded to underloaded machines [6]. Gathers system information with a gossip protocol.
6.1. Experimental methodology and environment
We first present the metrics and statistical methods used to compare PackStealLB with other LBs. Then, we describe our experimental environment. Finally, we give a brief overview of the molecular dynamics benchmark and we discuss our experimental design. The raw result files and scripts for their analysis are available in [48].

6.1.1. Metrics and statistical methods
Our methodology involves the evaluation of three factors:

1.
Total execution time (makespan): how long an application takes to execute;

2.
Load balancing invocation time: the time between invoking the LB and resuming the application after migrations;

3.
Useful application time: total execution time, excluding the load balancing invocation times.

Minimizing the application total execution time is the most important objective, which is the factor that enables the execution of high-scale scientific applications. Nonetheless, a low LB overhead (coming from the load balancing time and migrations) is important as it allows LBs to scale with applications as systems grow larger.

All comparisons of these metrics are based on a confidence interval threshold of 95% (significance of 5%) for the different statistical methods used. We start our comparisons by organizing the samples (e.g., total execution times for different LB and problem sizes), and then checking if they follow normal distributions (Kolmogorov–Smirnov test). If we do not reject the null hypothesis in any tests (i.e., all p-values ), then we use parametric methods for our comparisons (Welch Two Sample t-test). Otherwise, we move to nonparametric methods (Mann–Whitney U test, or Wilcoxon signed-rank test for dependent samples). For all these methods, a -value  means that we reject the null hypothesis that the compared versions perform the same, meaning that they perform differently.

6.1.2. Experimental environment
We carried out the experiments on the Joliot-Curie SKL supercomputer.5 It contains NUMA compute nodes interconnected with EDR Infiniband. Each node features two 24-core Intel Xeon 8168@2.7 GHz CPUs and 192 GB ECC RAM DDR4 memory@2666MHz. In our experiments, we employed 20 compute nodes for a total of 960 cores.

Joliot-Curie SKL runs on Red Hat Enterprise Linux 7.6, loading OpenMPI 2.0.4, and C/C++ Intel 17.0.6.256 modules. Charm++ version 6.9.0 was installed in the machine using the build target mpi-linux-x86_64 and option --with-production. Both Charm++ and the molecular dynamics benchmark were compiled with the -O3 flag.

6.1.3. Molecular dynamics benchmark and experimental design
We selected the molecular dynamics benchmark LeanMD for our experiments. LeanMD is based on (and performs core computations of) the Gordon Bell award-winning application NAMD [10], providing a realistic scenario to measure the impact of novel LBs.

The parallel implementation of LeanMD uses a 3D spatial decomposition approach, where the 3D space consisting of atoms is divided into cells. Our experiment used the standard LeanMD configurations available online,6 parameterized with  cells of dimensions 15 × 15 × 30. Cells are further divided into computes, which are the actual chares. These contain multiple particles, and manage communication among them.

The parameter  was varied from sizes  to . Each execution of LeanMD comprises  iterations, executing the first load balancing call at the 40th iteration and every  iterations after that, summing up a total of  LB calls. These parameter combinations generate simulations ranging from 1.15 to 3.08 millions of atoms.

Our experimental evaluation was carried out with  repetitions for each parameter combination (input size and load balancing algorithm). More specifically, as Joliot-Curie SKL is a supercomputer with a job scheduler and multiple users at the same time, we organized our experiments in four jobs (two for  and two for ). Each job contains  repetitions for all LBs and input sizes involved. For each repetition, all pairs of input size and LB were executed in a random order with the objective of avoiding having noise from other users affecting a single LB.

The objective of executing LeanMD with these different input sizes is to measure the capability of LBs in dealing with varying problem sizes in a large scale. This benchmark creates  particles per cell following Eq. (10). (10)
 
where  ranges from  to . This way, as we scale any of the dimension parameters we allow the particles to be more spread in the simulation area. This also leads to higher imbalance in the application as the input size increases.

6.2. Result analysis and discussion
The total execution times of LeanMD with different input sizes and load balancers are illustrated as boxplots7 in Fig. 1. Only the Baseline, PackDropLB and PackStealLB were executed for input sizes 240 and 320 due to the increasing time it takes to run LeanMD for larger sizes, and to the performance results seen for the other LBs with smaller input sizes.

At a first glance, the results portrayed in Fig. 1 show that PackStealLB is the only LB to consistently reduce the total execution time of LeanMD compared to the baseline. PackDropLB is a close second, failing only to outperform the baseline for the scenario with . RefineLB and DistributedLB achieved performances close to the baseline or worse than it, and GreedyLB always increased the total execution time of LeanMD, proving itself to be a bad fit for this situation.


Download : Download high-res image (371KB)
Download : Download full-size image
Fig. 1. Boxplots of the total execution times of LeanMD with different input sizes and load balancers on Joliot-Curie SKL. Sizes indicate a variation of the application dimension parameter . Each figure has a vertical axis starting at its own value to emphasize performance differences.


Download : Download high-res image (176KB)
Download : Download full-size image
Fig. 2. LeanMD speedups with different load balancers.

In order to verify if the performance of the algorithms were statistically different, we applied Welch’s t-test to some pairs of samples. This test was chosen because all samples followed normal distributions. The comparisons of PackStealLB or PackDropLB to the baseline all resulted in p-values , so we can conclude that their execution times are actually different from the baseline as first suspected. Using the same test, we also verify that PackStealLB outperforms PackDropLB for , it is outperformed by PackDropLB for , and that no difference can be seen between them for  (-value ). Finally, there is no statistical difference between the performances measured for RefineLB or DistributedLB to the baseline for  (p-values  and 0.402, respectively), and for RefineLB to the baseline for  (-value ).

Fig. 1 also indicates that both PackStealLB and PackDropLB have a greater impact to LeanMD’s total execution time as its input size increases. We can observe the speedup achieved over the baseline with load balancing for the different input sizes in Fig. 2. PackStealLB achieved speedups of 1.09, 1.38, 1.38, 1.47, and 1.41 for increasing input sizes. Starting on , PackStealLB and PackDropLB lead to significantly better results than competing LBs, which emphasize their scalability and capability to handle load imbalance.


Download : Download high-res image (324KB)
Download : Download full-size image
Fig. 3. Median load balancing invocation times for the first, second, and third LB calls during application execution. Data is displayed in  scale.

As mentioned earlier, the total execution time of LeanMD can be decomposed into two parts: (1) the load balancing invocation times, which act as an overhead during the execution of the application; and (2) the useful application time, which represents the actual time computing the molecular dynamics solution. We first focus on the load balancing invocation times in Fig. 3. The figures display the median load balancing invocation time for each of the three calls by LB and problem size. These times are shown in log scale due to their differences of multiple orders of magnitude. We chose the median for these samples because some of them did not come from normal distributions (p-values ).

The results displayed in Fig. 3 highlight multiple aspects of the LBs. Some of these aspects are as follows:

1.
The first load balancing invocation usually takes longer than the other ones for all scenarios. The differences between the first and second invocation are significant for most scenarios (Wilcoxon signed rank tests with p-values ) with the exceptions of the baseline for  (-value ), PackDropLB for  (-value ), and PackStealLB for  and  (-value  and 0.191, respectively). As this behavior is seen even for the baseline execution, we can conclude that these times are affected by the behavior of the application or even by Charm++’s LB framework. In this situation, comparisons between LBs must be limited only to invocation times of the same type (first, second, or third).

2.
We can observe the high overhead of the centralized synchronization and data organization for the baseline execution. As it invokes Charm++’s DummyLB, it takes from 0.35 s up to 7.15 s for one LB call.

3.
We can notice that GreedyLB and RefineLB take longer than the baseline, but RefineLB takes the longest. RefineLB’s longer times come from its more demanding decisions and not from a larger number of migrations. For instance, RefineLB migrates only a few hundred tasks on each LB invocation, while GreedyLB migrates almost all tasks (hundreds of thousands) every time.

4.
The efficiency of diffusive algorithms is evident when compared to centralized approaches for this kind of large-scale application and platform. The differences span from two to three orders of magnitude.

5.
We can see the difference between packing-based algorithms (such as PackStealLB) and DistributedLB. For instance, Fig. 3a shows that PackStealLB takes from 4.27 to  on its LB calls for problem sizes from  to , while DistributedLB takes from 19.43 to . Additionally, PackStealLB’s invocation times are statistically different from the ones of PackDropLB. The only exceptions happen for the second and third LB calls for  (Mann–Whitney U tests with p-values ).

When the LB invocation times are subtracted from the total execution times, what remains are the useful application times. The useful application times for the different tested scenarios are presented in Table 2. Each line represents one LB, and each column shows the average useful times for a given problem size. The average time was chosen here because all samples follow normal distributions.

The results in Table 2 extend the insights from the total execution times in Fig. 1 in a few ways. For instance, besides PackStealLB, RefineLB is also able to improve LeanMD’s performance for all tested problem sizes. This highlights the importance of keeping a low overhead when using periodic LB algorithms, as all gains from a better task distribution are erased by the LB invocation times of RefineLB. Still, PackStealLB achieves useful application times that are clearly better for , which emphasizes the quality of its scheduling decisions. On the opposite sense, we see that DistributedLB achieves worse times than the baseline for . This shows that just having a diffusive algorithm does not lead to performance improvements automatically. Finally, we see that GreedyLB always increases the useful application times, even though one would expect the application to become well-balanced with it. Given that the main difference between GreedyLB and RefineLB is how much of the original mapping they preserve, reasoning follows that the main performance issue being generated by GreedyLB comes from its disregard to the original task locality. Conversely, the preservation of locality is an important characteristic of our packing scheme, which helps explain how PackStealLB and PackDropLB outperform the other algorithms.

In conclusion, these experimental results show how PackStealLB and our packing scheme achieve their objectives of improving the scheduling process, as we see that: (i) they reduces the total and useful execution times, and scale to large platforms and input sizes; (ii) they achieve smaller load balancing times than other diffusive algorithms; and (iii) they also preserve some of the original locality of the application with their small number of groups of tasks migrating together.


Table 2. Average useful application times in seconds for different LBs and problem sizes.

LB	Problem sizes
80	120	160	240	320
Baseline	29.382	65.287	102.999	201.139	325.936
GreedyLB	36.865	77.158	133.254	–	–
RefineLB	28.688	56.395	89.500	–	–
DistributedLB	34.014	69.520	120.045	–	–
PackDropLB	32.769	49.035	77.174	160.049	266.891
PackStealLB	28.535	51.097	78.936	146.021	238.611
7. Conclusion
In this paper we have developed the idea of workload discretization (packing) for periodic LB, and presented PackStealLB, a new diffusive scheduler that employs this technique. PackStealLB is a pull-based LB that employs WS heuristics such as constrained and randomized victim selection [7], [15].

We implemented PackStealLB in the Charm++ RTS and ran experiments with a molecular dynamics benchmark (LeanMD). We have compared PackStealLB with our previous push-based LB (PackDropLB) as well as with multiple LBs available in Charm++ [12] in the Joliot-Curie SKL supercomputer.

The results of our experiments have shown PackStealLB as the most effective LB among the tested algorithms. PackStealLB achieved speedups of up to 1.49 over the total execution time of the baseline (a dummy LB that only collects statistics of the application), and of up to 1.39 when considering only the useful application time (execution time without any LB overhead). It also achieved a speedup of up 1.12 over the second best algorithm in the experiments, PackDropLB.

The success of PackStealLB comes from a combination of factors. PackStealLB was able to balance the application’s load without disturbing much of its original locality thanks to its packing scheme, whereas other algorithms (GreedyLB and DistributedLB) increased the useful application time due to their disregard for locality. Additionally, its low overhead (a few milliseconds for every LB invocation) enables the application to benefit from the improved task distribution over its whole execution (tens or hundreds of seconds). This is in sharp contrast to what was seen for RefineLB whose LB invocation times hid any benefits coming from its scheduling decisions. Finally, when compared to the also packing-based PackDropLB, PackStealLB’s WS-based heuristics were shown to perform similarly or better for large problem sizes. This emphasizes that PackStealLB’s gains were not only due to its diffusive nature or workload discretization, but also from its own scheduling heuristics.

Our results lead us to believe that the development of new and effective distributed load balancers is indeed crucial for future applications and parallel systems. In this scenario, we believe that studying established distributed scheduling algorithms is a path that has still to be explored to achieve exascale grade scheduling. Developing distributed schedulers targeting HPC applications based on concepts such as Deterministic Load Balancing [50] and Selfish Load Balancing [39] is part of the work we have in mind moving forward. Additionally, we intend to develop new packing strategies that leverage the task communication pattern and the network topology in order to place tasks that communicate more often (or exchange more data) in the same pack whenever possible. Finally, we plan to evaluate PackStealLB with a broad range of workloads and real-world applications to determine its best usage scenarios.