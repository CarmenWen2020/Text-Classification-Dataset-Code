Abstract
In light of the emergence of Database-as-a-Service paradigm, secure data outsourcing has become one of the crucial challenges which strongly imposes itself. In such a scenario, access control is considered as a major challenge. In fact, access control policies of the data owner must be preserved when data is moved to the cloud. Here, we address this problem by considering inference leakage that could be produced by exploiting functional dependencies. The proposed approach is based on vertical partitioning to produce a set of secure sub-schemas stored in separated partitions in the distributed system. Then, we extend this approach by presenting a secure query processing model to preserve access control policies when querying data from distributed partitions. The effectiveness of our algorithms is confirmed through observations from a variety of conducted experiments.

Previous
Next 
Keywords
Access control

Inference control

Data dependencies

Distributed databases

Security and privacy

1. Introduction
In recent years, with the increasing volume and variety of data collected from diverse sources, costs of in-house data storage and processing have become highly significant. Hence, data owners place their data among Cloud Storage Service Providers (CSSP) in order to increase flexibility, optimize storage and sharing, enhance data manipulation and decrease processing time. In spite of that, access control is considered as a major barrier to cloud computing and data outsourcing arrangements [32]. Indeed, access control policies of the data owner must be preserved when data is moved to the cloud. If an access was prohibited initially by the owner's access control policies, it should be also prohibited when data is hosted in the CSSP. Several research studies have investigated this problem while maintaining a balance with data utility [2], [10], [11], [14], [15], [21]. In fact, researchers have attempted to enforce access control policies of the data owners at the CSSP level by keeping some sensitive data separated from each other. Moreover, these works have pointed out that data utility was maintained through the exploitation of the query workload.1 However, these research efforts have not treated the case of indirect access that could occur bypassing access control policies. Indirect accesses via inference channels occur when a malicious user combines the legitimate response received from the system with semantic constraints (e.g. data dependencies). In addition, the authors have highlighted that the query workload is required to maintain data utility. However, due to errors in physical parameter estimations, the query workload could be not available or useless. Also, previous works have not focused on establishing an efficient distributed query evaluation strategy to secure the data residing in the CSSP through a big data-based system. Thence, in this research, we are dealing with three main challenges as follows: How database roles can be exploited to reduce the amount of servers that a user query has to span? How to capture inference leakage and how to control them? How to evaluate distributed queries derived from the aggregation of partial results from distributed partitions while retaining access control policies?

In this paper, we propose a graph-based approach to efficiently preserve owner's access control policies when data is externalized to the cloud. Firstly, the proposed approach relies on semantic relatedness measure between users roles and schema attributes to derive an optimal vertical partitioning. Optimal partitioning is the minimization of the number of distributed queries (minimizing the average query span, i.e., the average number of servers involved in the execution of a query [28]). Minimizing distributed queries will provide higher performance and speedup since the distributed servers have the possibility to process user query with less communication between them in the CSSP [18]. It is important to note that in this case, we suppose that every sub-schema assigned a partition where each one is hosted in one server. Moreover, we enhance a graph-based approach proposed in [34] to detect inference channels that could be deduced using functional dependencies. Then, on the basis of the hypergraph theory, we propose a hypergraph partitioning algorithm to compute the set of secure partitions stored in CSSP level. The proposed partitioning algorithm takes advantage from the distributed system to enforce confidentiality constraints related to access and inference controls by keeping some attributes separated from each other while preserving visibility constraints related to optimal distribution. Then, We propose a monitoring module to monitor at run time distributed queries and block suspicious ones.

The remainder of this paper is organized as follows: Section 2 highlights research efforts related to our work. Preliminaries and motivating scenarios are given in section 3. The proposed methodology is described in section 4. Section 5 examines the experiments. Section 6 is dedicated to the conclusion and future works.

2. Related works
In recent years, a growing body of literature has investigated data outsourcing related to access control mechanisms, two techniques have been proposed to enforce access control rules in CSSP level: The first one have attempted to enforce access control with non-communicating servers [2], while the second one have studied the impact of communication between servers on query execution and how this communication could lead to security breaches [7], [8], [19], [20]. In the following, we will investigate those works and their relation with the inference problem, we will also provide a survey on data outsourcing with optimality constraints.

2.1. Data outsourcing in presence of access control policies
2.1.1. Secure data outsourcing with non-communicating servers
The study in [2] was the first one that attempted to enforce access control in database outsourcing using vertical fragmentation. Under the assumption that servers do not communicate, the research aimed to split the database into two untrusted servers while preserving data privacy with some of the attributes possibly encrypted. The work of [24] have introduced an approach to enforce confidentiality and privacy while outsourcing data to CSSP. The proposed technique was based on vertical fragmentation and applied only minimal encryption to preserve data exposure to malicious party. However, the fragmentation algorithm enforced the database logic schema to be in third normal form to produce a good fragmentation design, also the query execution's cost was not proved to be minimal. In [17], researchers have treated the concept of secure data publishing in presence of confidentiality and visibility constraints. By modeling these two latter as Boolean formulas and fragment as complete truth assignment, authors have used Ordered Binary Decision Diagrams (OBDD) technique to check whether a fragmentation satisfies confidentiality and visibility constraints. Authors of [37] have studied the problem of finding secure fragmentation with minimum cost for query support. Firstly, they have defined the cost of a fragmentation F as the sum of the cost of each query Qi executed on F multiplied by the execution frequency of Qi. Secondly, they have resorted to heuristic local search graph-based approach to obtain near optimal fragmentation. Researchers in [10] have proposed a technique separating duties by a vertical fragmentation to address the problem of confidentiality preserving when outsourcing data to CSSP. The separation problem has been treated as an optimization one to maximize the utility of the fragmented database and to enhance the query execution over the distributed servers. The NP-Hardness proof of the separation of duties problem discussed in [10] was proved in [11] by the combination of the two famous NP-Hard problems: Bin packing and Vertex coloring.

2.1.2. Secure data outsourcing: the case of communicating servers
A few works have investigated the problem of data outsourcing with communicating servers [7], [8], [19], [20]. These works have implemented secure query evaluation strategies to retain access control policy when servers communicate with each other. Authors in [8] have built on [7] to propose an approach that securely outsources data based on fragmentation and encryption. It has also enforced access control when querying data by resorting to query privacy technique. The proposed approach has assumed that the distributed servers could collude to break data confidentiality so that the connection between servers was based on primary-key/foreign-key. In addition, the query evaluation model based on private information retrieval ensure data unlinkability performed by malicious user using semi-join query. Even though, the proposed technique has enforced database schema to be normalized, and it has generated a huge number of confidentiality constraints due to the transformation of inter-table constraints to singleton and association constraints which could affect the quality of the fragmentation algorithm. Join query integrity check has been tackled through the work in [19] where researchers were inspired from [20] to propose a new technique in order to verify the integrity of join queries computed by potentially untrusted cloud providers. The authors aimed also to prevent servers to learn from answered queries which could lead to breach users privacy. To do so, they have shown first how markers, twins, salts and buckets can be adapted to preserve integrity when a join query is executed as a semi-join, then, they have introduced two strategies to minimize the size of the verification: limit the adoption of buckets and salts to twins and markers only and represent twins and markers through slim tuples.

2.2. The inference problem
Traditional Access control models protect sensitive data from direct disclosure via direct accesses. However, they fail to prevent indirect accesses [22]. An indirect access is produced by malicious user through an inference channel. This inference channel is derived from the combination of legitimate answers received from the system with metadata (e.g. system's semantics, statistical information, exceptions, error messages, user's defined functions and data dependencies).

In [21], vertical database fragmentation technique was considered by authors to ensure data confidentiality in presence of data dependencies among attributes. Those dependencies allow unauthorized users to deduce information about sensitive attributes. To tackle these issues, the authors have reformulated the problem graphically using a hypergraph representation and then have computed the closure of a fragmentation by deducing all information derived from its fragments via dependencies to identify indirect access. Nevertheless, this approach have explored the problem only in single relational database. Also, the inference problem was tackled only through data dependencies. In our approach, we will consider multi relational database and we will deal with inference leakages caused by functional dependencies. To the best of our knowledge, our work is the first one that treats inference channels caused by functional dependencies in a data outsourcing scenario. Although data outsourcing was not mentioned in [34], the authors were inspired from [33] to control inference problems caused by functional dependencies and meaningful joins proactively by decomposing the relational logical schema into a set of views (to be queried by the user) where inference channels cannot appear. In this work, functional dependencies are represented as a graph in which vertices are attributes and edges are functional dependencies. Inference channel is then defined as a process of searching a sub-tree in the graph containing the attributes that need to be related. Compared to approach [33], in this one, the cut of the inference channel is relaxed by cutting the chains only at a single point, consequently minimizing dependencies loss. Nevertheless, like the technique in [33], it leads to semantic loss and need query rewriting techniques to query decomposed views.

2.3. Optimized schema partitioning
Some researchers have devoted their research efforts to address the problem of vertical database schema partitioning by considering optimality constraints. These constraints minimize the total resource consumption in the CSSP. In [18], the authors have elaborated an experimental study to prove the relation between resource consumption and the queries distributed over multiple servers. In reality, minimizing the number of servers invoked in the users queries enhances throughput and decreases latency which consequently will decrease resource consumption in CSSP level. Literature works dealing with this field of study could be classified in two categories:

•
Workload-aware approaches [14], [18], [28], [37]: These approaches have focused on the workload (information regarding queries, their frequencies, involved attributes, arrival patterns, and so on) to derive an optimal schema distribution in the CSSP level. For example, in [18], [28], authors have relied on a hypergraph representation of the database where the tuples are represented by vertices and transactions (queries) are hyperedges that relate them. Hence, the partitioning algorithm have minimized the number of cross-partition transaction by minimizing cuts of hyperedges in the hypergraph. However, these approaches are not always applicable since the query workload is not available in all scenarios or it is useless due to errors in physical parameters' estimations. For this reason, some workload-independent techniques have been proposed.

•
Workload-independent approaches [9], [35], [36]: These approaches have attempted to exploit database semantics without queries in order to derive optimal vertical schema partitioning. Nonetheless, these approaches are not in line with our methodology for the following reasons: The work in [9] has attempted to exploit the logical database schema by extracting the functional dependencies from database tables and use them to perform partitioning. This could produce as discussed in [25], [26], [27] inference leakage by malicious users and then violate access control policies. Moreover, the work in [36] has used an ontology-driven partitioning method to horizontally partition the data loaded in the relational schema according to their semantic similarity. Nevertheless, the adopted Ontology in this work has heavily relied on structure designed primarily for is-a relation and it couldn't detects similarity between concepts connected by other kinds of relations. Authors in [35] have proposed a clustering based fragmentation and use replication technique to derive semantic fragmentation of data in the database for flexible query answering. They have used medical taxonomy to measure semantic similarity between medical expressions. However, this methodology, same as the previously discussed approaches, requires data to be loaded in the database schema which is not the case in our situation.

The most relevant related works are classified in Table 1. The classification process is based on the following criteria: Confidentiality constraints support (these confidentiality constraints enforce access control rules), visibility constraints support, inference control support and query evaluation strategy support.


Table 1. A comparison between existing works and the proposed model.

Confidentiality constraints	Inference control	Visibility constraints	Query evaluation
[8]	yes	no	yes	yes
[11]	yes	no	yes	no
[37]	yes	no	yes	yes
[6]	yes	yes	no	no
[17]	yes	no	yes	yes
[13]	yes	no	yes	yes
[14]	yes	no	yes	yes
[20]	yes	no	no	yes
[21]	yes	yes	no	no
[33], [34]	yes	yes	yes	no
[10]	yes	no	yes	yes
[2]	yes	no	no	yes
[24]	yes	no	no	no

Required model	yes	yes	yes	yes
The literature review provided above, highlights research efforts devoted to ensure data confidentiality when this later is externalized to the cloud. As we can deduce from Table 1, none of the proposed approaches investigated the problem considering simultaneously the following criteria: Access control support, inference leakage through semantic constraints, reducing distributed transactions through visibility constraints and maintaining a secure query evaluation strategy. In fact, there is no significant contribution on data outsourcing guaranteeing the control of inference leakage caused by functional dependencies. Also, none of the cited works established a secure query evaluation strategy on a big data-based system. Through this research paper, we will address these limits. In reality, our contribution is seen through the adoption of vertical partitioning taking into consideration the most extreme situation faced by the database designer. Indeed, we suppose that neither the query workload is available, nor the database schema is loaded with data. Only the database schema with its semantic constraints and users privileges are available to the database designer. This further confirms to us the novelty of the research we propose.

3. Preliminaries and motivating scenario
Prior to detailing our approach, we would like to introduce basic definitions in relation with our research.

Definition 1

Confidentiality constraint [11]
Let R(A) be a relation schema over the set of attributes A. A confidentiality constraint on  is defined by a subset of attributes  stating that a partition is not allowed to store the combination of attributes contained in c. However, any proper subset of c may be revealed. Without loss of generality we consider confidentiality constraints with cardinality greater or equal to 2 ().

Definition 2

Visibility constraint [17]
Let  be a relation schema over the set of attributes A. A visibility constraint is a monotonic Boolean formula over attributes in . A Visibility constraint  states that attributes  must be jointly visible in the same partition.

Definition 3

Functional dependency [1]
A functional dependency (FD) over a schema R is a statement of the form:  where I, J two sets of attributes ⊆ schema R stating that each I value in R is associated with precisely one J value in R. We refer to I as the left hand side (LHS) and J as the right hand side (RHS) of the functional dependency .

Definition 4

Meaningful join [34]
Meaningful join is briefly an equi-join operation between a foreign and primary keys. The inference problem is defined on inhibiting all possible meaningful joins, among the attributes of confidentiality constraints.

Definition 5

Violating transaction [23]
A violating transaction  is a set of select queries such that if they are executed and their results combined, they will lead to disclosure of sensitive information and thus violating the confidentiality constraint.

Definition 6

Hypergraph [12]
It is a generalization of a graph in which an edge can connect any number of vertices. Formally, a hypergraph H is a pair  where X is a set of elements called nodes or vertices, and E is a set of non-empty subsets of X called hyperedges.

To illustrate the ideas behind our proposed methodology, a hospital database schema is considered as shown in Fig. 1. The schema is designed to store patient's medical records about addiction cure. Hence, it is clear that exposing such sensitive information to untrusted parties will bring security breaches.
Fig. 1
Download : Download high-res image (48KB)
Download : Download full-size image
Fig. 1. Hospital-db-schema.

We consider the roles: : doctor and : chemical-drug-agent

Relation: Patients(Patient_id, Diagnosis_id, Admission_date, Patient_details)

Functional dependencies: : ; : ; : ; : ; : .

Relation: Drugs(Drug_id, Drug_code, Drug_name, Drug_costs)

Functional dependencies: : ; : ; : .

Confidentiality constraint:  = {Drug_code, Patient_details}

We would like to mention that unlike previous works which impose that the attributes belonging to the same confidentiality constraint must be in the same relation, we authorize in our work the use of inter-relation confidentiality constraints like constraint .

Relation: Consumption(Patient_id, Drug_id)

Functional dependencies: : ; : 

It is clear that the association of sensitive attributes is the main cause of exposing data in cloud to unauthorized disclosure. According to , neither the doctor nor the chemical-drug-agent is allowed to see at the same time the visibility of attributes Drug_code and Patient_details. This confidentiality constraint is specified according to open policy where the access control rules specify all requests that are to be denied. Under such a policy, any request that is not denied by the access control rule is allowed by default. Let's see now how functional dependencies could be used by a malicious user to produce an inference channel and violate the confidentiality constraint . Let us assume that the following selection queries is issued by a doctor: ,  and . Combining the results of these queries and using functional dependencies {, , , }, the doctor can derive meaningful joins to obtain Patient_details and Drug_code simultaneously which induces the violation of the confidentiality constraint . These observations encourage the idea of partitioning vertically the relations into fragments in CSSP level to preserve confidentiality constraints. Also, they break inference channels while maintaining some attributes visible together to ensure data utility. Next, we propose an approach to solve this problem.

4. Secure data outsourcing: a graph-based approach
4.1. Overview of the proposed methodology
Our approach relies on the relational model, it aims, as shown in Fig. 2, to produce a set of secure sub-schemas, each sub-schema is represented by a partition  and each partition is stored exactly in one server in the CSSP. In addition, it introduces a secure distributed query evaluation strategy to efficiently request data from distributed partitions while retaining access control policies. To do that, our methodology takes as input a relational schema R to which is attached a set of confidentiality constraints C, a set of functional dependencies FD and a set of users roles , where m is the number of roles in the database, then it runs through the following phases:

1.
Constraints' generation: This phase aims to generate two types of constraints that in addition to the confidentiality constraints will guide the process of vertical schema partitioning. This will be done through the following steps:

•
Visibility constraints' generation: These constraints will be enforced as soft constraints in the partitioning phase and their severity is less than confidentiality constraints. To generate them, we perform a semantic analysis of the relational schema in order to measure semantic relatedness between attributes and users roles. These constraints will be preserved (stay visible) when the relational schema is fragmented (in other words, we aim to maximize intra-dependency between attributes that seem to be frequently accessed by the same role while minimizing the inter-dependency between attributes in separate partitions).

•
Constraints-based inference control generation: these constraints are enforced (like confidentiality constraints) as hard constraints. In this step, we resort to the method proposed in [34] to build a functional dependency graph and generate a set of join chains. Then, we use a relaxed technique to cut the join chain only at a single point in order to minimize dependency loss. We mean by cutting a join chain at a single point, the enforcement of the attributes in the LHS and RHS of the functional dependency representing the cut point as a confidentiality constraint. Consequently, we guarantee that the join chain will be broken.

2.
Schema partitioning: In this phase, we resort to hypergraph theory to represent the partitioning problem as a hypergraph constraint satisfaction problem. Then, we reformulate the problem as a multi-objective function F to be optimized. Therefore, we propose a greedy algorithm to partition the constrained-hypergraph into k partitions while minimizing the multi-objective function F.

3.
Query evaluation model: In this step, we propose a monitor module to mediate every query issued from users against data stored in distributed partitions. In the current version of our approach, the monitor module contains two mechanisms: a Role-Based Access Control mechanism and History-Based Access Control mechanism. The first mechanism checks the user role issuing the query if having the grants to execute distributed queries. If yes, his query will be forwarded directly to the desired partition. Otherwise, the query is forwarded to the History Access Control mechanism which takes as input a set of violating transactions to be prohibited. Also, it checks if the adding user past queries with current query would lead to a violating transaction. If it is the case, the query is revoked.

Fig. 2
Download : Download high-res image (268KB)
Download : Download full-size image
Fig. 2. The proposed methodology to generate secure partitions and lock suspicious queries.

4.2. Visibility constraints generation based on semantic relatedness measure
Definition 7 User-role

A role is a database object grouping a set of privileges and assigned to one or more user. A role is allowed to access a resource if there is no confidentiality constraint denying this access. Each role  is associated a head containing a set of terms  which semantically describes the role .

Definition 8 Semantic relatedness

It is a set of semantic relationship names describing relationships between entities, i.e., . Hence, the semantic relatedness between a role  and an attribute  is calculated as the average of semantic relatedness between this attribute and head terms. Consequently, it holds that:(1)
 
 where  denotes the set cardinality.

We propose in this section a workload-independent technique to generate the set of visibility constraints which in addition to other confidentiality constraints will guide the process of vertical schema partitioning. Our objective is to reduce the amount of servers that a user query has to span, so it reduces the communication cost over sites, avoids excessive network delays and improves data locality.

We suppose that each user is assigned at least to one role  in the database. Our work aims to maximize intra-dependency between attributes that seem to be frequently accessed by the same role assigning them to the same partition, while minimizing the inter-dependency between attributes in separate partitions. The main idea is to detect semantic relatedness between user role and each attribute in the logical schema. If the semantic relatedness is greater than a threshold α, then this attribute will be added to a visibility constraint . This constraint represents attributes that will be frequently accessed by the role . We produce the set of visibility constraints as specified in the following definition:

Definition 9 Visibility constraints generation

Let  be a relation schema over the set of attributes A and . Let  be the set of users roles described respectively by , and let α be a threshold such that . Then, a set of visibility constraints  holds if:

•
Vertical partitioning: for every visibility constraint , 

•
Completeness: for every attribute , there is a visibility constraint  in which  is contained

•
Reconstructability: 

•
Threshold: for every attribute ,  such that 

Algorithm 1 produces the set of visibility constraints based on the semantic relatedness measure.
Algorithm 1
Download : Download high-res image (58KB)
Download : Download full-size image
Algorithm 1. Generation of visibility constraints.

Let's consider our running example in Fig. 1 with roles doctor and chemical-drug-agent. To simplify the specification of the threshold α for the database designer in compliance with his requirements, let's consider the following ranges:

•
Weak relatedness: 

•
Medium relatedness: 

•
Strong relatedness: 

The semantic relatedness measure considered in this example is derived from the Unified Medical Language System (UMLS) as an ontology and the similarity interface on top of it.2 By considering a threshold α belonging to the medium relatedness range, the set of generated visibility constraints according to Algorithm 1 is represented as the following: where  is the visibility constraint assigned to role doctor,  is the visibility constraint assigned to role chemical-drug-agent, and  is used to ensure data lossless after the partitioning phase. Hence, under the assumption that the distributed servers could communicate between each other, a conceivable partitioning of the logical schema according to constraints ,  and  is shown in Fig. 3. Server 1 can be used to process queries issued from doctors and server 2 can process queries related to chemical-drug-agents. This partitioning will be refined by considering confidentiality constraints as explained in the following sections.
Fig. 3
Download : Download high-res image (83KB)
Download : Download full-size image
Fig. 3. An example of a partitioning w.r.t. visibility constraints v1, v2 and v3.

To compute semantic relatedness, several measures are possible. In this study, we rely on the measure proposed in [29]. This measure determines the relatedness between two terms/concepts by counting the number of overlaps between their corresponding definitions. However, when implementing this measure in Wordnet, the authors in [29] found that the definitions were short, and did not contain enough overlaps to distinguish between multiple concepts. Therefrom, the authors in [30] have extended this measure using second-order co-occurrence vectors. Here, a vector is created for every word in the concept's definition containing terms that co-occur with it in a corpus. These word vectors are averaged to create a single co-occurrence vector for the concept. The relatedness between the concepts is then calculated by taking the cosine between the concepts' second-order vectors.

UMLS computes the strength of semantic relatedness between any pair of concepts  and  by calculating the cosine of the angle between second-order vectors 
 and 
, [30]:(2)
 
 This formula is applied to our proposal by considering each attribute in  and each term belonging to role-head as a concept. Therefore, each concept  describes a term . Hence, to measure semantic relatedness between a role r and an attribute a, this formula is computed n times between every term t in the role-head and the attribute a.

4.3. Inference control
In this phase, we rely on the work of [34] to propose a set of algorithms which will help identify and block inference channels performed with meaningful join (joins on key). The proposed technique performs a  on the join chains which will consequently minimize dependency loss.

Phase 1: Building the Functional Dependency Graph 

The aim of the  is to list all the join chains that could be derived from a confidentiality constraint using functional dependencies. To build G, we resort to Algorithm 2 as follows:

1.
Decompose all functional dependencies in F, such that each functional dependency will have a single attribute in the RHS.

2.
Create an individual vertex for all attributes in schema and add to V.

3.
Create vertices for the attribute sets with more than one element, which exists in the LHS side of any functional dependency and does not exist in V.

4.
Create for each relation  an individual node in V.

5.
For each  in F, add an edge to E if  and  are different vertices in V.

Algorithm 2
Download : Download high-res image (128KB)
Download : Download full-size image
Algorithm 2. Building functional dependency graph G(V,E).

Phase 2: Generating Join Chain Set

Definition 10

Common Ancestor of a confidentiality constraint [34]
It is a vertex in the Functional Dependency Graph, from which there exist simple paths to each attribute of the confidentiality constraint.

In Fig. 4, () is the common ancestor for the confidentiality constraint .

Definition 11

Join Chain of a confidentiality constraint [34]
(Denoted as JC hereafter) is the set of edges of simple paths in the Functional dependency Graph, from a common ancestor to the confidentiality constraint.

This step aims at identifying the set of join chains that can be used from a malicious user to derive a confidentiality constraint using meaningful join. The steps of the join chain detection algorithm are as follows:

1.
All edges are reversed.

2.
Taking each element of attribute set (A) as starting vertex, apply DFS up to all connected vertices and all possible simple paths are determined for each end vertex.

3.
If there exist simple paths to the same end vertex, which are common (common ancestors in original FDG) for all set attributes (assumed to be starting vertices), all combinations of constructed simple paths, starting from different set attribute and ending in the same vertex is a join chain.

4.
If a chain composes another, it is discarded.

After applying Algorithm 3 to our example with the confidentiality constraint  and common ancestor , the following set of join chain is generated (the join chains are emphasized with blue color in Fig. 4). Phase 3: Detecting relaxed_cut

Definition 12 relaxed_cut detection

It consists in detecting the minimum number of functional dependencies in the FDG in order to cut the join chains of the confidentiality constraints to satisfy all security requirements (not allowing a malicious user to derive confidentiality constraint attributes using join chains).

It has been shown in [4] that the relaxed_cut problem is equivalent to Minimum Hitting Set Problem and thus it is NP-Complete. Hence, we give here the Algorithm 4 inspired from [34] to greedily resolve this problem. The steps of the algorithm are as follows:
1.
Compute all join chains () for each confidentiality constraint (Algorithm 3).

2.
For each edge in the FDG, determine the number of times (SecurityCount) it appears in join chains.

3.
Sort the edges first according to their SecurityCount in descending order, then, the number of attributes on the nodes at both sides of the edge, in ascending order (in order to detect lower chains first to cut).

4.
Cross the sorted list and mark each join chain as cut, if the edge is contained. These edges are selected ones and to be a selected one, an edge should be an element of at least one unmarked join chain. A set of selected edges will be enforced in the next algorithm (phase 4) as new generated confidentiality constraints.

Algorithm 3
Download : Download high-res image (147KB)
Download : Download full-size image
Algorithm 3. Join chain detection.

Algorithm 4
Download : Download high-res image (179KB)
Download : Download full-size image
Algorithm 4. Detecting relaxed_cut.

Now, let's execute Algorithm 4 on the previous example by considering FDG in Fig. 4 and the set of join chain {}. A naive approach could choose to select the edge  as a cut point (since the dependency  is already enforced as a confidentiality constraint and this will minimize dependency loss). However, this will break only  and it will not break the join chain . According to Algorithm 4 edge  () is the preferred cut point since this will break  and  simultaneously while minimizing dependency loss (we mean by minimizing dependency loss breaking  and  by placing the LHS and RHS of the selected edge in different partitions when processing the partitioning step).

Phase 4: Constraints-based inference control generation Unlike the technique proposed in [34] to break the join chains which leads to data loss and needs query rewriting techniques to query decomposed views, we propose here an algorithm to enforce each cut point selected by Algorithm 4 as a confidentiality constraint. This will help the database designer to break join chains by placing the LHS and RHS of the selected edge in different parts when processing the partitioning step. Hence, a malicious user will need to perform a distributed query against the distributed partitions to derive a confidentiality constraint with a join chain. Indeed, this malicious technique will be treated in subsection 4.5 when we will elaborate a secure distributed query evaluation strategy. We would like to mention also that in order to guarantee data lossless when distributing fragments over partitions we proceed as follows: When a functional dependency of the form  is cut a tuple identifier  is added to both fragments containing attributes I and J. Algorithm 5 proceeds as follows:

Algorithm 5
Download : Download high-res image (41KB)
Download : Download full-size image
Algorithm 5. Constraints-based inference control generation.

Applying Algorithm 5 to the set of edges generated from Algorithm 4 will extend the set of confidentiality constraints as the following:

Until now, we have produced from the constraints generation step of our proposed methodology three types of constraints: confidentiality constraints, visibility constraints and constraints-based inference control which will be considered as confidentiality constraints in the rest of this paper. In the next subsection, we will prove how these two constraint types will guide the process of schema partitioning to produce a secure and optimal distribution.

4.4. Schema partitioning
In this phase of our approach, we resort to hypergraph theory to produce an optimal attribute placement decision while preserving confidentiality constraints. We first use hypergraph to model our partitioning problem graphically then we propose a greedy algorithm to produce an optimal partitioning while retaining access control constraints.

Hypergraph partitioning have been previously used in database partitioning [18], [28]. In these approaches, authors have used a workload based approach to database partitioning, by relying on a hypergraph representation of the database where the tuples are represented by vertices (horizontal distribution) and transactions (queries) are hyperedges that relate them. Then, the partitioning algorithm minimizes the number of cross-partition transaction by minimizing cuts of hyperedges in the hypergraph.

Our choice of hypergraph theory was inspired by the comparison study provided in [18], where authors have assessed the quality of partitioning in terms of the number of distributed transactions which is the same criteria of optimality adopted in this paper. The comparison study has evaluated the number of distributed transactions produced by hypergraph partitioning algorithm, the manual partitioning (manual), replication of all tables (replication) and hash partitioning on the primary key or tuple id (hashing). The study has been evaluated on 9 datasets in all of it hypergraph partitioning technique has demonstrated its performance compared to other algorithms.

Hypergraphs and constraint satisfaction problems [12]:

A constraint satisfaction problem, P is defined as a tuple: where W is a finite set of variables, D is a finite set of values which is called the domain of P,  is a constraint,  is an ordered list of  variables, called the constraint scope, and  is a relation over D of arity , called the constraint relation.

To a constraint satisfaction problem, we can associate a hypergraph in the following way:

–
The vertices of the hypergraph are the variables of the problem.

–
There is a hyperedge containing the vertices  when there is some constraint  with a scope .

By considering our running example, vertices represent the attribute set A and hyperedges represent both visibility and confidentiality constraints. Fig. 5 illustrates our partitioning problem with respect to visibility constraints  and confidentiality constraints . Hyperedges  represent the set of visibility constraint  while hyperedges  represent the set of confidentiality constraint  and . For the sake of simplicity, Table 2 illustrates the mapping between the attribute set A and the set of vertices  in the hypergraph.

Fig. 5
Download : Download high-res image (30KB)
Download : Download full-size image
Fig. 5. Hypergraph representation of the partitioning problem.


Table 2. Attribute to vertex mapping.

Vertex	Attribute
a	Patient_id
b	Diagnosis_id
c	Admission_date
d	Patient_details
e	Drug_id
f	Drug_code
g	Drug_name
h	Drug_costs
To each hyperedge , we assign a weight  (we will explain the utility of this parameter when we introduce the multi-objective function). The dotted line in Fig. 5 states that the two hyperedges  and  should be cut through the partitioning algorithm since they represent confidentiality constraints. However, hyperedges ,  and  which represent visibility constraints will be preserved. Therefrom, we see that our objective function should maximize the hyperedges cut that represent confidentiality constraints and minimizes hyperedges cut representing the set of visibility constraints.

The traditional hypergraph partitioning techniques allow to optimize one of the objective functions: Minimize the communication cost between parts by minimizing hyperedge cut (min-cut), or declustering vertices in the same hyperedge by maximizing hyperedge cut (max-cut). Thus, we propose a multi-objective function that joins the two objectives max-cut and min-cut together. We use a priority-based formulation in which objectives are handled separately. Our greedy algorithm computes a k partitions such that it simultaneously optimizes all objectives, giving preference to the objectives with higher priorities. In our algorithm, we try to minimize the following function:(3) 
  
 

•
 and  are hyperedges representing confidentiality constraints and visibility constraints respectively.

•
: K-partitions to be computed.

•
 and .

•
 and  are the weights of the hyperedge representing confidentiality and visibility constraints respectively. The first term is the cost of violating a confidentiality constraint and the second one is the cost of violating a visibility constraint.

Please note that if a constraint is certain (the case of confidentiality constraints), its corresponding hyperedge in  should have a high weight. In such a case, the algorithm will prioritize solutions that do not violate this constraint. If a constraint is uncertain (the case of visibility constraints), then its corresponding hyperedge in  should be given a small weight, so the algorithm might give priority to solutions with a smaller first term.

Computing K-balanced partitions

We propose a greedy algorithm to compute k balanced partitions while minimizing the objective function F. Our algorithm is an attribute-to-partition mapping between individual vertex in  and partition labels. We observed later in our experiments that this greedy approach mostly determines the optimal solution compared to other state of the art approaches. First, we would like to highlight the balance constraint parameter. This latter enforces the partitioning algorithm to generate partitions where the difference in the number of vertices between these partitions must not exceed a threshold fixed according to the imbalance tolerance. We tuned our partitioning algorithm to generate balanced partitions according to the number of vertices in  as long as we place ourselves in the most extreme case where the workload is not available.

Let  be the imbalance tolerance. The balance constraint BC is defined as:(4)
 
 
 
 

•
 
 is the total number of vertices in the hypergraph divided by the number of partition K.

•
 is the number of vertices assigned to partition .

Our algorithm minimizes F by prioritizing the cut of hyperedges representing confidentiality constraints over preserving hyperedges that represent visibility constraints. A cut of a confidentiality constraint is ensured by assigning vertices of the corresponding hyperedge to different partitions. We follow a greedy method by assigning vertices one by one while retaining the balance constraint. Considering our previous example, Fig. 6 illustrates a possible partitioning according to Algorithm 6 with the confidentiality constraints {, }, the visibility constraints {, , }, a balance constraint  and a number of partition . The tuple identifier  is an artificial attribute added to ensure data lossless when a functional dependency involving a primary key was cut during the partitioning process.

Fig. 6
Download : Download high-res image (68KB)
Download : Download full-size image
Fig. 6. A refined partitioning of the Hospital-db schema w.r.t. C, V, BC = 2 and K = 4.

Algorithm 6
Download : Download high-res image (103KB)
Download : Download full-size image
Algorithm 6. Computing K-balanced partitions.

4.5. Query evaluation model
Definition 13 Role label

Let  be the set of all users roles in the database, and let  and  be two labels:

•
 states that the role  is allowed to execute distributed queries over K-partitions.

•
 states that the role  is allowed to execute only local queries (every query issued from this role cross exactly one partition).

We have developed our querying model, as shown in Fig. 7, based on a Spark architecture where data is manipulated by users through DataFrames since this latter support all common relational operators. We use Spark SQL for relational processing because it provides high performance using established DBMS techniques and enables extension with advanced analytics algorithms such as graph processing and machine learning [3]. Our querying model is composed of the following components:
•
Monitoring module: It is composed of two sub modules: A Role-Based Access Control module (RBAC) implemented using Apache Sentry, and a History-Based Access control module (HBAC). The monitoring module mediates all user queries and enforces a runtime approach which consists in monitoring the execution of queries and revokes those queries that could lead to the violation of access control policies.

•
Master node: It contains the cluster manager and it is in charge of decomposing and forwarding user queries from the monitoring module to the workers.

•
Worker nodes: They contain confidential data and responsible for relational queries processing.

In the rest of this section, we will introduce our query evaluation model which is composed of the two following steps: Violating transactions detection and query lock.
Fig. 7
Download : Download high-res image (66KB)
Download : Download full-size image
Fig. 7. A query evaluation strategy augmented with a monitoring module.

Violating transactions detection

By exploiting the functional dependency graph (defined in Section 4.3), we introduce an algorithm for automatically enumerating the set of queries having the following property: When all the queries of a given set are combined then the combination of their results will produce a violating transaction VT. These violating transactions occur using functional dependencies and considering the confidentiality constraints as queries that need to be forbidden. To cope with this problem, we firstly propose an algorithm to enumerate all violating transactions. Then, we propose a runtime approach to prohibit violating transaction completion.

Please note that to produce a violating transaction, we push the malicious user to perform a distributed query since we guarantee through our partitioning technique that the confidentiality constraint cannot be violated from a single partition.

Let's consider our example with confidentiality constraint . By running Algorithm 7 on the functional dependency graph  with join chains {}, the following set of violating transactions is generated: , , , , , .

Algorithm 7
Download : Download high-res image (104KB)
Download : Download full-size image
Algorithm 7. VT_Track.

Hence, it is clear that a malicious user could combine authorized queries with functional dependencies to evaluate all the queries of a violating transaction and by consequence violates the confidentiality constraint .

Query lock

Our query lock technique heavily relies on Role-Based Access Control Model (RBAC) and History-Based Access Control Model (HBAC) [31]. In RBAC, a set of privileges are grouped into a role. Roles are created for various job functions in an organization and users are assigned roles based on their responsibilities and qualifications. Hence, a user is assigned to this role by activating his session. HBAC consists in monitoring the access requests and revokes those that could lead to the violation of access control policies. The main idea of HBAC module is to compute the cumulation of the user's past and current accesses when he launches an access request to the system. If this cumulation can complete a violation of an access control policy, then the access request is revoked. Otherwise, the access is allowed.

In order to ensure maximal availability of data while ensuring the non disclosure of sensitive information, we propose a runtime approach which consists in monitoring the execution of queries and revokes those queries that could lead to the violation of policies. Under the assumption that malicious users cannot collude to produce a violating transaction, the main idea of the monitoring module is the following: When a user launches a query to the system, this later is automatically mediated by a Role Based Access Control mechanism. Then, the user role label is verified, if the label indicates that the user is not granted to execute distributed queries (the user label is equal to ), the query will be forwarded to the master node for relational processing. Otherwise, if the user label is equal to  then the query is forwarded from the Role Based Access Control mechanism to the History Based Access Control Mechanism. This latter takes the set of violating transactions to be prohibited as input and computes the cumulation of user past queries and current query. If the cumulation can complete a violating transaction then the query is locked, otherwise, the query is forwarded to the master node. Algorithm 8 describes how the monitoring model processes user queries.

Algorithm 8
Download : Download high-res image (80KB)
Download : Download full-size image
Algorithm 8. Query lock.

5. Experiments
5.1. Experimental design
We provide an experimental study of our proposed approach and analysis of the proposed algorithms. We have developed a prototype of our methodology written in Java: an inference control module, a partitioning module and a module for the access control based on user history. Apache sentry 2.1.0 was used to enforce the access control based on user role. To evaluate the queries execution on a big data framework, we have used Cloudera QuickStart VM 5.16.1 with Apache Spark 2. This evaluation was compared to another running MySQL DBMS. Experiments were conducted on an eight core PC (Inter(R) Core(TM) i5-8CPUs ≈1.8 GHz) running CentOs 6.7 with 12 GB of RAM.

For our benchmarking, we used the relational database schema retail_db (downloaded from the following link: [RT]). The relational dataset was converted to csv and produced the following files:

•
customers.csv: containing 12435 records

•
order_items: containing 172198 records

•
orders: containing 68884 record

•
products.csv: containing 1345 records

•
categories.csv: containing 58 records

•
departments.csv: containing 6 records

5.2. Evaluation
Fig. 9 shows a number of trials we performed so that for each run we consider a random number (ranging from 10 to 50) of functional dependencies of the form . We observed that the running time is proportional to the number of considered functional dependencies. This can be explained by the fact that the expansion of the join chain path in the functional dependency graph will push the algorithm to take more time to identify the relaxed_cut. For a scenario where we could have 50 functional dependencies per join chain the algorithm will take 6 ms to generate the constraint-based inference control set which we considered a reasonable timing. Although the algorithm shows promising results, we saw in some tests when the number of functional dependencies is greater than 100 (which is probably hard to find in real word scenarios), the algorithm takes an exponential behavior. This will be investigated in our future works.

Fig. 9
Download : Download high-res image (39KB)
Download : Download full-size image
Fig. 9. Impact of functional dependencies on the required timings to generate the constraint-based inference control set.

In Fig. 10, we depict the impact of the variation of the number of attributes on the partitioning algorithms. We compared our algorithm (HyP) to other ones proposed in the literature (Heuristic search HS [14] and complete search [16] algorithms). We observed that when the number of attributes exceed a threshold (≈15 attributes), HS and complete search algorithms take an exponential behavior until they became unfeasible. By contrast, the execution time of our algorithm always remains low. For 100 attributes, it takes 200 ms and this guarantees its applicability to large relational scheme since our algorithm will be performed offline only one time by the database designer to generate the distributed database schema.

Fig. 10
Download : Download high-res image (47KB)
Download : Download full-size image
Fig. 10. Impact of the number of attributes on the partitioning algorithm.

In Fig. 11, we fixed the number of attributes at each run and we varied the number of confidentiality constraints in Trial 1, and the number of visibility constraints in Trial 2 as shown in Table 3. After examining the execution time of our partitioning algorithm, we saw that the increase of the number of confidentiality constraints for the same number of attributes and the same number of visibility constraints has importantly increased the execution time due to the fact that a large number of confidentiality constraints will likely result in highly fragmented schema. However, increasing the number of visibility constraints while fixing the number of confidentiality constraints has slightly affected the execution time of the partitioning algorithm.

Fig. 11
Download : Download high-res image (44KB)
Download : Download full-size image
Fig. 11. Computational time varying the number of confidentiality constraints and visibility constraints.


Table 3. Number of confidentiality constraints C and visibility constraints V for each run.

Trial 1	C = 2, V = 6	C = 4, V = 6	C = 7, V = 6	C = 10, V = 6	C = 12, V = 6
Trial 2	C = 6, V = 2	C = 6, V = 4	C = 6, V = 7	C = 6, V = 10	C = 6, V = 12
We compared in Fig. 12 the execution time between MySQL database and SparkSQL for local and join queries. The database was firstly setup in a centralized server running MySQL, and a first trial was performed in which we had computed execution time. The number of queries was ranged from 5 to 100 queries. In the second trial, we had partitioned the database vertically into 6 partitions according to our proposed algorithms (we considered 6 visibility constraints and 12 confidentiality constraints) and we deployed it on a Spark cluster running 5 worker nodes. As illustrated in Fig. 12, when the number of executed queries is less than 40 the MySQL database shows good performance than SparkSQL, but when the number of distributed queries becomes large SparkSQL shows less execution time than MySQL DBMS. The optimization of local queries was reached through the maximization of data locality adopted in our partitioning technique while the optimization of join queries was reached through the parallelization of query execution.

Fig. 12
Download : Download high-res image (49KB)
Download : Download full-size image
Fig. 12. Comparison of query execution time between MySQL and SparkSQL.

In Fig. 13, we performed several runs. For each run, we fixed the number of issued queries from the user and we evaluated the required time to block a suspicious query. We note that the time required to block a suspicious query increase with the number of queries issued from the user. Observed results confirmed that our monitoring module took a reasonable timing to enforce both access control mechanisms RBAC and HBAC. However, dealing with a very large number of queries is an open issue which we will investigate in the future.

Fig. 13
Download : Download high-res image (65KB)
Download : Download full-size image
Fig. 13. Time required to lock a suspicious query.

The conducted experiments have showed the practicability of our methodology. Nevertheless, to ensure the usability of the approach, the following parameters should be taken into consideration:

•
Number of functional dependencies: The proposed methodology showed some limitations only when it deals with a large number of functional dependencies. We believe that finding such typical scenario with large number of functional dependencies is probably hard.

•
Value of the threshold α: Indeed, it is very important to choose an adequate value of α for the simple reason that this parameter directly affects the visibility constraints which in turn affect the partitioning process. Some studies have been performed to determine optimal threshold according to the research domain. For example, in bioinformatics, the work in [5] attempted to derive an optimal threshold for interpreting semantic similarity and particularity.

•
Since we store every generated partition in exactly one server, the number of allocated workers in the cloud should be equal to the number of generated partitions K.

•
The relational schema given as an input should be static in all different steps involved in our methodology. Any change brought to the database schema will lead to execute the approach from scratch.

6. Conclusion and perspectives
In this paper, we investigated the problem of secure data outsourcing in presence of inference leakage caused by functional dependencies. By exploiting semantic relatedness and vertical partitioning, we gave more flexibility to the database schema to answer user queries while minimizing distributed transactions. Then, by modeling the database schema as a graph of functional dependencies, we captured join chains that could lead to data disclosure. As well, we resorted to hypergraph theory to derive a partitioning algorithm aiming to break join chains, preserving access control and optimizing schema fragmentation. Lastly, we built a secure query evaluation model to block suspicious queries.

There are many research directions to pursue:

•
When the workload becomes available after the database is set up in the cloud, the challenge is how to dynamically reallocate the distributed database fragments among distributed partitions while retaining access control policies? Indeed, developing a dynamic reallocating algorithm by taking into consideration the query workload changes and confidentiality constraints will be a challenging task. We believe that machine learning could be a suitable technique in this situation towards an intelligent cloud database system.

•
Another interesting issue is the collaborative inference. Indeed, we proposed to block a violating transaction from being achieved to prevent inference leakage, but what if these violating transactions results from a combination of a set of queries from more than one user?

•
Furthermore, we aim in our future works to consider other semantic constraints as a source of inference leakage, for example, inclusion dependencies, join dependencies and multivalued dependencies.

•
It is obvious that maintaining the record of user history in the HBAC mechanism without an optimal archiving policy has operational drawbacks and high costs. Hence, a further interesting research challenge is the establishment of an optimal archiving policy to efficiently record user history in the HBAC mechanism. Indeed, we believe this will enhance the query runtime monitoring technique adopted in this paper.