Abstract
There is a need for simple, fast, and memory-efficient multidimensional data structures for dense and sparse storage that arise with numerical methods and in software applications. The data structures must perform equally well across multiple computer architectures, including CPUs and GPUs. For this purpose, we developed MATAR, a C++ software library that allows for simple creation and use of intricate data structures that is also portable across disparate architectures using Kokkos. The performance aspect is achieved by forcing contiguous memory layout (or as close to contiguous as possible) for multidimensional and multi-size dense or sparse MATrix and ARray (hence, MATAR) types. Our results show that MATAR has the capability to improve memory utilization, performance, and programmer productivity in scientific computing. This is achieved by fitting more work into the available memory, minimizing memory loads required, and by loading memory in the most efficient order. This document describes the purpose of the work, the implementation of each of the data types, and the resulting performance both in some simple baseline test cases and in an application code.


Keywords
Performance
Portability
Productivity
Memory efficiency
GPUs
Dense and sparse storage


1. Introduction
There are three main problems in code design in computational physics. They are: making the code easy to write and upkeep (e.g. making the programmer productive), making the code performant, and making the code portable across multiple modern computer architectures. These problems all arise when working through the low-level data layout for a given code. To help simplify this process, we have developed a software library, MATAR,1 that provides the flexible data structures needed by scientific applications and delivers high performance code even within complex routines. In many of our applications, the layout of data is a major hurdle when it comes to the performance of the code. Sparse data and unstructured data are commonplace and add complexity to our data structures. Furthermore, common array data structure implementations, especially multidimensional, are often inadequate for achieving desired performance. MATAR looks to improve the performance of these data structures within a simple interface that is portable across multiple architectures. MATAR also provides a GPU capability using the Kokkos portability library. While the dense arrays are straightforward to implement on the GPU, the sparse data representations were very difficult to get working within the constraints of the GPU and the languages.

The key contributions of this paper are:

1.
Providing data structures that map problem data efficiently in memory.

2.
Providing an easy to use interface for fast data access and modifier operators.

3.
Creating data structures using Kokkos that run on multiple CPU and GPU architectures in a performant and portable manner.

The remainder of this document covers the background problems that led to the decision to undertake this project, the overall design goals of creating MATAR, the methodology behind the code and a discussion of the mental model for how data are laid out and accessed. We then discuss the process of making MATAR portable across multiple computer architectures using Kokkos and give performance results from multiple test cases on CPUs and GPUs. Finally, to show the utility of MATAR, we show how a contact detection method used in a production geophysics code can be sped up using MATAR, and also discuss using MATAR in a new Finite Element library (ELEMENTS [29]).

2. Background
Dynamic memory allocation was introduced with the C programming language in 1978 [22]. Since that time, there has been a lot of research into efficient memory allocation. What is notable in this and many other reviews of memory allocators is that the performance criteria is based mostly on how fast the allocators execute and how much space is used by them. While the time taken for memory allocation is important, we are more interested in the performance when using the data. The performance while using the data is mostly orthogonal to the time taken for the initial allocation though there is certainly an impact from the algorithms that are used in the allocation. What we want is more data locality so that all of the data is accessed contiguously. Early work focused on the memory allocation within the kernel. Bonwick [5] looks at performance impacts on the usage of the memory with his Slab allocator. The Slab allocator groups memory allocations into subgroups organized around memory pages. One of the major impacts is that by co-locating memory allocations on a page, the number of translation lookaside buffer (TLB) misses can be reduced, significantly improving performance. Another possible performance issue is that smaller memory allocations can have a pattern such as landing in the first part of a page that causes them to all hit the same cache lines. Larger allocations can potentially spread out the mapping to the cache lines in a more uniform manner. There are some tools that help with data locality such as NUMA partitioning of memory [24], but this does not meet our needs. This necessitates a multidimensional array allocator and other contiguous memory approaches.

A recent paper by Elias, Matias, Fernandes, et al. [12] reviews six different allocators in a multi-processor environment. Recently memory allocation packages such as the jemalloc package for FreeBSD [13], tcmalloc by Google [19], and Hoard [4] have focused on supporting multi-threaded and multi-processor applications.

Another common memory management strategy for high-performance applications is the use of a memory pool. Memory pools utilize pre-allocated buffers which the application uses for quick allocations. These memory pools are tuned for the particular application needs and can result in better performance than a generic memory allocator. The use of memory pools pre-dates the C dynamic memory allocator. They were often used in Fortran applications to divide up a large heap space as a work-around for the lack of a dynamic memory allocation capability. Memory pools are still common in applications needing high-performance, ranging from network communication to computer games to get high frame rate graphics. Most memory pools are coded directly into applications, but there are some general purpose pool tools such as the Boost Pool library [35]. In the high-performance computing space with GPUs, the Umpire [3] package provides a memory pool where the use of memory can be overlapped between computational modules. Umpire used some of the ideas in the CNMeM package [1] which provides a GPU memory pool for neural network applications.

Interest in proper multidimensional array support has long existed in the C and C++ programming community. There has always been the capability for using the Standard Template Library (STL) vector of vectors to provide a two-dimensional (2D) array, but it is not a performant method, nor does it scale reasonably to n-dimensional arrays much larger than two dimensions. The STL and other collections of memory such as the Java Collections Framework are focused on convenience rather than performance. Extensive work has provided alternatives to the Java Collections Framework [9] that have demonstrated better performance. One of their conclusions is that any Array Lists implementation is a great improvement over the Linked List collection type. Jung, et al. [21] developed a tool that improves performance simply by analysing program usage of the STL and recommending better containers. Unfortunately they don't suggest better usage such as specifying the initial size of the container to avoid expensive reallocation costs. They also don't quantify the cost compared to explicit memory management. In a parallel code seeking high performance, having one processor slowed by a reallocation while all the others are waiting on it must be avoided. Smart usage of the STL can minimize the costs, but it is all too easy to fall prey to the convenience and be unaware of the performance penalties.

The Boost Multidimensional Array Library [16] has provided a multidimensional array class template for C++ programmers since 2002. The library provides a dense n-dimensional array class template, which end-users can use to create contiguously allocated arrays that interoperate well with C++ Standard Library containers and algorithms. In addition, the library allows programmers to:

•
create sub-views of arrays, i.e., arrays that contain the same number of dimensions or fewer than the original array;

•
specify the memory layout of arrays, e.g., create arrays that follow C-style or Fortran-style memory layouts;

•
specify array bases, i.e., how arrays are to be indexed;

•
reshape arrays, i.e., change an array's dimensionality along each dimension while preserving the number of array elements; and

•
resize arrays, i.e., change an array's dimensionality along each dimension without necessarily preserving the number of array elements.

Some of the design for the Boost Multidimensional Array Library has come from the well-known Blitz++ library, which also provides powerful multidimensional array capabilities. Another software package, the Matrix Template Library [36], develops a flexible multidimensional data structure for use in matrix operations for numerical solvers.
The Warwick Data Store (WDS) [23] is another C++ template library that provides similar multidimensional array capabilities, but with a focus on letting programmers write data structures that are highly interoperable with existing codebases and incur minimal overhead. Specifically, the Warwick Data Store is a collection of three classes: a controller class, which provides high-level functionality to programmers; the variable classes, which allocate and manage the underlying memory for the desired data structures; and the view classes, which provide quick and easy access to the underlying memory for the desired data structures. By being composed of these three class collections, the Warwick Data Store makes it easy for programmers to replace existing data structures — from simple 2D arrays to highly specialized containers. Both the Warwick Data Store and our work was inspired by the performance study of compressed sparse multi-material data structures by Fogerty, Martineau, Garimella, et al. [15]. By providing a higher-level interface to these performant data structures, Warwick Data Store can be used in existing codebases without writing significant conversion code. For example, if a codebase contains a function that works with a regular 2D array and a programmer wants to call the function with a Warwick Data Store data structure instead (that has the same intended memory layout of the original array), the programmer can do so without modifying the function to account for the memory layout of the replacement data structure — this is handled entirely by Warwick Data Store.

The latest multidimensional array implementation to come to C++ is the mdspan proposal that has been accepted into the C++ standard [20]. Mdspan originated as the multidimensional array object in the Kokkos performance portability library [11].

Many of these libraries cite their motivation being multidimensional arrays that are easier and more powerful to use in C++ applications and the better performance of contiguous memory implementations over the vector of vectors approach. Each of these tools have a targeted purpose. Most are focused on the CPU application space with regular multidimensional arrays. Kokkos has extended support to the GPU for these multidimensional arrays.

Sparse data arrays are important in high-performance computing, especially in the matrix solver community. Many different sparse storage schemes have been developed over the years. We'll delve into sparse data arrays in Section 4.3 and then develop more simplified ways to create these types of arrays in a contiguous memory space. One of our long-term goals in this work is to extend Kokkos and eventually the C++ standard to support sparse arrays and a broader set of data representations. The explorations of implementations by these libraries should eventually coalesce into a model for more general usage.

3. Design and goals
One of the most difficult parts of code design for computational physics is deciding on how data will be laid out in memory, and how to make accessing that data efficient. Contiguous memory access is generally the most performant on any hardware since it allows for data streaming and vectorization. But contiguous memory is rarely the most efficient from the programmers perspective since the data structures for contiguous memory become more challenging as the complexity of the problem being solved increases, and the problems themselves are rarely set up to make contiguous memory access easy to achieve.

One example of where these problems arise is in methods that involve polytopal meshes like the one discussed in [26]. Polytopal meshes like the one shown in Fig. 1 are difficult to build since the connectivity structure is entirely based on the initial point cloud used to generate the cells and no symmetry or repeated structure can be used to simplify the mesh building process. For example, cell 1 in Fig. 1 has 6 nodes and 6 neighbors, cell 2 has 6 nodes and 9 neighbors, cell 3 has 7 nodes and 7 neighbors, and cell 4 has 4 nodes and 7 neighbors. A data structure to hold the information for this will necessarily be “ragged” since the amount of information required to be stored for each cell varies. There are few tools for building these meshes [32] because of the complexity in their construction, but they show great promise for meshing complex geometry.

Fig. 1
Download : Download high-res image (51KB)
Download : Download full-size image
Fig. 1. Polytopal meshes do not lend themselves to easily implementable contiguous connectivity structures since the amount of information required to represent the connectivity between each cell is not the same.

For another example, it is often the case in Eulerian codes that multiple materials exist inside each cell, and since the materials move through each cell the number of materials changes throughout the run-time of the code. A few methods that take this approach are [2], [7] and an example of this type of problem is shown in Fig. 2. To make these multi-material codes performant the information required needs to be laid out and accessed contiguously in memory. There are two main ways to lay out memory for these applications, cell centric or material centric. The performance of each of these is analyzed in [17], but each still results in a ragged data layout.

Fig. 2
Download : Download high-res image (77KB)
Download : Download full-size image
Fig. 2. In some applications there can be multiple materials per cell, and for Eulerian or Arbitrary Lagrangian Eulerian (ALE) methods, these materials move through the cells. This results in a ragged memory layout to describe either the materials in a cell, or the cells which have a material depending on if your code is cell centric or material centric [17].

Also, numerical methods are derived and designed using multidimensional matrices for the calculation, and most people think about these methods as object manipulation (e.g. matrix-matrix multiply) instead of how the processor manipulates the data (e.g. contiguous-access, fused multiply-add). The goal of MATAR is to build data structures that are easy to use and give the feeling of object manipulation, but lay out and access the data contiguously in memory so the end-user gets the performance benefits of data-oriented design with the productivity benefits of object-oriented design. But what do we mean by data-oriented design and the better known term, object-oriented design? We first review the definitions of the two terms.

3.1. Object-oriented programming and data-oriented design
Introduced around the late 1980s, object-oriented programming (OOP) is a prominent programming paradigm and has become a preferred way of programming in many fields [37]. Object-oriented programming solves problems by breaking down the problem into its constitutive parts and their relations. These objects are then manipulated through the process of solving a problem instead of manipulating all of the bits of information individually.

Although object-oriented programming is a great mental map for the relationship of objects and their capabilities, there are some performance limitations that are often overlooked. The most significant drawback of object-oriented programming is that it is typically not an efficient paradigm. For example, in Listing 1, we create a Circle class with some variables and functions that will be used. It is then possible to create many circle objects. Having millions of Circle objects will lead to poor performance because of the sub-optimal layout of the data in memory. The non-contiguous memory layout for each variable, i.e. x, y, radius will lead to more memory loads for each object and cache misses if the data is not used sequentially [33]. The memory loads become a bottleneck, making it more difficult to optimize the code for performance.

Listing 1
Download : Download high-res image (74KB)
Download : Download full-size image
Listing 1. An example of Object-Oriented Programming. The object is the circle and the method manipulates traits of the circle object.

Fig. 3 shows how the Circle class is laid out in memory. If you want to use the radius of each circle sequentially in a calculation, then you will generally only be getting one value per cache load. If the memory is laid out so that the radius value is contiguous (sequential) in memory, then you would load multiple values into cache that can all be used, which is both faster and allows for vectorization. While this object-oriented programming example creates many circle objects, the object-oriented programming approach can be applied to multi-physics and multi-material implementations [2], [7], [17] by creating many cell or material objects.

Fig. 3
Download : Download high-res image (16KB)
Download : Download full-size image
Fig. 3. The memory layout for the array of classes Circle. Notice how the data are loaded for each object in memory instead of having the data, e.g., variable r contiguously in memory. If the data is not used sequentially, more memory loads are needed leading to a memory bottleneck.

If we want performance on large problems the focus needs to shift from the objects themselves to the data layout and access patterns required to solve the problem in order to reduce memory loads. As others have suggested in [27] and more in-depth in [14], a data-oriented design (DOD) approach would be more performant than the programmer-oriented object-oriented programming paradigm because it places the focus on how the data is being used and where it is in memory. Shifting the focus onto how the data is being used and accessed will lead to better data use patterns. Laying out the data contiguously in memory so that it matches the access patterns used by the algorithm will lead to sequential data access, maximizing cache usage and providing more opportunities for vectorization [27]. Also, for the case of ragged data structures, the amount of memory required to describe the problem is minimized.

Having the data contiguous in memory in a bandwidth-limited code will also yield high performance benefits from getting full use of a cache line. For example, changing from the use of only half a cache line to a full cache line will generally double performance. Going from only using one value in a cache line to using all eight double precision values will roughly increase performance by a factor of eight. Even when getting mostly full cache lines, getting memory on the same pages of memory will avoid additional page loads and their associated costs. Penalties are high if data is not in the page cache and it has to walk all the pages to find the right page to load.

3.2. Design of dynamically allocated multidimensional arrays
One of the important features of C++ is that it offers its users the ability to dynamically allocate arrays via the keywords new and delete. Though there is no explicit multidimensional array in C++, the dynamic memory allocation of arrays can be extended to create multidimensional arrays. Listing 2 shows one of the conventional ways for dynamically allocating memory on the heap for a 2D array in C++.

Listing 2
Download : Download high-res image (77KB)
Download : Download full-size image
Listing 2. An example of dynamically allocating and deleting a 2D array.

In instances where programmers want to create a small multidimensional array in C++ for one-off usage, they can use the approach shown in Listing 2. However, when programmers want to create a conventionally allocated, large multidimensional array, e.g., a five-dimensional array, they will find that writing the corresponding code is cumbersome and error-prone. A common error is forgetting to deallocate the memory upon termination of the application, which leads to memory leaks during run-time and can crash the code. In addition, there are deeper problems with the approach shown in Listing 2, with one of the most pressing issues being that multidimensional arrays that are conventionally allocated do not necessarily reside contiguously in memory. An example of what this can look like in memory is given in Fig. 4, and this can lead to a high performance cost.

Fig. 4
Download : Download high-res image (203KB)
Download : Download full-size image
Fig. 4. Generally, when using malloc or new to allocate multidimensional arrays in C and C++ you are not necessarily getting memory that is contiguously laid out on hardware. This can severely limit performance as memory has to be moved from random places instead of streaming continuously.

Having the data in the array scattered in global memory impacts how many flops can be done per load and lowers overall cache efficiency. This impacts the array's cache efficiency, but more importantly, any code that uses such arrays is not necessarily portable and parallelizable. Multidimensional arrays allocated this way cannot be trivially passed between programming languages, GPUs cannot work with the array, the array cannot be written in block format to a file, and multiple processors cannot easily work with different portions of the array [33].

Of course, programmers do not have to dynamically allocate multidimensional arrays row by row; they can allocate multidimensional arrays contiguously in memory in C++, and one such way is given in Listing 3.

Listing 3
Download : Download high-res image (83KB)
Download : Download full-size image
Listing 3. This code allocates a 2D array contiguously in C++. As you increase the number of dimensions the complexity required to allocate the array contiguously increases, and the number of times the memory must be deleted grows. This generally leads to more errors in a code.

The method shown in Listing 3 is certainly not the only way to contiguously allocate memory for multidimensional arrays in C++. But even with this rudimentary approach we see that the programmer is able to obtain better memory allocation efficiency and cache efficiency. In addition, the portability problems that were identified earlier are now addressed. However, this approach is error prone; the programmer still has to exercise great caution when allocating and deallocating the array. This process is made more difficult when the programmer wants to work with larger multidimensional arrays. These are just a few of the common issues that motivated the creation of MATAR.

3.3. Goals of MATAR
MATAR is a C++ header library in development at Los Alamos National Laboratory that aims to address the issues identified in previous sections regarding contiguous memory layout for multidimensional and sparse data. MATAR aims to provide the end-user data structures that are:

1.
easy-to-use, i.e., for each data structure, the library exposes a clean interface for intuitive data access and modification;

2.
portable, i.e., each data structure is able to be used in both serial and parallel settings, and on multiple CPU and GPU architectures using the Kokkos programming model, and

3.
performant, i.e., each data structure, regardless of the data sparsity use case, maps, accesses, and modifies data as efficiently as possible in memory.

MATAR's data structures use a minimum amount of memory to describe complex data layouts and allows programmers to access and manipulate the data easily. It is also designed to give good performance with today's deep-cache hierarchies.

4. Methodology
The high level view of MATAR data structures combines the organization and object focus of object-oriented programming and data allocation and management of data-oriented design. MATAR data structures are pseudo-objects that allocate multidimensional, dense, and sparse data in one contiguous block of memory and access the elements in memory as a one-dimensional (1D) array.

From a high level, every MATAR data structure consists of one or more the following:

1.
a 1D array allocated contiguous in memory;

2.
meta-data that specifies the arrays properties including number of dimensions; and

3.
views of the data allowing different treatments

The views allow the end-user to access and modify the elements of the array in a way that is consistent with the mental model of the MATAR data structure, i.e., if the end-user is expecting to work with a 2D array, then the class exposes a method to access the array with two indices rather than forcing the user to do index calculations. MATAR also prevents the end-user from working with the 2D array as an 1D array or a higher-dimensional array when compiled with a debug flag. This allows for ease of debugging, but does not affect performance during a production run.

In addition, MATAR takes care of additional low-level details, including deciding where the memory for the underlying arrays and associated meta-data is allocated, i.e., on the CPU or GPU.

MATAR's data structures are categorized by the following criteria:

1.
access pattern - refers to whether the data is laid out in column-major format or row-major format;

2.
indexing pattern - whether the data is 0-indexed or 1-indexed; and

3.
data sparsity - is the data is densely packed or sparsely packed.

Fig. 5, Fig. 6 indicate how the various MATAR data structures are grouped by the above criteria. In the following sub-sections we will go into greater detail about the importance of these criteria and how each of these structures are allocated and accessed.
Fig. 5
Download : Download high-res image (56KB)
Download : Download full-size image
Fig. 5. How MATAR's data structures for dense data access and modification are organized by memory access and indexing patterns.

Fig. 6
Download : Download high-res image (58KB)
Download : Download full-size image
Fig. 6. How MATAR's data structures for sparse data access and modification are organized by memory access and indexing patterns (all the sparse data structures are 0-indexed).

There are a few important distinctions we will make for the remainder of the paper. Our MATAR Array (and Matrix) data structures allocate memory and store attributes such as length and dimension size. Ragged data structures also allocate memory and contain additional metadata such as a stride array (which is passed in as an argument). MATAR Views do not allocate any data, instead they store similar metadata as MATAR Arrays, with a memory pointer to a pre-existing data structure. This distinction is important because our MATAR Views are a method of accessing and manipulating data, rather than a data structure itself.

4.1. Row-major and column-major data layout
The high-performance computing (HPC) community extensively uses Fortran, C, and C++, and many HPC practitioners call Fortran code, e.g., numerical linear algebra subroutines provided by LAPACK, from C and C++ code bases. One of the difficulties in getting existing Fortran code to work correctly within C and C++ code bases is ensuring that existing C and C++ access and modify multidimensional arrays appropriately with respect to Fortran code, and this difficulty arises from how Fortran, C, and C++ lay out data in memory and how memory is allocated.

In Fortran, multidimensional arrays are laid out in memory in column-major layout, and Fortran gives end-users the ability to allocate arrays contiguously in memory via the contiguous attribute keyword, but in practice, most heavily-used Fortran compilers allocate arrays contiguously without the programmer explicitly requiring it to [33]. In C and C++, multidimensional arrays are laid out in memory in row-major layout, and both languages allow programmers wide leeway in determining how the memory is allocated, as seen in earlier sections. As shown in Fig. 7, Fortran stores the multidimensional data as a one-dimensional array column wise while C++ stores a one-dimensional array row wise.

Fig. 7
Download : Download high-res image (56KB)
Download : Download full-size image
Fig. 7. Memory layout pattern for a 2D array in Fortran, C, and C++. Notice how Fortran data is laid out traversing the columns while C and C++ data is laid out by row. MATAR considers both memory layout patterns for optimal performance.

It is important to consider the memory layout pattern of the language because better performance is achieved when data access patterns match how the data is laid out in memory (e.g. the loop structure must walk over the data the way it is laid out in machine memory). Listing 4 shows the fastest and slowest loop structures in C++ as an example. The loop structure will be reversed for Fortran code.

Listing 4
Download : Download high-res image (71KB)
Download : Download full-size image
Listing 4. C/C++ stores memory in a row major order, so the index that is contiguous in memory is always the last one. Data is accessed contiguously by looping over this index at the lowest level of the loop structure, and if the loops are interchanged, the memory will be fetched in jumps that are the size of the second index of the array. This kills cache performance and vectorization.

Accessing the data in the same pattern as how it is laid out in memory ensures a cache line is fully utilized. MATAR has dense data structures (matrices and arrays) that take care of the contiguous memory allocation for each either row major or column major for up to seven-dimensional (7D) data, which is consistent with the Fortran standard. Another benefit of including both row major and column major is it simplifies the process of converting scientific Fortran code to C++ code.

4.2. Dense data structures
MATAR data types for dense data are Array and Matrix. The data is allocated contiguously in memory (either column or row major) and accessed with the () operator. The only difference between an Array and Matrix is the index start value (0 for Array, and 1 for Matrix). Looking at Fig. 8, an Array follows the traditional C indexing where 
 and 
 and Matrix indices are from 
 in order to be consistent with the how indices are normally defined in mathematics.

Fig. 8
Download : Download high-res image (49KB)
Download : Download full-size image
Fig. 8. MATAR C-style (row-ordered) two-dimensional data object. The 3×3 square is how the user will interpret the object while the top shows how the object is laid out contiguous in memory.

In Listing 5, we show some examples of how to create a CArray and CMatrix objects with different dimensions. When creating a MATAR data structure, the desired data type is specified in the chevron brackets <> and the size for each dimension in parentheses () separated by a comma. MATAR currently supports 1D to 7D arrays and matrices, which follows the Fortran standard.

Listing 5
Download : Download high-res image (133KB)
Download : Download full-size image
Listing 5. A few examples of how to create and access CArrays and CMatrixs in MATAR. For each of these data elements the last index is contiguous in memory. These data elements can be any of the primitive data types such as an int, float or double, but it can also be a more complex type such as a struct. But we caution that a struct would be an Array of Structures and may give poor performance. However an Array of Structures of Arrays where the inner array type is a short length that matches the vector length can perform well. Creation of FArrays and FMatrixs is exactly the same, but with the notable exception that the first index is contiguous in memory.

If you are working on a legacy code where it is unfeasible to go through a large code and replace all the traditional arrays with MATAR data types, then Views are a great alternative. Views allow the programmer to take a pre-existing array of data and access it as its own object. The Views allow the programmer to walk over an existing array that is contiguously allocated in memory and treat it as a multidimensional array for manipulation. One of the main benefits of Views is that they allow the programmer to pass sections of pre-existing blocks of memory into functions by reference. That view of the memory is then accessed contiguously in memory and manipulated in place. Views work similarly to arrays when it comes to accessing memory, and the main difference is that the Views do not allocate memory. They simply take in the pointer to the first index in memory of some pre-existing data and manipulate that data as if it is its own object. Fig. 9 gives an example of how a contiguous block of memory can be accessed as a 3D array using Views, and Fig. 10 shows how the programmer can start a view at any point in memory. Views can also be passed by reference to functions as shown in Listing 6. One thing to note is all memory is modified in place. This means that if you make a View of a pre-existing array and modify the data in the View, then the data in the pre-existing array is also modified. Views let you treat the underlying data as any size and dimension as long as the total size does not go out of bounds of the underlying array. This means the user must make sure the operations done on Views are numerically consistent with whatever operation is being done on the data. For this reason, the underlying structure of the data must be known by the programmer.

Fig. 9
Download : Download high-res image (131KB)
Download : Download full-size image
Fig. 9. Here a View is created by specifying the desired index of some pre-existing memory and “viewing” it as a 3D array of size 2x2x3. This view can be accessed the same way as a CArray.

Fig. 10
Download : Download high-res image (103KB)
Download : Download full-size image
Fig. 10. Views do not necessarily have to start at the first index of a pre-existing array. The user defines the start point, dimension, and size of each dimension when creating the view.

Listing 6
Download : Download high-res image (97KB)
Download : Download full-size image
Listing 6. Views can be passed by reference to functions where it can be modified in place.

Listing 7 shows how Views can be created using some pre-existing array. One benefit of using Views is you can treat the same data as different size arrays and each view will still access and modify data contiguously in memory. It is also possible to make Views of Views, as shown in Listing 7. To reiterate, the views are not “slices” that one might expect from python or Matlab, they are simply ways of accessing underlying contiguously allocated data. This means if you view a 3x3 matrix as a 1x9 array you will be walking over the 9 values in whatever order the 3x3 matrix was stored in memory. Listing 7 should clarify this further.

Listing 7
Download : Download high-res image (100KB)
Download : Download full-size image
Listing 7. A View allows the programmer to access an already existing contiguous array and treat it as its own object. Views can be treated as having a higher, lower, or the same dimension as the underlying data depending on what is needed.

4.3. Sparse data structures
MATAR also provides data structures for sparse data access and modification, and these data structures fall under one of the following categories:

1.
ragged data structures,

2.
dynamic ragged data structures, and

3.
compressed sparse arrays.

We will provide an overview of these three categories of data structures.
4.3.1. Ragged data structures
In computational physics codes, it is common to work with data that are grouped by rows (or columns), where each row (column) may not be the same length. Fig. 11 gives an example of the type of data layout where these memory issues arise. The 2D array is laid out in row-major order, but each row is a different length.

Fig. 11
Download : Download high-res image (95KB)
Download : Download full-size image
Fig. 11. How a contiguous array might have unevenly populated, i.e., “ragged”, rows. It is non-trivial to allocate memory efficiently for these types of structures with the standard C/C++ approach. The programmer can either allocate each row individually, allocate it as a dense array where large sections are unpopulated, or as a linked list, which is not performant or necessarily contiguous in memory.

It is inefficient and expensive with respect to run time and memory usage to treat sparse arrays as dense arrays. While programmers can create a sparse array by allocating memory row-by-row for data in C and C++, to gain the full benefits of this approach, they will have to ensure that the underlying memory is contiguously allocated. In these instances, MATAR's ragged data structures offer programmers the benefits of working with sparse data intuitively and efficiently without the pitfalls of standard methods of allocation. Fig. 12 shows how the programmer can visualize the data from Fig. 11 as a RaggedRightArray object, a MATAR data structure that lays out its associated data in row-major layout and is 0-indexed.

Fig. 12
Download : Download high-res image (147KB)
Download : Download full-size image
Fig. 12. How the programmer can visualize the data from the previously shown sparse array as a RaggedRightArray object and how MATAR lays out the associated data in memory. The underlying memory is contiguously allocated, and an extra array is used to store the start and stop index.

Implementing the ragged data representations for the GPU with Kokkos was challenging. The sparse data representations pushed the limits of Kokkos, C++ 17, and CUDA 11. It took effort to find clever solutions that work within the limitations of languages and hardware.

MATAR also provides a RaggedDownArray data structure that is nearly identical to the RaggedRightArray structure except that it allows programmers to access and modify data as if it were laid out in column-major format instead of row-major.

4.3.2. Dynamic, ragged data structures
MATAR also provides programmers with dynamic, i.e., extendable, ragged data structures, which are similar to the ragged arrays discussed earlier except that every row (or column) has an additional buffer that is available for the programmer to add more entries. Thus, the programmer is able to increase the size of an individual row (or column), whenever the need arises, up to the buffer length that is specified upon the object's construction. For example, consider the dense array in Fig. 11. This dense array can be treated as a dynamic, ragged right array as shown in Fig. 13. If for this application the programmer knows that each row will be resized often, they can use MATAR's dynamic ragged right array. This larger buffer size in the ragged data structure leads to a slight performance hit, but it will be faster overall than if the memory had to be re-allocated and propagated each time the row size is changed. If the programmer wants to traverse the same data in column-major format, they can use MATAR's DynamicRaggedDownArray data structure.

Fig. 13
Download : Download high-res image (161KB)
Download : Download full-size image
Fig. 13. How the programmer can visualize the data from the previously shown sparse array as a DynamicRaggedRightArray object and how MATAR lays out the associated data in memory. The underlying memory is contiguously allocated, as in the case with a regular RaggedRightArray object, and an extra array of metadata is used the store the start and stop indices for each row. Furthermore, the DynamicRaggedRightArray allows the programmer to append additional data to a given row, up to the provided buffer size.

4.3.3. Motivation for ragged data structures
A linked list (LL) is a basic data structure in which objects are arranged through a linear series of pointers, where the order is determined by a pointer in each object [8]. Typical linked list implementations provide simple and flexible representations of dynamic sets; new elements can be deleted and added, unlike a traditional array where the size is set. Although linked lists are flexible in terms of their size, there are significant drawbacks such as

1.
Accessing the sequence of lists must be done in order; therefore a search can quickly become expensive if the list is not sorted.

2.
No guarantee the linked list is contiguous in memory (i.e. poor data locality). The lists may be scattered throughout global memory, similarly to Fig. 4.

3.
They can not easily be ported to the GPU.

4.
Repeatedly adding elements to an list may cause re-allocations of heap memory; a prohibitively memory- and time-intensive operation.

Thus, linked lists are generally avoided in parallel numerical codes. They are especially avoided when it is expected that they will be heavily modified, i.e., elements will be inserted at the end of lists and deleted from lists.

MATAR offers dynamic ragged data representations that address the drawbacks with linked lists. The ragged dynamic data structures allow programmers to leverage the flexibility of working with linked lists while being usable in parallel settings (CPUs and GPUs) and offers better data locality through contiguous memory allocation. The biggest difference between a MATAR dynamic data structure and a linked list is where the new elements are added. While in a linked list the nodes can be added throughout the list, for a dynamic ragged structure elements can only be appended in the buffer at the end of a row shown as DynamicRaggedRight or DynamicRaggedDown for a column in Fig. 13.

4.3.4. Compressed sparse arrays
It is common to work with large arrays that have very few non-zero entries (for example, finite difference matrices), and it is typically desirable to work only with the non-zero entries of these arrays. Two commonly used formats for compressing and working with sparse, 2D, and (typically) non-symmetric arrays are the following:

1.
compressed row storage (CRS) format (also known as the Yale format), where the data is “compressed” row-wise, and

2.
compressed column storage (CCS) format (also known as the Harwell-Boeing format), where the data is “compressed” column-wise.

For brevity's sake, we will discuss only the first format. Given a sparse, 2D, and (typically) non-symmetric array A that is  and has ℓ non-zero entries (where ℓ is typically much smaller than ), common CSR implementations store A's non-zero data via the following three, 1D arrays:
1.
val, an ℓ long array that stores A's non-zero values as A is traversed row-by-row;

2.
row_ptr, an  long array (where the last entry of row_ptr is ) that stores the indices of val's entries that start rows in A; and

3.
col_ind, an ℓ long array that stores the column indices of A that correspond to val's entries.

For example, consider the sparse, 2D array is shown in Fig. 14. The array would be stored in CSR format as follows: MATAR provides CPU and GPU implementations of both CSR and CCS data structures.

Fig. 14
Download : Download high-res image (41KB)
Download : Download full-size image
Fig. 14. An example of a sparse, 6 × 6 array with 17 non-zero entries.

5. Parallelization
MATAR is built with performance in mind and with ease of use for developers and scientists alike. In following with that trend, we wanted to pair it with a technology that also focused on performance as well as portability. Kokkos [11], [20] was the obvious choice for this coupling because it provides a programming model that focuses on similar concepts. The objective of Kokkos is to create an environment where the same code (or nearly the same) can be run with high performance on multiple CPU and GPU architectures. For MATAR specifically, we incorporate it as a means to target many different GPU architectures on diverse HPC systems. By leveraging Kokkos, MATAR is able to provide complex data structures to users with a simple interface.

5.1. Adding Kokkos to MATAR
Extensive collaboration with Kokkos developers was done to incorporate Kokkos inside of MATAR while still providing a simple interface to users. A large portion of this work involved providing MATAR class data and functions to a user that hides all GPU pointers and memory management, as well as complex indexing, from the inheriting libraries. GPU memory does not have to be managed by a MATAR user, and even some costly initialization is parallelized and optimized inside of the MATAR library. Specific class functions must be labeled with the Kokkos macro KOKKOS_FUNCTION for class functions to return data members on the GPU. The data structures themselves are allocated and initialized almost exactly as they are for their non-Kokkos counterparts, using Kokkos Views (not to be confused with our MATAR Views) instead of the C++ arrays.

The key difference between the MATAR classes containing Kokkos and those that do not occur within the Ragged data structures. Here, the constructor has preliminary calculations that are necessary for keeping track of the ragged array start indexes. Ideally this is a simple conversion of for-loops to Kokkos parallel loops. However only the latest Cuda version supports lambda capturing inside of class functions, and even then these lambdas (extending further to Kokkos parallel for) are not allowed in a class constructor. In order to preserve the functionality and appearance we present in the other MATAR classes, extra background work within MATAR had to be done such that initialization is shown to the user in the same way as the other classes. Furthermore, to get the latest Kokkos class functionality, we need to use GCC 9 coupled with Cuda 11.0 to work with C++ 17.

5.2. Virtual functions
MATAR has the ability to hold data types that differ from a simple integer or double type. When creating a library of data structures, as we have done here, it is important to demonstrate the ability to contain data types that are more complex. We show the ability of MATAR to hold class data types, including those involving virtual functions. This is fairly trivial on a CPU, but is a complicated process when we involve a GPU. Kokkos views have the ability to hold class data types by copying all of the class attributes and functions, including constructors and destructors, at the time of instantiation. However, virtual functions are not able to be copied at this time. Extra steps need to be taken to ensure that the virtual functions and their derived counterparts are seen on the device (GPU), rather than on the host (CPU). The Kokkos framework gives an adequate explanation for this problem specific to Kokkos, which we echo here for simplification.

The problem is that the class is initialized on the host, so the object points to a host VTable. When these objects are accessed on a GPU, an error occurs because the device has no knowledge of that host pointer. The class needs to be initialize on the GPU itself so that the virtual function pointers hold a GPU pointer value. Unfortunately, this does not occur when creating a Kokkos view of that class type, because the class is being initialized on the host for that Kokkos GPU view. The situation becomes even more complex when we are working with a derived class that lies within a different class. The solutions accommodating this situation are obviously out of the scope of most applications, even though it is a common application design. We have made the process as simple as possible for a programmer using MATAR. This is especially helpful when the abstraction is extended and we are working with derived classes inside other classes, a common situation in multiphysics applications. Algorithm 1 shows the steps to ensure correct pointer location.

Algorithm 1
Download : Download high-res image (126KB)
Download : Download full-size image
Algorithm 1. Allocating polymorphic classes with virtual function classes in MATAR.

6. Results
One of the goals of MATAR is to be as or more performant than standard C/C++ data structures (such as linked list, arrays, vectors). “More performant” can be measured by a variety of metrics. For example, we could see an improvement in run time of the application or lower data volume. Here we show some performance results between MATAR and dynamically allocated C++ arrays with the performance metrics: memory read bandwidth, memory write bandwidth, total memory bandwidth, advanced vector extensions (AVX), and double precision (DP) MFlops/s. To obtain the performance metrics mentioned, our performance tool of choice is likwid (Like I Know What I'm Doing) [38], a Linux-specific suite of command line tools that use hardware counters to let programmers measure and report information about an application. Specifically likwid-perfctr version 4.3.4. We use the marker API mode, where we section off and gather data only on the kernels of interest; parts of the code such as the initialization are not measured.

Timing studies and performance diagnostics were gathered on the Babel STREAM Benchmark [10], a matrix-matrix multiply (MMM), a matrix-vector product (MVP), and a contact detection algorithm in the HOSS geophysical application. LANL's HOSS software integrates solid mechanics using the finite-discrete element method and computational fluid dynamics. HOSS is capable of simulating high strain rate events in harsh subsurface environments for applications including oil and gas, construction, mining, and defense. This provides a real-world test of the technique in a multi-physics, production application.

All CPU tests are run on an Intel Skylake Gold CPU with a 2.10 GHz clock speed using gcc/9.2.0. Details for each test are given in their respected subsection and detailed examples of how to reproduce the results are available on the GitHub site for MATAR.

With these tests, the focus is on showing that (1) MATAR multidimensional data structures that are contiguously allocated in memory are faster than traditional, dynamically allocated C++ arrays, and (2) intricate sparse MATAR data structures are more efficient than traditional sparse representations in a real application.

6.1. CPU
In the CPU performance portion, we focus on understanding and comparing the baseline run time performance, bandwidth, data volume, and vectorization of MATAR CArrays and ViewCArray against a traditional dynamically allocated C++ array (denoted by Trad. Array in figures and tables following) using the Babel Stream Benchmark. A matrix-matrix multiply (MMM) and matrix-vector product (MVP) are also tested because these operations are heavily used in numerical methods. These operators are simple yet well suited to validate the effectiveness of MATAR's data-oriented design methodology. The matrix data structure is not presented because the only difference between MATAR arrays and matrix is the starting and ending indexing (refer to section 3); the array and matrix are allocated identically in memory. The Fortran-style is also not presented in the Babel Stream benchmark for the same reason. The results from CArray are sufficient to show how MATAR's design ideology compares against traditional arrays, since all of MATAR's data structures are contiguously allocated and accessed. Contiguous memory, whether it be Fortran-style (column-wise) or C-style (row-wise) is expected to perform similarly on the Babel Stream benchmark. Therefore the results of the C-style array types can be extended to matrix types and Fortran style data structures.

6.1.1. Babel STREAM benchmark
The STREAM benchmark is a set of simple kernels (copy, scale, sum, triad) to test the theoretical sustainable memory bandwidth in MB/s. The arrays are allocated on the stack and the size must be large enough to require storage in main memory. In our case, since we want to compare the differences of contiguous memory and dynamically allocated memory, we followed the modified Babel STREAM, which includes a dot product kernel and arrays allocated on the heap [10], [28].

The results for the Babel STREAM on a MATAR CArray, a ViewCArray of a traditional array, and traditionally dynamically allocated C++ array in 1D and 3D are presented. The 1D array is meant to serve as a proof-of-concept that we are at least up to par with traditional arrays, and the three-dimensional case will give a representation of MATAR's capabilities for the intended use case. The array size for the 1D array is 16,777,216 which is large enough to be in main memory and divisible by eight, making it ideal for vectorization. For the 3D case, each dimension is 256 (
), so the amount of work being done is equal to the 1D case.

From Table 1, MATAR ViewCArray and CArray are slightly under performing the traditional C++ array in the copy and scale kernels and vary in the sum, triad, and dot product kernel. The copy kernel is the slowest for both the CArray and


Table 1. Run time (in milliseconds) of the 1-dimensional BabelStream Benchmark Test (size 16,777,216). The results show some variation between the MATAR CArray and ViewCArray. Both are slower in the copy and scale kernel, but the ViewCArray is faster than the CArray in the dot product. Overall, there is no significant overhead from using MATAR for 1D memory.

Data Structure	Tests	Avg.	Min.	Max.
Trad. Array	Copy	20.064	19.905	20.164
Scale	20.262	20.079	20.425
Sum	30.263	29.988	31.880
Triad	30.370	30.305	31.5794
Dot Prod.	23.474	23.275	23.766

CArray	Copy	20.648	20.433	22.572
Scale	20.399	20.195	23.213
Sum	30.069	29.911	33.061
Triad	30.557	30.430	32.135
Dot Prod.	23.847	23.636	25.169

ViewCArray	Copy	20.573	20.426	20.733
Scale	20.743	20.606	21.028
Sum	30.438	30.285	31.659
Triad	30.359	30.311	30.514
Dot Prod.	23.331	23.117	24.573
ViewCArray. Even with these minor variations it is clear that there is no significant overhead from the implementation of the MATAR data structures.

The run time results for both the 1D and 3D case are shown in Table 1, Table 2. The test was run 101 times, but the average was computed from the 
 to the 
 time; the first timing is ignored since the cache is warming up. The reported times are using the standard chrono::steady_clock and is truncated after five significant digits.


Table 2. Run time (in milliseconds) of the 3-dimensional Babel STREAM Benchmark Test (size 2563 total). The results in the table show MATAR excels with operations on higher dimensional data. Every kernel excluding the dot product is about 40% faster. The decrease in run time is most likely attributed to the contiguous memory layout and maximizing cache usage. In general, C++ multidimensional arrays are not guaranteed to be continuous and are often scattered throughout global memory.

Data Structure	Tests	Avg.	Min.	Max.
Trad. Array	Copy	94.319	94.013	94.812
Scale	96.192	95.584	113.412
Sum	96.087	95.449	129.506
Triad	100.803	99.700	101.518
Dot Prod.	31.222	31.443	31.465

CArray	Copy	56.812	56.765	57.185
Scale	57.160	57.111	58.313
Sum	60.867	60.759	61.967
Triad	58.370	58.318	58.679
Dot Prod.	23.655	23.460	23.972

ViewCArray	Copy	55.459	55.344	56.305
Scale	55.902	55.864	56.294
Sum	58.298	58.218	59.493
Triad	58.809	58.756	59.244
Dot Prod.	23.686	23.526	23.938
Table 2 shows the timings for the 3D test, and we can see MATAR CArray and ViewCArray are the fastest for all kernels. Both MATAR CArray and ViewCArray are 40% faster over all the tests except the dot product, which is about 24% faster. Fig. 15 shows the results for the triad kernel. We expect the speed-up in the run time with MATAR to be significant with the higher dimensional data structures because in C++, as shown in Fig. 4, dynamically allocated multidimensional arrays are not guaranteed to be contiguous and are often scattered throughout global memory. The performance benefit shown in Table 2 is likely due to the contiguous memory allocation. The data is laid out in memory in the order it will be used therefore maximizing cache usage, needing less memory loads per operation and hence, we see a decrease in run time.

Fig. 15
Download : Download high-res image (108KB)
Download : Download full-size image
Fig. 15. Memory bandwidth and data volume for the 3D triad kernel. Both the CArray and the ViewCArray outperformed the traditional C++ array by about 40%, yet the bandwidths rate are on the opposite side of the spectrum. The ViewCArray has the highest bandwidth while the CArray has the lowest. The traditional array has the highest data volume overall.

Explicit #pragma omp directives were included for all tests in each dimension and vectorization flags are added automatically using a FindVector.cmake module. This vector module attempts to optimize the compiler flags based on the architecture [33]. For each 1D data structure, all of the double precision flops were vectorized for the scale, sum and triad kernels, however consistent vectorization for the 3D case was difficult to achieve. One would expect to see vectorization for all dimensions of the MATAR CArray's and ViewCArray's since multidimensional data is unrolled and allocated as a 1D contiguous array (see Fig. 8).

Further experiments were done with the high-dimensional data to understand the vectorization inconsistencies. In Table 3, the triad kernel is the tested for 3D and 4D arrays on the gcc/9.2.0 and Intel/19.0.5 compiler. The size of the 4D tests is of size 64 in each dimension (
). The triad was run on different compilers because it is the most arithmetic intensive test that was able to be vectorized (as seen in Table 1). For the test in Table 3, experimentation with the #pragma omp directives was done. We found that vectorization is achievable on the gcc compiler without the pragma, whereas the Intel compiler vectorized more with the pragma directives.


Table 3. Megaflops per second for the Intel and gcc compiler on a 3D and 4D triad example. The Intel compiler required #pragma omp directives, whereas the gcc compiler did not. All FLOPS were able to vectorize as shown by the vectorization (AVX) numbers being equal to the double precision (DP) numbers. The large variation in FLOPS is due to how the compilers implement the loops. Future research needs to be conducted on the compiler and vectorization flag sensitivities for both MATAR and multidimensional traditional arrays.

gcc/9.2.0	Intel/19.0.5
DP	AVX	DP	AVX
3D	CArray	469.69	469.69	565.52	565.52
Trad. Array	595.12	595.12	4409.12	4409.12
View	466.08	466.08	514.93	514.93

4D	CArray	520.10	520.10	5532.28	5532.28
Trad. Array	664.04	664.04	4548.79	4548.79
View	811.85	811.85	6087.59	6087.59
It is still unclear why MATAR and traditional arrays are compiler sensitive to the pragma directives. Obtaining consistent vectorization is a complicated task and merits its own research, and the scope of this paper will not cover why we see these compiler sensitivities. That being said, from experience using MATAR in an applied setting, the data structures vectorize well once the correct combination of compiler flags (per compiler version) is found. A deeper analysis of the individual compilers and vectorization flags is left for future studies.

For all test cases, the traditional array has the most amount of data volume, meaning the hardware had to do more loads since the data was not contiguous in memory. The higher data volume could be a strong indicator as to why the traditional C++ array is significantly slower than its counterparts. For the higher dimensional data structures, the data has a greater chance of being scattered throughout memory (recall Listing 3), therefore increasing the amount of memory loads per operation.

6.1.2. Matrix operations performance
Tensor inner products are a common computation in many numerical methods [2], [30]. Therefore, a timing study on a simple yet heavily used operation such as a matrix-matrix multiply (MMM) and matrix-vector product (MVP) is important. The matrix size for this test is  (32002 = 10,240,000), and the vector is .

Both the MMM and MVP tests include an extra example: a MATAR CArray × FArray. In matrix multiplication, the right matrix is traversed column-wise as shown in Fig. 16, which matches the memory layout of a Fortran array.

Fig. 16
Download : Download high-res image (14KB)
Download : Download full-size image
Fig. 16. The figure shows the order of a matrix-matrix multiply. Each element of the resulting matrix is the inner product on a row in the left matrix and column in the right matrix.

Table 4 shows the run time for the MMM test. The combination of a CArray and FArray performed the fastest while the CArray performed the slowest. Although the CArrays are contiguous in memory, it was expected that the CArray would perform poorly in this case. In order to maximize the performance with MATAR CArrays, the access patterns should match how the CArray is unrolled in memory, which is row wise. However, the right matrix in a MMM is traversed column wise. Elements already loaded into cache are not used and more memory loads are needed in order to obtain the column elements needed for each matrix product computation. It should be noted that if we perform the same test, but in a more memory complex setting the Trad. Array becomes fragmented in global memory and the performance can be slowed down drastically. Fig. 17 illustrates the actual access pattern for the right matrix does not match the optimal access pattern of the CArray memory layout. In fact, the access pattern is identical to the memory layout pattern for the FArray. We can see from Fig. 18 that the CArray test had the highest amount of read data volume and overall data volume, showing the extra memory fetches needed to obtain for the correct elements. The significant slow down in the CArray test raises a key point that contiguous memory alone does not guarantee better performance; contiguous memory must match the access pattern to be optimally performant.


Table 4. Timing results for Matrix-Matrix Multiply (3200x3200) in seconds. The combination matrices are the fastest while the CArray ended up being the slowest. This is likely due to the expected row-access that the CArray layout expects. This test showed that accessing pattern and contiguous memory layout is key for the best performance.

Data Structure	Avg.	Min	Max.
Trad. Array	88.54	88.30	89.76
CArray	145.74	145.73	145.77
C x F	39.72	39.65	39.93
Fig. 17
Download : Download high-res image (25KB)
Download : Download full-size image
Fig. 17. The figure shows a 2D CArray. As explained in section 3, the memory layout for a CArray is contiguous in memory row-wise. In order to maximize cache usage, the access pattern should be row by row. However, for a matrix-matrix multiply operation, the right matrix traverses the columns. Elements already loaded into cache are ignored and more memory loads are needed per calculation. The bottom array shows the actual access pattern for the latter matrix.

Fig. 18
Download : Download high-res image (101KB)
Download : Download full-size image
Fig. 18. Memory bandwidth and data volume for the matrix-matrix multiply. This figure supports the claim that matching access pattern and contiguous memory layout is vital for a performance benefit. The CArray has the highest data volume (read and overall). The access pattern for the right matrix does not match the memory layout of the CArray and in turn, more memory loads were needed in order to obtain the correct values for the operation.

Continuing the analysis of the bandwidth and data volume from Fig. 18, we see the C × F had the highest bandwidth, but the data volume closely matches the traditional array.

Now we look at a Matrix-Vector Product (MVP) with the same examples as the MMM: a traditional dynamic array for both the matrix and the vector, a CArray matrix and CArray vector, and a CArray matrix and FArray vector. The operation order for the matrix-vector product is shown in Fig. 19. Unlike the results from the MMM, this time there should not be a significant difference between the two vector options since the memory allocation and access pattern for a 1D FArray and CArray are identical. In Table 5, traditional array stands for a dynamically allocated 2D array (matrix). The 1D array (vector) CArray results are for the CArray vector, C×F is the CArray 2D matrix and the FArray vector.

Fig. 19
Download : Download high-res image (9KB)
Download : Download full-size image
Fig. 19. Figure showing the order of a matrix-vector product. Each element of the resulting vector is the result of an inner product with each row of the matrix and the column vector. With the vector being a column, we are inclined to think a FArray is more suitable. But for a 1D case both the memory allocation and access pattern for the FArray and CArray are the same.


Table 5. Timing results for the Matrix-Vector Product for a matrix size of 3200x3200 in milliseconds. The CArray and FArray vector combination ended up performing the fastest. Both MATAR data structures were faster than the traditional 2D C++ array by a small amount.

Data Structure	Avg.	Min	Max.
Trad. Array	12.91	12.83	13.20
CArray	12.24	12.10	13.66
C x F	12.03	11.98	13.20
Table 5 shows the results for the matrix size 3200 × 3200. Both MATAR data structures are faster than the traditional array, by a small amount, but we see a deviance between the CArray and FArray vector. As mentioned, we should not expect a difference between them because the 1D memory layout and access pattern is the same. In practice, if the data access pattern is known, there should be some consideration for creating the appropriate data structure (i.e. FArray compared to a CArray).

6.1.3. Ragged data structure performance
One of the more novel contributions of this work are the ragged data structures in MATAR. These data structures allow for efficient representation of the types of ragged data that appear in many numerical methods in computational physics such as [2], [7], [26]. This issue also arises when there are multiple types of physics or material present in a simulation. Having a way to represent “ragged” data as efficiently as possible is important in computational physics applications.

The STREAM benchmark test was used to analyze the performance of MATAR's Ragged-Right array and a ragged 2D traditional C++ array. The Trad. Array was a dynamically allocated 2D array with varying sized columns. The number of rows was set to be 64,000 while the size of the columns varied randomly from 6 to 20,004. The average number of columns each row had was 10023.7 for this particular test. Table 6 displays the run time and Table 7 displays the double precision flops and vectorization.


Table 6. The run-time for the Ragged-Right (RR) STREAM Benchmark Test. Units are in milliseconds.

Data Structure	Copy	Scale	Sum	Triad
RR MATAR	810	810	1,170	1,190
RR Trad. Array	840	840	1,160	1,240

Table 7. Double Precision Flops and Advanced Vectorization Extension for the Ragged-Right Stream Benchmark. We see both the traditional array and the Ragged-Right were able to vectorize their flops. Flops are in hundreds.

Data Structure	Flop Type	Copy	Scale	Sum	Triad
RR MATAR	DP	2.58e-03	3.56	2.80	8.86
AVX	0	3.56	2.80	8.86

RR Trad. Array	DP	1.27e-04	2.37	4.64	4.23
AVX	0	2.37	4.63	4.23
The MATAR Ragged-Right (RR) data structure is faster than the traditional array in the copy, scale and triad kernel, showing an increase of about 2%. Since the traditional array is dynamically allocated and every row has a varied column size, the data may be more scattered throughout memory leading to a slower performance. This improvement in memory coherence is more commonly observed for the GPU runs. A simple copy operation is roughly seven times faster on our ragged data structure than on an equivalent 2D dense array.

The compiler was able to vectorize both the traditional array and the MATAR RR for the scale, sum and triad kernel. Ragged-Rights are initialized and accessed like a 2D object using (i,j) where i is the fixed number of rows and j is the varying column size for each row. In order to collapse two for loops using pragma directives, the dimensions need to be constant. Therefore a #pragma omp directive is placed on the inner loop.

Fig. 20 shows the memory bandwidth and data volume plots for the triad benchmark. The figures show some inconsistency between the bandwidth, data volume and run time, similar to the 3D stream benchmark. In the copy and scale kernels, the bandwidth is higher for the Ragged-Right but the data volume is lower. We associated a lower data volume with a decrease in run time in the MMM and MVP, but in this test case for the sum and triad kernel, the same does not hold.

Fig. 20
Download : Download high-res image (104KB)
Download : Download full-size image
Fig. 20. Memory bandwidth and data volume for the Ragged-Right(RR) triad kernel. The RR is about 3% faster- the largest speed up compared to the other test. However the bandwidth and data volume are higher for RR than the traditional array.

It is unclear why the sum kernel is the only test where the Ragged-Right is slower, since the Ragged-Right showed the best speed up in the triad kernel. Overall, the Ragged-Right arrays were faster and the in-depth analysis regarding the data volume and bandwidth is limited by how the profiling tool is using the hardware counters.

Table 8 shows metrics related to the L2 cache performance. We look at the 1D and 3D data structures, as well the Ragged Right, comparing MATAR to the traditional array. The two areas of interest are overall bandwidth, which includes read and write speeds, and cache misses. The bandwidths were comparable across the board, with the exception of the ragged right structures. As expected, the MATAR variation more than doubled the bandwidth of the traditional ragged right implementation. Additionally, MATAR data structures consistently have fewer misses than their traditional counterparts, sometimes close to an order of magnitude fewer. This memory performance helps give a deeper insight into the cause of other improvements we see with the MATAR data structures.


Table 8. Performance comparison of MATAR data structures and traditional arrays looking at of the L2 cache bandwidth and cache misses. Tests are done using the Triad operation.

Data Structure	Bandwidth (Mbytes)	Misses
MATAR	1D	3905	9.18e04
3D	3439	2.26e05
RR	6872	6.10e05

Trad. Array	1D	3871	2.59e05
3D	3251	2.36e05
RR	3414	1.02e06
To verify the potential gains of using MATAR's dynamic ragged data structures over linked lists, we looked at the HOSS geophysics application [25], specifically within the contact detection algorithm [34]. Within HOSS, every element within a simulation has its own linked list that contain the neighbors of that element. As the simulations progress, nodes from the linked list are inserted if elements come into contact or removed if they are no longer in contact. For a more in-depth understanding of the contact detection algorithm, we refer the reader to [34]. Two separate code bases were maintained, (1) the existing code base that used the linked list, and (2) a modified code base that used MATAR's DynamicRaggedRight array (DRRA) data structure. The size of the test problem is 10 million elements, with the input file containing the cartesian coordinates for the elements that form the cells and the points that are allowed to freely move.

To compare the performance of the contact detection algorithm with the two data structures we used the command line tool likwid to gather information about the bandwidth, data volume, and vectorization. More information about likwid and how it was used can be found in section 5. Both codes were compiled with gcc-9.2.0 and ran on an Intel Xeon Gold 6152 CPU (2.10 GHz), without any accelerators.

The performance between the linked list and the dynamic ragged right data structure was evaluated for just one iteration of the HOSS contact detection algorithm to evaluate the difference in time it takes to build the data structure in the first test, and the second test was done to quantify the difference in speed between the two structures while they are being used. To simulate use in a production run, row statistics (row average, row minimum, row maximum, and row sum) were calculated over each row of the respective data structure. This was repeated for 10,000 iterations to get reasonable run-times.

Table 9 shows the results for one iteration of HOSS during the building of the data structures. The time it takes to build each structure was comparable, with the dynamic ragged right array showing a slight improvement over the linked list. Table 10 and Fig. 21 show the run-time and data results for using the different data structures. Fig. 21 shows that the dynamic ragged right array only had to load half the memory of the linked list, which is expected to speed up the code drastically, shown by the 44% decrease in run-time.


Table 9. LIKWID results from running the contact detection algorithm on competing HOSS implementations. The dynamic ragged right array is about 3% faster for building and initializing the data structures.

Performance Metric	LL	DRRA
Run time (RDTSC) [s]	16.1768	15.7079
DP MFLOP/s	119.9842	123.7221
AVX DP MFLOP/s	0.7669	0.8489

Table 10. LIKWID results from computing row statistics on competing HOSS implementations over 10,000 iterations. We see a drastic improvement in run time with the DDRA over the linked list. The increased performance is due to the contiguous memory access, which minimizes the amount of memory that has to be loaded.

Performance Metric	LL	DRRA
Run time (RDTSC) [s]	580.4707	325.6728
DP MFLOP/s	453.9550	568.5682
AVX DP MFLOP/s	0	0
Fig. 21
Download : Download high-res image (90KB)
Download : Download full-size image
Fig. 21. Memory bandwidth and data volume for the HOSS stress test. The linked list has higher bandwidth and data volume for the stress test (10,000) runs. The greater data volume is attributed to the linked-list data being scattered throughout global memory. When the dynamic ragged-right array is allocated contiguously, less memory loads are needed. Note, the memory write bandwidth and data volume is significantly smaller than the read and overall, so they are not shown in the plots.

The DynamicRaggedRight was used in the HOSS code on a 2D test case. The run-time of the contact surface method, which is one of the most expensive methods in HOSS, was reduced by 40% with the Intel compiler and by 55% with the gcc compiler. The overall run-time of the HOSS code reduced by 20% (with Intel) and 25% (with gcc). These results indicate that the overall HOSS code and similar algorithms that need to keep track of element neighbors that routinely change will benefit from using dynamic ragged arrays in place of linked lists. More importantly, dynamic ragged arrays in MATAR offer features that linked lists can not easily handle. Linked lists are not portable across multiple computer architectures and they are difficult to parallelize. MATAR was designed for portability and parallelization. Also, the elements of a dynamic ragged right array can be accessed as a 2D array, i.e. (i,j), eliminating the sorting overhead that most linked lists require.

6.1.4. QR decomposition
Now we study the performance of the data representation on a QR decomposition. Given matrix A such that A∈ 
, then A can be factored into the product of orthonormal matrix Q∈ 
 and upper triangular matrix R ∈ 
; A = QR. For this test, the orthonormal column vectors of Q are obtained using the Gram-Schmidt process. There are other variations to obtain Q and R by using Givens rotations, or Householder transformations as shown here [6].

The Gram-Schmidt process orthogonolizes a set of vectors by the means of vector projection. For QR, the columns of A are orthogonialized [18].(1) 
  We define the projection of a vector u onto a vector a as:(2)
 
 where  defines an inner product. The Gram-Schmidt process is then as follows:(3a)
(3b)
(3c)(3d)
 
 Where the columns of the intermediate matrix U are orthogonal. Let 
, then Q and R are defined as:(4) 
 (5)
 

For this example, the matrix A comes from a 1D discretized Laplacian operator with homogenous Dirichlet boundary conditions.(6)
 
 Table 11 shows the average, minimum, and maximum elapsed time in seconds of the QR algorithm for three different sizes. The algorithm was repeated five times in order to obtain the average. Since we are working with a square matrix, 
, the total problem size is given by 
. For the three sizes shown in Fig. 22, the FArray is the most efficient data representation while the Carray is significantly underperforming. The CArray exhibits a similar slow down in the Matrix-Matrix example, confirming our observation that a mismatch of memory layout and access pattern will have a negative impact on performance. The QR algorithm works with the columns of matrix A, and with a row representation, so more fetches are needed to load the correct values.


Table 11. Run time results in seconds of the QR decomposition on an A∈ 
 matrix using MATAR and dynamic arrays. On average, the FArray out-performed the traditional array by at least 50%. The CArray under performed significantly as the dimension increased, following the same behavior in the matrix-matrix multiply example.

N	Array	Avg.	Max.	Min.
1024	CArray	8.718	8.761	8.699
FArray	1.591	1.599	1.55
Reg.	5.046	11.214	2.212

2048	CArray	133.575	134.575	133.050
FArray	13.207	13.216	13.200
Reg.	27.920	59.497	19.887

4096	CArray	2260.075	2267.294	2255.076
FArray	106.172	106.192	106.149
Reg.	249.813	416.813	208.099
Fig. 22
Download : Download high-res image (172KB)
Download : Download full-size image
Fig. 22. Average elapsed time of the QR decomposition for the different data representations. Note the total size of the problem is N2. As the size increases, the FArray remains superior to a regular dynamic C++ array. The under performance of the CArray is due to the row-layout that is not ideal for an algorithm that works with the columns of the matrix. A similar situation was seen with the Matrix-Matrix product.

Given that many linear algebraic algorithms are based off of matrix multiplies and vector-vector operations, we expect to see similar run-time savings with MATAR on other like algorithms.

6.2. Kokkos performance
We show that our performance results are almost identical to using a traditional Kokkos View. For 1D views, this is not surprising, because the data structures being compared are identical. In fact, this is also true for our 3D example, because Kokkos does a similar data conversion to one dimension with their view allocation. Our breakthrough in doing these performance tests was discovering the importance of data layouts and loop indexing on the GPU. Kokkos provides many ways to control the data layout of the Kokkos Views on the GPU. Additionally, Kokkos allows the user to handle the order in which loop lambda arguments are applied. All of these play a factor in performance on the GPU, and MATAR adds additional options by choosing CArrayKokkos versus FArrayKokkos. Even with optimal GPU data layout, loop indexing matters. A Kokkos lambda of (int i, int j, int k) will not give the same performance as (int k, int j, int i) (assuming access to the same data structure). Using MATAR, we can keep this optimization complexity hidden by simply allowing the user to choose between CArrayKokkos and FArrayKokkos, depending on the target architecture. Table 12, Table 13 show the timings for the 1D and 3D parallel runs on both CPU and GPU.


Table 12. 1D (size 16,777,216) and 3D (size 256x256x256) STREAM Benchmark Test with Kokkos (CPU using OpenMP with max thread count). Units are milliseconds (averaged over 100 runs). Benchmark was run on an IBM Power 9.

Data Structure	Copy	Scale	Sum	Triad	Dot Prod.
1D	CArrayKokkos	6.01	8.10	5.99	6.16	6.12
Trad. Kokkos View	5.79	8.45	5.78	6.41	6.15
ViewCArrayKokkos	5.84	7.92	5.97	6.21	6.04

3D	CArrayKokkos	6.56	6.26	8.16	8.26	6.37
Trad. Kokkos View	6.57	6.49	8.51	9.00	7.08
ViewCArrayKokkos	6.56	6.26	8.16	8.26	6.37

Table 13. 1D (size 16,777,216) and 3D (size 256x256x256) STREAM Benchmark Test with Kokkos (GPU). Units are milliseconds (averaged over 100 runs). Benchmark was run on a Nvidia V100.

Data Structure	Copy	Scale	Sum	Triad	Dot Prod.
1D	FArrayKokkos	0.44	0.44	0.49	0.49	0.45
Trad. Kokkos View	0.45	0.44	0.49	0.49	0.45
ViewFArrayKokkos	0.45	0.44	0.49	0.49	0.45

3D	FArrayKokkos	0.45	0.45	0.50	0.50	1.83
Trad. Kokkos View	0.41	0.41	0.49	0.49	1.69
ViewFArrayKokkos	0.45	0.45	0.50	0.50	1.83
The Kokkos optimization within MATAR will be specific to whichever data layout is being chosen. As a result of our investigation into the optimal layout combinations, we discovered that not all mathematical operations can be optimized under a single data layout formula. For example, simple multiplications of array elements perform best with our FArraysKokkos (assuming default Kokkos settings for the GPU). However, a matrix multiplication performs best multiplying a FArrayKokkos by a CArrayKokkos. MATAR views are key in facilitating this optimization because a data structure can be declared as one layout, but can have a different view of the same data, resulting in a different layout, depending on which layout gives better performance. The caveat to this is that the user has to take special care to ensure the operations are still valid with the “new” access pattern. For example, a matrix multiply of AxB would need to become 
 if A and B are not symmetric to ensure the same result with the “opposite” layout since the opposite layout of A is 
. Fig. 23 Shows the scaling for the 2D matrix multiply across serial, CPU parallel, and GPU parallel runs.

Fig. 23
Download : Download high-res image (58KB)
Download : Download full-size image
Fig. 23. 2D matrix-matrix multiply run-time results on the IBM Power 9 architecture and Nvidia V100 GPU. The run-time is shown for CPU serial, CPU parallel (with max threads), and GPU parallel runs.

7. Summary and conclusion
We introduced a new C++ library for performance portability and productivity with dense and many sparse data representations.2

Multidimensional dense data are unrolled as a 1D array based on Fortran (i.e. FArray) or C++ memory layout (i.e. CArray). These data structures are allocated contiguously in memory. Sparse data structures were created for ragged arrays, dynamic ragged arrays, and sparsely populated arrays. The sparse data is unrolled as a 1D array and is allocated contiguously in memory or nearly contiguously in the case of the dynamic ragged right. Various tests were presented to validate the idea that contiguous memory layout paired with sequential data access improves run time performance. Furthermore, performance portability tests show favorable run times across CPU and GPU architectures for all data structures.

On a CPU, the MATAR data structures for dense arrays were competitive in the run-time to the traditional dynamically allocated 1D array and were 40% faster than the traditional 3D array. The ragged right data structures yield slightly faster run-times (by around a few percent) than the traditional dynamically allocated ragged right arrays on most test cases. It was shown that the MATAR dynamic ragged right data structure (DynamicRaggedRight) is nearly twice as fast compared to a dynamic linked list used in the HOSS contact-detection algorithm; furthermore, DynamicRaggedRight is portable across architectures. The DynamicRaggedRight in the HOSS code yielded a 40% (Intel) and 55% (gcc) reduction in the run-time for the contact surface method (one of the most expensive methods in the code) that resulted in a 20% (Intel) and 25% (gcc) reduction in the overall run-time for a 2D test case.

The performance portability of MATAR was demonstrated using the STREAM benchmark on a multi-core IBM Power 9 CPU and a V100 GPU. Run-time results using the MATAR dense array types compare favorably to the traditional Kokkos views on both architectures. The run-time results on the V100 GPU were 13X to 16X faster (depending on the test case) than using all cores on the CPU with the max number of threads. The performance portability of MATAR was further demonstrated using a large matrix-matrix multiply test case on the multi-core Power 9 CPU and a V100 GPU. Linear scaling was observed on the multi-core CPU. The V100 GPU results were 30X faster than using all cores on the CPU with the max number of threads.

MATAR data structures present a competitive advantage over traditional arrays and linked lists due to the performance gain from contiguous memory, portability to other architectures through Kokkos, and the addition of sparse data representations. MATAR has already been extensively used in a high order Finite Element library called ELEMENTS3 that includes routines for a wide range of numerical methods, including continuous and discontinuous Galerkin methods, and mesh data structures for linear and curvilinear elements. An early version of the code is discussed in [29] and the code can be found on GitHub.

Although the presented paper focuses on simple benchmarks to validate the theory behind data-oriented design, further work remains for obtaining consistent vectorization for MATAR array dimensions ranging from 1D to 7D on various compilers. A theoretical study and a deeper focus on the performance portability aspect can be explored with quantitative measurements such as the Pennycook metric [31].