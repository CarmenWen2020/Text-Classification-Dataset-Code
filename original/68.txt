Abstract
Artificial intelligence and algorithmic decision-making processes are increasingly criticized for their black-box nature. Explainable AI approaches to trace human-interpretable decision processes from algorithms have been explored. Yet, little is known about algorithmic explainability from a human factors’ perspective. From the perspective of user interpretability and understandability, this study examines the effect of explainability in AI on user trust and attitudes toward AI. It conceptualizes causability as an antecedent of explainability and as a key cue of an algorithm and examines them in relation to trust by testing how they affect user perceived performance of AI-driven services. The results show the dual roles of causability and explainability in terms of its underlying links to trust and subsequent user behaviors. Explanations of why certain news articles are recommended generate users trust whereas causability of to what extent they can understand the explanations affords users emotional confidence. Causability lends the justification for what and how should be explained as it determines the relative importance of the properties of explainability. The results have implications for the inclusion of causability and explanatory cues in AI systems, which help to increase trust and help users to assess the quality of explanations. Causable explainable AI will help people understand the decision-making process of AI algorithms by bringing transparency and accountability into AI systems.

Previous
Next 
Keywords
Explainable Ai

Causability

Human-ai interaction

Explanatorycues

Interpretability

Understandability

Trust

Glassbox

Human-centeredAI

1. Introduction
The role of algorithms in our lives is growing rapidly, from simply recommending online content or online search results, to more critical usages, like diagnosing human cancer risk in medical fields (Chazette and Schneider, 2020). Algorithms are widely used for collection, computing, data processing, and automated decision-making. By widely mediating and assisting in human decision-making, algorithms are becoming a ubiquitous part of human lives (Rai, 2020). While algorithms can offer highly personalized and relevant services and content, the effectiveness of artificial intelligence (AI) systems is limited by the algorithm's current inability to explain their decisions and operations to users. Complicated matters, such as fairness, accountability, transparency, and explainability (FATE) are inextricably linked to algorithmic phenomena (Ferrario, Loi, and Viganò, 2020; Shin, Zhong, and Biocca, 2020). Questions regarding how to safeguard the goals, services, and underlying processes of AI, who should be held liable for the consequences of AI, and whether AI is doing things that humans believe are ethical remain unclear and controversial (Dörr and Hollnbuchner, 2017). Thesesubjects, including FATE and ethical concerns regarding how we address and govern issues will be critical to AI development and innovation (Crain, 2018).

The black-box nature of algorithm processes had led to calls for research on explainability in AI (Castelvecchi, 2016; Holzinger, 2016), for example, explore the effects of explainability and transparency in the adoption of personalized news recommendations. Shin (2020) proposes an idea of algorithmic trust in terms of transparency in the content recommendation context. How users interpret algorithmic features and how users understand algorithm-based systems will be important questions to address as AI becomes more widespread (Shin, 2020). Particularly, this topic will be even more critical in news recommendation systems, where fairness, accountability, and credibility are inherent journalistic values (Dörr and Hollnbuchner, 2017). There has been increasing pressure to give the right explanation on how and why a result was provided (Hoeve et al., 2017). Despite their importance, few studies have examined the roles of explainability and interpretability in AI. Recent research on algorithm acceptance (Shin et al., 2020) suggests the heuristic role of explainability in the acceptance of algorithm/AI services. When users interact with an algorithm, they inevitably encounter issues of algorithm functions, which are essentially subjective insofar as they are dependent upon human judgment and context (Shin and Park, 2019). Thus, along with explainability, it is important to examine how users interpret such explanations, how they reason causality and causal inference (Arrieta, 2020), and the process through which people work to understand the issues in algorithms that are ambiguous and uncertain (Vallverdú, 2020). Against the increasing concerns about the opacity of black-box AI, this study operationalizes trust in algorithms by clarifying the role of explainability in reference to causability. It examines FATE in the context of algorithm processing and to clarify its roles and influence in the user interaction with AI. The following research questions (RQ) are formulated based on the research gaps:

RQ1: How does explainability play out in user heuristics and systematic evaluations for personalized and customized AI news?

RQ2:How do users perceive/evaluate the given explanations and how do we measure the quality of explanations?

RQ3:How do explainability combined with causability affect trust and the user experience with a personalized recommender system?

Findings reveal a user dual-process that users go through: a heuristic process by causability and a systematic process through explainability evaluating algorithm features and deciding how and whether to continue to use AI services during their evaluations. Whenever people encounter algorithms, they must make decisions as to whether, how, and to what extent to trustalgorithm-based services (Wölker and Powell, 2020). Heuristically, users evaluateexplanations based on their existing knowledge and beliefs, and partly based on their understanding of the algorithms. Users evaluate the quality of explanation based on their own level of interpretability and understandability (Samek, Binder, Montavon, Lapuschkin, and Muller, 2017). Systematically, users explore AI product information when evaluating algorithmic functionality. In the process, issues of FAT play roles as heuristic cues, triggering user trust. Levels and kinds of FAT are perceived as a function of user appraisal of explainability (Moller, Trilling, Helberger, and van Es, 2018). When such explanations are reasonable and understandable, users begin to accept FAT and trust the AI system.

The causal implications of trust and algorithmic explainability provide important directions for academia and practice. Theoretically, clarifying the role of explainability in AI would make meaningful contributions to the ongoing discussion of human-AI interaction (HAII; Sundar, 2020). Particularly, the human-interpretable heuristic processes of explainable AI (XAI) from a human factor's perspective is useful because they provide new ways of designing and developing causable XAI (Combs, Fendley, and Bihl, 2020). The findings contribute to theformalisation of the field of explainability and causability in HAII by showing how the concepts are conceptualized, by illustrating how they can be implemented in user interfaces, and by examining how the effect and the quality of explainability is measured (Samek et al., 2017). From a practical standpoint, the heuristic role of causability and the systematic dimension of explainability in algorithms lends strategic direction on how to design and develop XAI and user-centered algorithms in order to facilitate algorithm adoption in mainstream services. As the current AI models are increasingly criticized for their black-box nature, the roles of explainability and related causability will surely give insights into user confidence in algorithms (Shin, 2020).

2. Literature review
2.1. XAI: finding correlation and causation
XAI refers to machine learning and AI technologies that can offer human-understandable justifications for their output or procedures (Gunning et al., 2019). Explainability along with transparency are two very important elements related to XAI (Ehsan and Riedl, 2019). While there is no uniformly accepted definition of explainability in AI, it can be conceptualized as the ability to explain the way in which an algorithm works in order to understand how and why it has delivered particular outcomes (Arrieta, 2020). Humans, by nature, may wonder about the reasons why and how algorithmic decisions were made (Shin and Park, 2019). As the complexity of the AI systems and algorithms grow, people increasingly consider them as “black-boxes” that defy understanding in the sense that increasing amounts of specialized expertise and knowledge are required to understand the AI decision or performance (Castelvecchi, 2016). Increasing complexities result in a lack of transparency that hinders understanding and negatively influences trust. Non-expert end-users do not know how the exact cascades of algorithmic code resulted in a particular decision. The issues related to AI decision-making being a “black-box” are significantly worsened when dealing with ordinary users who lack technical knowledge and are still required to interact with AI systems. Most of the users have little visibility and knowledge of how AI systems make the decisions they do. Thus, explainability is critical in building faith, rapport, and bonding with AI, especially when it comes to understanding malfunctions and undesirable consequences. Explainability gives users assurance and confidence that AI systems work well, help developers understand why a system works a certain way, and safeguard against prejudice.

The essence of explainability lies in sense-making. The explain-"ability" of a technological system is often reliant on the human's capacity to make sense of its working. Thus, sense-making is a contextual process where the arrangement of situated epistemologies of the user and the system needs to take place. Nevertheless, the over-emphasized technical discussion of explainability in AI often leads to human's inability to understand the technicality. Explainability in AI is as much a problem in HAII as it is a problem in AI research. Recent studies have shown that AI with explanations allows users to have more confidence in an AI system, and have faith and trust in the algorithm results. For example, Lipton (2018) examined the interpretation of machine learning systems from a human perspective and identified transferability, trust, information, causality, and fair decision-making as key aspects. Rosenfeld and Richardson (2019) provided the taxonomy of explainability in relation to related factors such as interpretability, transparency, explicitness, and faithfulness. Anjomshoae, Najjar, Calvaresi, and Främling (2019) presented a systematic review and clustered the results with respect to the demographics, the application scenarios, the intended purpose of an explanation, and if the studies were grounded in a social science or psychological background. Alonso and De La Puente (2018) proposed a review of the system's transparency in algorithm framework, stressing the role of transparency in flexible and efficient human-robot collaborations. While transparency is becoming increasingly critical, the abstract quality nature of transparency along with fairness/accountability should be better understood and related to mechanisms that can promote it. An important first step is to identify user requirements in terms of explainable AI behavior. This study tests the relationship between causability and explainability and analyze its impact on AI quality.

2.2. Heuristics systematic process
In AI and algorithms, a user normally faces questions when encountering recommended results: What is it doing, why is it doing that, and what needs to be done? (Combs et al., 2020). These questions are closely related to users’ heuristic and systematic evaluation: how people interact with AI becomes an important question to justify for algorithm design and development. User heuristics regarding algorithmic qualities regard questions like: How do users figure out the qualities or features of AI? How do people perceive them and with what sense? Because algorithm-based content brings numerous competitive edges, it is essential to investigate what users’ a priori expectations are and how these expectations are realized. Also important is how users’ trust affects their emotions and satisfaction, which subsequently influences their intentions. The heuristic systematic model (HSM) is an appropriate tool for this task insofar as the model theorizes thatpre-behaviorsand post-experiencesinfluence user evaluation,whichinturn,lead tosatisfactionandintentions (Chaiken, 1980). HSM is used as a frame to trace user sensemaking with AI through understanding the role that the algorithmic features play in shaping users’ sensemaking of algorithmic explainability, as well as how users’ interpretations affect their sensemaking. As algorithms and AI afford people new experiences, HSM can be extended by integrating AI-specific dimensions as antecedents of trust and by incorporating accuracy and personalization as an algorithmic performance.

3. Hypotheses proposition: causability and explainability in AI
The proposed model includes users’ cognitive and emotional responses to causability-explainability in AIFig. 1. Causability is proposed as a predecessorof explainability, which is posited as antecedents of FAT.

3.1. How do users perceive/evaluate explainability? Sense making with AI
With the rise of automated algorithms in every sector of our lives, FAT is becoming a key concept in AI (Shin and Park, 2019). There are increasing concerns about the use of data, which may be shared illegally or abused by others for the sake of content automation. Automated data decisions may be incorrect, unfair, nontransparent, or unaccountable (Crane, 2016). Recommending contents/items requires a more detailed engagement with issues of FAT. Taken together, FAT brings up key considerations in the design and development of algorithm services (Shin et al., 2020). Algorithm services are basically designed to produce accurate predictions (Renijith, Sreekumar, and Jathavedan, 2020). How the processes are done, whether the results actually reflect user preferences, and whether the results are reasonably accountable remain unanswered questions. While previous studies have shown that fairness, transparency, and accountability determine user trust and subsequent attitudes/behavior (Shin et al., 2020), the effect of explainability remains largely unknown.

User awareness and understanding of how and why a particular recommendation is produced and how their input impacts the result have been found to be significant. Clear transparency and good visibility for relevant feedback improve search performance and satisfaction with recommendation systems. The work of Kizilcec (2016) shows that utilizing explanations can improve positive attitudes and overall satisfaction with a recommendation system. A user's confirmation level is associated with satisfaction in the context of technology adoption (Shin and Biocca, 2018). Numerous research have reported a causal relationship between explainability and assurance in the context of algorithm services (e.g.,Zhang, Wang, and Jin, 2014). Ehsan and Riedl (2019) show that human-like rationales promote feelings of trust, intimacy, rapport, and comfort in non-experts operating AIs. Based on the existing research, it can be rightly inferred that XAI would help users to understand the process and thus increase users’ faith in the system. People are inclined to use explainable systems because they would like to understand how data are collected, processed, and thus how recommendations are produced (Rai, 2020). When there is a transparent mechanism, users can increase their input data to improve recommendation outputs. Algorithm users are also able to understand the logic of a recommendation system (Renijith et al., 2020). Providers of algorithms are encouraged to ensure the accuracy and legitimacy of results in order to increase user trust.

H1: Explainability positively influences user perception of AI transparency.

H2: Explainability positively influences user perception of AI fairness.

H3: Explainability positively influences user perception of AI accountability.

3.2. Causability: quality of explainability
Related to explainability, causability emerges as a coupling concept with explainability, although, as yet, the relation has not been well researched. It is not clear whether users actually see explanations as a means to better understand AI systems. For this gap, Holzinger, Langs, Denk, Zatloukal, and Mueller (2019) conceptualize the notion of causability in the AI context based on a well-established usability scale. They defined causability as the extent to which an explanation of a statement to a user achieves a specified level of causal understanding with effectiveness, efficiency, and satisfaction in a specified context of use. Based on a previously developed usability scale, they develop a system causability scale. Causability is closely connected to explainability in the same way that usability encompasses measurements for the quality of use. Causability includes measurements for the quality of explanations generated by explainable AI methods. While explainability is a property of an AI system, causability is the property of users (Holzinger, Carrington, and Müller, 2020). In the journalism and media domain (e.g., news recommendation AI, algorithm journalism), it is important to enable readers to understand why algorithm journalism came up with certain news or content. For example, just like the Right of Reply in the traditional journalism, the Right of Explanations in AI has been highlighted (Goodman and Flaxman, 2017) and accepted as a legitimate right (as exemplified in the EU's General Data Protection Regulation). These rights can be nicely consonant with the idea of causability as it can provide an underlying rational basis for such new rights. There has been little research explicitly on the users’ views on the properties of explanations and on the perceived impact of explanations on FAT (Chazette and Schneider, 2020). Beyond the existence of explanations, it is important to examine how users really understand the given explanations. Given that user feedbacks are a key source of AI requirements, understanding their views and what they expect from explanations is critical. For this, Holzinger (2016) proposes a notion of quality of explanations from a user perspective; how to evaluate the quality of explanations provided by explainable AI systems. Based on the causability discussion by Holzinger et al. (2019); 2020), it can be postulated the antecedent role of causability to explainability as well as the mediating role in the path of FAT to trust.

H4:CausabilitypositivelyinfluencesexplainabilityofAI.

3.3. Mediating role of causability
Research into the role of explanation on trust and attitude has yielded consistent results that explainability plays a key role in user perception. Shin's finding (2020) highlights the importance of transparent explanation, specifically that explanations provide empirical validation that is a cue that users can interpret as transparency and accountability. The existence of the trust, along with explanation, is key to promoting technology acceptance (Ehsan and Riedl, 2019; Hong and Cha, 2013). As explanations have shown a significant effect in accounting for user attitude and trust, it is reasonable to think that causability would have similar roles. It can be posited that the effects of FAT on trust are mediated by causability, meaning that understandable explanatory cues also influence user trust. In the same manner, mediating effects can be also posited between causability and performance. The objectives of the mediation analyses are twofold: to determine the mediating effect of explanation in FAT and trust and to test the mediating effect of explanation on trust and performance expectancy.

H5: Causability positively mediates the path between transparency and trust.

H6: Causability positively mediates the relationship between fairness and trust.

H7: Causability positively mediates the path between accountability and trust.

3.4. Normative belief and trust
In the context of AI, trust is considered as the belief that a vendor's services and/or reported results are reliable and trustworthy, and that the vendor will fulfill obligations in an exchange relationship with the user (Shin and Park, 2019). In the news recommendation context, trust refers to a reliable belief in the accuracy of news recommendations (that is, the quality or state of being correct or the precision of news recommendations) and for user readiness to use the recommender system's capabilities. Thus, trust denotes how reliable and credible a system is. Many trust dimensions determine a user's decision to engage with technology, but few studies to date have researched algorithm services, particularly AI services.

People are inclined to use trustworthy systems because they are familiar with how data is collected, processed, and thus how recommendations are produced (Rai, 2020). When there is a transparent mechanism, users can revise their input data to improve recommendation outputs. Algorithm users are able to understand the logic of a recommendation system (Renijith et al., 2020). Providers of algorithms are encouraged to ensure the accuracy and legitimacy of results in order to increase user trust. Together, transparency, fairness, and accuracy play critical roles in algorithm services by improving user trust in algorithms (Shin et al., 2020). High levels of transparency in an algorithm can afford users a sense of personalization. Fair and accountable news affords users a sense of confidence, which, in turn, promotes a sense of satisfaction and continued use. User awareness and understanding of why and how a certain recommendation is generated have been found to be significant. Great visibility and clear transparency for relevant feedback improve search performance and satisfaction with recommendation systems. Using explanations can improve users’ overall satisfaction with a recommendation system (Kizilcec, 2016). Cramer et al. (2008) argue that a user's perceived transparency influences trust in the context of content recommendation. It has been confirmed a relationship between explainability and assurance in the context of algorithm services (e.g., Zhang et al., 2014).

H8: Perceived transparency positively influences the user trust in AI.

H9: Perceived fairness positively influences the user trust in AI.

H10: Perceived accountability positively influences the user trust in AI.

3.5. Algorithmic performance
The performance of algorithms is a process of making evaluative judgment about algorithms. Performance analysis of an algorithm depends upon two factors, accuracy and personalization (Shin, 2020). Algorithm systems represent a set of personalization features that help people search through a massive amount of information. Personalized content needs to be accurate as users expect personalized recommendations to match their preferences. Personalization and accuracy are the two important criteria determining a user's perceived utility of the system. Accuracy is about whether the recommender system predicts those items that people have already evaluated or interacted with; recommender systems with optimized precision will prioritize more related items for its users (Shin et al., 2020).

Overall, personalization can be a key attribute of an algorithm system (Soffer, 2019). A personalized news recommendation service acts as an information filter with the capability of learning a user's interests and preferences, according to their profile or history. When users get the sense that news recommendations are personalized to their needs, they rate the service useful, and feel more satisfied with the content (Kim and Lee, 2019). Users view the algorithm as easy to use and convenient as long as they perceive the recommended items or contents accurately. Empirical evidence has confirmed these relationships in various algorithm services (Li, 2016), in which personalization and accuracy are found to be determinants of trust and satisfaction (Shin and Park, 2019). As algorithms for content curation show people what is relevant to users, users consider AI qualities in terms of how accurate and how customized AI services are when it comes to actual use (Bedi and Vashisth, 2014). Thus, a final hypothesis regarding algorithmic performance are developed:

H11: Trust has a significant effect on the perceived performance of AI.

4. Methods
4.1. Data collection and sample
This study recruited a total of 350 individuals through online (Qualtrics) and offline (local universities) in exchange for monetary compensation and class credits. The data were merged and analyzed using SPSS AMOS. The sample was confined to respondents who had prior experience with algorithm services (automatic recommendation, content suggestions, online news aggregation, etc.). To warrant the reliability and the validity of responses, a series of validation confirm questions were added into the survey. Of the collected responses, 18 incomplete responses with missing information were excluded, resulting in the use of a total of 350 responses for data analysis. Among the participants, 54% were male. With regard to age, 29% were between the ages of 30 and 39, 49% were between the ages of 20 and 20, 13% were between the ages of 40 and 49, 8% were between the ages of 50 and 59, and 1% were over 60.

Respondents were invited to a media lab equipped with computers Fig. 2. Respondents were asked to surf, view, and read autogenerated news on algorithm-based sites for about 1–2 h (Fig. 2). The recommendation services are available through smartphones as well as computers. Participants can surf through the news/item recommendations through the sites. They were told that the news items/content were generated by algorithms and enabled by machine learning and artificial intelligence mechanisms. They were also briefed about FATE in the specific context of the algorithm because these concepts are complex and possibly outside of normal definitions for persons that do not specialize in algorithm research or practice. After the viewing and reading of media, participants were given surveys to complete. The respondents were hired through college classes related to digital media, usability, and algorithm services.

Fig. 1
Download : Download high-res image (238KB)
Download : Download full-size image
Fig. 1. Causability and explainability in human-AI interaction.

Fig. 2
Download : Download high-res image (374KB)
Download : Download full-size image
Fig. 2. Explainability and causability in AI recommendation system.

4.2. Scales and measurements
The scales used in this study comprise of 21 measurements. All of the measurements were developed from the human-computer interaction literature (e.g., Shin and Park, 2019) and user experience (UX) research (e.g., Konstan and Riedl, 2012). Measurements were mixtures of previously used metrics and modified metrics from previous versions. This was necessary because, in our analysis, some measurements needed to be changed to reflect salient features of algorithms and AI services.

Thirty graduate students with knowledge of and experience with using algorithm services or AI applications participated in a pretest over a three-week interval. Cronbach's α was utilized to examine the reliability of measurement items, their scales, and questions, and correlation coefficients were calculated to assess the concurrent validity of the instrument. Reliability values show acceptable levels ranging from 0.782 to 0.906 (Table 1). A confirmatory factor analysis (CFA) was calculated to validate the findings of the exploratory factor analysis, with analysis showing that the items had satisfactory factor loadings. The factor loadings for all measurements were significant, providing evidence for valid internal consistency. To assess validity, correlation tests were performed to determine reciprocal relationships among variables. A simple linear correlation (Pearson's r) was employed to assess the significance of observed relationships. The intercorrelations among the variables showed no signs of multicollinearity. The square root of the average variance extracted (AVE) from the construct was significantly higher than the variance shared between the construct and the other constructs in the model. Thus, discriminant validity is established. Taken all together, the data from the factor loadings, alpha-values, correlations, composite reliabilities, and AVE values for each construct suggest that the indicators account for a large portion of the variance of the corresponding latent construct and thus provide evidence for measurement modeling.


Table 1. Reliability and validity.

Variables	Mean	Standard Deviation	Cronbach's alpha	AVE	Composite reliability
Transparency	4.58	1.026			
4.63	1.134	0.855	0.763	0.906
4.40	1.071			
Accountability	4.32	1.229			
4.55	1.213	0.845	0.762	0.905
4.03	1.004			
Fairness	4.01	1.267	0.784	0.755	0.787
4.45	1.210			
4.06	1.066			
Explainability	4.05	1.210		0.719	0.771
4.02	1.276	0.769		
4.13	1.294			
Performance	4.39	1.134	0.784	0.714	0.882
4.05	1.295			
4.18	1.216			
Trust	4.35	1.145	0.906	0.841	0.940
4.22	1.074			
4.10	1.066			
Causability	3.89	1.234	0.782	0.795	0.894
3.65	1.400			
4.04	1.309			
In evaluating the model, goodness-of-fit indices were examined with their respective acceptable values. Selected goodness-of-fit indices were assessed with prespecified cutoff values–the chi-squared value per degrees of freedom (χ2/df), the normed fit index (NFI), the root mean square error of approximation (RMSEA), the Tucker-Lewis Index (TLI), and the incremental fit index (IFI). Most of the indices indicate a good fit and thus there is a high probability of a good fit(Table 2).


Table 2. Model fit indices.

Fit statistics	Model	Suggested value
χ2 (df)	2359/181	
p-value	0.000	< 0.05
RMSEA	0.067	0.05<x< 0.10
CFI	0.826	> 0.90
NFI	0.904	> 0.90
IFI	0.826	> 0.80
TLI	0.884	> 0.85
RFI	0.864	> 0.80
Akaike Information Criterion (AIC)	504.00	
Expected Cross Validation Index (ECVI)	7.169	
5. Results
5.1. Structural model testing
Structural path testing revealed that the relations drawn in the hypotheses were largely supported (Fig. 3 and Table 3). All the path coefficients were statistically significant(p<.001 or p<.05). Trust is significantly influenced by FAT, which is determined by causability and explainability. These factors altogether account for 58.0% of trust variance (R2=0.581). Performance expectancy values are greatly influenced by the trust. The model explained a significant portion of the variance in each construct.

Fig. 3
Download : Download high-res image (163KB)
Download : Download full-size image
Fig. 3. Mediating roles of causability in AI.


Table 3. Path results.

Paths	Standardized Coefficient	S.E.	C.R.	p	Supported
H1: Explainability → Transparency	0.762	0.050	13.727	***	Yes
H2: Explainability→ Fairness	0.535	0.028	2.306	.021*	Yes
H3: Explainability→Accountability	0.693	0.035	12.274	***	Yes
H4: Causability → Explainability	0.939	4.317	2.010	.044*	Yes
H5: Transparency → Trust	0.535	0.053	10.042	***	Yes
H6: Fairness → Trust	0.242	0.617	2.262	.024*	Yes
H7: Accountability → Trust	0.349	0.054	7.277	***	Yes
H8: Trust → Performance	0.911	0.060	16.201	***	Yes
*1.96: 95% (0.05), **2.58: 99% (0.01), ***3.29: 99.9% (0.001).

The strong paths imply a fundamental connection between trust and its antecedents. Given the significant effect of trust on performance expectancy, it would be desirable to examine the possible mediating effects of trust on other variables. For example, trust affords user allowance of the collection of more data, and in turn, more data yields better predictive analytics. Subsequently, users are more gratified with highly accurate and transparent results.

5.2. Tests for mediation
One of the main effects of the explainability of algorithms is viewed in the user's perception of trust. The mediating effects of the causability of algorithms is that user's perception of trust is increased along with algorithmic performance when people can easily understand the explanations Fig. 3.

A non-parametric bootstrapping approach was used to analyze the significance of the mediating effect. Mediation analyses assessed the indirect effect of explainability on the association of FAT and trust. The SPSS PROCESS macro was employed to conduct the mediation analysis (Preacher and Hayes, 2008). Bootstrapping techniques can be used when examining mediation to gain confidence limits for specific indirect effects (Hair, Hult, Ringle, and Sarstedt, 2013). Variance accounted for (VAF) is to evaluate the indirect effect and the value of greater than 80% is full mediation, while greater than 20% but less than 80% is partial mediation (per Hair et al., 2013). The 95% confidence interval for the indirect effect via explainability was obtained using bootstrapped resampling. Mediation is confirmed if such a confidence interval does not contain zero (Hayes, 2013). Table 4 shows the results of mediating effects with VAF values. There are partial mediations, which mean that explainability has partial effects on the relationships, which can be significantly reduced without explainability, but the relationships still hold.


Table 4. Results of mediation effect.

Effects	Direct effect (t)	Indirect effect (t)	Total effect	VAF (%)	Results
Transparency→Causability→Trust	0.22**(8.29)	0.01*(1.37)	0.26	49.73	Partial
Fairness→ Causability →Trust	0.24**(11.04)	0.06 (2.19)	0.34	33.53	Partial
Accountability→ Causability →Trust	0.27** (4.34)	0.15** (3.18)	0.35	49.23	Partial
Trust→ Causability →Perform.	0.32**(7.35)	0.07* (2.11)	0.39	31.54	Partial
*t>= 1.96 at p = .05 level; **t>= 3.29 at p=.001.

6. Discussion: bridging the gap of explain ability and human cognition
The model illustrates that interacting with algorithms engages a series of intersecting cognitive processes, wherein features of algorithms are used to formulate a heuristic for user motivation and to trigger user action when using AI services. The findings of this study offer interesting insights into the links between causability and explainability, and further the dynamics of heuristics, quality, and trust in algorithms. The findings of this study lay forth an argument that human-centered AI should be designed with social responsibility in mind, such as addressing FAT as well as a broader scope of human conditions like interpretability/causability.

First and foremost, this study examined whether and how causability plays a role in the user experience of AI journalisms. Using the causability measurements developed by Holzinger et al. (2020), this study not only demonstrates that causability represents the quality of explanations, but also shows that causability plays an antecedent role to explainability and an underlying part in trust. The findings provide conceptual clarifications between explainability and causability and further illustrate a use-case of the explanations of AI and the interpretations of humans. Thus, we can infer from the results that while explainability is related to the quality of an AI system, causability comes from the users who try to understand the explanations. Per Holzinger et al. (2019), human efforts to understand how an AI decision is made and evaluate the quality of the explanation is critical in AI development and machine learning. Providing explanations can be necessary conditions, but not necessary-sufficient conditions for glass-box human-centered AIs (Riedl, 2019). The identified paths regarding causability open a new window of opportunity for AI services. The high opacity nature of algorithms can be decreased, and trust can be established, which can enhance user acceptance and promote human-AI interaction.

Second, using the HSM as a theoretical backdrop, this study confirmed how algorithmic explanations influenced users’ trust and performance through two different routes of cognitive processing. It can be inferred from observed relationships that algorithmic features and service values are positively associated with trust. Perceived notions of FAT are positively associated with algorithmic performance through causability mediation. That is, users assess the accuracy and personalization of AI through a dual process; first through FAT heuristics and second via acceptance of the systematic process that proceeds through trust. Users process algorithm services both heuristically and systematically (Combs et al., 2020; Sokol and Flach, 2020). Heuristic processing involves the use of simplifying FAT assessment through given explanations to quickly permit assessment of service quality. Systematic processing entails the deliberative processing of accuracy and personalization through the interpretive process using causability. Trust connects the two processes linking heuristics and systematic mechanisms (Shin, 2020). This trust link can be a key clue about algorithmic qualities, algorithm experiences, and users’ interactions with AI. Certain algorithmic features afford users cues for trust, which allows them to interact with algorithms with feelings of effectiveness and efficacy. Per Sokol and Flach (2020), trust shaped through heuristic processing is more likely to have cognitive attributes that reflect the FAT assessment, whereas that shaped through systematic processing is more likely to have effects on performance evaluation due to the reliance on established FAT cues.

Third, the findings suggest the significant role of trust in HAII. With the pervasive role of algorithms in our lives, a question is how do people trust an algorithm's decision? How trust is formed and evolves in the course of interaction may provide valuable clues when designing and developing AI services. This is important because more and more people realize that algorithms are not neutral and that they may have human prejudices. People would like to understand how algorithms work, how their data are analyzed, and to what extent their results are fair. The model in this study provides a guideline for how trust is created and with respect to what factors. Although previous research has consistently confirmed the effects of trust in AI and algorithms (e.g., Bussone, Stumpf, and O'Sullivan, 2015), this study empirically demonstrates the role of trust in AI, its antecedents, mediating role, and heuristic-systematic process. Users get a sense of trust in algorithms when they are assured of their expected level of FATE. When users trust algorithm systems, they tend to believe that the contents are useful and convenient (Shin and Park, 2019). Trust significantly mediates the effects of FATE on users’ satisfaction. Satisfaction stimulates trust, and in turn, leads positive user perception of FATE. Higher satisfaction leads to greater trust and suggests that users are more likely to continue to use an algorithm. Affording more user trust may reassure users that their personal data would be used in legitimate and transparent processes, thereby producing positive trust of the AI algorithms, ultimately leading to heightened levels of satisfaction. Trust between algorithms and human agents is the underlying key factor in the heuristics and acceptance of AI. Trust serves as a bonding mechanism between humans and AI (Shin, 2020) and an essential drive force for improving algorithmic performance and thus creating human-centered AI.

7. Implications: how to overcome the black-box pit fall of AI
The impacts of this study are twofold, managerial and theoretical. Practically, the findings of the study have design implications regarding what AI practitioners should do to support effective HAII, specifically, how to implement effective explainability in the AI interface. Theoretically, this study confirmed the heuristic-systematic process together with the liaison role of user trust in AI (Ferrario et al., 2020). It is implied that algorithms should be designed with principles that AIs are part of a larger system consisting of users.

7.1. Theoretical implications: measuring the quality of explanation
This study contributes to the understanding of causability, explainability, trust, and algorithmic qualities in the context of AI. First and foremost, based on the conceptual foundation of causability by Holzinger et al. (2019)), the findings of this study highlight the importance and the role of causability-explainability, which is central for users to understand, trust, and appreciate the reasoning behind suggestions and predictions. The study identifies the antecedents of algorithmic features and the relations among them, user trust in AI, and tests the heuristic role of those antecedents and performance. This finding stands to contribute to theoretical advances by proposing how algorithmic trust is created and what effects of trust are present in AI users, and from there, how trust can be theorized, measured, and analyzed, with reference to AI qualities and features. Explainability provides users with a cue. Not only does explainability provide interpretable ideas of AI, but also it provides a cue for users evaluating FAT. In other words, the presence of explanations itself help users to heuristically interpret the outcomes of AI by turning black-box into glass-box; explainability itself serves as a cue revealing some level of transparency and accountability. The presence of an explanation can hold an AI system liable and accountable for their recommendations by ensuring that they are compliant to rules, help verify the system, make the best effort to produce the right decisions.

Previous technology acceptance models and the traditional notions of trust may not be applicable when seeking to explain users’ interaction with AI because the algorithm is drastically different from previously existing technologies. Previous models mainly focus on users’ adoption of well-known technologies, whereas AI is fairly new to users and they are unsure about what AI can do it for them. AI services represent a novel paradigm with a new ecosystem of socio-technical issues (Rai, 2020). Through conceptualization and development of the FATE scales, this study contributes to the efforts of how to ensure such issues in AI, how we can best utilize AI to support users, and offer enhanced insights while allowing users to avoid unfair and discriminatory situations, and how we can balance the demand for technological innovation and public interest with accountability and transparency for users. As AI becomes increasingly ever-present and everyday reality, FATE will be even more important. The finding of the relation of FATE to trust is particularly notable as it is a new attempt to establish such a relation. While the components of FATE have been considered critical factors for AI users (Shin and Park, 2019), how users process FATE information and how it influences trust remains unknown. The established relations between FATE and trust will be a stepping stone to further explore the role of FATE in AI design.

Second, the heuristic-systematic approach in the model advances the literature on the user experience of AI (algorithm experience), specifically the user cognitive process literature, by clarifying the dual roles played by causability and explainability and the underlying relationships among its closely associated measures (Konstan and Riedl, 2012; Shin and Park, 2019; Thurman, Moeller, Helberger, and Trilling, 2019). Our findings not only support the HSM's key argument that decision-making is largely influenced by heuristic cues (Chen, Duckworth, and Chaiken, 1999) but also provide additional insights for the positive trust plays a liaison role between heuristics and systematic processing (Shin et al., 2020). Previous research on HSM focuses on the co-occurrence and separate processes of heuristics and systematics, while neglecting how the two are related and intersect. Our results show that trust plays both a liaison and interface role between heuristic and systematic processing, facilitating user assessment, experience, and AI service adoption. Users are engaged with heuristic processing when evaluating FATE, which affects user trust, and this trust influences the systematic evaluation of AI performance. This view is an advancement of previous works (e.g., Chaiken and Ledgerwood, 2012), which have studied dichotomic heuristic and systematic processes as a dichotomy (e.g., which effects are stronger). The effect of heuristic processing has rarely been examined in the context of AI, let alone the relation of such a process to a systematic one. Our findings suggest that perceived FATE for AI may lead to the establishment of user trust, which leads to a more systematic in-depth evaluation of its performance.

Lastly, our model provides a glimpse of the role of FAT with respect to AI use. Not only do FAT qualities play a key role in establishing trust, but they also play an anchoring role in developing user evaluation of AI performance; how useful and convenient did the users perceive AI.User reactions to perceived algorithmic performance are not automatic; rather, they are dependent upon, or at least closely related to, how users recognize, understand, and process the information regarding FAT. Such a relationship can be described as heuristic insofar as users rely on FAT to determine their feelings about accuracy and personalization when using algorithm services. In other words, users figure out algorithmic performance according to the FATE of their content. This finding is in line with the arguments of previous studies, which have shown the contextual nature of such variables (Shin and Park, 2019). The functional features of algorithms are processed through the users’ understanding and wishes for FAT, in which underlying mental models, attitudes, and perceptions. Users have positive emotional valence when their assessment through heuristic and systematic processes are relevant and reasonable. Our finding exemplifies how quality perception can function as a positive heuristic cue that can lead to performance expectancy and human emotion with respect to AI.

7.2. Managerial implications: human-centered AI
To build AI-based systems that users can justifiably trust, one needs to understand how AI technologies impact the trust put in these services. The FATE frameworks provide practical guidelines for the development of user-centered or trust-based algorithm design. For the providers of AI and algorithms, the suggestions of this study can be useful in designing AI interfaces and UXs. As AI continues to transform the way we interact with technologies, how to build a transparent interaction, fair algorithm, and how to include explainability in the interface are important questions to address.

Our findings have practical implications for FATE in algorithms. Issues of causability and explainability have been urgent issues in AI, and users seek guarantees on such issues when using AI. Based on the FATE model, we can infer that trust is closely embedded with these issues as it plays a key role in developing user confidence and credibility. Only when users are assured that their data and privacy are secured with FATE, the user trust is unfolded, and users are willing to provide more of their data to AI. The more trust between people and AI, the more transparent processes can be put into practice. In turn, greater amounts of data enable AIs to produce more precise and reliable results customized and tailored to user preferences and personal histories. Trust serves as a critical platform to bridge between users and AI systems enabling positive feedback loops. The results of this study offer guidelines on how to actualize and integrate FATE issues with other factors — for instance, how to collect user data and/or implicit feedback effectively while upholding users’ trust and emotions. Also, our finding of explainability gives insights on how to address a right to explanation when using AI (Goodman and Flaxman, 2017).

This study highlighted the need for adopting FATE by a design approach when developing AI systems and applications. An important implication of this is that building consensus and achieving collaboration across key stakeholders (i.e., clients, users, and developers) is a prerequisite for the successful adoption of AI in practice. Another key question with regard to XAI is how did an AI service makes a specific recommendation and why didn't that AI do something else? One practical strategy for gaining faith for an AI system is to use algorithms that are inherently explainable and interpretable. For instance, basic elements of an algorithm such as logic classifiers, path trees, and algorithm code that have certain levels of transparency and traceability in their AI decision-making can provide the visibility needed for users critical of AIs.

The second practical implication is that industries should address the algorithm experience in AI. In order to understand user attitudes and algorithm behaviors, researchers must consider algorithm features, user heuristics, and value (Bolin and Schwarz, 2015). In particular, insights derived from the user heuristic can be used for designing heuristic algorithms. Developing user-centered algorithm services as opposed to data-centered approaches involves an understanding of users’ cognitive processes together with the ability to reflect these processes in algorithm designs (Shin and Park, 2019). User perceptions and psychological states of mind are critical in rationalizing how and why users interact, what they do about the issues surrounding AI, as well as how users accept and experience AI services (Vallverdú, 2020). An eventual goal of causable AI is to enable humans to understand, appropriately trust, and effectively manage the emerging development of AI.

8. Conclusion and future studies: beyond explainable AI
AI will be developed to offer truly personalized, algorithm-supported news that is based on the user's past behavior and expressed interests (Shin, 2019). However, the AI industry should do this in a way that observes the FAT principles and respects the users’ right to explanations. This implies that AI and future algorithms must look beyond superficial fairness and legality, or perfunctory accuracyand fulfill genuine user needs and requirements. Modeling the algorithm experience would be important for forecasting users’ future interests for the sake of better performance. This work will be even more challenging because users have ever-changing needs and rising expectations of algorithm services. The user model in this study provides strategic ideas as well as theoretical frames as to how to integrate FATE with usability factors and user behavioral intentions. The eventual goal of AI is to develop user-centered algorithm operations. Algorithms that are FATE-equipped, together with trust-based feedback loops, are critical for designing such user-centered AI and human-centered algorithm systems, which are designed with social responsibility in mind, such as effectively addressing issues of FAT and a broader scope of human conditions. The relations identified through this model serve as a very important step toward achieving these long-term goals. The future endeavor can widen the issues of causability and explainability in diverse emerging AI technologies in greater detail. Particularly, future studies should conduct experiments over a longitudinal design (as users’ trust upon AI varies over time), not just within a single session just like we did in this study. This study concludes by making a plea for more causability and explainability in AI.

