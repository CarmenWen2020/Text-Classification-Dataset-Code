Abstract
We present a self-stabilizing leader election algorithm for general networks, with space-complexity O(logΔ+loglogn) bits per node in n-node networks with maximum degree Δ. This space complexity is sub-logarithmic in n as long as Δ=no(1). The best space-complexity known so far for general networks was O(logn) bits per node, and algorithms with sub-logarithmic space-complexities were known for the ring only. To our knowledge, our algorithm is the first algorithm for self-stabilizing leader election to break the Ω(logn) bound for silent algorithms in general networks. Breaking this bound was obtained via the design of a (non-silent) self-stabilizing algorithm using sophisticated tools such as solving the distance-2 coloring problem in a silent self-stabilizing manner, with space-complexity O(logΔ+loglogn) bits per node. Solving this latter coloring problem allows us to implement a sub-logarithmic encoding of spanning trees — storing the IDs of the neighbors requires Ω(logn) bits per node, while we encode spanning trees using O(logΔ+loglogn) bits per node. Moreover, we show how to construct such compactly encoded spanning trees without relying on variables encoding distances or number of nodes, as these two types of variables would also require Ω(logn) bits per node.

Keywords
Self-stabilisation
Leader election
Coloring
Spanning tree construction

1. Introduction
1.1. Motivation
This paper tackles the problem of designing memory efficient self-stabilizing algorithms for the leader election problem. Self-stabilization [22] is a general paradigm to provide recovery capabilities to networks. Intuitively, a protocol is self-stabilizing if it can recover from any transient failure, without external intervention. Leader election is one of the fundamental building blocks of distributed computing, as it enables a single node in the network to be distinguished, and thus to perform specific actions. Leader election is especially important in the context of self-stabilization as many protocols for various problems assume that a single leader exists in the network, even after faults occur. Hence, a self-stabilizing leader election mechanism enables such protocols to be run in networks where no leader is given a priori, by using simple stabilization-preserving composition techniques [22].

Memory efficiency relates to the amount of information to be sent to neighboring nodes for enabling stabilization. As a result, only mutable memory (used to store variables) is considered for computing memory complexity of a self-stabilizing protocols, while immutable memory (used to store the code of the protocol) is not considered. A small space-complexity induces a smaller amount of information transmission, which (1) reduces the overhead of self-stabilization when there are no faults, or after stabilization [1], and (2) facilitates mixing self-stabilization and replication [31].

1.2. Related work
A foundational result regarding space-complexity in the context of self-stabilizing silent algorithms2 is due to Dolev et al. [23], stating that in -node networks,  bits of memory per node are required for solving tasks such as leader election. So, only talkative algorithms may have -bit space-complexity for self-stabilizing Leader Election solution. Several attempts to design compact self-stabilizing leader election algorithms (i.e., algorithms with space-complexity  bits) were performed but restricted to rings. The algorithms by Mayer et al. [37], by Itkis and Levin [34], and by Awerbuch and Ostrovsky [7] use a constant number of bits per node, but they only guarantee probabilistic self-stabilization (in the Las Vegas sense). Deterministic self-stabilizing leader election algorithms for rings were first proposed by Itkis et al. [35] for rings with a prime number of nodes. Beauquier et al. [8] consider rings of arbitrary size, but assume that node identifiers in -node rings are bounded from above by , where  is a small constant. The best result known so far in this context [15] is a deterministic self-stabilizing leader election algorithm for rings of arbitrary size using identifiers of arbitrary polynomially bounded values, with space complexity  bits per node.

In general networks, self-stabilizing leader election is tightly connected to self-stabilizing tree-construction. On the one hand, the existence of a leader enables time- and memory-efficient self-stabilizing tree-construction [12], [16], [18], [24], [36]. On the other hand, growing and merging trees is the main technique for designing self-stabilizing leader election algorithms in networks, as the leader is often the root of an inward tree [2], [3], [5]. To the best of our knowledge, all algorithms that do not assume a pre-existing leader [2], [3], [5], [10] for tree-construction use  bits per node. This high space-complexity is due to the implementation of two main techniques, used by all algorithms, and recalled below.

The first main technique is the use of a pointers-to-neighbors variable, that is meant to designate unambiguously one particular neighbor of every node. For the purpose of tree-construction, pointers-to-neighbors variables are typically used to store the parent node in the constructed tree. Specifically, the parent of every node is designated unambiguously by its identifier, requiring  bits for each pointer variable. In principle, it would be possible to reduce the memory to  bits per pointer variable in networks with maximum degree , by using node-coloring at distance 2 instead of identifiers to identify neighbors. However, this, in turn, would require the availability of a self-stabilizing distance-2 node-coloring algorithm that uses  bits per node. Previous self-stabilizing distance-2 coloring algorithms use variables of large size. For instance, in the algorithm by Herman et al. [32], every node communicates its distance-3 neighborhood to all its neighbors, which yields a space-complexity of 
 bits. Johnen et al. [30] draw random colors in the range 
, which yields a space-complexity of  bits. Finally, while the deterministic algorithm of Blair et al. [9] reduces the space-complexity to  bits per node, this is achieved by ignoring the cost of storing another pointer-to-neighbor variable at each node. In absence of a distance-2 coloring (which their algorithm [9] is precisely supposed to produce), their implementation still requires  bits per node. To date, no self-stabilizing algorithm implements pointer-to-neighbor variables with space-complexity  bits in arbitrary networks.

The second main technique for tree-construction or leader election is the use of a distance variable that is meant to store the distance of every node to the elected node in the network. Such distance variable is used in self-stabilizing spanning tree-construction for breaking cycles resulting from arbitrary initial state (see [2], [3], [5]). Clearly, storing distances in -node networks may require  bits per node. There are a few self-stabilizing tree-construction algorithms that are not using explicit distance variables (see, e.g., [20]), but their space-complexity is huge (e.g.  bits of memory per node [20]). Using the general principle of distance variables with space-complexity below  bits was attempted by Awerbuch et al. [7], and Blin et al. [13]. These papers distribute pieces of information about the distances to the leader among the nodes according to different mechanisms, enabling to store  bits per node. However, these sophisticated mechanisms have only been demonstrated in rings. To date, no self-stabilizing algorithms implement distance variables with space-complexity  bits in arbitrary networks.

Our results
In this paper, we design and analyze a self-stabilizing leader election algorithm with space-complexity  bits in -node networks with maximum degree . This algorithm is the first self-stabilizing leader election algorithm for arbitrary networks with space-complexity  (whenever 
). It is designed for the standard state model (a.k.a. shared memory model) for self-stabilizing algorithms in networks, and it performs against the unfair distributed scheduler.

The design of our algorithm requires overcoming several bottlenecks, including the difficulties of manipulating pointers-to-neighbors, and distance variables using  bits in arbitrary networks. Overcoming these bottlenecks was achieved thanks to the development of sub-routine algorithms, each deserving independent special interest described hereafter.

First, we extend the bit-wise ring publication technique [13] to arbitrary topologies. Our approach retains the  bits per node complexity for storing identifiers, and is thus independent of the degree.

Second, we propose the first silent self-stabilizing algorithm for distance-2 coloring that breaks the space-complexity of  bits of memory per nodes. More precisely this new algorithm achieves a space-complexity of  bits of memory per nodes. As opposed to previous distance-2 coloring algorithms, we do not use full identifiers for encoding pointer-to-neighbor variables (this would require  bits per node). Instead, our compact representation of the identifiers (using  bits per node) enables symmetry breaking. This distance-2 coloring permits to distinguish parent and children and a node’s neighbors, allowing the design of a compact encoding of spanning trees.

Third, we design a new technique to detect the presence of cycles given by the current set of pointers-to neighbors. This approach does not use distances, but it is based on the uniqueness of each identifier in the network. Notably, this technique can be implemented by a silent self-stabilizing algorithm, with space-complexity  bits of memory per nodes.

Last but not least, we design a new technique to avoid the creation of cycles during the execution of the leader election algorithm. Again, this technique does not use distances but maintains a spanning forest, which eventually reduces to a single spanning tree rooted at the leader at the completion of the leader election algorithm. Implementing this technique results in a self-stabilizing algorithm with space complexity  bits of memory per nodes.

2. Model and definitions
2.1. Protocol syntax and semantics
We consider a distributed system consisting of  processes that form an arbitrary communication graph. The processes are represented by the nodes of this graph, and the edges represent pairs of processes that can communicate directly with each other. Such processes are said to be neighbors. Let  be an -node graph, where  is the set of nodes, and  the set of edges and  the degree of the graph.

Space-complexity in self-stabilization considers only volatile memory (that is, memory whose content changes during the execution of the protocol), while non-volatile memory (whose content does not change during the execution of the protocol, used e.g., to store the code and the constants, and in particular the node unique identifier) is not included in the space complexity. Volatile memory includes the space allocated for protocol variables, and in particular the program counter (that commands the next line of code to execute). So, to achieve  bits of memory per node, we define helping functions that enable volatile memory to remain below that threshold.

A node  has access to a constant unique identifier 
, but can only access its identifier one bit at a time, using the 
 function, which returns the position of the  most significant bit equal to  in 
. Even though identifiers require  bits of memory per node in the worst case, the  function can be stored in the immutable code portion of the node. We present here the pseudocode for the 
 function at a particular node . Note that since nodes have unique identifiers, they are allowed to execute unique code. For example, suppose node  has identifier  (in decimal notation), or  (in binary notation). Then, one can implement 
 as follows for : 
≔
 

Since we assume that all identifiers are  bits long, the 
 function only returns values with  bits. Also, when executing Function 
, the program counter only requires  values. In turn, this position can be encoded with  bits when identifiers are encoded using  bits, as we assume they are. A node  has access to locally unique port numbers associated with its adjacent edges. We do not assume any consistency between port numbers of a given edge. In short, port numbers are constant throughout the execution but initialized by an adversary. Each process contains variables and rules. Variable ranges over a domain of values. The variable 
 denote the variable  located at node . A rule is of the form  [21]. A guard is a boolean predicate over process variables. A command is a set of variable-assignments. A command of process  can only update its own variables. On the other hand,  can read the variables of its neighbors. This classical communication model is called the state model or the state-sharing communication model. This model is also used in stabilization-preserving compilers that produce actual code [6], [17], [19], [38].

An assignment of values to all variables in the system is called a configuration. A rule whose guard is true in some system configuration is said to be enabled in this configuration. The rule is disabled otherwise. The atomic execution of a subset of enabled rules (at most one rule per process) results in a transition of the system from one configuration to another. This transition is called a step. A run of a distributed system is a maximal alternating sequence of configurations and steps. Maximality means that the execution is either infinite or its final configuration has no rule enabled.

2.2. Schedulers
The asynchronism of the system is modeled by an adversary (a.k.a. scheduler) that chooses, at each step, the subset of enabled processes that are allowed to execute one of their rules during this step. Those schedulers can be classified according to their characteristics (like fairness, distribution, etc.), and a taxonomy was presented By Dubois et al. [25]. Note that we assume here an unfair distributed scheduler. This scheduler is the most challenging since no assumption is made of the subset of enabled processes chosen by the scheduler at each step. We only require this set to be nonempty if the set of enabled processes is not empty in order to guarantee progress of the algorithm.

2.3. Predicates and specifications
A predicate is a boolean function over configurations. A configuration conforms to some predicate , if  evaluates to true in this configuration. The configuration violates the predicate otherwise. Predicate  is closed in a certain protocol , if every configuration of a run of  conforms to , provided that the protocol starts from a configuration conforming to . Note that if a protocol configuration conforms to , and the configuration resulting from the execution of any step of  also conforms to , then  is closed in .

A specification for a processor  defines a set of configuration sequences. These sequences are formed by variables of some subset of processors in the system. This subset always includes  itself.

Problem specification prescribes the protocol behavior. The output of the protocol is carried through external variables, that are updated by the protocol, and used to display the results of the protocol computation. The problem specification is the set of sequences of configurations of external variables.

A protocol implements the specification. Part of the implementation is the mapping from the protocol configurations to the specification configurations. This mapping does not have to be one-to-one. However, we only consider unambiguous protocols where each protocol configuration maps to only one specification configuration. Once the mapping between protocol and specification configurations is established, the protocol runs are mapped to specification sequences as follows. Each protocol configuration is mapped to the corresponding specification configuration. Then, stuttering, the consequent identical specification configurations, is eliminated. Overall, a run of the protocol satisfies the specification if its mapping belongs to the specification. Protocol  solves problem  under a certain scheduler if every run of  produced by that scheduler satisfies the specifications defined by . A predicate  is an invariant of protocol  if every run of  that starts in a state conforming to  satisfies  in every subsequent configuration. Given two predicates 
 and 
 for protocol , 
 is an attractor for 
 if every run of protocol  that starts from a configuration that conforms to 
 contains a configuration that conforms to 
. Such a relationship is denoted by 
. Also, the  relation is transitive: if 
, 
, and 
 are predicates for , and 
 and 
, then 
. In this last case, 
 is called an intermediate attractor towards 
.

Definition 1 Self-Stabilization

A protocol  is self-stabilizing [22] to specification  if there exists a predicate  for  such that:

1.
 is an attractor for true,

2.
Any run of  starting from a configuration satisfying  satisfies .

Definition 2 Leader Election

Consider a system of processes where each process’ set of variables is mapped to a boolean specification variable leader denoted by . The leader election specification sequence consists in a single specification configuration where a unique process  maps to 
, and every other process  maps to 
.

Definition 3

In the state model, a protocol is silent if and only if every execution is finite. Otherwise, the protocol is talkative.

We measure time complexity with respect to individual steps performed by each process. So, the time complexity of a self-stabilizing leader election algorithm is the highest number of individual steps before a single leader is elected, starting from an arbitrary configuration.

3. Compact self-stabilizing leader election for networks
Our new self-stabilizing leader election algorithm is based on a spanning tree-construction rooted at a maximum degree node, without using distances. If multiple maximum degree nodes are present in the network, we break ties with colors and if necessary with identifiers.

Theorem 1

Algorithm called C-LE solves the leader election problem in a talkative self-stabilizing manner in any -node graph, assuming the state model and a distributed unfair scheduler, with  bits of memory per node.

Our talkative self-stabilizing algorithm reuses and extends a technique for obtaining compact identifiers of size  bits of memory per node presented in Section 3.1. Then, the leader election process consists in running several algorithms layers using decreasing priorities (see also Fig. 1):

1.
An original silent self-stabilizing distance-2 coloring presented in Section 3.2 that permits to implement pointer-to-neighbors with  bits of memory per node.

2.
A silent self-stabilizing cycle destruction and illegitimate sub spanning tree-destruction reused from previous work [11], [13] presented in Section 3.3.

3.
A new silent self-stabilizing cycle detection that does not use distance to the root variables presented in Section 3.4.

4.
An original talkative self-stabilizing spanning tree-construction, that still does not use distance to the root variables, presented in Section 4. This algorithm is trivially modified to obtain a leader election algorithm.

5.
In Section 5, we describe how to integrate all previous components into a leader election protocol for general graphs.

3.1. Compact memory using identifiers
As many deterministic self-stabilizing leader election algorithms, our approach ends up comparing node unique identifiers. However, to avoid communicating the full  bits to each neighbor at any given time, we reuse the scheme devised in previous work [13] to progressively publish node identifiers. Let 
 be the identifier of node . We assume that 
. Let 
 be the set of all non-zero bit-positions in the binary representation of 
. Then, 
 can be written as 
, where 
. In the process of comparing node unique identifiers during the leader election algorithm execution, the nodes must first agree on the same bit-position 
 (for ); this step of the algorithm defines phase . Put differently, the bit-positions are communicated in decreasing order of significance in the encoding of the identifier. In turn, this may propagate it to their neighbors, and possibly to the whole network in subsequent phases. This propagation is used in the following to break symmetries in the coloring problem or to detect a cycle in spanning tree construction.


Download : Download high-res image (134KB)
Download : Download full-size image
Fig. 1. Overview of algorithm.

If all identifiers are in 
, for some constant , then the communicated bit-positions are less than or equal to , and thus can be represented with  bits. However, the number of bits used to encode identifiers may be different for two given nodes, so there is no common upper bound for the size of identifiers. We circumvent this problem using a ranking on bit-positions that is agnostic on the size of the identifiers. We extract of our previous works the part dedicated to the propagation of the identifier bit by bit in phases, remark that we slightly modify our previous work. Since we do not assume that the identifiers of every node are encoded using the same number of bits, simply comparing the th most significant bit of two nodes is irrelevant. Instead, we use variable 
, which represents the most significant bit-position of node . In other words, 
 represents the size of the binary representation of 
. The variables ,  are the core of the identifier comparison process. Variable 
 stores the current phase number , while variable 
 stores the bit-position of 
 at phase . Remark that the number of non-zero bits can be smaller than the size of the binary representation of the identifier of the node, so if there are no more non-zero bit at phase 
, we use 
. To make the algorithm more readable, we introduce variable 
, called a compact identifier in the sequel. When meaningful, we use 
, where 
.

Node  can trivially detect an error (see predicate ) whenever its compact identifier does not match its global identifier, or its phase is greater than 
. (1)
Moreover, the phases of neighboring nodes must be close enough: a node’s phase may not be more than  ahead or behind any of its neighbors; also a node may not have a neighbor ahead and another behind. Predicate  captures these conditions, where  denotes a subset of neighbors of . The set  should be understood as an input provided by an upper layer algorithm. (2)
If  detects an error through  or , it resets its compact identifier to its first phase value (see command ). In a talkative process, node identifiers are published (though compact identifiers) infinitely often. So, when node  and all its active neighbors have reached the maximum phase (i.e. 
),  goes back to phase one. Then, if  has 
 and an active neighbor  has 
, it is not an error. But if  has 
, one active neighbor  has 
, and another active neighbor  has 
, then an error is detected. (3)

If  detects an error through ,  or , it resets its compact identifier to its first phase value: (4)(5)
≔
 This may trigger similar actions at neighbors in , so that all such errors eventually disappear.

The compact identifier of  is smaller (respectively greater) than the compact identifier of , if the most significant bit-position of  is smaller (respectively greater) than the most significant bit-position of , or if the most significant bit-position of  is equal to the most significant bit-position of ,  and  are in the same phase, and the bit-position of  is smaller (respectively greater) than the bit-position of : (6)
When two nodes  and  have the same most significant bit-position and the same bit position at phase 
, they are possibly equal with respect to compact identifiers (denoted by 
). (7)
Finally, two nodes  and  have the same compact identifier (denoted by 
) if their phase reaches the size of the binary representation of the identifier of the two nodes, and their last bit-position is the same. (8)
See Fig. 2 for examples of comparison between compact identifiers.

The predicates 
 and 
 check if a node  can increase its phase (or restarts 
), the first one is dedicated to the silent protocols, the second one is dedicated to the talkative protocols. The command  is dedicated for increasing phases or restarting 
. Last, the command  assigns at a node  the minimum (or maximum) compact identifier in the subset of neighbors . We have now, all the principals ingredients to use compact identifiers.

Predicate 
 is true if for every node  in , either 
, or 
. (9)

Similarly, 
 is true if for every node  in , either 
, or 
. Remark that when the self-stabilizing algorithm is talkative, when the phase reaches the maximum, the publication restarts at the first phase. As a consequence, the next phase to compare when a node reaches the maximum phase is the phase 1. (10)

When 
 or 
 is true,  may increase its phase: (11)
≔
 

In some case, we need to compute the minimum or the maximum on compact identifiers. Let  denote a function that is either minimum or maximum. Let us denote by 
 the minimum or the maximum most significant bit of nodes in . (12)
To compare compact identifiers, one must always refer to the same phase; we always consider the minimum phase for nodes in . (13)
Finally, we compute the minimum or the maximum bit position. (14)
Predicate  checks if 
 is equal to the minimum among nodes in : (15)
The predicate  does the same for the maximum: (16)

Node  may use  to assign its local variables the minimum (or maximum) compact identifier in . (17)
≔
≔
≔
 

We have now, all the ingredients to use compact identifiers.

3.2. Silent self-stabilizing distance-2 coloring
In this section, we provide a new solution to assign colors that are unique up to distance two (and bounded by a polynom of the graph degree) in any graph. Those colors are meant to efficiently implement the pointer-to-neighbor mechanism that otherwise requires  bits of memory per node. The remaining of the section is organized as follows: Section 3.2.1 presents high level concepts of our solution, Section 3.2.2 lists functions and predicates used in the algorithm, Section 3.2.3 formally presents the algorithm, while Section 3.2.4 is dedicated to establishing its proof of correctness.

3.2.1. Self-stabilizing algorithm high level description
Our solution uses compact identifiers to reduce memory usage. When a node  has the same color as (at least one of) its neighbors, then if the node  has the smallest conflicting color in its neighborhood and is not the biggest identifier among conflicting nodes, then  changes its color. To make sure a fresh color is chosen by , all nodes publish the maximum color used by their neighborhood (including themself). So, when  changes its color, it takes the maximum advertised color plus one. Conflicts at distance two are resolved as follows: let us consider two nodes  and  in conflict at distance two, and let  be (one of) their common neighbor; as  publishes the color of  and , it also plays the role of a relay, that is,  computes and advertises the maximum identifiers between  and , using the compact identifiers mechanisms that were presented above; a bit by bit, then, if  has the smallest identifier, it changes its color to a fresh one. To avoid using too many colors when selecting a fresh one, all changes of colors are made modulo an upper bound on the number of neighbors at distance 2, which is computed locally by each node.

In our self-stabilizing coloring algorithm, called C-Color, each node  maintains a color variable denoted by 
 and a degree variable denoted by 
. A variable 
 stores the minimum color in conflict in its neighborhood (including itself). The variable 
 stores the maximum color observed in its neighborhood. We call  a player node when  has the minimum color in conflict. Also, we call  a relay node when  does not have the minimum color in conflict, yet at least two of its neighbors have the minimum color in conflict. The nodes continuously update their variables according to the minimum and maximum colors published by their neighbors. A player node whose compact identifier is smaller than the compact identifier of its neighbors (be them players or relay nodes) becomes a loser node and changes its color. If all the neighborhood of a player node  has the same phase and the same compact identifier, then  increases its phase, until it becomes a loser or no conflict remains in ’s neighborhood. When a player node and its neighborhood reach the maximum phase, they restart at the first phase. A relay node continuously takes the value of the greatest compact identifier of its player neighbors, for the purpose that at least one of them becomes a loser. Note that, a node may alternate between being a relay and a player, until the coloring is complete. This algorithm is silent, once the system reaches a distance two coloring, the color variables remain the same.

3.2.2. Functions, predicates and actions used by algorithm C-Color
Note that all rules are exclusive, because a node  cannot be both  and . Let us now describe the functions, predicates and actions used by algorithm C-Color. Remember that . Function  returns the maximum degree between  and its neighbors, and is used to define the range 
 of authorized colors for a node : (18)

The function  returns the minimum color in conflict at distance one and two: (19)≔

The function  returns the maximum color used at distance one: (20)≔

The predicate  is true if  has not yet set the right value for either  or . Moreover, this predicate checks if ’s compact identifier matches its global identifier (see predicate (1): ) and if the phases of the subset of ’s neighbors  are coherent with  (see predicate (2): ). (21)

The predicate  is true if  has the minimum color in conflict (announced by its neighbors or by itself). Observe that the conflict may be at distance one or two: (22)

The predicate  is true if  does not have the minimum color in conflict, and at least two of its neighbors have the minimum color in conflict (see Fig. 3): (23)

The function  returns the subset of ’s neighbors that have the minimum color in conflict, when  is a relay node: (24)≔
The function  returns the subset of ’s neighbors that are in conflict with  at distance one, or the set of relay nodes for the conflict at distance two, when  has  equal to : (25)≔


Download : Download high-res image (1MB)
Download : Download full-size image
Fig. 3. An example execution of our coloring scheme is presented for a line network of size . The color of the main rectangle denotes the node’s color (variable 
), the upper left disk color denotes the maximum color used at distance  (variable 
), the lower left disk color denotes the minimum color in conflict at distance  or  (variable 
). The light gray color denotes the absence of any color. The rule immediately under a node denotes which rule is executed to reach the next configuration of the execution. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.)

The predicate  is true whenever a competing player of  has a greater bit position at the same phase. A node whose identifier is maximum among competitors does not change its color, but any losing competitor does. (26)

The predicate  is true if a relay node is not according to its player neighbors, like we decide to change the color of the node with the minimum identifier the relay node stores the maximum compact identifier of its player neighbors: (27)

The action  updates the variables 
 and resets the variables relatives to the identifier (see command (v) in Eq. (5)). (28)
≔
≔

When a node changes its color, it takes the maximum color at distance one and two plus one modulo 
, and then adds one to assign colors in the range 
. (29)
≔

3.2.3. Algorithm C-Color
The rule 
 assures that the degree variable is equal to the degree of the node. Each node  must maintain its color in range 
 to satisfy the memory requirements of our protocol, where  is a function that returns the maximum degree of its neighborhood (including itself). Whenever ’s color exceeds its expected range, rule 
 resets the color to one. Rule 
 is dedicated to updating the variables of  whenever they do not match the observed neighborhood of  (see ), or when a player node has an erroneous phase variable when comparing its identifier with another player node (see function ). In both cases, the  computes the minimum and maximum color and resets its compact identifier variable (see command  (28)). The rule 
 increases the color of the node  but maintains the color in some range (see command  (29)), when  has the minimum color in conflict and the minimum identifier. The rule 
 increases the phase of , when  is a player and does not have the minimum identifier at the selected phase. The rule 
 updates the identifier variable when  is a relay node.


Download : Download high-res image (140KB)
Download : Download full-size image
3.2.4. Correctness
Theorem 2

Algorithm  solves the vertex coloration problem at distance two using 
 colors in a silent self-stabilizing manner in graph, assuming the state model, and a distributed unfair scheduler. Moreover, if the  node identifiers are in 
, for some , then  uses  bits of memory per node.

In the details of lemmas that are presented in the sequel, we use predicates on configurations. These predicates are mean to be intermediate attractors towards a legitimate configuration (i.e., a configuration with a unique leader). To establish that those predicates are indeed attractors, we use potential functions [4], [39], [40], that is, functions that map configurations to non-negative integers, and that strictly decrease after any algorithm step is executed. In the remaining of the paper, the potential functions we define closely match the proof arguments of the following Lemma/Theorem. That is, various invariants are defined for each property we wish to prove, and the potential function makes sure the output value decreases until the invariant is reached.

To avoid additional notations, we use sets of configurations to define predicates; the predicate should then be understood as the characteristic function of the set (that returns true if the configuration is in the set, and false otherwise).

Lemma 1

Using a range of 
 for colors at node  is sufficient to enable distance-2 coloring of the graph.

Proof

In the worst case for the number of colors, all neighbors at distance one and two of  have different colors. Now,  has at most  neighbors at distance one, each having  other neighbors than . In total,  has at most 
 neighbors at distance up to two, each having a distinct color. Using a range of 
 for ’s color leaves at least  available colors for node .  □

Let 
 be the color of  in configuration .

Let  be the following function: 
 Let  be the following potential function: 

Now, let 
 be the color of  in configuration 
, 
 being defined as the configuration where 
 reaches zero.

Let  be the following function:  
 

Let  be the following potential function: 

We denote by 
 the configuration after activation of (a subset of) the nodes in 
 where 
 denotes the enabled nodes in  due to rule 
. We can now prove the following result: 
 for every configuration  where 
 is not empty.

Lemma 2

 for every configuration  such that 
 is not empty.

Proof

We consider a node 
. After executing rule 
,  takes a color: 
As a consequence  decreases by at least one, so 
.  □

Let  be the function defined by: 
 

Let  be the potential function defined by: 

Let 
 be the potential function defined by: 

The comparison between two configurations  and 
 is by using lexical order. We denote by  the (subset of) enabled nodes (for any rule of our algorithm) in configuration . Note that the algorithm is stabilized when every node is neither a player nor a relay, that is the nodes have no conflict at distance one and two, when . We define 

Lemma 3

 and 
 is closed.

Proof

The function  decreases by any execution of rules 
 and 
. Remark that  is considered a non corruptible local information, so once  has executed 
, this rule remains disabled afterwards. Moreover, 
 maintains the value of the color inferior (or equal) to 
, and other rules modifying the color maintain this invariant. Hence, if the scheduler activates rules 
 or 
, we obtain 
, otherwise if other rules are activated, then 
. We already saw that, when the scheduler activates a node  for rule 
, we obtain 
. Overall, if the scheduler activates rules 
, 
, or 
 we obtain 
. We now consider the cases where the scheduler activates other rules.

First, we focus on rule 
. Let us consider 
, the set of nodes enabled in configuration  for this rule, and a node  such that 
. In other words,  has  (see predicate (21)). If  has 
 in , then after activation of , we obtain 
 and 
 in 
 by the command  (see command (29)), because  and  depend only on the color of the neighbors of  (see Function (19), (20)). The same argument applies for , because 
 is computed only with the identifier of . So, after execution of 
 by , we obtain 
, thanks to the execution of the command (v) (see command (28)) that assigns  to . Remark that, if the color of the neighbors of  does not change, rule 
 remains disabled. Now, if the color changes,  still decreases thanks to Lemma 2.

Let us consider now a configuration where the rule 
 is disabled for every node. Rule 
 increases the phase of a player node, so after activation of this rule we obtain 
. Executing rule 
 decreases also 
 due to  (see predicate (27)), because when all nodes in  (see function (24)) have increase their phases,  decreases for all ’s neighbors. Note that, when a conflict for a color  is resolved, a node can become a player node or a relay node for another color 
, but then 
. So 
, hence  decreases.

If a node has no conflict at distance one or two, it never changes its color, so once the system reaches a distance-2 coloring, it remains with the same coloring.  □

Lemma 4

Algorithm  requires  bits of memory per node.

Proof

The variables 
 take  bits. The compact identifier 
 takes  bits per node.  □

Proof of Theorem 2

Direct by Lemma 1, Lemma 3, Lemma 4.  □

Lemma 5

Algorithm C-Color converges in 
 steps.

Proof

Direct by the potential function .  □

3.3. Cleaning a cycle or an impostor-rooted spanning tree
The graph  is supposed to be colored up to distance 2, thanks to our previous algorithm. To construct a spanning tree of , each node  maintains a variable 
 storing the color of ’s parent ( otherwise). The function  to return the subset of ’s neighbors considered as its children (that is, each such node  has its 
 variable equal ’s color). Note that the variable parent is managed by the algorithm of spanning tree-construction.

An error is characterized by the presence of inconsistencies between the values of the variables of a node  and those of its neighbors. In the process of a tree-construction, an error occurring at node  may have an impact on its descendants. For this reason, after a node  detects an error, our algorithm cleans  and all of its descendants. The cleaning process is achieved by Algorithm , already presented in previous works [11], [13]. Algorithm  is run in two cases: cycle detection (thanks to predicate , presented in Section 3.4), and impostor leader detection (thanks to predicate , presented in Section 4). An impostor leader is a node that (erroneously) believes that it is a root.

When a node  detects a cycle or an impostor root,  deletes its parent. Simultaneously,  becomes a frozen node. Then, every descendant of  becomes frozen. Finally, from the leaves of the spanning tree rooted at , nodes delete their parent and reset all variables that are related to cycle detection or tree-construction. So, this cleaning process cannot create a livelock. Algorithm Freeze is a silent self-stabilizing algorithm using  bits of memory per node.

It is important to note that a frozen node, or the child of a frozen node, does not participate in cycle detection or spanning tree-construction.

We now recall Freeze in Algorithm 2. This algorithm uses only one binary variable . This approach presents several advantages. After  detecting a cycle, the cycle is broken ( deletes its parent), and a frozen node cannot reach its own subtree, due to the cleaning process taking place from the leaves to the root. So, two cleaning processes cannot create a livelock.


Download : Download high-res image (78KB)
Download : Download full-size image
Theorem 3

Algorithm Freeze deletes a cycle or an impostor-rooted sub spanning tree in -nodes graph in a silent self-stabilizing manner, assuming the state model, and a distributed unfair scheduler. Moreover, Algorithm Freeze uses  bits of memory per node and converges in  steps.

Proof of Theorem 3 is due to Blin et al. [11].

3.4. Silent self-stabilizing algorithm for cycle detection
We present in this subsection a self-stabilizing algorithm to detect cycles (possibly due to initial incorrect configuration) without using the classical method of computing the distance to the root. We first present our solution with the assumption of global identifiers (hence using  bits for an -node network) and then using our compact identifier scheme postponed in Section 3.4.3.

3.4.1. Silent Self-stabilizing algorithm with identifiers
The main idea to detect cycles is to use the uniqueness of the identifiers. We flow the minimum identifier up to the tree to the root, then if a node whose identifier is minimum receives its identifier, it can detect a cycle. Similarly, if a node  has two children flowing the same minimum identifier,  can detect a cycle. The main issue to resolve is when the minimum identifier that is propagated to the root does not exist in the network (that is, it results from an erroneous initial state).

The variable 
 stores the minimum identifier collected from the leaves to the root up to node . We denote by 
 the minimum identifier obtained by  during the previous iteration of the protocol (this can be ). A node  may select among its children the node  with the smallest propagated identifier stored in 
, we call this child kid returned by the function .

Predicate  is the core of our algorithm. Indeed, a node  can detect the presence of a cycle if it has a parent and if (i) one of its children publishes its own identifier, or (ii) two of its children publish the same identifier. Let us explain those conditions in more detail. We consider a spanning structure , a node  and let  and  be two of its children. Suppose that  and  belong to a cycle , note that, since a node has a single parent,  cannot belong to any cycle (see Fig. 4). Let 
 be the minimum identifier stored by any variable 
 such that  belongs to . So,  is either in , or in the subtree rooted to , denoted by 
.

First, let us consider the case where 
 is stored in 
. As any node selects the minimum for flowing the  upstream, there exists a configuration  where 
, and a configuration 
 where 
. In 
,  can detect an error, due to the uniqueness of identifier, it is not possible for two children of  to share the same value when there is no cycle.


Download : Download high-res image (74KB)
Download : Download full-size image
Fig. 4. Spanning structure.

Now, let us suppose that 
 is in , and let 
 be the node with the smallest identifier in , so 
 or 
 (
 means that the identifier 
 does not exist in .) If 
, as any node selects the minimum for flowing the  upstream, there exists a configuration  where 
 and 
 is the child of 
 involved in , then 
 can detect an error. Indeed, due to the uniqueness of identifier, it is not possible that one of its children stores its identifier when there is no cycle. The remaining case is when 
. In this case, as any node selects the minimum for flowing the  upstream, there exists a configuration  where 
, with  belonging to . When a node , its parent and one of its children share the same minimum, they restart the computation of the minimum identifier. For this purpose, they put their own identifier in the  variable. To avoid livelock, they also keep track of the previous 
 in variable 
. Now 
, so the system reaches the first case. Note that the variable 
 blocks the live-lock but also the perpetual restart of the nodes, as a result of this, a silent algorithm. Moreover, a node  collects the minimum identifier from the leaves to the root, if 
 contains an identifier bigger than the identifier of the node , then  detects an error. The same holds, when  has a 
 smaller than 
 with  children of , since the minimum is computed between 
 and its own identifier. 

Our algorithm only contains three rules. The rule 
 updates the variable 
 if the variable 
 of a child  is smaller, nevertheless this rule is enabled if and only if the variable 
 does not contain the minimum 
 published by the child. When  and its relatives have the same minimum,  declares its intent to restart a minimum identifier computation by erasing its current (and storing it in 
). The rule 
 is dedicated to declaring its intent to restart. When all its neighbors have the same intent,  can restart (see rule 
).

An example execution of Algorithm Break is depicted in Fig. 5.


Download : Download high-res image (69KB)
Download : Download full-size image

Download : Download high-res image (500KB)
Download : Download full-size image
Fig. 5. An example execution for Algorithm Break is presented on a ring shaped network. Only id, 
, and 
 are represented for each node. Nodes colored in light gray are activated in each step by the scheduler. The node colored in orange detects a cycle. Only one of the many possible execution is represented. Below each configuration , we also indicate the value of the smallest identifier  such as , along with the value of the corresponding  (See Section 3.4.2 for the definition of .). (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.)

3.4.2. Correctness of the algorithm 
Theorem 4

Algorithm Break solves the detection of cycle in -node graph in a silent self-stabilizing manner, assuming the state model, and a distributed unfair scheduler. Moreover, if the  node identifiers are in 
, for some , then algorithm Break uses  bits of memory per node.

Let  be a natural integer in 
, representing a possible node identifier. Then, let us denote by  the set of nodes on the path between  and , where  is the nearest descendant of  () such that 
, if such a node exists. We suppose that every node  in  has . The value  can improve the value 
 if and only every node  in  has a 
 and 
. Also, if 
, the value vanishes during the execution, note that may be . Predicate  captures this fact. Predicate  is defined recursively as, to improve a node, its descendants must be improved first. Only when no descendent can be improved is the node itself bound for action. (30)

Let  be the function defined by: 
 

Let  be the function defined by: 

Fig. 5 depicts the evolution of the values returned by  for the minimum identifier  such that .

Let 
 be the function defined by: The comparison between two configurations  and 
 is performed using lexical order. In the following, 
 denotes the variable 
 in configuration . Note that the algorithm is stabilized when no value  can improve the value stored in 
, that is when . We define 

Lemma 6

 and 
 is closed.

Proof

The starting predicate being true, all possible configurations are taken into consideration. We simply review the effect of executing the three rules of the algorithm:

•
Rule 
: 
, so for 
, we have 
. Note that for 
,  has no effect on  and 
. Now, 
 because 
 is enabled for  only if 
, and 
 because we have 
, thus 
. So, if the scheduler activates  with rule 
, we obtain 
.

•
Rule 
:

–
: 
 because 
 (otherwise an error is detected). Also, if  can improve 
, it can also improve 
.

–
: Rule 
 needs 
, so 
. Now, we have 
, and we obtain 
.

As a consequence, 
 for 
 and 
. So, if the scheduler activates only  for rule 
, we obtain 
.
•
Rule 
: 
 and 
 by definition of rule 
, so in , we have 
 because 
 and 
 can improve 
. Now, we have 
 because 
. 
, so if the scheduler activates only  for rule 
, we obtain 
.

To conclude, 
 for every configuration  and 
, when 
 occurs later than .  □

Proof of Theorem 4

Now we prove that, in 
 if the spanning structure  contains a least one cycle , then at least one node  in  has . For the purpose of contradiction, let us assume the opposite. Let 
 and every node  in the cycle  in  has . By definition all the nodes in  have a parent, and all the nodes have 
. Now, if a node  shares the same  with its parent and its child, then rule 
 is enabled for , a contradiction with . In a cycle, it is not possible that all nodes have 
 (due to well foundedness of integers, at least one node  has 
), which is a contradiction with the assumption that every node  is such that .  □

Lemma 7

Algorithm Break converges in 
 steps.

Proof

Direct by the potential function .  □

3.4.3. Silent self-stabilizing cycle detection with compact identifiers
We refine algorithm Break to make use of compact identifiers (of size  instead of global identifiers (of size ). With compact identifiers, the main problem is the following: two nodes  and  can deduce that 
 if and only if they have observed 
 during every phase , with 
. A node  selects the minimum compact identifier stored in variable  in its neighborhood (including itself). If it is the case that in a previous configuration, one of ’s children presented  a compact identifier smaller than its own,  became passive (Variable 
), and remained active otherwise (Variable 
). Only active nodes can continue to increase their phase. Moreover, a node increases its phase if and only if its parent and one of its children  has the same information, namely 
. Observe that in a spanning tree, it is possible that several nodes do not increase their phases. For example, leaf nodes are in such a situation. To explain why the absence of phase increases does not cause trouble, let  be the node with the smallest identifier involved in a cycle and let us suppose that  has two children, one child  involved in the cycle, and another child  that is not. In some configuration,  may not be able to increase its phase, but then  reaches the same phase of the active node , so  increases its phase, and the system reaches a configuration where 
. Then,  detects an cycle error. Variable 
 combined to this compact identifier usage permits to obtain a silent algorithm.

Predicate  now takes into account the error(s) related to compact identifiers management. It is important to note that the cycle breaking algorithm does not manage phase differences. Indeed, a node  whose phase is bigger than that of one of its children  assigns 
 to 
, if and only if 
 or no child of  has the same compact identifier as . The 
 variable is compared using lexicographic order by rule 
. The modifications to algorithm  are minor. We add only one rule to increase the phase: 
. Only a passive node can restart. Remark that now the  variable uses  bits. As the  variables store a color, we obtain a memory requirement of  bits per node.

3.4.4. Algorithm C-Break and predicates
Predicate  must be updated to take into account this extra care. We denote by 
 the child of  with minimum compact identifier stored in 
. Moreover, predicate  now takes into account the error(s) related to compact identifiers management (see Eq. (1): ). It is important to note that the cycle breaking algorithm does not manage phase differences. The compact identifier stored in 
 is compared using lexicographic order by rule 
. (31)


Download : Download high-res image (100KB)
Download : Download full-size image
Theorem 5

The algorithm C-Break solves the detection of cycle in arbitrary -node graph in a silent self-stabilizing manner, assuming the state model, and a distributed unfair scheduler. Moreover, if the  node identifiers are in 
, for some , then algorithm C-Break uses  bits of memory per node and converges in 
 steps.

The proof of Theorem 5 mimics the proof of algorithm . The extra  steps factor (with respect to algorithm ) results from the number of comparisons that are necessary when using compact identifiers.

4. Talkative spanning tree-construction without distance to the root
Our approach for self-stabilizing leader election is to construct a spanning tree whose root is to be the elected leader. Two main obstacles to self-stabilizing tree-construction are the possibility of an arbitrary initial configuration containing one or more cycles, or the presence of one or more impostor-rooted spanning trees. We already explained how the cycle detection and cleaning process takes place, so we focus in this section on cycleless configurations.

The main idea is to mimics the fragments approach introduced by Gallager et al. [29]. In an ideal situation, at the beginning each node is a fragment, each fragment merges with a neighbor fragment holding a bigger root signature, and at the end remains only one fragment, rooted in the root with the biggest signature (that is, the root with maximum degree, maximum color, and maximum global identifier). To maintain a spanning structure, the neighbors that become relatives (that is, parents or children) remain relatives after that. Note that the relationship may evolve through time (that is, a parent can become a child and vice versa). So our algorithm maintains that as an invariant (see Lemma 8).

Indeed, when two fragments merge, the one with the root with smaller signature 
 and the other one with a root with bigger signature 
, the root of 
 is re-rooted toward its descendants until reaching the node that identified 
. This approach permits to construct an acyclic spanning structure, without having to maintain distance information. The variable 
 stores the signature relative to the root (that is, its degree, its color, and its identifier). Note that, the comparison between two  is done using lexical ordering. The variable 
 stores the color of the neighbor  of  leading to  with 
 if there exists such a node, and  otherwise. The function  returns the color of the neighbor of  with the maximum root (see (32)).

4.1. Algorithm description
Let us now give more details about our algorithm (presented in Algorithm 5). If a root  has a neighbor  with 
, then  chooses  as its parent (see rule 
). If a node  (not a root) has a neighbor  with 
, it stores its neighbor’s color in Variable 
, and updates its 
 to 
. Yet, it does not change its parent. This behavior creates a path (thanks to Variable ) between a root  of a sub spanning tree 
 and a node contained in an other sub spanning tree 
 rooted in 
, with 
 (see rule 
). The subtree 
 is then re-rooted toward a node aware of a root with a bigger signature . Now, when 
’s neighbor  becomes root, it takes  as a parent (see rules 
 and 
). Finally, the descendants of the re-rooted root update their root variables (see rule 
). The predicate  (see (33)) captures trivial errors and impostor-root errors for the construction of the spanning tree. Note that it is used in  only (and not in ) as these errors are never created by  and  has higher priority than  (see Section 4.2).


Download : Download high-res image (122KB)
Download : Download full-size image
4.1.1. Predicates
The function  returns the color of the neighbor of  with the maximum root: (32)

We now present a list of trivial errors and impostor-root errors for the construction of the spanning tree. The explanations of the different elements composing the predicate  follow: (1) A node without relative has its root signature different to its own variables. (2) The variable 
 is not equal to the degree of . (3) The invariant is not satisfied. (4) A node with 
 (that is,  is not involved in a rerouting process) has a root signature bigger than that of its parent; (5) A node with 
 (that is,  is involved in a rerouting process) has a root signature different from that of its tentative new parent. (6) A root  with 
 and a signature 
 that does not match is own. (7) A node involved in a rerouting process whose parent’s parent is itself. (33)
 
 

Examples of the possible situations are presented in Fig. 6.


Download : Download high-res image (437KB)
Download : Download full-size image
Fig. 6. We present various situations arising when evaluating predicate  to true. The node color denotes its 
 variable, the top right circle denotes its 
 variable (light gray represents no color), the bottom right square denotes 
 variable (light gray represents no color), and the arrow visually represents the parent–child relationship. Under each situation, the multiset of numbers represents which conditions of predicate  evaluate too true. For example, in the first situation,  has degree four yet announces a degree of , hence condition (2) of predicate  evaluates to true. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.)

4.2. Correctness
Theorem 6

Algorithm ST solves the spanning tree-construction problem in a silent self-stabilizing way in any -node graph, assuming the absence of spanning cycle, the state model, and a distributed unfair scheduler, using  bits of memory per node.

Proof of Theorem 6 Let 
 
 denote the color of ’s neighbor with the maximum degree, and in case there are several such neighbors, the one with maximum color. (34)
 

Lemma 8 Invariant

For every node  such that 
 and , 
 
 remains true.

Proof

Proof by induction

Basis case:
When a node  starts the algorithm it chooses for a parent the node with the maximum degree, if there exist more than one it chooses the one with the maximum identifier among the ones with the maximum degree. So if a node picks a parent  at the first execution of the algorithm it takes 
 
 so for these nodes the invariant is preserved. For the nodes  which are a maximum local. We denote by  the node 
 
. Suppose that at the first execution of ,  choose the node  as a parent, that means 
 and 
. So after this execution 
, so now  can choose  as a parent and the invariant is preserved for node . So after one execution of the algorithm for all the nodes the invariant is preserved.

Assumption:
Assume true that after  steps of execution, the algorithm preserved the invariant.

Inductive step:
Let us consider a node , by the assumption we have 
 
The node  cannot change its children it can only change its parent, and only if  is a root (see rules 
 and 
 of Algorithm 5). So for 
 
, the rule 
 assigns as a parent a new neighbor  () so the invariant is preserved. The rule 
 assigns as a parent of  a child of  so the invariant is preserved. □

Lemma 9

The descendants  of  with 
 have 
.

Proof

Proof by induction on the value 
 with  descendants of 

Base case:
Each node  with 
 and  has 
 and 
, otherwise an error is detected. A node  takes a parent iff there exists a neighbor  of  such that 
, and in this case  maintains its variable 
, so the claim is satisfied (see Rule 
).

Assumption:
Assume that there exists a configuration  where for every node , all the descendants  of  with 
 have 
.

Inductive step:
We consider Configuration . For a node  and every descendant , the assumption gives the property that if 
, then 
. Let us now consider the case where there exists a neighbor  of  with 
.

–
If  is the parent of , 
 takes the value of 
 (see rule 
). By the induction assumption, we have 
 (as a parent of ,  is also a descendant of ). So, 
 remains inferior or equal to 
.

–
By the induction assumption, if 
, then  cannot be a descendant of .

–
If  is not in the same subtree of ,  cannot change its parent because  is not a root (see rule 
). So  changes its 
 to 
, but it sets 
 (see rule 
).

Now, if there exists a neighbor  of  such that 
, then to execute rule 
,  must be a root. We obtain a contradiction with our assumption that  is a descendant of . To conclude, if  is the descendant of  in configuration  and it remains a descendant of  at configuration , and the value of 
 remains empty, then 
 in configuration . □
Lemma 10

If there exists an acyclic spanning structure in Configuration , then any execution of a rule maintains an acyclic spanning structure in Configuration .3

Proof

Proof by induction on the size of the acyclic spanning structure.

Basis case:
By contradiction: Remark that, thanks to Algorithm , there exist a total order between the neighbors of a node. Let us consider three neighbor nodes  such that in Configuration ,  and  have no relatives. Then all three nodes are enabled by rule 
. Let us suppose for the purpose of contradiction that in Configuration  a cycle exists. More precisely: 
, 
 and 
, to achieve that:

1.
 must choose  as a parent, for that 

2.
 must choose  as a parent, for that 

3.
 must choose  as a parent, for that 

We obtain a contradiction between (1), (2) and (3).
Assumption:
Assume true that in configuration  there exists an acyclic spanning structure.

Inductive step:
Let us consider a node , a node  takes a new parent only in two cases, and in both cases,  must be a root. Let us consider first rule 
, let  be the neighbor of  with 
 and 
. By Lemma 9,  is not a descendant of , so if  takes  as a new parent, an acyclic spanning structure is preserved. Now, we consider rule 
. Let  be the neighbor of  such that 
 and 
. In this case,  is either a child of  with 
, or  is not a child of  with 
. If  is a child of ,  takes  as a parent. Remark that the first action of  is to delete its parent (see rule 
, and consider the fact that all other rules require 
), so we do not consider this case as a cycle. If  is not a child of  with 
, by Lemma 9  is not a descendant of . Now, when  takes  as a parent, this action maintains an acyclic spanning structure. To conclude, Configuration  maintains a acyclic spanning structure. □

Lemma 11

If  is a node such that 
, and every ancestor of  (and  itself) has . Then  is an ancestor of , or  itself.

Proof

Suppose for the purpose of contradiction that , and  is not an ancestor of . By Lemma 10,  is an element of a sub spanning tree . Let  be the oldest ancestor of  such that 
. By hypothesis, every ancestor  of  (including ) has 
. By Lemma 9, we have 
, which contradicts 
.  □

Lemma 12

Executing Algorithm  constructs a spanning tree rooted in the node with the maximum degree, maximum color, maximum identifier, assuming the state model, and a distributed unfair scheduler.

Proof

Let  be the function defined by: 
where  is the node with 
 with . Now, let  be the function defined by: 
 Remark that a node  cannot have 
 and 
. Otherwise an error is detected through predicate .

Let 
 be the potential function defined by: 

Let  be a configuration such that , and let  be a node in  such that  is enabled by a rule of Algorithm . If  executes rules 
, 
, or 
, then 
 increases and we obtain 
. Now, if  executes rule 
, this implies 
 is not empty. After execution of 
, 
 become empty, so  decreases by one. Finally, if  executes rule 
, it implies that  had 
, and now 
. As a result, 
.

Therefore, we obtain 
. By Lemma 10, Lemma 11 we obtain the property that when , a spanning tree rooted in  is constructed.  □

Lemma 13

Algorithm ST converges in 
 steps.

Proof

Direct by the potential function .  □

We adapt ST to use compact identifiers and obtain Algorithm C-ST. It is simple to compare two compact identifiers when the nodes are neighbors. Along the algorithm execution, some nodes become non-root, and therefore the remaining root of fragments can be far away, separated by non-root nodes. To enable multi-hop comparison, we use a broadcasting and convergecast wave on a spanning structure to assure the propagation of the compact identifier.

Theorem 7

C-ST solves the spanning tree-construction problem in a talkative self-stabilizing way in any -node graph, assuming the absence of spanning cycle, the state model, and a distributed unfair scheduler, in  bits of memory per node.

Let  a node that wants to broadcast its compact identifier. We add a variable  to our previous algorithm. This variable checks whether every descendant or neighbor shares the same compact identifier at the same phase before proceeding to the convergecast. More precisely, a node  must check if every neighbor  has 
, and if every child has 
. If so, it sets its variable 
, and the process goes on until node . As a consequence,  increases or restarts its phase and assigns  to .

Lemma 14

Algorithm C-ST converges in 
 steps.

The proof of Theorem 7 mimics the proof of Theorem 6.

5. Self-stabilizing leader election
We now present the final assembly of tools we developed to obtain a self-stabilizing leader election algorithm. We add to Algorithm  an extra variable  that is maintained as follows: if a node  has no parent, then 
, otherwise, 
. Variable 
 is meant to be the output of the leader election process.

Our self-stabilizing leader election algorithm results from combining several algorithms. As already explained (see Fig. 1), a higher priority algorithm resets all the variables used by lesser priority algorithms. Moreover, lesser priority algorithm does not modify the variables of the higher priority algorithms. Algorithms are prioritized as follows: , ,  and . First, starting from an arbitrary configuration,  eventually guaranteed that colors form a distance two coloring in the network, and those colors never change thereafter (the algorithm is silent). Then,  ensures that no cycles or fake spanning tree go undetected forever. If one is found,  is used (with a higher priority) to destroy it. So, after  terminates (and it does since it is silent), no cycle of fake spanning tree exists.

Only algorithm  is talkative, but the number of steps before electing a single leader forever is bounded once Algorithms , , and  all terminate. Thanks to Theorem 7, we obtain in a finite number of steps a spanning tree rooted in the node with the maximum degree, maximum color, and maximum identifier (in lexicographic order). Adding a leader variable as suggested in this section to Algorithm  guarantees that only the root of the spanning tree  has 
 and every other node  has 
. Since the root of the spanning tree remains the same forever, so does the elected leader.

Theorem 1

Algorithm called C-LE solves the leader election problem in a talkative self-stabilizing manner in any -node graph, assuming the state model and a distributed unfair scheduler, with  bits of memory per node.

Proof of Theorem 1

We first need to show that the number of activations of rules of algorithm  are bounded if there exist nodes enabled by ,  or . Let us consider a subset of the nodes  enabled for at least one of these algorithms, and by  the nodes enabled by rule . The nodes in  belong to some spanning trees (possibly only one), otherwise at least one of rules of  or  would be enabled. So, there exists a node in  that is enabled by algorithm . Algorithm  is talkative, but it runs by waves, and its waves require that all neighbors of a node  have the same  at each phase. As we consider connected graphs only, there exists at least one node  in  with a neighbor  in . Then, there exists a configuration 
 where the rules of  are not enabled, because  cannot have the same  at each phase (since  is not enabled by rules of ). So, only Algorithms , , and  may now be scheduled for execution, as they have higher priority. As they are silent and operate under an unfair distributed scheduler, we obtain convergence.

Let us now consider a configuration  where no node is enabled for Algorithm , , and C-Break. There exists a node enabled by Algorithm . Thanks to Theorem 7, we obtain a spanning tree rooted at the node with the maximum degree, maximum color, and maximum identifier. As a consequence, only the root  has 
 and every other node  has 
.  □

6. Conclusion
We presented the first self-stabilizing leader election for arbitrary graphs of size  that uses  bits of memory per node, breaking a long-standing lower bound. Our solution does not require any weakening of the usual self-stabilization model, in particular it withstands the most general scheduling assumption: the unfair scheduler. Besides tree construction and leader election, our research paves the way for new memory efficient self-stabilizing algorithms. For example, some of the solutions for self-stabilizing maximal matching construction use a fixed number of “pointer to neighbor” variables [33]. Using our distance two coloring process would permit to go from  to  bits of memory per node.

In the case of ring shaped networks, an important byproduct of our approach is that, with respect to previous work [15], we no longer require the hypothesis of weak fairness (simple progress is sufficient), while the space complexity is not altered. Indeed, in a ring, , so our space complexity becomes  bits per nodes, which is the same as in previous work [15].

Although there exist several techniques and methods to prove self-stabilization is a systematic manner [20], [26], [27], [28], [41], in the context of self-stabilization they are currently limited to systems that reach a fixed point (a.k.a. a fixed single configuration) after finite time. This implies they may only be used for silent protocols. Alas, silent solutions to leader election require  bits per node [23]. Instead, we developed a systematic approach based on potential functions that allow to obtain both correctness proofs and step complexity results. We plan to further formalize these techniques in future work.