Abstract
Recent trends in high-performance computing and deep learning have led to the proliferation of studies on large-scale deep neural network training. However, the frequent communication requirements among computation nodes drastically slow the overall training speeds, which causes bottlenecks in distributed training, particularly in clusters with limited network bandwidths. To mitigate the drawbacks of distributed communications, researchers have proposed various optimization strategies. In this paper, we provide a comprehensive survey of communication strategies from both an algorithm viewpoint and a computer network perspective. Algorithm optimizations focus on reducing the communication volumes used in distributed training, while network optimizations focus on accelerating the communications between distributed devices. At the algorithm level, we describe how to reduce the number of communication rounds and transmitted bits per round. In addition, we elucidate how to overlap computation and communication. At the network level, we discuss the effects caused by network infrastructures, including logical communication schemes and network protocols. Finally, we extrapolate the potential future challenges and new research directions to accelerate communications for distributed deep neural network training.

Previous
Next 
Keywords
Distributed deep learning

Communication optimization

Parallel algorithms

Network infrastructure

1. Introduction
Currently, we are in an unprecedented and historic era of deep learning research in which waves of deep neural networks have swept through several application domains, ranging from autonomous driving [24], computer vision [29], [134], and natural language processing [18], [28] to recommendation systems [34]. The popularity of deep learning promoted the development of DNN architectures, including fully connected neural networks (FCNN), convolutional neural networks (CNN), recurrent neural networks (RNN) and its variants (LSTM [61], GRU [28]). These neural networks have achieved state-of-the-art performance across various domain-specific tasks. Taking computer vision as an example, well-designed DNNs, such as GoogLeNet [109] and ResNet-50 [58] trained on ImageNet [97] dataset, have beaten humans on image classification tasks.

Regarding good performance, DNNs tend to be deeper and more sophisticated and are trained with the larger datasets. The rapid increase of data volumes and model sizes has resulted in large number of computations, indicating that DNN training is time-consuming and can extend over several days or weeks. High-performance hardware, such as graphics processing units (GPU) [79] and tensor processing units (TPU) [67], are applied to accelerate the training time.

Beyond using high-performance hardware, paralleling and deploying DNN training tasks on multiple nodes (consisting of one or more machines) is another practical approach. Under these conditions, each node only executes part of an entire computation task. However, due to the frequent communication requirements for exchanging large amounts of data among the different computation nodes, communications overhead is a critical bottleneckin distributed training. With the growth of the cluster scale, communications overhead has increased explosively. Such a phenomenon considerably diminishes the advantage of parallel training, as a majority of the training time is spent on transferring data. When high-performance hardware accelerators are used, the proportion of time spent on communication increases further because they only decrease the computation overhead, whereas the communications overhead is unchanged.

In this paper, we primarily investigate how to deal with communications overhead for distributed DNN training. Because distributed deep learning is a cross-disciplinary field, both deep learning and distributed network communities have proposed communication optimization strategies from their own perspectives. In this survey, we bring together, classify, and compare the large body of work on communications optimization for distributed DNN training from the different research communities that contribute to this area. An overview of optimization strategies is shown in Fig. 1.


Download : Download high-res image (370KB)
Download : Download full-size image
Fig. 1. Overview of communication optimization strategies. We divide optimization strategies into two categories: communication reduction and scheduling algorithm part (referred to as algorithm level) and network traffic execution part (referred to as network level).

Previous studies have covered many research domains ofdistributed deep learning. Ben et al. [13] provided a detailed concurrence analysis of DNNs from different levels. Various training algorithms and modern state-of-the-art training frameworks were studied by Chahal et al. [19]. A recent review by Mayer et al. [83] discusses the challenges of managing large deep learning systems on a distributed infrastructure. Mittal et al. [86] reported optimization techniques for deep learning applications of GPUs from an architecture and a system-level aspect. Some surveys have discussed communication optimization issues in distributed deep learning, but we provide a broader investigation. In particular, we discuss optimization from high algorithms level to low network level, which is a perspective that was missing from previous surveys.

The rest of this paper is organized as follows: In Section 2, we introduce deep learning’s background and provide an overview of distributed DNN training. In the next two sections, we discuss the optimization of algorithms and networks, respectively. Section 3 discusses communication rounds reduction, gradient compression and computation–communication overlap, whereas Section 4 introduces logical communication architectures and network protocols. Finally, we conclude the entire paper and highlight potential challenges and research directions in Section 5.

2. Background
In this section, we provide a brief introduction to large-scale distributed deep neural network training. Currently, the most popular method to train DNNs on a single-machine remains mini-batch based stochastic gradient descent (SGD) algorithm [16], [17] with error back-propagation [96]. When DNN training moves to parallelization, several problems need to be considered: (i) which part of the training task can be parallelized, (ii) the architectures of the computation nodes, and (iii) when to synchronize gradients.

2.1. Stochastic gradient descent
SGD and its variants have become the workhorse algorithms for training DNN models. Suppose that we want to train a DNN model to minimize the loss function  averaged on a given dataset , where  represents the DNN’s output with parameter , and  is an input data instance corresponding with the true label . The application-specific loss function  measures the difference between outputs and true labels. SGD will update model parameters and iteratively converge towards the minimum of the loss function as follows [16]: 
 
where  is the number of training instance in each mini-batch,  is learning rate and 
 are the gradients of current mini-batch.

2.2. Data and model parallelism
Data parallelism and model parallelism are two commonly training methods used in distributed deep learning. In data parallelism, the entire training dataset is randomly and equally divided into  parts and dispatched on  nodes, see in Fig. 2(a). Each node maintains a model replica along with its local parameters 
. The training process is as follows:

Step 1.
Each node reads a mini-batch of the training data and executes forward and backward propagations to calculate its local gradients 
.

Step 2.
Each node sends the local gradients to a master node. After receiving gradients from all the nodes, the master aggregates these gradients and updates the model by 
 
.

Step 3.
The master broadcasts the latest model parameters to all other nodes.

Step 4.
Repeat these three steps until the model converges.

With respect to model parallelism [36], [45], different parts of the DNN model are split into different nodes. Hence, the number of parameters on a single node is reduced. Nodes, where the input layer is located, are responsible for reading the entire dataset. For example, only Node 1 and Node 3 read input dataset in Fig. 2(b). Correspondingly, nodes where the output layer is located are responsible for outputting the predicted value of the DNN. The calculations between nodes are no longer independent of each other. Only neurons with connections that cross computation nodes (thick line in Fig. 2(b)) will need to exchange gradients and model parameters. Clearly, data parallelism is suitable for a large training dataset, while model parallelism is applicable when a model is too large to fit in a single node. In this paper, we primarily focus on data parallelism.

2.3. Centralized and decentralized architectures
The (logical) architecture of the computation nodes can affectthe communication modes and the network’s performance.Parameter server [73], [74] is the most popular centralized architecture in distributed deep learning. A parameter server usually includes a server node and several worker nodes. The server maintains global model parameters, whereas each worker stores a local model replica. If the server node has more than one machine, each machine maintains a partition of the entire model parameters. For workers, each worker stores an entire model replica in data parallelism or a part of the model in model parallelism. Workers communicate with the server via a push/pull operation, whereas there is no communication between any workers. The drawback of the parameter server is the bandwidth bottleneck on the server-side with the increment of workers.

Due to this drawback of the parameter server, decentralized architectures have attracted considerable research attention because they incur fewer communication traffic issues [76]. Similar to the parameter server, each node in a decentralized architecture holds an entire model replica, but they use collective communication operation Allreduce instead of push/pull to exchange gradients and parameters. Nevertheless, the Allreduce operation has a variety of implementations with notable differences in performance, which means that it may affect the communications overhead. We will discuss related issues in Section 4.

2.4. Synchronous and asynchronous updates
Owing to differences in network bandwidth and computing power, some nodes may calculate gradients faster, while other nodes may be slower. The main challenge that comes with such circumstances is determining when to synchronize the gradients among multiple computation nodes. There are three different methods that reasonably solve this problem: synchronous, asynchronous, and bounded delay updates.

2.4.1. Synchronous
In the synchronous update, the server does not update the model until it receives gradients from all the workers at each iteration. In other words, faster workers will wait for slower workers. One well-known implementation of the synchronous update is bulk synchronous parallel (BSP) [110]. A characteristic of the synchronous mode is that the server will always receive the latest gradients of all the nodes, which does not affect the model’s convergence. However, fast nodes idle when waiting for slow nodes, leading to a waste of resources and causing the straggler problem that slows the overall training time.

2.4.2. Asynchronous
Asynchronous algorithms such as Hogwild! [92] overcome the above problems. In asynchronous updates, fast workers do not wait for slow workers. One worker may be sending its local gradients to the server while others are calculating their gradients, as shown in Fig. 3. For example, the first worker calculates and pushes its local gradients when  whereas the second worker pushes gradients when , although they both pull parameters from servers at the same timestamp ().

The primary challenge raised by asynchronous updates is data staleness, because fast workers may always use stale parameters which jeopardizes model’s convergence. In addition, the fastest worker is equivalent to update its local parameters by its sub-dataset, which causes the local model to deviate from the global model. To overcome the drawbacks of asynchronous updates, researchers have attempted to limit the staleness of the parameters. In bounded stale updates, fast workers will use stale parameters but the staleness (same as delay in Fig. 3) is limited [32], [60]. The limitation on staleness mitigates the straggler problem to some extent and increases training throughput. However, determining ways to choose the limitation on staleness is a question that is worth discussing because an excessively large value means completely asynchronous updates, while a small value is similar to synchronous updates.


Download : Download high-res image (131KB)
Download : Download full-size image
Fig. 3. Asynchronous updates, where green nodes represent servers while blue nodes correspond to workers. In this instance,  is the timestamp and the arrows indicate the data transmission directions (green for model weights and blue for gradients). (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.)

Adapted from [39].
3. Algorithm level optimization
In this section, we demonstrate how to reduce the communications overhead in distributed DNN training from an algorithm perspective. Algorithm optimizations include reducing communication rounds and volumes as well as increasing the computation–communication overlap ratio. These optimizations are compatible with the underlying network, and most of these can run on top of various network infrastructures andprotocols.

3.1. Communication rounds
When using SGD to train deep neural networks, the entire training process usually consists of multiple epochs and iterations. In regard to parallelization, nodes often exchange data at the end of every iteration. One intuitive way to cut down the communication overhead is reducing the number of data exchanges, or the communication rounds. The number of rounds for a node is related to the batch size and the communication period. Larger batch sizes and more extended communication periods can both reduce the number of data exchanging rounds.

3.1.1. Large batch training
Batch size is a significant hyperparameter that controls the amount of data read by a node in each iteration. A large batch size usually better approximates the distribution of the input data and introduces fewer variances into the gradient estimates than a small batch. Additionally, a large batch of data will take a longer time to process and incur fewer model parameter update; this finding is primarily due to the relationship between batch size, the number of iterations, and training data size. The following equation shows that using a large batch size leads to a reduction in the number of iterations; hence, the parameter updates are infrequent. (1)
 

In distributed training circumstances with a data parallelism scheme, the batch size is the sum of all the local batch sizes of each node. Recall that in conventional distributed deep learning, nodes exchange gradients and model parameters at the end of each iteration. Since the shape and size of gradients and model parameters only depend on the DNN itself, the single iteration communicated message size remains constant, as changing the batch size does not change their shape and size. Therefore, increasing batch sizes reduces the number of iterations and the communications rounds. Take ResNet-50 as an example, if we fix the number of epochs at 100 and set the batch size to 1024 on two machines, the entire training process requires 250,000 iterations, in contrast, with a batch size of 8192 on 16 machines, only 15 625 iterations are required [129]. Table 1 shows ResNet-50 training results with different configurations.

Nevertheless, by directly deploying a parallel SGD with a huge batch size, in practice it will likely suffer generalization ability degradations compared with training a small batch [48], [69]. Fig. 4 illustrates this situation in which, when the batch size exceeds  (we use  to denote  samples), the error on the validation set increases dramatically as the batch size increases. The reason of this finding is that large batch methods tend to converge to a sharp minima of the training function [69]. These minima are characterized by a significant number of large positive eigenvalues in the Hessian matrix, indicating that the curvature around these minima is large. As shown in Fig. 5, sharp minima often generalize less well. Conversely, small batch method converge to flat minima, which is characterized by having many small eigenvalues for the Hessian matrix and a small curvature. Observations show that the loss function landscape of a DNN is such that large-batch methods are attracted to regions with sharp minima and are unable to escape from them [69].


Table 1. Compare ResNet-50 training detail with different works.

Works	Batch size	Hardware	Top-1 Accuracy	Training time
He et al. [58]	256	Tesla P100 × 8	75.3% (baseline)	29 h
Goyal et al. [48]	8k	Tesla P100 × 256	76.3%	1 h
Cho et al. [26]	8k	Tesla P100 × 256	75.0%	50 min
Smith et al. [104]	8k16k	TPU (256 tensor cores)	76.1%	45 min
Codreamu et al. [33]	32k	KNL ×1024	75.3%	42 min
You et al. [125]	32k	KNL ×2048	75.4%	20 min
Akiba et al. [3]	32k	Tesla P100 × 1024	74.9%	15 min
Jia et al. [66]	64k	Tesla P40 × 1024	76.2%	8.7 min
Ying et al. [124]	32k	TPU ×1024	76.3%	2.2 min
Mikami et al. [85]	54k	Tesla V100 × 3456	75.3%	2.0 min
Yamazaki et al. [122]	80k	Tesla V100 × 2048	75.1%	1.2 min
Many methods have been proposed to prevent models from converging into sharp minima, including learning rate scaling rules [48], [70], various warm-up schemes [48], [126], and layerwise adaptive rate scaling (LARS) [125], [128], [129]. The scaling rules dictate that the learning rate should increase to prevent a loss of accuracy with an increase of the batch size. There are two rules of increasing learning rate: Sqrt Scaling Rule, [70] and Linear Scaling Rule, [48]. When the batch size is multiplied by , the first rule multiplies the learning rate by  to keep the variance of the gradient estimator constant [70], while the second rule multiplies the learning rate by  based on an assumption introduced by [48]. When using the linear scaling rule, the learning rate usually is too large at the beginning for the model to converge. Hence, at the very beginning of training (i.e., 5 epochs), we slowly and smoothly increase the learning rate to the suggested value of the linear scaling rule. This scheme is called the gradual warm-up [48]. To deploy a warm-up on the RNN (i.e., LSTM and GRU) training process, You et al. [126] proposed a linear-epoch gradual warm-up (LEGW) scheme, in which the warm-up epochs are multiplied by  when increasing the batch size  times. LARS applies different learning rates for different layers in DNN because the distribution of gradients and parameters varies for different layers [125], [128], [129]. However, LARS performs poorly for attention models, indicating that its performance gains are not consistent across all tasks [127]. Therefore, You et al. [127] proposed a general layerwise adaptive large batch optimization technique called LAMB, which performs well across various tasks such as BERT [38] and ResNet-50 training with minimal hyperparameter tuning. For RNN training, Chen et al. [22] proposed a blockwise model update filtering (BMUF) to train LSTM on 16 GPUs and achieved a near-linear speedup.


Download : Download high-res image (135KB)
Download : Download full-size image
Fig. 5. A conceptual sketch of flat and sharp minima. The -axis indicates value of the loss function and the -axis the variables (parameters).

Adapted from [69].
3.1.2. Periodic communication
Recall that when training DNNs using a traditional distributed SGD, communication often occurs at the end of each iteration. Suppose that we parallel a DNN training task on  nodes with  iterations, the complexity of rounds of a vanilla parallel SGD is . Due to such high complexity of communication rounds, many research studies suggest reducing the frequency of exchanging gradients and parameters.

In model averaging, individual model parameters trained on local nodes are averaged periodically. We consider averaging operations occur at most for  iterations, where  is a factor of the period. Averaging occurs in every iteration, i.e., , which is identical to a vanilla parallel SGD. Conversely, averaging occurs only at the end of the training, i.e., , which is equal to one-shot averaging. The case of  is another common setting. Fig. 6 depicts three cases. Experimental works [84], [108], [133], [138] have verified that model averaging can reduce the communications overhead of the training time as long as the period is suitable. In addition, some theoretical studies [7], [105], [132], [137] have analyzed why model averaging can achieve good convergence rates (see Table 2).


Table 2. Comparison of periodic averaging SGD.

Works	Communication rounds ()	Linear speedup
vanilla Parallel SGD [37]		✓
One-Shot [84], [138]		
K-AVG [137]		
PR-SGD [132]	
✓ when 
Local SGD [105]	
✓
RI-SGD [55]	
LUPA-SGD [54]	
✓
COCOD-SGD [101]	
✓

Download : Download high-res image (86KB)
Download : Download full-size image
Fig. 6. Comparison of vanilla parallel SGD, local SGD and one-shot averaging, where a green block indicates computation and a yellow block corresponds to communication. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.)

The one-shot averaging, which only requires communication at the end of the training, is an extreme case of model averaging. It has been shown that one-shot averaging has satisfying performance on both convex [138] and some nonconvex [84] optimization problems. However, Zhang et al. [133] reported that certain nonconvex optimization problems cannot be solved by one-shot averaging. As a remedy, these researchers suggested that more frequent averaging can improve performance.

Between one-shot averaging and vanilla parallel SGD, tremendous works have focused on periodical averaging, in which communication occurs every  iterations. There is another aspect of research which attempts to reduce communication cost by adding redundancy into a training dataset. For instance, Haddadpour et al. [56] show that by adding a proper amount of redundancy through coding theoretic means, a linear regression can be solved through one round of communication. Additionally, Haddadpour et al. [55] demonstrate that by properly infusing redundancy into the training data with model averaging, it is conceivable to significantly reduce the order of communication rounds as PR-SGD [132] with less accuracy degradation for a general nonconvex optimization. The advantages of model averaging have been examined from a practical point of view in [108]. Specifically, these findings show that model averaging performs well empirically in terms of reducing communication costs for a given accuracy. Arjevani et al. [7] studied the theoretical lower bounds of the number of communication rounds required for different settings in order to solve distributed convex learning and optimization problems.

In addition to model averaging, an asymmetrical push/pull operation can also increase the communication period. Dean et al. [36] proposed a feasible solution called asymmetrical push/pull, in which workers request update parameters from servers every 
 step, and send gradients to servers every 
 step (
 may not equal to 
). However, they only provide experimental results for asymmetrical push/pull, and the convergence analysis is not provided. Chen et al. [21] developed Lazily Aggregated Gradient (LAG) methods, in which workers and servers synchronize gradients and parameters only when some conditions are violated. These conditions are generated and exported by the estimation of the objective value descent for one iteration. LAG has convergence guarantees both experimentally and theoretically. When using nine workers to train a linear regression model, LAG can decrease the parameter server communication complexity from 5283 to 1756 , compared with the vanilla parallel SGD.


Download : Download high-res image (211KB)
Download : Download full-size image
Fig. 7. Different compression schemes. The first is one layer’s original gradient as a matrix, while others are the output of various compression schemes that take the first as input. Red means negative values and green means positive values. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.)

Adapted from [112].
3.2. Gradients compression
Each computation node requires communication operations to exchange gradients and model parameters during the distributed training. Using 32 bits variables to represent each element in the gradients and model parameters is the most commonly used configuration. When the size of the gradients and model parameters is large, the communication bottleneck caused by exchanging a large amount of 32 bits single-precision variables impairs the advantages of parallel SGDs. For example, consider training the BERT architecture for neural language processing tasks with approximately 340 million parameters, which implies that each full precision exchange between nodes is over 1.2 GB [38]. Therefore, we will discuss how to reduce message size in this section.

Gradient compression reduces the communication traffic by compressing the gradients transmitted during training. It is easy to implement and performs very well on some DNN models. However, the model’s performance will be affected by these lossy compression methods, which is unacceptable in some areas (i.e., recommendation systems) where the accuracy is sensitive. Currently there are mainly three bandwidth-efficient compression approaches: quantization, sparsification and decomposition. The first, substitutes 32 bits variables for low-precision variables (i.e., 8 bits, 4 bits, or even 1 bit), whereas the second only transports important variables to avoid unnecessary overhead. Since these two approaches are orthogonal, they can be combined to compress the communication further. The third is not as popular as the first two; it transmits small matrices instead of large matrices by a matrix decomposition. Fig. 7 compares various compression methods.

3.2.1. Quantization
As a low-precision data representation method, quantization first discretizes continuous values and maps them to different integers in a range. A standard method of quantization is to use the sign of each element to represent the gradient. As far as we know, one-bit SGD [99] has pioneered the research of communication quantization for distributed deep learning. Recently, a similar algorithm called signSGD [14] was proposed for nonconvex optimization.

Because the difference between gradients processed by sign-based quantization and original gradients is too large, the quantized gradient is often a biased estimate of the original gradient [68], which makes the model converge slowly with a significant accuracy loss [50]. To address this issue, we use the error feedback technique to correct the deviations of direction accumulated in the previous iterations. Error feedback maintains a vector  to store the accumulated difference between the quantized and the original gradients. The following equations give the rationale of the error feedback, where subscript  represents the  th iteration,  is a decaying factor [118] and  is the quantizer: (2a)
(2b)

Using error feedback, one-bit SGD achieves  speedup in training a speech DNN with little accuracy loss [99]. Karimireddy et al. [68] fixed signSGD using error feedback techniques and performed a theoretical analysis of the convergence of the method on nonconvex smooth functions.

Although error feedback can prevent the model from significant accuracy loss, gradients processed by such aggressive sign-based quantizer are still biased [106]. The other way to cope with biased gradients is to avoid producing them; hence, many unbiased methods have been proposed. These methods introduce randomness into the quantization operator, meaning that the quantized value of each element is subject to one probability distribution. A function 
 that maps a deterministic input to a random vector with a quantization level  is called a stochastic quantizer, if the following holds for 
: (1)unbiased quantization 
; (2) bounded variance 
 where 
 is a function of  and . Due to the extra randomness introduced by stochastic quantization, such quantizers must be an unbiased estimate of the original gradients and have a variance bound that ensures convergence as in a vanilla SGD.

In two parallel works, TernGrad [116] and QSGD [5], both use stochastic unbiased gradients. For a given gradient 
, TernGrad [116] compresses each values into three levels  by the following function: (3)
where (4)
is the maximum norm of vector , and  is a random binary vector that follows a Bernoulli distribution [116]. It can be proven that the quantized gradient 
 using a ternary function is an unbiased estimate of original gradient [116]. QSGD [5] has a similar but more sophisticated quantization function and provides a hyperparameter  to control the representation’s precision levels. A smaller  means less communication overhead with a higher level of information loss, whereas a larger  means a lower level of information loss but more communication overhead.

Bidirectional quantization is often used in parameter server architecture to reduce worker–server communication in both directions. SignSGD [14] applies the majority vote on the server-side to enable double quantization. TernGrad [116] offloads the model update operation to workers, and only the compressed gradient is transmitted between the workers and the server. In contrast, Asy-LPG [131] quantizes the gradients and the model parameters in worker–server and server–worker direction respectively.

To further reduce communication traffic, several works [5], [77], [118] apply efficient lossless coding techniques (i.e., Elias coding [42]) after quantization.

3.2.2. Sparsification
A limitation of quantization is that it can only compress a communication message up to 32 times, because we need at least one bit to represent numbers. However, sparsification does not have such a limitation; it only transmits values that play essential roles in the model update. A zero in the gradient means that the associated parameter is rarely updated, meaning that such gradient values have little effect on parameter updates. Therefore, it is a waste of bandwidth to transmit gradients that contains many zeros.

As far as we know, Strom et al. [107] were the first to clip gradients via a static threshold, the element will not be transmitted if its absolute value below this static threshold. However, it is not very easy to select a reasonable threshold for various DNNs. To remedy this issue, subsequent works [20], [41] applied local selections and dynamic threshold instead of a static threshold. A variant of threshold sparsification is the Top-K method, in which each node only transmits  largest (absolute) values [2], [78], [95], [98]. Theoretical work [6] gave a convergence analysis of Top-K sparsification under some assumptions, and pointed out that sparsification is equivalent to a stale update.

To ensure model convergence, we usually select values from gradient residuals, rather than manipulating the original gradients directly. The gradient residual is the sum of all previous gradients accumulated locally at each node [107]. Stich et al. [106] proved that with accumulated gradients, sparsified SGD had the same convergence rates as vanilla parallel SGD. Deep gradient compression (DGC) [78] introduced momentum correction, local gradient clipping, momentum factor masking, and warm-up training to achieve better performance. By these methods, DGC compresses the gradient size of ResNet-50 from 97 MB to 0.35 MB without accuracy loss [78]. Wangni et al. [115] proposed random sparsification, which drops out indices of gradients randomly. To guarantee the sparsified vector is unbiased, the remaining part is appropriately amplified. Experiments show that random dropping causes little accuracy losses of 3-layer CNNs on CIFAR-10 datasets [71].

Since quantization and sparsification are two orthogonal methods, they can be integrated for deep compression. The integration is straightforward based on a centralized architecture (Parameter Server). However, there is a challenge that needs to be resolved for sparsification in a decentralized setting. Recall that sparsification only transmits “important” values in the gradient, which means that each node may receive different nonzero indices (dimensions) for the same gradient [13]. Fortunately, there are a series of implementation [44], [87], [95], [136] for sparse communication. Using such sparse collective communication protocols, we can implement a sparse gradient exchange in a decentralized architecture and combine it with quantization.

3.2.3. Matrix decomposition
In the field of compression, an emerging method is to decompose a large gradient matrix into several small matrices before transmission, and then reconstruct after receiving it. Transmitting two small matrices has less communication overhead than transmitting one huge matrix. This method is feasible because of the correlation between gradients. GradiVeQ [130] exploited such linear correlations between CNN gradients and applied principal component analysis (PCA) [117] to reduce the gradients’ dimensions. In addition, the proposed method GradiVeQ also enables direct aggregation of compressed gradients. ATOMO [114] is a general compression method built on top of a singular value decomposition (SVD) [46]. For a given gradient, ATOMO will produce a random unbiased gradient with minimal variance [114]. PowerSGD [112] performs a low rank decomposition with error feedback, and avoids the computationally expensive SVD step in ATOMO [114] to achieve better scalability.

3.3. Computation–communication overlap
Since the gradient is generated in order from the last layer to the first layer during back propagation, there is no need to wait for the calculation of the previous layer to complete before sending the gradient of the later layer. In other words, former layers’ computation is independent of the latter layers’ communication, and the latter layers’ parameters update is independent of the former layers [135]. Hence, we can transmit the gradient of the th layer while computing the th layer’s gradient. In fact, computation–communication overlap does not really reduce the communication time, it just performs computation and communication simultaneously as possible. Therefore, this approach can combined easily with other optimization strategies.


Download : Download high-res image (231KB)
Download : Download full-size image
Fig. 9. Parameter server architectures.

Poseidon [135] provided a wait-free backward propagation (WFBP) scheduling algorithm. WFBP makes each layer start its communication once its gradients are calculated after backward propagation. However, different layers may have different computation and communication times, which means Poseidon may not outperform than the FIFO scheduling on some specific network models.

Shi et al. [102] pointed out that there are three cases of WFBP. Fig. 8 shows that both Case 1 and Case 2 are ideal cases in which we can easily hide the communication time. Case 3 could happened more often, especially in the high latency or low bandwidth network environment. They find that two or more small messages can be merged into one large message before being sent, and the merged gradients can be communicated with a smaller cost that is easily hidden by the computation. The newly proposed method, named merged-gradients WFBP (MG-WFBP), achieves much better scaling efficiency than WFBP.

Priority-based parameter propagation (P3) [63] extendedWFBP with priority-based scheduling, in which the first layer obtains the highest priority, and the last layer obtains the lowest priority. The tensor with the highest priority is processed first during the communication phase, regardless of when it is generated. P3 uses the tensor partition technique to handle Case 3, shown in Fig. 8. Tensor partition technique splits layers’ parameter matrices into small proper pieces and assigns priorities to every slice based on their parent layers’ processing order in the forward propagation. Hashemi et al. [57] exploited the execution order of computational graphs and proposed two heuristic scheduling algorithms: Timing-Independent Communication (TIC) and Timing-Aware Communication (TAC). Both algorithms are built on the properties of communication operations, such as communication dependency, communication time, and directly dependent compute load, etc. ByteScheduler [89] applies tensor partition and priority-based scheduling like P3, and designs a credit-based preemption approach to fully utilize the network’s bandwidth. Credit-based preemption works similar to a sliding window, where credit is the window size. Small tensor pieces in the sliding window are sent simultaneously. ByteScheduler uses Bayesian optimization to find the ideal credit and partition sizes.

4. Network level optimization
In this section, we mainly concentrate on optimizing low-level network infrastructures, including advanced centralized and decentralized architectures, messaging libraries, and network protocols. The benefits of network level optimization are intuitive. Modifying the low-level communication protocol will not have too much impact on the high-level training algorithm. However, network level optimization maybe not easy to implement, and the performance depends on the design of the distributed training system.

4.1. Logical architectures
Centralized and decentralized architectures have differentcommunication patterns and performance. We will discuss modern and advanced architectures in this subsection.

4.1.1. Parameter server
Fig. 9(a) depicts the traditional parameter server architecture, in which the server is responsible for storing and updating global model parameters. The server is prone to network bottlenecks, especially when there are many working nodes. Tree-based parameter servers [53], [62], [81] alleviate such bottlenecks to a certain extent.

Mai et al. [81] treat each server as the root and build a spanning tree connecting all workers. All workers are leaf nodes in the spanning tree, whereas other nodes in the tree are servers. Each worker pushes gradients to their parents. The parents aggregate all received gradients and push the results upstream towards the root where the last step of aggregation is performed. The global weights are multicasted from the top root server down to the leaf worker. Through such a spanning tree, the communication overhead, in both push and pull operations is reduced. Gupta et al. [53] proposed similar tree-based architectures like [81]. To further alleviate the network traffic, the root server broadcasts the global weights directly down a tree constructed within all workers, see in Fig. 9(b). Heish et al. [62] considered the physical distance of the parameter server and employed an intelligent communication mechanism over wide area networks (WAN) to efficiently utilize the bandwidth. In addition, some works such as Project Adam [25] and Geeps [35] improve the throughput of parameter servers by caching and by isolated communications.


Download : Download high-res image (214KB)
Download : Download full-size image
Fig. 10. The Ring Allreduce algorithm. The first two steps corresponding to Reduce-Scatter, and the last two steps corresponding to Allgather.

Adapted from [100].

Download : Download high-res image (368KB)
Download : Download full-size image
Fig. 11. 2D-Mesh Allreduce across a hypothetical 3 × 3 torus. In the left part, the first half of the tensors (blue) are summed along the vertical dimension while the second half of the tensors (red) are summed concurrently along the horizontal dimension. In the right part, the dimensions are flipped. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.)

Adapted from [124].
4.1.2. Allreduce
The key issue with traditional communication strategies is that, as the number of GPUs increases, the communication costs increase linearly. A classical implementation of Allreduce is a combination of Reduce operation followed by Broadcast which sends the result from the root to all processes. This implies a bottleneck on the root process. The optimized algorithms are based on a few principles: recursive vector halving, recursive vector doubling, recursive distance halving, recursive distance doubling, binary blocks, and ring [91].

To the best of our knowledge, Baidu first introduced a ring-based Allreduce into distributed deep learning [11]. A Ring Allreduce is made of two phases: Reduce-Scatter and Allgather; each phase includes  communication steps when we use  GPUs, see in Fig. 10. Each GPU maintains its local gradients, which are equally divided into  chunks. In the reduce-scatter phase, each node sends and receives different chunks of a stored tensor. For the received chunk, each node adds it to the corresponding position in the buffer. After  steps, each node holds a different part of the global result. In the all-gather phase, each node sends the part of the global result maintained by itself and receives other parts of the global result from other nodes. Each node holds a complete global result after  steps. Hence, Ring Allreduce needs a total of  communication steps. The complete communication process is described in Fig. As early as 2009, Patarasuk et al. [88] proved that a ring-based Allreduce has the optimal bandwidth of the Allreduce algorithms.

Mikami et al. [85] proposed a 2D-Torus Allreduce topology in which the GPUs are arranged in a 2D grid. Each row contains  GPUs while each column contains  GPUs. There are three phases in a 2D-Torus Allreduce: Reduce-Scatter, vertical Allreduce, and Allgather. Although a 2D-Torus Allreduce has one phase more than a Ring Allreduce, its overall communication overhead is still smaller because  and  are less than  [85]. A similar study by Ying et al. [124] aggregates gradients in two phases with a 2D-Mesh topology, which utilizes two parallel ring-based reductions, each summing different halves of the payload along the horizontal and the vertical dimension, see in Fig. 11. The 2D-Mesh algorithm has twice the throughput of gradient aggregations than the 1D Ring Allreduce [124].

Jia et al. [66] proposed hierarchical Allreduce to resolve the problem for a small tensor communication. These researchers split all  GPUs into several groups and conducted a three-phase reduction. Fig. 12 shows the three-phase operations. The first phase is a separate ring Allreduce operation in independent groups, each of which consists of  GPUs. In the second phase, a master node from each group operates Ring Allreduce to get a global result. Finally, the master node in each group broadcasts the global result to every GPU in its group. Compared with a Ring Allreduce, this three-phase hierarchical Allreduce decreases the running steps from  to .


Download : Download high-res image (136KB)
Download : Download full-size image
Fig. 12. Three phases of hierarchical Allreduce, where each orange entry is local master node in each group [66]. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.)

4.2. Message level libraries
The performance of different parameter server systems and various Allreduce algorithms partially depends on the implementation of the message communication libraries. The parameter server is usually run on top of ZeroMQ [59] or gRPC [47]. ZeroMQ is a high performance and low latency asynchronous messaging library that supports multiple communication patterns, and gRPC is a high-performance remote procedure call (RPC) framework developed by Google. As far as we know, the most commonly used message communication libraries in the parameter server are still ZeroMQ and gRPC.

At present, there are many message level communication libraries that implement various Allreduce algorithms and other collective communication algorithms efficiently, including MPI [49], Gloo [43], NCCL [64], Baidu Allreduce [11], Aluminum [40] and BlueConnect [27]. Thanks to the high performance of MPI, there are many optimizations based on MPI_Allreduce, including Horovod [100], MXNet-MPI [82] and TensorFlow-MPI [111]. To reduce communications, Horovod uses tensor fusion that sends several small tensors simultaneously [100]. NCCL implementsmulti-GPU and multinode collective communication primitives that are optimized for NVIDIA GPUs [64]. A series of studies by Awan et al. [8], [10] optimized Bcast operation based on NCCL and CUDA-Aware MPI, respectively. BlueConnect [27] decomposes a single Allreduce operation into a series of parallelizable Reduce-Scatter and Allgather operations to reduce the communication cost. Experiments show that the performance of BlueConnect incurs less communication overhead than Gloo and Baidu Allreduce with more GPUs [27].

4.3. Network protocols
Traditional message level communication libraries are implemented based on the TCP/IP protocol, which handles data sending and receiving by sockets. Each node must create a socket object and establish a connection to the receiver before sending data. Data are processed by the operating system and encapsulated with different protocol headers until its copied into the network interface controller (NIC) buffer, as shown in Fig. 13(a). Such operations are wasteful in distributed training that needs low network latency. Therefore, high performance and low latency networks that run on top of associated network hardware (i.e., InfiniBand) attract much research attention; the two most common are remote direct memory access (RDMA) [93] and internet protocol over InfiniBand (IPoIB) [30]. As illustrated in Fig. 13(c), RDMA allows one machine to directly read and write the memory of another machine without involving the operating system, which enables high performance and low latency networking. The network interface in RDMA is called verbs, which provide two types of communication paradigms: message and memory. IPoIB, as the name implies, encapsulates IP datagrams over an InfiniBand card and enables TCP/IP applications to run on top of InfiniBand without any code modification [30]. However, as shown in Fig. 13(b), IPoIB cannot bypass the host operating system like RDMA in Fig. 13(c).

With the advent of RDMA and IPoIB, tremendous works have been done on improving the performance of distributed training systems, including MXNet [75], [80], TensorFlow [15], [65], [121], CNTK [12], [31], Caffe [9] and the IBM deep learning platform [94], to leverage its high bandwidth and low latency. The memory communication paradigm is the most popular method due to its low memory demands [94], [121]. Some works [31], [94], [121] consider communications between GPUs and explore GPU direct RDMA (GDR), which allows an RDMA NIC to access GPU memory directly without going through host memory. Gradients are aggregated in GPUs by using the GDR technique. Furthermore, Biswas et al. [15] designed an adaptive RDMA-based gRPC to dynamically adjust communication mechanisms for different message sizes in a deep learning workload.

According to experimental results [65], [75], [121], using RDMA and IPoIB to replace the TCP/IP protocol can significantly accelerate training. In addition, RDMA performs better than IPoIB in distributed training [75], [80]. Experiments by Liu et al. [80] report that compared with IPoIB (53%), the RDMA-capable network achieves near-linear (96%) speedup when scaling Inception-v3 training on 100 GPUs. The result also fits the previous description of IPoIB and RDMA verbs.

Recently, Xia et al. [119] rethought the distributed training process of DNNs and designed a bounded loss tolerance transmission protocol, which ignores a fraction of the packet loss during training. The new protocol achieves shorter job completion times than the traditional TCP/IP protocol. However, the random dropping rate has to be tuned when changing the DNN architecture. It is challenging to choose a reasonable dropping rate for various DNNs.

5. Conclusion
It has been shown that communications overhead is a significant obstacle to achieving a desirable performance for distributed DNN training. In this paper, we provide a comprehensive survey of the recent research on communication optimization techniques for distributed DNN training. Both theoretical and experimental studies are investigated. We divide these communication optimization strategies into two dimensions: algorithm level optimization and network level optimization. From the algorithm perspective, we elaborate on techniques to reduce communication volumes and computation–communication overlaps. In terms of network level optimization, we discussed the impact of different topologies and network protocols on distributed deep learning. Below, we highlight potential new research directions and challenges.

Focusing on different deep learning tasks and neural network models. At present, there are a considerable number of communication optimizations that focus on image classification tasks, especially ResNet-50 trained on ImageNet dataset. Natural language processing and recommendation system related tasks have not received notable research attention. For example, the large embedding matrix in the recommendation system model may become a new bottleneck in communication optimization.

Local SGD for nonconvex problems. For periodic communications, there still remains some research opportunities for nonconvex problems. Despite many theoretical works on model averaging, the research on whether the linear speedup with  can be preserved for nonconvex optimizations is still unexplored. Individually, the lower bounds on the number of communication rounds for nonconvex optimizations to achieve linear speedup are an interesting research direction.

Trade-off between model accuracy and compression ratio. The core challenge that requires consideration in gradient compression is the trade-off between model accuracy and thecompression ratio. Conventional approaches to prevent models from diverging include error feedback (for quantization) and local gradient accumulation (for sparsification). More advanced methods such as squared error feedback [77] need to be explored in future works.

Higher computation–communication overlap ratio. The ratio of computation to communication is essential for deploying pipeline training. For the sake of a higher overlap rating, variousalgorithms use different strategies to arrange communication operations and shrink communication time. However, these algorithms are generally heuristic and achieve nonoptimal solutions in such scheduling problems. Better optimization algorithms, such as dynamic programming, maybe be suitable for solving this problem.

Large-scale DNN training on top of different datacenter network topology. The physical topology of the datacenter network has a significant impact on distributed deep learning. A recent work by Wang et al. [113] deployed a parameter server over BCube [51] instead of a traditional Fat-Tree [4] architecture and achieved good performance on LeNet-5 [72] and VGG-19 [103] training. Other network topologies, such as DCell [52], may further accelerate large-scale neural network training.

Communications overhead analysis tools are required. Another critical research issue is the performance model and measurement tools of distributed DNN training. Performance modelsallow us to theoretically analyze the various costs of distributed training [90], [123], and the measurement tools help us study communication behavior and find the bottlenecks in the distributed training tasks. Although some deep learning frameworks including Tensorflow [1] and MXNet [23] currently provideperformance analysis tools, these tools cannot analyze network behavior. Advanced tools for network analysis such as Horovod timeline [100] and SketchDLC [120] are still required.