Abstract
Graph-tensor learning operations extend tensor operations by taking the graph structure into account, which have been applied to diverse domains such as image processing and machine learning. However, the running time of graph-tensor operations increases rapidly with the number of nodes and the dimension of data on nodes, making them impractical for real-time applications. In this paper, we propose a GPU library called cuGraph-Tensor for high-performance graph-tensor learning operations, which consists of eight key operations: graph shift (g-shift), graph Fourier transform (g-FT), inverse graph Fourier transform (inverse g-FT), graph filter (g-filter), graph convolution (g-convolution), graph-tensor product (g-product), graph-tensor SVD (g-SVD) and graph-tensor QR (g-QR). cuGraph-Tensor supports scalar, vector, and matrix data processing on each graph node. We propose optimization techniques on computing, memory accesses, and CPU–GPU communications that significantly improve the performance of the graph-tensor learning operations. Using the optimized operations, cuGraph-Tensor builds a graph data completion application for fast and accurate reconstruction of incomplete graph data. In the experiments, the proposed graph learning operations achieve up to 142.12× speedups versus CPU-based GSPBOX and CPU MATLAB implementations running on two Xeon CPUs. The graph data completion application achieves up to 174.38× speedups over the CPU MATLAB implementation, and up to 3.82× speedups with better accuracy over the GPU-based tensor completion in the cuTensor-tubal library.


Keywords
GPUGraph-tensor
Graph operations
Graph data processing
Library

1. Introduction
There is a huge amount of data generated in diverse domains, such as social networks, sensor networks, biomolecular networks, citation and authorship networks, and e-commerce networks [13]. Many aspects of daily life are being recorded at all levels generating large-scale datasets such as phone trajectories, health data on monitoring devices, banking and financial records, shopping preferences, and so on. These data are irregular with complex structures [9]. Graphs offer the ability to characterize the complex interactions by data modeling. Thus researchers proposed various graphs to represent real-world data, such as scale-free graphs, ring graphs, nearest-neighbor graphs, and random geometric graphs [26]. Entities such as users on Facebook or sensors in a field are modeled as nodes and the connections among users or sensors as edges. For graphs that have a data matrix residing on each node, these data matrices form a tensor with graph structures called graph-tensor, where each frontal slice corresponds to a graph node and the connection between two frontal slices is an edge, as shown in Fig. 1.

Graph-tensor operations are widely used in various applications, such as graph neural networks (graph convolution, graph SVD, and graph QR), computer vision (graph convolution), image processing (graph filter), data completion (graph Fourier transform and inverse graph Fourier transform), and video compression (graph SVD). However, existing works are insufficient to support efficient large-scale graph-tensor computations. First, the running time of CPU-based graph-tensor operations increases rapidly with the number of nodes or the dimension of data on nodes; thus existing CPU-based graph-tensor tools cannot meet the real-time requirements of applications, such as localization and online recommendation. There is a growing impact of graph computing on high-performance GPUs [24]. Second, there are no high-performance graph-tensor computation libraries that support key graph-tensor operations as far as we know. Researchers have to implement and optimize their own graph-tensor operations in a case-by-case manner, which is inefficient and error-prone. For instance, GSPBOX [27] is a popular CPU-based graph processing toolbox. However, the graph operations in GSPBOX have long running time for big graphs, as revealed in our preliminary experiments (Fig. 18 shows g-FT, inverse g-FT, and g-filter on a graph vector of length 20,000 take 3022.36 s, 3006.91 s, and 20,443.11 s, respectively). Moreover, GSPBOX supports only scalar data on nodes and lacks several key graph operations such as graph convolution, graph shift, g-product, g-SVD, and g-QR, which have wide applications in network analysis and image processing. Although the cuTensor-tubal library [36], [37], [38] provides high performance tensor computations that are closely related to our work, it does not support the structural properties of graphs and is unsuitable for graph analyzing applications. Therefore, we are motivated to develop a library of high-performance graph-tensor operations to support diverse applications.

In this paper, we implement high-performance graph-tensor operations for big data and IoT applications on GPU. We design, implement, and optimize a library called cuGraph-Tensor for fast and accurate graph-tensor operations. cuGraph-Tensor implements eight key graph-tensor operations and provides three data types on graph nodes: scalar, vector and matrix. Graphs with these data types can model many real-world applications such as sensor networks, social networks, IoT wireless camera networks, and so on. A graph data completion application is presented in Section 4.3 to demonstrate the usage of the proposed graph-tensor operations. We further propose optimization techniques to improve the computation and memory access efficiency and reduce CPU–GPU communications, which contribute significant performance improvement. We build the cuGraph-Tensor library on top of existing highly optimized NVIDIA CUDA [7] libraries, including cuBLAS, cuSolver, and existing libraries, including Magma [1], KBLAS [4] for efficient GPU computations, as shown in Fig. 2.


Download : Download high-res image (123KB)
Download : Download full-size image
Fig. 1. A third-order graph-tensor is constructed by stacking the matrices on graph nodes in node order. Each data matrix becomes a frontal slice of the graph-tensor.


Download : Download high-res image (230KB)
Download : Download full-size image
Fig. 2. Position of the cuGraph-Tensor library in the system.

Our contributions are summarized as follows.

•
We develop a high-performance GPU library called cuGraph-Tensor of eight graph-tensor operations, including graph shift (g-shift), graph Fourier transform (g-FT), inverse graph Fourier transform (inverse g-FT), graph filter (g-filter), graph convolution (g-convolution), graph-tensor product(g-product), graph-tensor SVD (g-SVD) and graph-tensor QR (g-QR). We encapsulate these operations into an open-source library and provide BLAS-like interfaces for ease of use.

•
We propose optimization techniques on batched computing, computation reduction, memory accesses, and CPU–GPU communications to improve the performance. As a demonstration, we further develop a graph data completion application using the high-performance graph-tensor operations of the cuGraph-Tensor library.

•
We perform extensive experiments to evaluate the performance of the graph-tensor operations and the graph data completion application. The g-shift, g-FT, inverse g-FT, g-filter, g-convolution, g-product, g-SVD and g-QR operations achieve up to , , , , , ,  and  speedups, respectively, comparing with CPU-based GSPBOX [27] and CPU MATLAB implementations running on two Xeon CPUs. These graph-tensor operations with the optimizations in Section 3.2 are on average , , , , , , , and  faster over the GPU baseline implementation. The graph data completion application achieves up to  speedup over the CPU MATLAB implementation, and up to  speedup with higher accuracy over the GPU-based tensor completion in the cuTensor-tubal library [36], [37], [38].

The remainder of this paper is organized as follows. In Section 2, we describe the notations and eight graph-tensor operations. Section 3 shows the implementation and optimizations of the graph-tensor operations on GPU. In Section 4, we evaluate the performance of the graph-tensor operations and the graph data completion application. Section 5 discusses the related works. The conclusions are drawn in Section 6.

2. Graph-tensor operations on GPU
The graph-tensor operations in the cuGraph-Tensor library combine the graph signal processing operations in [29], [30] with the tubal-rank tensor operations in [17], [36], [37], [38]. We introduce key graph-tensor operations and then briefly analyze the GPU parallelisms of these operations.

2.1. Notations and basic operations
Notations: We use lowercase boldface letters 
 to denote vectors, uppercase letters 
 to denote matrices, and uppercase calligraphic letters 
 to denote tensors. We use 
 to denote the number of nodes in a graph, and  for temporary indexing of nodes. Let 
 denote the Hermitian transpose of a complex matrix , and 
 denote the pseudo inverse of .

In the graph-tensor model, the connection relations are modeled as a graph G  (), where 
 is the set of 
 nodes (i.e., vertices) and 
 is the corresponding weighted adjacency matrix. A nonzero weight 
 indicates that there exist the relations (e.g., dependency or similarity) between the th and the th nodes. The neighbors of node 
 are denoted as set 
. For undirected graphs,  is symmetric with 
.

Degree matrix: The degree matrix  is an 
 diagonal matrix, where the value of the th diagonal element is the sum of the weights of associated edges of node 
, i.e., 
 for 
.

Laplacian matrix and normalized Laplacian matrix: The Laplacian matrix  is . The normalized Laplacian matrix  is 
. Let the eigen decomposition of 
, where 
 is the eigen vector matrix.


Table 1. Eight graph-tensor operations in the cuGraph-Tensor library.

Operations	Input	Output	Computation cost	Space cost
g-shift	
, 
g-FT	
, 
Inverse g-FT	
, 
g-filter	
, 
g-convolution	
, 
, 
g-product	
, 
, 
g-SVD	
, 
, 
, 
g-QR	
, 
, 
Graph-tensor: Given a graph G of 
 nodes, without loss of generality, we assume the graph-tensor  is real-valued and define  as a mapping: (1)

It is convenient to write a graph-tensor 
 as 
 matrices 
, whose physical meaning is a graph G of 
 nodes and the th frontal slice 
 associates with node 
. The data element on a node can also be scalar or vector, depending on applications. For tensor 
, the th entry is concisely represented as 
.

2.2. Key graph-tensor operations
We implement eight graph-tensor operations on GPU, including two types: basic operations and composite operations. The basic operations include g-shift, g-FT, and inverse g-FT. The composite operations include g-filter, g-convolution, g-product, g-SVD, and g-QR, which are constructed based on the basic operations. These graph-tensor operations are widely used in various applications such as graph neural networks [31], video compression [35], data completion [16], machine learning [19], pattern recognition [12], feature learning [2], and network analysis [30]. The input and output of the eight graph-tensor operations are summarized in Table 1.

g-shift: The g-shift 
 of 
 is a graph-tensor of size 
, 
, for 
.

g-FT: We define 
 as the spectral domain representation of  by taking the graph Fourier transform. 
.

inverse g-FT: We can also compute  from 
 through 
.

The composite graph-tensor operations require matrix multiplication and matrix decomposition, including eigenvalue decomposition, singular value decomposition, and QR.

g-filter: For 
 on a graph of 
 nodes, the g-filter 
, where  represents a specific transformation on matrix  and 
 operates on diagonal elements of . We first perform the eigen decomposition of  to obtain  and . Then we perform the matrix multiplication between the filter matrix  and each frontal slice of the reorganized graph-tensor 
 called matrix–tensor multiplication. It takes 
 times of matrix–matrix multiplications.

g-convolution: On a graph of 
 nodes, the g-convolution 
 of 
, where 
 is the convolution kernel and  represents Hadamard product (element-wise multiplication). We first perform the graph Fourier transform of the graph kernel . Then, we perform the Hadamard product between the result 
 and the reorganized graph-tensor 
. It takes 
 times of Hadamard product.

g-product: On a graph of 
 nodes, the g-product  of 
 and 
 is a graph-tensor of size 
. We perform matrix multiplication between eachfrontal slice of graph-tensor 
 and 
 called tensor-sliced multiplication. It takes 
 times of matrix–matrix multiplications.

g-SVD: For 
 on a graph of 
 nodes, the g-SVD is given by 
, where  and  are orthogonal tensors of sizes 
 and 
, respectively.  is an f-diagonal tensor of size 
. We perform singular value decomposition of each frontal slice of the graph-tensor 
. It takes 
 times SVD.

g-QR: For 
, the g-QR is given by 
, where  is a tensor of size 
 and  is an 
 upper triangular tensor. We perform QR decomposition of each frontal slice of the graph-tensor 
. It takes 
 times QR.

2.3. Parallelization analysis for GPU architecture


Download : Download high-res image (96KB)
Download : Download full-size image
In the GPU baseline implementation, we implement the matrix–tensor multiplication by exploiting the cuBLAS library instead of multiple scalar multiplication of tensor, which denotes a constant  multiplies a tensor 
 represented as 
 for 
 to improve performance. To enable efficient multiplication, we perform two times of tensor reorganization, which denotes reorganizing the tensor 
 to the tensor 
 for each graph-tensor operations on GPU. We implement multiple matrix computations, including matrix multiplication and matrix decomposition through the for-loop. These computations have high parallelism. In Section 3.2.1, we use the batched computing scheme to implement multiple matrix computations.

For the eight graph-tensor operations, their major computations on GPU are summarized as follows:

1.
g-shift: a matrix–tensor multiplication of the adjacent matrix  and the graph-tensor.

2.
g-FT and inverse g-FT: (a) performing matrix decomposition on the normalized Laplacian matrix  to obtain the eigenvector matrix ; (b) performing matrix–tensor multiplication of  (or 
) and the graph-tensor.

3.
g-filter, g-convolution, g-product, g-SVD, and g-QR: (a)performing g-FT; (b) conducting filtering, convolution,tensor-sliced multiplication, tensor SVD, and tensor QR, respectively in the spectral domain; (c) performing inverse g-FT.

The computations of these eight graph-tensor operations consist of many linear algebra routines such as matrix decomposition, matrix transposition, matrix multiplication, matrix–tensor multiplication, and tensor-sliced multiplication which are well supported on GPU. In addition, the data storage and memory access of graph data need to be delicately arranged to improve data access efficiency.

For example, to compute a g-product , we first compute 
 and 
 obtained by applying the g-FT on  and , then perform matrix multiplication for the frontal slices of these two tensors in the spectral domain to derive the frontal slices of 
. Finally,  is obtained by applying the inverse g-FT on 
, as shown in Alg. 1.

3. The cuGraph-Tensor library on GPU
We design the cuGraph-Tensor library on top of CUDA libraries including cuBLAS [7], cuSolver [7], and existing libraries including Magma [1], and KBLAS [4], as shown in Fig. 2. We present the workflow and layered design of the cuGraph-Tensor library. Then, we propose optimization strategies and efficient graph-tensor learning operation implementations based on these optimizations.


Download : Download high-res image (344KB)
Download : Download full-size image
Fig. 3. The workflow of the cuGraph-Tensor library.

3.1. The architecture of the cuGraph-Tensor library
Fig. 3 presents the workflow of the cuGraph-Tensor library. At the beginning, CPU allocates memory and loads graph data, including the weighted adjacency matrix  and the graph-tensor . CPU sends the matrix  together with the graph-tensor  to GPU. Then GPU utilizes  to compute the normalized Laplacian matrix  (for g-FT, inverse g-FT, g-convolution, g-product, g-SVD, and g-QR) or weighted adjacency matrix  (for g-shift and g-filter) and the specified graph-tensor operations. Finally, GPU sends results to CPU. Note that the computations of graph-tensor learning operations are entirely performed on GPU. As shown in Section 4, graph-tensor operations in the cuGraph-Tensor on GPU are much faster than these on CPU. On the other hand, adding CPU into computation as a hybrid CPU–GPU implementation incur additional CPU–GPU synchronization and communication cost.

The cuGraph-Tensor library adopts a layered design, as shown in Fig. 4. The bottom two layers are supporting routines of the library, including the data storage handler for efficient storing of three data types, the communication handler for data transfer between CPU and GPU, and the memory access handler for efficient memory accesses on GPU. The middle layer contains eight graph-tensor operations. g-shift, g-FT, and inverse g-FT are basic operations that do not rely on other operations. By contrast, g-filter, g-convolution, g-product, g-SVD, and g-QR are composite operations that depend on g-FT and inverse g-FT, as elaborated in Section 2.3. Table 1 summarizes the input and output of these operations. The entire cuGraph-Tensor library is running on top of NVIDIA CUDA [7] libraries including cuBLAS and cuSolver, and existing libraries including Magma [1] and KBLAS [4] to support upper applications. These graph-tensor operations are widely used in various applications such as video compression [35], data completion [16], data compression [30], machine learning [19] and pattern recognition [12], feature learning [2], knowledge discovery [5] and network analysis [30]. We develop a graph data completion application to demonstrate the usage of the cuGraph-Tensor library in Section 4.3.

3.2. Optimizations for graph-tensor operations on GPU
The baseline implementation in Section 2.2 is not optimal since they do not fully utilize thousands of computing cores and high memory bandwidth of GPUs. We propose optimization techniques to improve the performance of graph-tensor operations.

3.2.1. Batched computing scheme to improve GPU utilization and reduce kernel invocations
The unoptimized implementation in Section 2 performs a single matrix computation on each frontal-slice of a graph-tensor, leading to low GPU utilization and high kernel invocation cost. We design a batched computing scheme to organize the computations and launch only one CUDA kernel for matrix computing in graph-tensor operations. The kernel utilizes a batched routine to compute multiple matrix computations in a graph-tensor operation parallelly, as shown in Fig. 5.

Batched matrix–tensor multiplication: The graph-tensoroperations such as g-shift, g-FT, and inverse g-FT in the cuGraph-Tensor library perform a tensor–matrix multiplication computation intensively. For instance, Fig. 6(a) shows the matrix–tensor multiplication of the eigenvector matrix 
 with a tensor 
 in graph Fourier transform on graph-tensors. To compute this matrix–tensor multiplication, GPU needs to perform 
 matrix multiplications of 
 and each frontal slice of 
 (
 for 
). Conventional GPU implementation uses a for-loop to compute these 
 matrix multiplications sequentially.

We propose a batched matrix–tensor multiplication scheme to improve GPU utilization. In this scheme, we organize multiple matrix multiplications and compute them parallelly on GPU. As shown in Fig. 6(b), we compute 
 matrix multiplications of 
 and 
, 
 parallelly on GPU if there is enough GPU memory. Otherwise, we split the 
 matrix multiplications into several partitions according to GPU memory capacity, where each partition contains multiple matrix multiplications and the computations of partitions are independent of each other. Then we compute these partitions one by one on GPU. With this scheme, the GPU utilization is significantly improved. Analogously, we propose a batched tensor-sliced multiplication scheme to improve GPU utilization for g-product in the cuGraph-Tensor. Fig. 7 shows the batched matrix–tensor multiplication scheme achieves up to  speedup over the GPU baseline implementation without the batched scheme.

Batched tensor SVD: The g-SVD operation requires computing multiple matrix SVD decompositions on GPUs in the matrix computation step. Existing GPU BLAS libraries do not support batched SVD routines for big matrix size. For instance, NVIDIA cuBLAS library and the KBLAS library only support batched SVD of matrix size up to 32 × 32 and 512 × 512, respectively. So we exploit the KBLAS library to compute multiple matrix SVD decompositions parallelly in the matrix computation step of g-SVD.

Batched tensor QR: The g-QR operation requires computing multiple matrix QR decompositions in the matrix computation step on GPU. NVIDIA cuBLAS library do not support batched QR of matrices. So we exploit the Magma library to compute multiple matrix QR decompositions parallelly in the matrix computation step of g-QR.


Download : Download high-res image (290KB)
Download : Download full-size image
Fig. 7. The performance of the batched matrix–tensor multiplication scheme.

3.2.2. Reducing CPU–GPU communications
In this work, we mainly focus on external Nvidia GPUs since they have much higher computation capability than integrated Nvidia GPUs. Since the bandwidth of the PCIe bus connecting CPU and external GPU is small (around 12 GB/s for PCIe gen3 × 16) compared to the 900 GB/s bandwidth of the GPU global memory, reducing CPU–GPU communication cost is critical for performance.

As described in Section 2.3, the computations of graph-tensor operations consist of multiple steps and involve several linear algebra routines (i.e., vector, matrix, or tensor computations). In conventional GPU implementation, the calling of each linear algebra routine would incur a CPU to GPU communication to transfer input data to GPU, and a GPU to CPU communication to move results back to CPU. As a result, there will be multiple CPU–GPU communications in a graph-tensor operation. The communication cost is high, especially when the data size is large (e.g., large graph-tensors).

We propose a pre-allocation scheme to reduce intermediate CPU–GPU communications. In this scheme, we pre-allocate intermediate variables such as the eigenvector matrix  or 
 and the result 
. During the computation of a graph-tensor operation, we provide these pre-allocated variables to linear algebra routines and eliminate the storing of intermediate results back to CPU. Therefore, there is only one CPU–GPU data transfer at the beginning and another data transfer at the end of an operation, respectively. We utilize asynchronous data transfer to further reduce the communication cost.

3.2.3. Optimizing data storage and memory access
The graph-tensor operations in the cuGraph-Tensor library support three data types on graph nodes: scalar, vector, and matrix. For a graph G with 
 nodes, the data of all nodes form a 
-length vector, an 
 matrix, or an 
 tensor when each node has a scalar, 
-length vector, or 
 matrix, respectively. We call such vectors, matrices, and tensors as graph-vectors, graph-matrices, and graph-tensors, respectively. Using these three data types, the cuGraph-Tensor library can support various applications. For instance, if we model sensor nodes in a field as graph nodes and the network connection among sensors as edges, then each graph node can have scalar data (one sensor reading), vector data (a time series of sensor readings), or matrix data (time series of multiple sensors). The data storage handler handles these three data types on GPU.

To improve memory access efficiency on CPU and GPU, the memory access handler in the cuGraph-Tensor library uses the appropriate memory storage layout to ensure continuous memory access, as shown in Fig. 8. For the scalar data type, we store the data elements of graph nodes into a one-dimensional array continuously following the indices of graph nodes. For the vector data type, we arrange the vector of each graph node as a column in the matrix following the indices of graph nodes and adopt the column-major layout to store the matrix into memory. For the matrix data type, we arrange the matrix of each graph node as a frontal slice in the tensor following the indices of graph nodes. For a graph G with 
 nodes and each node has an 
 matrix data, all data form a tensor 
. We flatten this tensor  into a one-dimensional array , such that tensor element 
 is stored in 
. During the computation of graph-tensor operations, GPU threads need to fetch and store the data elements of graph nodes. The memory access handler incorporates memory access operators and enables a GPU thread to fetch and store a scalar, vector, or matrix of a specific graph node conveniently. For instance, there is a pair of memory access operators “Slice-fetch” and “Slice-store” for the loading and storing of a frontal slice from a graph-tensor.


Download : Download high-res image (182KB)
Download : Download full-size image
Fig. 8. Data representation and the corresponding memory storage.

3.2.4. Further optimizations on computations
(a). Optimization of reorganizing graph-tensors. At the beginning of graph-tensor operations, we need to reorganize the input tensor 
 to 
 on GPU for subsequent computation. Conventional implementation allocates GPU memory and launches massive threads (e.g. 
 threads) to reorganize the input data in a straightforward manner. We propose a matrix-transpose-based scheme to replace the straightforward reorganization on GPU to achieve better memory access efficiency and reduce computations. In Section 3.2.3, an input graph-tensor 
 is stored as a 1D array in memory. Accessing this array in the column-major view forms a size-specific matrix 
. Performing matrix transposing on  will produce a transposed matrix 
, which is equivalent to the tensor 
 needed.

(b). Optimization to reduce computations. For the specific size tensor 
 and 
, the g-product is equivalent to  instead of . For g-filter 
, we first take the graph Fourier transform of the graph-tensor , followed by pointwise multiplication in the spectral domain of the graph Fourier transform tensor () by the filter spectral response . Finally, an inverse graph Fourier transform computes the output back in the time domain. This is the graph filtering theorem that reduces graph filtering to two graph Fourier transforms and a pointwise multiplication in the spectral domain [29]. Likewise, graph convolution is a specific graph filter. Using the theorem, we can compute g-convolution by 
 instead of 
, where  converts a vector into a diagonal matrix. This optimization technique reduces computations and the number of kernel invocations for g-product, g-filter, and g-convolution operations.

(c). Optimization on matrix decompositions. The computations of these graph-tensor operations utilize matrix decomposition to obtain eigenvector matrix  by decomposing the normalized Laplacian matrix . In NVIDIA CUDA [7] libraries, there are multiple matrix decomposition routines, such as the singular value decomposition (SVD), lower–upper (LU) decomposition, and the eigen decomposition. For undirected graphs, we can utilize eigen decomposition on  and obtain 
. Because matrix  is symmetric, it must be diagonalized. The eigen decomposition of the matrix is faster than singular value decomposition and saves memory space at the same time.


Table 2. API functions in the cuGraph-Tensor library.

1.Data storage
cugraphStatus_t CuGraph_ALLOCATE_GRAPH(float 
, size_t 
)
Allocate memory for the weighted adjacency matrix 
 of a graph.
cugraphStatus_t CuGraph_ALLOCATE_GRAPH_Tensor(float 
, size_t 
, size_t 
, size_t 
)
Allocate memory for a graph-tensor 
. In GPU memory, a graph-tensor  is stored using a one-dimensional array , such that tensor element 
 is stored at 
.
2.Graph operations
cugraphStatus_t CuGraph_GRAPH_Shift(float 
, float 
, size_t 
, size_t 
, size_t 
, float 
)
Compute graph shift. 
 is the weighted adjacency matrix and 
 is the input graph-tensor. The output is 
.
cugraphStatus_t CuGraph_GRAPH_FourierTransform(float 
, float 
, size_t 
, size_t 
, size_t 
, float 
)
Compute graph Fourier transform and obtain the spectral domain representation of the graph-tensor. 
 is the weighted adjacency matrix and 
 is the input graph-tensor.The output is 
.
cugraphStatus_t CuGraph_GRAPH_InverseFourierTransform(float 
, float 
, size_t 
, size_t 
, size_t 
, float 
)
Compute inverse graph Fourier transform. 
 is the weighted adjacency matrix and 
 is the graph Fourier transformed graph-tensor.The output is 
.
cugraphStatus_t CuGraph_GRAPH_Filter(float 
, float 
, size_t 
, size_t 
, size_t 
, float 
)
Compute graph filter. 
 is the weighted adjacency matrix and 
 is the input graph-tensor. The output is 
.
cugraphStatus_t CuGraph_GRAPH_Convolution(float 
, float 
, float 
, size_t 
, size_t 
, size_t 
, float 
)
Compute graph convolution. 
 is the weighted adjacency matrix, 
 is the input graph-tensor and 
 is the graph convolution kernel. The output is 
.
cugraphStatus_t CuGraph_GRAPH_Product(float 
, float 
, size_t 
, size_t , float 
, size_t 
, size_t 
, float 
)
Compute graph product. 
 is the weighted adjacency matrix. 
 and 
 are two input graph-tensors. The output is 
.
cugraphStatus_t CuGraph_GRAPH_SVD(float 
, float 
, size_t 
, size_t 
, size_t 
, float 
, float 
, float 
)
Compute graph SVD. 
 is the weighted adjacency matrix and 
 is the input graph-tensor. The outputs are 
, 
, 
.
cugraphStatus_t CuGraph_GRAPH_QR(float 
, float 
, size_t 
, size_t 
, size_t 
, float 
, float 
)
Compute graph QR. 
 is the weighted adjacency matrix and 
 is the input graph-tensor. The outputs are 
, 
.
3.3. Efficient implementation of graph-tensor operations
Using the optimization strategies of 3.2, the graph-tensor operations in the cuGraph-Tensor library are all optimized. We introduce the optimized implementation of the graph-tensor operations in detail.

g-shift, g-FT, inverse g-FT: For an input graph-tensor 
, we first reorganize  to 
, then perform the batched matrix–tensor multiplication between  (or eigenvector matrix  of ) and the graph-tensor 
 by exploiting the cuBLAS library. Finally, we reorganize the graph-tensor back to get the output graph-tensor of size 
.

The optimized composite graph tensor operations, including g-filter, g-convolution, g-product, g-SVD, and g-QR can be efficiently computed in the spectral domain in the following three steps:

1.
perform the optimized g-FT to obtain the spectral domain representation of the input graph-tensor on the GPU;

2.
In the spectral domain, the graph-tensor operations are decomposed into multiple independent matrix computations that possess strong parallelism. We batch these matrix operations for computation on the GPU;

3.
perform optimized inverse g-FT to return to the time domain from the spectral domain on the GPU.

g-filter: We first perform the eigen decomposition of  to obtain  and  by exploiting the cuBLAS library. Then we perform the batched matrix–tensor multiplication between the filter matrix about  and the graph-tensor 
 by exploiting the cuBLAS library.

g-convolution: We first perform the graph Fourier transform of the graph kernel 
. Then we perform the batched matrix–tensor multiplication between the result 
 and the graph-tensor 
 by exploiting the cuBLAS library.

g-product: In the second step, we perform the batched tensor-sliced multiplication between the graph-tensor 
 and 
 by exploiting the cuBLAS library.

g-SVD: In the second step, we perform batched tensor SVD of the graph-tensor 
 by exploiting the KBLAS library.

g-QR: In the second step, we perform batched tensor QR decomposition of the graph-tensor 
 by exploiting the Magma library.

The cuGraph-Tensor library is built on top of NVIDIA CUDA and existing libraries, including Magma and KBLAS. It can be used on Nvidia’s CUDA-enabled GPUs, including external GPUs and integrated GPUs. The cuGraph-Tensor library provides BLAS-like interfaces for upper algorithms and applications, as shown in Table 2.

4. Performance evaluation
We first evaluate the performance of the cuGraph-Tensor library by measuring the running time and speedups of eight key graph-tensor operations. In addition, cuGraph-Tensor develops a data completion application using these graph-tensor learning operations for efficient and accurate graph data reconstruction. Then, we evaluate the performance of the graph data completion application.


Table 3. Evaluation platforms.

Component	Type	Detailed parameters
CPU	Two Intel Xeon E5–2640	10 core @ 2.4 GHz (20 cores totally)
GPU	NVIDIA Tesla V100	5120 CUDA cores @1.53 GHz, 32 GB DDR memory
Host memory	DDR4	80 GB @ 2.133 GHz
4.1. Evaluation settings
The detailed configurations of the experiment platform are listed in Table 3. The server has 80 GB DDR4 memory, two Intel Xeon E5-2640 V4 CPUs, and a Tesla V100 GPU whose peak single-precision performance is 14 tera flops. Each CPU has 25 MB L3 cache and 10 physical cores that support 20 threads with hyperthreading technology. The server runs Ubuntu Linux 18.04 with kernel version 4.15.0.

We generate synthetic graph-tensor 
 to test eight graph-tensor operations. We use the CPark video [25] to evaluate the graph data completion application. Each experiment was repeated five times and the average results are reported.

Two metrics are used to evaluate the algorithm performance: running time and recovery error. For the running time, we vary the tensor sizes and measure the execution time on CPUs and the Tesla V100 GPU, respectively. The speedups are calculated as: . For the recovery error, we measured the relative square error (RSE) in the data completion application, which is defined as 
.

•
cuGraph-Tensor: The proposed GPU-based library consists of eight graph-tensor operations.

•
GSPBOX [27]: A CPU-based toolbox contains GSP operations. We compare g-FT, inverse g-FT, and g-filter with GSPBOX [27] (There are no g-shift, g-convolution, g-SVD, and g-QR operations in GSPBOX [27]). GSPBOX [27] only supports processing scalar data on nodes, so we use loops in GSPBOX [27] implementations for processing vector data types.

•
CPU MATLAB implementation: We compare these eight graph-tensor operations with CPU implementations in MATLAB for processing matrix data types (i.e. graph-tensor). We compare g-shift, g-convolution, g-product, g-SVD, and g-QR with CPU implementations in MATLAB for processing vector data types (i.e. graph matrix).

•
cuTensor-tubal [36], [37], [38]: A GPU-based library contains tensor operations and a tensor completion implementation. We compare the graph data completion application with the tensor completion application in cuTensor-tubal [36], [37], [38].

The CPU and GPU implementations run on MATLAB R2017a and CUDA 10.1 [7], respectively. Note that there is no GPU-based graph-tensor operation library for comparison with cuGraph-Tensor, to the best of our knowledge.

4.2. Library performance of the graph-tensor operations
We evaluated the cuGraph-Tensor library performance on graph-tensor operations with varying graph sizes.

Fig. 9 shows the running time and speedups of the optimized and the unoptimized basic graph-tensor operation on the Tesla V100 GPU, and the MATLAB implementation running on two Xeon CPUs. Fig. 9(a) shows g-shift achieved a maximum speedup of  compared with the CPU MATLAB implementation. With the design and the optimization techniques in Section 3, g-shift is significantly faster than the CPU MATLAB implementation. By comparison, the unoptimized g-shift employed none of the optimization techniques presented in Section 3.2 and achieved a maximum speedup of . As a result, it conducts single (i.e., not batched) computation and suffers high data transfer and memory access cost, leading to low GPU utilization and performance. Fig. 9(b) shows g-FT achieved a maximum speedup of  compared with the CPU MATLAB implementation. g-FT significantly outperforms the MATLAB implementation of graph Fourier transform. By comparison, the unoptimized g-FT without the optimization techniques in Section 3.2 achieved a maximum speedup of . Fig. 9(c) shows the inverse g-FT achieved a maximum speedup of  compared with the CPU MATLAB implementation. With the design and the optimization techniques in Section 3, inverse g-FT achieved much higher performance than the CPU implementation. By comparison, the unoptimized inverse g-FT achieved a maximum speedup of . The unoptimized inverse g-FT exhibits low GPU utilizations and high data transfer and memory access cost, leading to low performance.

Fig. 10 shows the running time and speedups of the optimized and the unoptimized g-filter, g-convolution, and g-product on the Tesla V100 GPU, and the MATLAB implementation running on two Xeon CPUs. Fig. 10(a) shows g-filter achieved a maximum speedup of  compared with the CPU MATLAB implementation. g-filter obtained much higher performance than the MATLAB implementation. In contrast, the unoptimized g-filter achieved a maximum speedup of . The unoptimized g-filter does not effectively utilize the GPU resources and has high data access cost, thus obtained much lower performance than the optimized g-filter. Fig. 10(b) shows the g-convolution achieved a maximum speedup of  compared with the CPU MATLAB implementation. With the design and the optimizations on GPU, g-convolution significantly outperformed the MATLAB implementation. In contrast, the unoptimized g-convolution achieved a maximum speedup of . The optimized g-convolution is much faster than the unoptimized g-convolution since it utilized the optimizations in Section 3.2 and achieved higher GPU utilization. Fig. 10(c) shows the g-product achieved a maximum speedup of  compared with the CPU MATLAB implementation. With the design and the optimizations on GPU, g-product significantly outperformed the MATLAB implementation. In contrast, the unoptimized g-product achieved a maximum speedup . The optimized g-convolution is much faster than the unoptimized g-product since it utilized the batched tensor-sliced multiplication.

Fig. 11 shows the running time and speedups of the optimized and the unoptimized g-tensor decomposition operations on the Tesla V100 GPU, and the MATLAB implementation running on two Xeon CPUs. Fig. 11(a) shows the g-SVD achieved a maximum speedup of  compared with the CPU MATLAB implementation. With the design and the optimizations on GPU, g-SVD significantly outperformed the MATLAB implementation. In contrast, the unoptimized g-SVD achieved a maximum speedup of . The optimized g-SVD is much faster than the unoptimized g-SVD since it utilized batched graph SVD in Section 3.2 and achieved higher GPU utilization. Fig. 11(b) shows that compared with the CPU MATLAB implementation, the g-QR achieved a maximum speedup of . In contrast, the unoptimized g-QR achieved a maximum speedup of . The optimized g-QR is much faster than the unoptimized g-QR since it utilized the batched graph QR.


Download : Download high-res image (566KB)
Download : Download full-size image
Fig. 11. Running time and speedups of the graph-tensor decomposition operation on the Tesla V100 GPU and two CPUs, respectively.

For all graph-tensor operations, the average and the maximum performance of the optimized GPU implementation is much higher than that of the unoptimized GPU implementation on graph-tensors. The g-shift, g-FT, inverse g-FT, g-filter, g-convolution, g-product, g-SVD, and g-QR with the optimization techniques in Section 3.2 are on average , , , , , , , and  faster over the unoptimized graph-tensor operations. This demonstrates that the optimization techniques including the batched scheme, computation, memory access, and data transfer optimizations are effective and significantly improve performance.

In addition, from Fig. 9, Fig. 10, Fig. 11, we observed that the speedups of the optimized GPU implementations increase with the growing graph data sizes. With the growth of graph data sizes, there are more and more computations and data parallelisms. GPU effectively parallelizes and accelerates these computations. We also evaluated the performance of the cuGraph-Tensor library on graph vectors and graph matrices, and it obtained much higher performance versus the GSPBOX or CPU MATLAB implementations.

4.3. Library performance in the graph data completion application
In big data and IoT applications, data missing occurs because of various unpredictable or unavoidable reasons [18]. For instance, high data missing rates are reported in real-world IoT projects, namely, up to 23% in IntelLab [11], 35% in GreenOrbs [23], 55% in CitySee [22], and 64% in OceanSense [34]. The incomplete sensory data can be recovered by exploiting the spatial and temporal correlation among data [20]. Researchers have proposed diverse data completion approaches [15], [21], [33], [38] to reconstruct incomplete data.

Previously, the iterative algorithm called Tubal-Alt-Min [16] algorithm was proposed for robust data completion based on the low-tubal-rank tensor model. However, this algorithm is compute-intensive, and its running time increases exponentially with the growing of tensor size and dimension, which makes it impractical for real-time or data recovery. We solve this limitation by implementing this graph data completion on GPU and exploiting the high-performance graph-tensor operations in the proposed cuGraph-Tensor library, including g-FT and inverse g-FT.


Download : Download high-res image (126KB)
Download : Download full-size image
Fig. 12. The algorithm steps of the graph data completion application.

Without loss of generality, the graph data completion consists of three major steps, as shown in Fig. 12. The first step performs graph Fourier transform to obtain an initial partial tensor as input for the alternating minimization iterations. In the alternating minimization step, a pair of graph-tensors are iteratively and alternately refined using least squares and graph Fourier transform. Finally, perform inverse graph Fourier transform to obtain a reconstructed graph-tensor that approaches the original graph-tensor.

The least squares problem can be formulated as: (2)
 
where 
, 
, and 
. We perform QR factorization on  to obtain , where 
 and 
 is an upper triangular matrix. Then (3)

We run experiments on video recovery for wireless camera networks to evaluate the performance of the graph data completion application. As shown in Fig. 13, there is a wireless camera network consisting of multiple sensor nodes, and each node is equipped with a camera. The video frames captured by these camera sensor nodes are sent to a computing node for video recovery to fill in missing video frames due to wireless transmission or sensor node failure. The completed video data are then sent to upper applications such as security surveillance. We model the wireless camera network as a ring graph G with 
 camera sensor nodes as vertices. The camera sensor nodes have the same resolution of 360 × 640. Each time, each sensor node captures a video frame of 360 × 640, and all video frames form a 
 video. We use a missing rate to model the percentage of missing video frames among all video frames.


Download : Download high-res image (128KB)
Download : Download full-size image
Fig. 13. Illustration of the graph data completion task.

Running time and speedups: Fig. 14, Fig. 15 show the running time and speedups of the graph data completion application in cuGraph-Tensor, the tensor completion in cuTensor-tubal[36], [37], [38], and the CPU MATLAB implementation with varying graph-tensor sizes on the Tesla V100 GPU and two Xeon CPUs, respectively. The number of nodes 
 varied between 10 and 20. Both the GPU and the CPU implementations were executed for 8 iterations to converge. Compared with the GPU-based tensor completion in cuTensor-tubal [36], [37], [38], the graph data completion in cuGraph-Tensor achieved an average of  and up to  speedups. Compared with the CPU MATLAB implementation running on two CPUs, the graph data completion in cuGraph-Tensor achieved an average of  and up to  speedup. This shows that the graph data completion application obtains high performance.


Download : Download high-res image (196KB)
Download : Download full-size image
Fig. 14. The running time of three data completion implementations.


Download : Download high-res image (180KB)
Download : Download full-size image
Fig. 15. The speedups of the graph data completion module versus two implementations.

Recovery errors: This experiment compared the recovery errors of the graph data completion in cuGraph-Tensor, the tensor completion in cuTensor-tubal [36], [37], [38], and the CPU MATLAB implementation under different data missing rates. There are  graph nodes each having a 360 × 640 video frame, which form a graph tensor of size 360 × 640 × 10. We vary the missing rate of video frames from 10% to 90%. For instance, a 50% missing rate means  out of  video frames are missing. Both the GPU and CPU implementations ran for 8 iterations and terminated. As shown in Fig. 16, when the missing rate varied from 10% to 90%, cuGraph-Tensor and the CPU MATLAB implementation achieved much better RSEs than the tensor completion in cuTensor-tubal [36], [37], [38]. We observed that cuGraph-Tensor and the CPU MATLAB implementation achieved similar RSEs on all missing rates (the two curves in Fig. 16 overlap), which means that they had a similar recovery error performance. At a 50% missing rate, cuGraph-Tensor achieved an RSE of - while cuTensor-tubal [36], [37], [38] achieved an RSE of -. The proposed cuGraph-Tensor library achieved a better recovery error than the tensor completion in the cuTensor-tubal library [36], [37], [38] since it can utilize the correlation in graph structure for data completion.


Download : Download high-res image (209KB)
Download : Download full-size image
Fig. 16. The recovery error under varying missing rate of three implementations.

Visual result of the video recovery:

Fig. 17(a) and (b) show original video frames and recovered video frames for a graph-tensor of 640 × 360 × 10 with a 50% missing rate. We can see that 5 out of 10 video frames are missing in Fig. 17(a) (i.e., the black ones). After applying the graph data completion in the cuGraph-Tensor library, we can see in Fig. 17(b) that the missing video frames are reconstructed.


Download : Download high-res image (504KB)
Download : Download full-size image
Fig. 17. The original video frames and the recovered video frames.

5. Related works
We discuss the related works on graph operation libraries, GPU-based tensor computing and data completion, and graph algorithm libraries.

5.1. Graph operation libraries
To the best of our knowledge, this work is the first GPU-based library of graph-tensor learning operations. So we analyze CPU-based graph operation libraries and GPU-based tensor operation library cuTensor-tubal [36], [37], [38]. GSPBOX [27] and Grasp [10] are two CPU-based graph operation libraries in MATLAB, and PyGSP [8] is a CPU-based graph operation library in Python. We observed that they have the following limitations:

•
Low performance: These CPU-based graph operation libraries exhibit low performance, especially for a large number of graph nodes. For instance, Fig. 18 shows the running time of graph Fourier transform, inverse graph Fourier transform, and graph filter in GSPBOX [27] under a varying number of graph nodes with a scalar data on each node. Note that graph Fourier transform and inverse graph Fourier transform have similar computations, therefore two curves overlap. For a graph G with 20,000 nodes, graph filter takes 20,443.11 s (5.68 h) on two Xeon CPUs while graph Fourier transform and inverse graph Fourier transform take 3022.36 s and 3006.91 s, respectively.

•
Single data type: They only support scalar data on vertices, while real-world graph applications have more data types (e.g., vector and matrix) on vertices.

•
Limited graph operations: Each of these libraries misses some useful graph operations. GSPBOX [27], Grasp [10], and PyGSP [8] do not contain some of the eight operations in our library. We try to provide more useful graph-tensor learning operations in cuGraph-Tensor to support diverse applications.

•
Lacking structural properties: In the cuTensor-tubal library [36], [37], [38], each frontal slice of the tensor lacks structural properties. The graph-tensor learning operations use the graph structure between each frontal slice of the tensor.

5.2. GPU tensor computing and data completion
The cuGraph-Tensor library supports matrix data type on graph nodes. The graph nodes, along with a matrix on each node, form three-dimensional data. We model such data as tensor so we can exploit the tensor model and techniques for efficient processing and analyzing. Previously, we proposed a cuTensor-tubal library [36], [37], [38] on GPU that consists of seven key tensor operations to benefit diverse applications. There are some existing works that focus on designing a specific tensor operation on GPUs. NTF [3] is a GPU-based non-negative tensor factorization. CUSNTF [14] proposed an efficient tensor factorization. ReFacTo [28] is a tensor decomposition library for heterogeneous CPU–GPU clusters.

Data completion algorithms are iterative and computationally intensive, and therefore GPUs have been increasingly exploited to accelerate data completion. Lu et al. [21] proposed a GPU-based high-performance matrix completion for homomorphic systems. Li et al. proposed a GPU-based encoder and decoder scheme for robust data transmission in wireless networks. We proposed a GPU-based tensor completion in the cuTensor-tubal library [36], [37], [38]. Experiment results show that the graph data completion in the proposed cuGraph-Tensor library is up to  faster and more accurate than the tensor completion in the cuTensor-tubal library [36], [37], [38].

5.3. GPU graph algorithm libraries
nvGraph [6] is a GPU-based graph algorithm library in CUDA [7]. nvGraph provides functions for graph construction and manipulation primitives, and a set of useful graph algorithms, including SSSP (single-source shortest path), Pagerank, and Triangle counting. Similarly, the RAPIDS cuGraph library [32] is a GPU library of graph algorithms, including community detection, Centrality detection, BFS, SSSP, and pagerank. In comparison, the cuGraph-Tensor library consists of a set of high-performance graph operations, including g-shift, g-Ft, inverse g-FT, g-filter, g-convolution, g-product, g-SVD, and g-QR. nvGraph [6], cuGraph [32], and cuGraph-Tensor are all built based on existing CUDA libraries such as cuBLAS. They can be selectively used depending on that the application requires graph algorithms or graph operations.

6. Conclusion and future work
In this paper, we proposed a cuGraph-Tensor library of eight key graph-tensor operations to support high-performance graph data processing. The cuGraph-Tensor library exploited the separability in the graph-tensor operations and mapped the parallelism onto GPU architectures. We proposed optimization techniques on computing, memory accesses, and CPU–GPU communications for graph-tensor operations. Based on these efficient graph-tensor operations, cuGraph-Tensor developed a graph data completion application for fast and accurate reconstruction of incomplete graph data. Performance evaluations showed that the cuGraph-Tensor library achieved up to , , , , , , , and  speedups versus CPU-based GSPBOX [27] and MATLAB implementations for g-shift, g-FT, inverse g-FT, g-filter, g-convolution, g-product, g-SVD, and g-QR, respectively. With the proposed optimization techniques, the optimized GPU graph-tensor operations achieved on average , , , , , , , and  speedups, respectively, over the baseline GPU operations. The graph data completion achieved a maximum speedup of  over the CPU MATLAB implementations, and a maximum speedup of  with better accuracy over the GPU-based tensor completion in the cuTensor-tubal library [36], [37], [38]. In the future, we plan to improve the cuGraph-Tensor library on functionality, performance, and convenience. We are going to add more useful graph-tensor operations, scale the library onto multiple GPUs, and develop interfaces for Python and other frameworks.

CRediT authorship contribution statement
Tao Zhang: Writing - review & editing, Investigation, Supervision. Wang Kan: Software, Writing - original draft. Xiao-Yang Liu: Methodology.