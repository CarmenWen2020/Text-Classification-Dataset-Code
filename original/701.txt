Abstract
For diagonal sparse matrices that have many long zero sections or scatter points or diagonal deviations from the main diagonal, a great number of zeros need be filled to maintain the diagonal structure while using DIA to store them, which leads to the performance degradation of the existing DIA kernels because the padded zeros consume extra computation and memory resources. This motivates us to present an adaptive sparse matrix-vector multiplication (SpMV) for diagonal sparse matrices on the graphics processing unit (GPU), called DIA-Adaptive, to alleviate the drawback of DIA kernels for these cases. For DIA-Adaptive, there are the following characteristics: (1) two new sparse storage formats, BRCSD (Diagonal Compressed Storage based on Row-Blocks)-I and BRCSD-II, are proposed to adapt it to various types of diagonal sparse matrices besides adopting DIA, and SpMV kernels corresponding to these storage formats are presented; and (2) a search engine is designed to choose the most appropriate storage format from DIA, BRCSD-I, and BRCSD-II for any given diagonal sparse matrix; and (3) a code generator is presented to automatically generate SpMV kernels. Using DIA-Adaptive, the ideal storage format and kernel are automatically chosen for any given diagonal sparse matrix, and thus high performance is achieved. Experimental results show that our proposed DIA-Adaptive is effective, and has high performance and good parallelism, and outperforms the state-of-the-art SpMV algorithms for all test cases.


Keywords
Diagonal sparse matrices
Sparse matrix-vector multiplication
Sparse storage format
CUDA
GPU

1. Introduction
Given their many-core structures, graphics processing units (GPUs) have sufficient computation power for scientific computations. Processing big data by using GPUs has drawn much attention over the recent years. Following the introduction of the compute unified device architecture (CUDA), a programming model that supports the joint CPU/GPU execution of applications, by NVIDIA in 2007 [20], GPUs have become strong competitors as general-purpose parallel programming systems.

The sparse matrix-vector multiplication (SpMV) for diagonal sparse matrices is defined as(1) where 
 is the diagonal sparse matrix, 
 is the known vector, and 
 is the output vector. It has proven to be of particular importance in computational science [3], [10], [15], [19], [9], [12]. Different from other sparse matrices, nonzero values of diagonal sparse matrices are restricted to a small number of matrix diagonals. When implementing SpMV for diagonal sparse matrices on GPU, two main problems need to be solved: (1) appropriate storage format, and (2) highly efficient accesses to the matrix data in global memory.

Bell and Garland [3] show that DIA is the most appropriate storage format compared to CSR, COO, ELL, and HYB, and present an efficient SpMV kernel for DIA. DIA stores nonzero values according to the diagonal, and all nonzero entries on the same diagonal share the same index. However, a large number of zeros have to be filled to maintain the diagonal structure, when the diagonals are broken by long zero sections, or when many diagonals are far away from the main diagonal, or when there are many scatter points, as shown in Fig. 1. This may reduce the performance since the filled zeros consume extra computation and memory resources.

Fig. 1
Download : Download high-res image (175KB)
Download : Download full-size image
Fig. 1. A sample of diagonal sparse matrices. (For interpretation of the colors in the figure(s), the reader is referred to the web version of this article.)

To limit the number of filled zeros, Sun et al. [23], [24] propose a new storage format for diagonal sparse matrices, called compressed row segment with diagonal-pattern (CRSD), which alleviates the zero fill-in issue of DIA. The idea is to represent the diagonal distribution by defining the diagonal pattern, which divides diagonals into different groups, and to split the matrix into row segments. In each row segment, nonzero elements on the diagonals of the same group are viewed as the unit of storage and operation and are therefore stored contiguously. However, storing a diagonal sparse matrix using CRSD is complicated, and the implementation source code of the SpMV kernel for CRSD is unavailable [9]. Barbieri et al. [2] design the Hacked DIA (HDI) format to limit the amount of padding zeros, by breaking the original matrix into equally sized groups of rows, and then storing these groups as independent matrices in DIA. Because DIA is used for each submatrix, HDI cannot mitigate the padding when many diagonals are far away from the main diagonal. In this paper, we will further investigate how to optimize the sparse matrix-vector multiplication for diagonal sparse matrices on GPU. Using the symbols defined in Table 1, we divide the diagonal sparse matrices into the following three types:

(1)
Type I: The matrices satisfy that p_offset =0 and p_zero <α, where α is a small value, as shown in Fig. 2(a). Obviously, for the matrices in Type I, all diagonals have small deviations from the main diagonal, and there are no long zero sections or scatter points.

Fig. 2
Download : Download high-res image (138KB)
Download : Download full-size image
Fig. 2. Samples of three types of diagonal sparse matrices.

(2)
Type II: The matrices satisfy that p_offset >0 and label_lz is false and label_s is false, as shown in Fig. 2(b). For the matrices in Type II, there exist large diagonal deviations from the main diagonal, and there are not any long zero section and scatter point.

(3)
Type III: All cases except for matrices in Types I and II, as illustrated in Fig. 2(c).


Table 1. Symbols used in this paper.

Symbol	Remark
α	A predefined threshold
δ	A predefined threshold
n_diag	Total number of diagonals
n_offset	Number of diagonals whose distance from the main diagonal is more than δ
p_offset	n_offset/n_diag
n_elem	Total number of elements for DIA
n_zero	Number of filled zeros for DIA
p_zero	n_zero/n_elem
label_lz	As long as the diagonal matrix has a long zero section, it is true. Otherwise, it is false
label_s	As long as the diagonal matrix has a scatter point, it is true. Otherwise, it is false
For the matrices in type I, we note that although some padding can be needed for DIA, it is still the most appropriate to store them. The SpMV kernel for DIA in [3] is utilized in this case. For the matrices in types II and III, a great deal of padding is required when using DIA, so we present two new sparse storage formats BRCSD (Diagonal Compressed Storage based on Row-Blocks)-I and BRCSD-II for the two types of matrices to alleviate the drawback of DIA, respectively. Furthermore, two SpMV kernels, which are corresponding to BRCSD-I and BRCSD-II, respectively, are proposed. To identify its type for any given diagonal sparse matrix, a search engine is suggested. Using this engine, for any diagonal sparse matrix, its type is automatically identified and the most suitable storage format and kernel are chosen. Furthermore, in our algorithm, for the matrices in type II (or III), the BRCSD-I (or BRCSD-II) kernel codes all are different due to the fact that the loop unrolling optimization technique is utilized. Therefore, as long as the input matrix changes, the BRCSD-I (or BRCSD-II) kernel code must be rewritten. To alleviate this drawback, here we present a code generator that is used to automatically generate the kernel codes. Note that for matrices in type I, the code generator only needs to call the DIA kernel code in [3]. Experimental results show that our proposed algorithm is effective, and outperforms the state-of-the-art kernels such as the best of CSR-Scalar, CSR-Vector, ELL, DIA, and HYB in the CUSP library [4], HYBMV in the CUSPARSE library [21], CSR-Adaptive [6], HDI [2], and CRSD [23].

The main contributions in this paper are summarized as follows.

•
We propose two new sparse storage formats BRCSD-I and BRCSD-II and SpMV kernels that are corresponding to BRCSD-I and BRCSD-II for diagonal sparse matrices in Types II and III, respectively.

•
A search engine is proposed to automatically identify the type of diagonal sparse matrices.

•
A code generator is presented to automatically generate the kernel codes.

•
We present an adaptive SpMV algorithm for diagonal sparse matrices on GPU. This algorithm can automatically select the most suitable sparse storage format from DIA, BRCSD-I, and BRCSD-II, and thus achieves high performance for any diagonal sparse matrix.

The rest of this paper is organized as follows. Section 2 introduces the related work, Section 3 presents our algorithm, Section 4 analyzes and evaluates the proposed algorithm, and Section 5 contains our conclusions and points to our future work.

2. Related work
Sparse matrix-vector multiplication (SpMV) is so important in computational science that there has been a great deal of work on accelerating it. Initial work about accelerating the SpMV on the CUDA-enabled GPU is presented by Bell and Garland [3]. They suggest several well-known sparse matrix storage formats such as CSR, DIA, ELL, COO, and HYB, and their corresponding kernels. Each storage format has its own suitable matrix type. They conclude that the best performance of SpMV on GPU depends on the storage format. Since then, many new efficient SpMV kernels have been proposed for GPUs using the variants of the CSR, ELL, and COO storage formats such as the compressed sparse eXtended [17], bit-representation-optimized compression [25], block CSR [27], [5], ELLPACK-R [26], sliced ELL [5], [1], SELL-C-σ [18], sliced COO [7], and blocked compressed COO [30]. The overview of efficient storage formats and their corresponding SpMV kernels can be found in [9].

DIA is the most appropriate storage format for diagonal sparse matrices where all nonzeros on the same diagonal share the same index [3], [19], [13], [11]. However, DIA can potentially waste storage and computational resources because it requires zero padding for non-full diagonals in order to maintain the diagonal structure. To limit the padding for non-full diagonals, some new storage formats and corresponding kernels are presented [23], [2], [14]. These formats are efficient for some specific diagonal sparse matrices and can alleviate the padding of DIA. In this paper, unlike previous other work, we establish an adaptive SpMV algorithm for diagonal sparse matrices, which can automatically find the most appropriate storage format with less padding and an efficient kernel for any given diagonal sparse matrix.

3. GPU implementation
In this section, we present our proposed adaptive sparse matrix-vector multiplication for diagonal sparse matrices on GPU, called DIA-Adaptive. The proposed algorithm, whose main framework is shown in Fig. 3, includes the following components:

(1)
A popular sparse storage format DIA and its kernel, two new sparse storage formats BRCSD-I and BRCSD-II and their corresponding kernels. These formats are most suitable for diagonal sparse matrices in types I, II and III, respectively (see Sections 3.1, 3.2, and 3.3).

(2)
A search engine. Using this engine, the most appropriate storage format can be chosen from DIA, BRCSD-I, and BRCSD-II for any given diagonal sparse matrix.

(3)
A code generator. Using this generator, the above three types of SpMV kernels for diagonal sparse matrices can be automatically generated.

Fig. 3
Download : Download high-res image (293KB)
Download : Download full-size image
Fig. 3. Main framework of our proposed algorithm.

The symbols that are used in this search engine are listed in Table 1. For DIA-Adaptive, there are the following novelties: (1) the diagonal sparse matrices are divided into three types based on their characteristics, and our algorithm can automatically identify its type for any diagonal sparse matrix; and (2) our algorithm includes three storage formats and kernels: DIA format and its corresponding kernel [3] and two new proposed BRCSD-I and BRCSD-II formats and their corresponding kernels, which are most suitable for matrices in types I, II and III, respectively. This is obviously different from these algorithms with only one storage format; and (3) a search engine is presented to identify the type of the input matrix and choose the most appropriate storage format and kernel; and (4) a code generator is proposed to automatically generate the kernel code. The source codes of the search engine and the code generator are available in the supplemental material.

In the following subsections, we present DIA, BRCSD-I and BRCSD-II formats and their corresponding kernels, respectively, and investigate the selection of parameters in the search engine.

3.1. DIA format and kernel
3.1.1. DIA format
DIA is formed by two arrays: data, which stores the nonzero values, and offsets, which stores the offset of each diagonal from the main diagonal [3]. E.g., the diagonal sparse matrix shown in Fig. 4 is stored as
 

Fig. 4
Download : Download high-res image (133KB)
Download : Download full-size image
Fig. 4. An example of diagonal sparse matrix for DIA.

3.1.2. DIA kernel
3.2. BRCSD-I format and kernel
3.2.1. BRCSD-I format
For DIA, we observe that some zero values are filled into data because of the offset of diagonals. The greater the offset is, the more the zero values are filled. To alleviate this drawback of DIA, we present a new sparse storage format BRCSD (Diagonal Compressed Storage based on Row-Blocks)-I that is an extension of that in [29]. The procedure of generating BRCSD-I is shown in Fig. 6.

Fig. 6
Download : Download high-res image (53KB)
Download : Download full-size image
Fig. 6. Procedure of generating BRCSD-I.

Find the piece point set . Assume that for a diagonal matrix 
, its 
, the procedure of finding the piece point set is as follows. We first save 
, , and 0, and 
, , and  to the set , and delete the same values in . Then sort the values in  in ascending order.

For example, for the diagonal matrix in Fig. 4, the piece point set .

Trim . This step is to delete invalid points of  so that the distance between any two adjacent points in  is not less than η. Assume that  has l elements and let 
, where 
 and 
.

(1)
Take the first element 
 and the second element 
 of , and if 
, we delete 
 and then take 
, ⋯, until we find an element 
 so that 
; otherwise, goto (3);

(2)
Take 
 and 
, and if 
, we delete 
 and then take 
, ⋯, until we find a piece point so that the distance between it and 
 is not less than η; otherwise, we succeed to take 
 and 
, ⋯, and so repeated until 
 is taken. The trimming procedure is terminated.

(3)
Take 
 and 
, and if 
, we delete 
 and then take 
, ⋯, until we find a piece point so that the distance between it and 
 is not less than η; otherwise, we succeed to take 
 and 
, ⋯, and so repeated until 
 is taken. The trimming procedure is terminated.

For example, for  obtained by the previous step, the first two elements 0 and 1 are first taken, they are retained because . Second, we take the second and third elements 1 and 2, and delete 2 because . Similarly, 4 is retained. Finally, given that , we delete 4 because the last element must be kept. The trimming procedure is terminated.  is transformed into  after the trimming step.

Definition 1

Assume that p and q are two piece points and , if , the submatrix from the th row to the qth row is called a row piece; otherwise, the submatrix from the 0th row to the qth row is called a row piece.

Form row pieces without intersection. Based on , the matrix is divided into lots of row pieces without intersection according to Definition 1.

Output BRCSD-I arrays that represent the matrix. Assume that the number of row pieces that are obtained by the previous step is s, we represent the matrix by the following two arrays:

Here 
, , is the starting row number of the ith row piece;  and , , store the offset of each diagonal from the main diagonal and the nonzero values for ith row piece, respectively. E.g., the matrix shown in Fig. 7 is represented as follows: 
 

Compared to DIA, BRCSD-I has fewer filled zeros for this case where many diagonals are far away from the main diagonal.

3.2.2. BRCSD-I kernel
Parallelizing SpMV for BRCSD-I on GPU is straightforward. Given that the matrix is split into row pieces in BRCSD-I, a row piece is assigned to a thread block, and each row is processed by a thread in this block. For example, for the matrix in Fig. 7, the first row piece is assigned to one thread block and the second row piece is assigned to another thread block. Obviously, the loading of two thread blocks is unbalanced. To ensure the load balance of each thread block for our proposed BRCSD-I kernel, we try to let each thread block to process the same number of rows, . Thus, if a row piece size after splitting the matrix into row pieces is larger than , we will divide it into  row pieces again. Furthermore, based on the procedure of generating row pieces, we see that this procedure can not guarantee that the size of row pieces is a multiple of . Therefore, to assure the coalescing of accessing BRCSD-I arrays, the size of each row piece while splitting the matrix into row pieces is finely modified to be a multiple of the number of threads per block. A good choice for  is a multiple of warp size, and the best for it is to be equal to the number of threads per block.

In the following, we generate the BRCSD-I kernel. First, we get the thread block ID  and the local thread ID , and identify which offset array the thread block processes. Because a thread block deals with a row piece, block  processes the ith offset array only if satisfying the following condition:

Next, we generate the SpMV operations for block  with the ith offset array. For local thread , when processing the jth diagonal, the location of nonzero is 
, and its corresponding location in vector x is .

Finally, accumulating the product of all diagonals in the ith offset array, we store it to y at location .

3.3. BRCSD-II format and kernel
3.3.1. BRCSD-II format
When a matrix has long zero sections or scatter points on its diagonals, many zero values will be filled into data using DIA; moreover, using BRCSD-I still needs to fill in lots of zeros. For instance, for the diagonal sparse matrix shown in Fig. 9, using DIA and BRCSD-I, it is respectively stored as
 
 and 
 

Fig. 9
Download : Download high-res image (156KB)
Download : Download full-size image
Fig. 9. An example of diagonal sparse matrix for BRCSD-II.

Obviously, whether using DIA or BRCSD-I, many zeros are padded because of a scatter point and two long zero sections. Therefore, to alleviate the drawback of DIA and BRCSD-I in this case, we present a new sparse storage format BRCSD-II. The procedure of generating BRCSD-II is listed in Fig. 10.

Fig. 10
Download : Download high-res image (38KB)
Download : Download full-size image
Fig. 10. Procedure of generating BRCSD-II.

In the following part, we give a detailed explanation of the procedure of generating BRCSD-II in Fig. 10.

Set nrows. Based on the number of matrix rows m and GPU feature, we set , which is a multiple of warp size, and at most the number of threads per block. Note that  is equal to the number of threads per block in general.

Divide the matrix into row pieces. Equally divide the matrix into  row pieces according to the row. Let . For any row piece i, , we use the array toffsets[i] to represent the offset of each diagonal from the main diagonal, and store nonzero values to the array data[i] according to the diagonal.

As an example for the diagonal matrix in Fig. 9, assume that  is set to 2 in the previous step, we illustrate the procedure.

In this step, the matrix is equally divided into , as shown in Fig. 9. Let . In the first row piece, for the diagonal with number 3, it offset is −1, so that we obtain toffsets[0][0] =−1 and save numbers 0 and 3 to data[0][0] = 0 and data[0][1] = 3. Similarly, for the diagonal with numbers 1 and 4, we obtain toffsets[0][1] = 0 and save numbers 1 and 4 to data[0][2] = 1 and data[0][3] = 4; for the diagonal with numbers 2 and 5, we obtain toffsets[0][2] = 2 and save numbers 2 and 5 to data[0][4] = 2 and data[0][5] = 5. Thus for the first row piece, we have toffsets[0] =  and data. Similar to the first row piece, we have toffsets and data for the second row piece, and toffsets and data for the third row piece.

Accumulate offset arrays. For toffsets[i], , we take toffsets[0], and if toffsets[0] = toffsets[1], we compare toffsets[1] with toffset[2], ⋯, until we find toffsets[j] so that toffsets[] ≠ toffsets[j], and record 
; otherwise 
, and we compare toffsets[1] with toffset[2], ⋯, so repeated until we finish the comparison of toffsets[] and toffsets[]. Let , for each 
, , if 
, 
, and use the array offsets[c] = toffsets[i] and .

For example, for toffset[i] obtained by the previous step, , because toffsets[0] ≠ toffsets[1] and toffsets[1] = toffsets[2], we have 
, and 
. Let . For 
, because 
, we have 
, offset, and ; for 
, because 
, we have 
, offset; for 
, there are nothing to do because 
. The procedure is terminated. Thus the first row offset array offset, and the last two row pieces share the offset array offsets

Output BRCSD-II arrays that represent the matrix. Assume that the number of row pieces that are obtained by the previous step is p, and the size of offsets array after accumulating row pieces is s, we represent the matrix by the following two arrays:
 Here 
, , represents the number of row pieces that share the offset array offsets[i], and 
. For example, the diagonal sparse matrix shown in Fig. 9 is represented as follows:

We observe that compared to DIA, the filled zero values in BRCSD-II are reduced from 13 to 3, and compared to BRCSD-I, the filled zero values in BRCSD-II are reduced from 11 to 3 for this case.

3.3.2. BRCSD-II kernel
Parallelizing SpMV for BRCSD-II on GPU is also straightforward. Given that the matrix is split into row pieces in BRCSD-II, a row piece is assigned to a thread block, and each row is processed by a thread in this block.

We generate the BRCSD-II kernel in the following steps. We first get the thread block ID  and the local thread ID , and identify which offset array the thread block processes. Because a thread block deals with a row piece of , block  processes the ith offset array only if satisfying the following condition:
 
 

In the following, we generate the SpMV operations for block  with the ith offset array. When the local thread  handles the jth diagonal, the nonzero location is 
. , and its corresponding location in vector x is .

In the end, we accumulate the product of all diagonals for block  with the ith offset array, and store it to y at location .

Taking the diagonal sparse matrix shown in Fig. 9, assuming that , we list the main code of the BRCSD-II kernel that is generated by the above procedure, as shown in Fig. 11. For the BRCSD-II kernel, all operations for row pieces with the same offset array are executed in a loop, and the loop unrolling optimization technique is utilized.

Fig. 11
Download : Download high-res image (141KB)
Download : Download full-size image
Fig. 11. BRCSD-II kernel.

3.4. Parameter analysis
From the search engine in Fig. 3, we can observe that the selection of the most appropriate storage format for any given diagonal sparse matrix depends on two parameters δ and α except for p_offset, label_lz, and label_s, which directly influences the performance of DIA-Adaptive. Therefore, here we investigate how to choose the two parameters. For simplicity, we let  for the test matrices in the following experiment.

First, we investigate the selection of δ. To determine the value of δ, we consider only diagonal sparse matrices without any long zero sections and scatter points in this experiment. Assume that , where Z is a positive integer greater than 1. For any diagonal sparse matrix whose p_offset is equal to zero, only when the distance between each diagonal and the main diagonal all is close to δ, the most filling is needed for DIA, and the performance of the DIA kernel is the worst at this point. In this case, taking the number of diagonals = K, p_zero is roughly calculated as Obviously, when K is larger, p_zero is independent on n and K and only depends on Z. In the following, we investigate the threshold of Z so that DIA is better than BRCSD-I for this case. We set Z to  in that order, and then for each Z, a diagonal sparse matrix, which is obtained by taking  and  and letting the distance between each diagonal and the main diagonal all be close to , is set to a test matrix. The execution time curves of DIA and BRCSD-I kernels with the increasing Z are shown in Fig. 12. The unit is millisecond (ms). We observe that when , BRCSD-I outperforms DIA. Otherwise, the execution time of DIA is not more than that of BRCSD-I. Therefore, we can affirm that taking  is the best because this can generally guarantee that DIA is not worse than BRCSD-I for any diagonal sparse matrix with p_offset = 0.

Fig. 12
Download : Download high-res image (114KB)
Download : Download full-size image
Fig. 12. Execution time curves of DIA and BRCSD-I kernels with Z.

Second, we let  and investigate the selection of α. From the above experiment, we can conclude that for any diagonal sparse matrix without any long zero sections and scatter points, if p_offset = 0, the condition satisfying that DIA outperforms BRCSD-I is p_zero , where K is the total number of diagonals. Obviously,  is the upper bound of p_zero. For any diagonal sparse matrix with p_offset = 0, if p_zero , this will mean that this matrix can have long zero sections or scatter points and the performance of DIA should be worse than that of BRCSD-I, not to speak of BRCSD-II. Therefore,  is taken for this case.

4. Evaluation and analysis
Our experimental evaluations have two objectives, namely, (1) to evaluate the influence of parameters on DIA-Adaptive; and (2) to test the performance of DIA-Adaptive by comparing it with several state-of-the-art SpMV kernels.

Table 2 shows NVIDIA GPUs that are used in the performance evaluations. Our source codes are compiled and executed using the CUDA toolkit 9.0 [20]. The performance is measured in terms of GFLOPS that is calculated on the basis of the assumption of two flops per nonzero entry for a matrix [3], [16]. The measured GPU performance for all experiments does not include the data transfer (from the GPU to the CPU or from the CPU to the GPU).


Table 2. Overview of GPUs.

Hardware	K40c	GTX1070
Cores	2880	1920
Clock speed (GHz)	0.74	1.56
Memory type	GDDR5	GDDR5
Memory size (GB)	12	8
Max-bandwidth (GB/s)	288	256
Compute capability	3.5	6.1
The diagonal sparse matrices in this experiment come from the University of Florida Sparse Matrix Collection [8] and other publications [12], [23], [28]. Table 3 summarizes the information of the test matrices, including the dimension, number of diagonals, and total number of nonzeros. The first 10 matrices, whose  is equal to 0 except for wang3, wang4, kim1, and nemeth22 (Table 4) come from the University of Florida Sparse Matrix Collection and have been used in some publications [23], [2]; Matrices with number from 11 to 18, whose  is not equal to 0 and label_lz is true (Table 4), come from the University of Florida Sparse Matrix Collection [8] and a publication [12]; The last 2 matrices, whose  is not equal to 0 and label_lz is false (Table 4), are from the publication [28]. Obviously, these chosen test matrices well present the characteristics of scatter points, long zero sections, and large offsets.


Table 3. Descriptions of test matrices.

Matrix	Dimension	Diagonals	Nonzeros
wang3	26,064 × 26,064	21	177,168
wang4	26,068 × 26,068	23	177,196
s3dkt3m2	90,449 × 90,449	655	3,686,223
s3dkq4m2	90,449 × 90,449	661	4,427,725
kim1	38,415 × 38,415	25	933,195
kim2	456,976 × 456,976	25	11,300,020
nemeth21	9,506 × 9,506	169	1,173,746
nemeth22	9,506 × 9,506	197	1,358,832
af_1_k101	503,625 × 503,625	897	17,550,675
af_2_k101	503,625 × 503,625	897	17,550,675
crystk02	13,965 × 13,965	99	968,583
crystk03	24,696 × 24,696	99	1,751,178
ms_110	8,386,560 × 8,386,560	17	17,919,990
ms_112	8,386,560 × 8,386,560	20	22,497,270
m_110	4,190,000 × 4,190,000	26	10,317,193
m_112	4,190,000 × 4,190,000	32	10,911,913
Maxwell 2D512	786,432 × 786,432	18	5,898,222
Maxwell 2D1024	3,145,728 × 3,145,728	18	23,592,942
KGS 2D1024	2,097,152 × 2,097,152	7	12,574,720
KGS 3D96	1,769,472 × 1,769,472	9	14,045,184

Table 4. Estimated and measured optimal kernels on GTX1070.

Matrix	δ	p_offset	α	p_zero	label_lz	label_s	EOpt	MOpt
wang3	261	14/21	/	/	Y	N	BRCSD-II	BRCSD-II
wang4	261	16/23	/	/	Y	N	BRCSD-II	BRCSD-II
s3dkt3m2	905	0	0.009970	0.936350	/	/	BRCSD-II	BRCSD-II
s3dkq4m2	905	0	0.009970	0.924543	/	/	BRCSD-II	BRCSD-II
kim1	385	2/5	/	/	Y	N	BRCSD-II	BRCSD-II
kim2	4,570	0	0.009600	0.028261	/	/	BRCSD-II	BRCSD-II
nemeth21	96	0	0.009941	0.269383	/	/	BRCSD-II	BRCSD-II
nemeth22	96	6/197	/	/	Y	N	BRCSD-II	BRCSD-II
af_1_k101	5,037	0	0.009978	0.960079	/	/	BRCSD-II	BRCSD-II
af_2_k101	5,037	0	0.009978	0.960079	/	/	BRCSD-II	BRCSD-II
crystk02	140	2/3	/	/	Y	N	BRCSD-II	BRCSD-II
crystk03	247	66/247	/	/	Y	N	BRCSD-II	BRCSD-II
ms_110	83,866	12/17	/	/	Y	N	BRCSD-II	BRCSD-II
ms_112	83,866	3/4	/	/	Y	N	BRCSD-II	BRCSD-II
m_110	419,00	3/26	/	/	Y	N	BRCSD-II	BRCSD-II
m_112	419,00	9/32	/	/	Y	N	BRCSD-II	BRCSD-II
Maxwell 2D512	7,865	2/3	/	/	Y	N	BRCSD-II	BRCSD-II
Maxwell 2D1024	31,458	2/3	/	/	Y	N	BRCSD-II	BRCSD-II
KGS 2D1024	20,972	2/7	/	/	N	N	BRCSD-I	BRCSD-I
KGS 3D96	17,695	2/9	/	/	N	N	BRCSD-I	BRCSD-I
For simplicity, only the double-precision values are used for all computations in the following experiments.

4.1. Experimental analysis
First, we take GTX1070 to test the accuracy of our proposed search engine with the parameter values chosen in Section 3.4. The diagonal sparse matrices for this test are shown in Table 3. Table 4 lists the optimal kernels that are estimated by our proposed search engine as well as the corresponding optimal kernels that are measured on GTX1070 for the 20 matrices. The second, third, fourth, fifth, sixth, and seventh columns display the values of parameters δ, p_offset, α, p_zero, label_lz, and label_s for each matrix, respectively. The eighth column is the optimal kernel estimated by the search engine for each matrix and the corresponding measured optimal kernel is shown in the ninth column. The estimated and measured optimal kernels are matched very well for the test cases. The accuracy rate on GTX1070 is 100%.

Second, given that the choice of the optimized row-piece size is related to the thread-block size, we suppose the row-piece size equals the thread-block size, and then investigate the influence of the thread-block size on the DIA-Adaptive performance for the matrix of different sizes. We take the thread-block size as 128, 256, and 512 in that order, and show the DIA-Adaptive performance curves with the thread-block size on K40c and GTX1070 for 20 matrices of Table 3 in Fig. 13, Fig. 14, respectively. We can observe that given the thread-block size, the DIA-Adaptive performance is different for different GPUs. On the same GPU, for any matrix, the DIA-Adaptive performance has slight difference with different thread-block sizes. For all test matrices, DIA-Adaptive has the worst performance when the thread-block size is 512. This is due to the increase of padded zeros. For example, for s3dkt3m2, the padded zeros are 1,589,832, 1,589,832, and 2,010,225 in that order when the thread-block size is taken as 128, 256, 512, respectively. When the thread-block sizes are 128 and 256, the DIA-Adaptive performance is similar in both cases for all test matrices, and the DIA-Adaptive performance for the thread-block size being 128 is slightly better than that for the thread-block size being 256 on two GPUs. Therefore, we can conclude that the DIA-Adaptive performance is related to the GPU architecture. Because the large thread-block size can lead to the increase of padded zeros, the thread-block size is not easy to take too large for DIA-Adaptive. E.g., 128 or 256 can be better.

Fig. 13
Download : Download high-res image (596KB)
Download : Download full-size image
Fig. 13. DIA-Adaptive performance with thread-block size being 128, 256, and 256 on K40c.

Fig. 14
Download : Download high-res image (571KB)
Download : Download full-size image
Fig. 14. DIA-Adaptive performance with thread-block size being 128, 256, and 256 on GTX1070.

4.2. Performance evaluation
Given that DIA, CSR, and ELL all can efficiently store diagonal sparse matrices, we choose a popular SpMV kernel for CSR, CSR-Adaptive [6], and a representative of SpMV kernels for ELL, HYBMV in the CUSPARSE library [21], and three popular SpMV kernels for DIA, DIA in the CUSP library [4], HDI [2], and CRSD [23], and the best of CSR-Scalar, CSR-Vector, ELL, and HYB kernels in the CUSP library [4], for the performance comparison. The 20 diagonal sparse matrices in Table 3 are used for this test.

First, the performance of various algorithms for all test matrices on K40c and GTX1070 is shown in Fig. 15, Fig. 16, Fig. 17, Fig. 18, respectively. Table 5 presents speedups that are achieved by DIA-Adaptive over other algorithms on both GPUs for all test matrices.

Fig. 15
Download : Download high-res image (627KB)
Download : Download full-size image
Fig. 15. Performance comparison of DIA-Adaptive, CUSP, HYBMV and CSR-Adaptive on K40c.

Fig. 16
Download : Download high-res image (583KB)
Download : Download full-size image
Fig. 16. Performance comparison of DIA-Adaptive, DIA, HDI and CRSD on K40c.

Fig. 17
Download : Download high-res image (716KB)
Download : Download full-size image
Fig. 17. Performance comparison of DIA-Adaptive, CUSP, HYBMV and CSR-Adaptive on GTX1070.

Fig. 18
Download : Download high-res image (689KB)
Download : Download full-size image
Fig. 18. Performance comparison of DIA-Adaptive, DIA, HDI and CRSD on GTX1070.


Table 5. Speedups of DIA-Adaptive over other algorithms.

Matrix	Speedups on K40c	Speedups on GTX1070
CSR-Adaptive	CUSP	HYBMV	DIA	HDI	CRSD	CSR-Adaptive	CUSP	HYBMV	DIA	HDI	CRSD
wang3	4.86	1.54	1.44	2.83	1.89	1.03	2.48	1.29	1.21	2.43	1.65	1.28
wang4	4.75	1.50	1.38	2.63	1.71	0.99	2.38	1.24	1.13	2.24	1.52	1.21
s3dkt3m2	3.08	1.48	0.97	10.33	1.64	1.17	1.14	0.98	0.97	9.46	1.37	1.13
s3dkq4m2	2.90	1.27	0.96	8.44	1.43	1.27	1.22	0.99	0.97	7.77	1.28	1.20
kim1	4.30	1.79	1.57	1.25	1.27	1.48	2.04	1.56	1.40	1.13	1.18	1.46
kim2	2.24	1.52	1.51	1.22	1.08	1.00	1.72	1.48	1.44	1.24	1.04	1.01
nemeth21	2.36	1.36	1.02	1.44	1.12	1.94	1.26	1.03	1.07	1.09	1.09	1.48
nemeth22	2.23	1.32	1.04	1.36	1.18	1.78	1.21	1.10	1.14	1.09	1.13	1.85
af_1_k101	2.07	1.04	0.98	20.90	1.68	1.48	1.12	1.02	0.97	16.60	1.45	1.44
af_2_k101	2.07	1.04	0.98	20.90	1.68	1.48	1.12	1.02	0.97	16.60	1.45	1.44
crystk02	1.64	1.19	0.92	1.45	1.07	1.46	1.24	0.80	0.93	1.05	1.16	1.15
crystk03	1.77	1.33	0.94	1.40	1.44	1.43	1.35	1.03	1.02	1.03	1.07	1.27
ms_110	1.91	1.41	1.28	4.88	1.07	1.03	1.44	1.43	1.42	2.97	1.04	1.00
ms_112	2.16	1.61	1.47	5.68	1.08	1.00	1.64	1.64	1.63	3.68	1.09	1.00
m_110	1.92	1.90	1.34	7.10	1.09	1.19	1.47	1.80	1.62	5.83	1.05	1.13
m_112	1.89	1.96	1.41	7.85	1.09	1.28	1.51	1.94	1.76	6.03	1.07	1.20
Maxwell2D512	2.77	1.47	1.77	1.43	1.17	1.00	1.56	1.53	1.73	1.20	1.11	1.01
Maxwell2D1024	2.41	1.51	1.75	1.46	1.12	1.02	1.51	1.52	1.67	1.17	1.05	1.01
KGS2D1024	2.66	1.68	1.60	1.60	1.21	1.30	1.62	1.57	1.56	1.19	1.11	1.21
KGS3D96	2.33	1.69	1.84	1.44	1.23	1.61	1.84	1.57	1.55	1.17	1.14	1.56
From Fig. 15, Fig. 17, we can observe the performance comparison of DIA-Adaptive, CUSP, HYBMV and CSR-Adaptive on K40c and GTX1070, respectively. On the K40c GPU, DIA-Adaptive outperforms CUSP and HYBMV for all matrices except for s3dkt3m2, s3dkq4m2, crystk02, crystk03, af_1_k101, and af_2_k101. DIA-Adaptive has lower performance than CUSP and HYBMV for the six matrices due to the fact that ELL may be the most appropriate format for the two matrices. For s3dkt3m2, s3dkq4m2, crystk02, crystk03, af_1_k101, and af_2_k101, DIA-Adaptive has slight performance degradation, but it still achieves average performance improvement of 1.48 × over CUSP and 1.31 × over HYBMV for all test cases (Table 5). In addition, DIA-Adaptive has better behavior than CSR-Adaptive for all test matrices and achieves average performance improvement of 2.61 × over CSR-Adaptive (Table 5). Similar to K40c, we can also see that DIA-Adaptive has higher performance than CSR-Adaptive, CUSP, and HYBMV for all matrices except for s3dkt3m2, s3dkq4m2, af_1_k101, and af_2_k101, and crystk02 on GTX1070 (Fig. 17), and achieves average speedups of 1.53 × over CSR-Adaptive, 1.31× over CUSP, and 1.33 × over HYBMV, respectively (Table 5).

Furthermore, we show the performance comparison of DIA-Adaptive and three popular SpMV kernels for the diagonal storage format, DIA, HDI, and CRSD on K40c and GTX1070 in Fig. 16, Fig. 18, respectively. Obviously, the performance of the proposed DIA-Adaptive is almost much better than that of DIA, HDI, and CRSD for all matrices on two GPUs from Fig. 16, Fig. 18. DIA-Adaptive achieves speedups, ranging from 1.22 to 20.90 on K40c and 1.03 to 16.60 on GTX1070, over DIA, and the average speedups on K40c and GTX1070 are roughly 5.28 and 4.25, respectively (Table 5). The speedups of DIA-Adaptive over HDI range from 1.07 to 1.89 on K40c and 1.04 to 1.65 on GTX1070, and the average speedups on K40c and GTX1070 are roughly 1.33 and 1.21, respectively (Table 5). As compared to CRSD, DIA-Adaptive achieves speedups ranging from 0.99 to 1.94 on K40c and 1.00 to 1.85 on GTX1070, and the average speedups on K40c and GTX1070 are roughly 1.29 and 1.25, respectively (Table 5).

Second, we take GTX1070 to test the effective bandwidth of all algorithms. The effective bandwidth [22] is calculated by the following:(2)
 
 Here the effective bandwidth is in units of GB/s (second); 
 is the number of bytes read per kernel and 
 is the number of bytes written per kernel. The effective bandwidth of all algorithms for all test matrices on GTX1070 is shown in Fig. 19, Fig. 20. Compared to CSR-Adaptive, CUSP, and HYBMV, DIA-Adaptive has smaller effective bandwidth when the size and nonzeros of test matrices such as wang3, wang4, s3dkt3m2, s3dkq4m2, kim1, af_1_k101, and af_2_k101 are small; otherwise, it outperforms them for the effective bandwidth (Fig. 19). For all test cases in this test, DIA-Adaptive achieves average improvement of 1.12 × over CSR-Adaptive, 1.34 × over CUSP, and 1.05× over HYBMV, respectively. Furthermore, as shown in Fig. 20, DIA-Adaptive has better effective bandwidth for all matrices compared to the three popular SpMV kernels for the diagonal storage format, DIA, HDI and CRSD, and its average efficient bandwidth is almost 4.25, 1.20 and 1.25 times of that of DIA, HDI and CRSD, respectively.

Fig. 19
Download : Download high-res image (747KB)
Download : Download full-size image
Fig. 19. Effective bandwidth of DIA-Adaptive, CUSP, HYBMV and CSR-Adaptive on GTX1070.

Fig. 20
Download : Download high-res image (693KB)
Download : Download full-size image
Fig. 20. Effective bandwidth of DIA-Adaptive, DIA, HDI and CRSD on GTX1070.

4.3. Discussion and analysis
From the experimental results mentioned in the above section, we discover that DIA-Adaptive is advantageous over DIA, HDI and CRSD. In this subsection, we address what makes DIA-Adaptive achieve better performance than DIA, HDI and CRSD. All experiments are executed on GTX1070.

Compared to DIA, the performance improvement of DIA-Adaptive mainly derives from decreasing the zero padding. From Table 6, we can see that the number of padded zeros in the DIA-A format is much smaller than that in the DIA format for all test matrices. Note that here we uniformly use DIA-A to denote the sparse storage formats (DIA, BRCSD-I and BRCSD-II) included in DIA-Adaptive, and DIA-A presents different forms for different types of diagonal sparse matrices. For example, for matrices in type I, DIA-A is denoted as the DIA format; and for matrices in type II, DIA-A is displayed in the BRCSD-I format; and DIA-A is denoted as the BRCSD-II format for matrices in type III.


Table 6. Number of padded zeros.

Matrix	DIA	HDI	CRSD	DIA-A
wang3	370,176	7,376	3,920	5,408
wang4	422,368	8,176	4,056	5,788
s3dkt3m2	55,557,872	1,736,264	1,471,831	1,589,657
s3dkq4m2	55,359,064	2,071,741	1,808,475	1,925,774
kim1	27,180	16,986	13,898	15,081
kim2	94,380	51,148	47,388	48,092
nemeth21	432,768	375,926	359,102	375,098
nemeth22	513,850	419,600	410,448	418,192
af_1_k101	434,200,950	11,326,367	11,142,962	11,226,892
af_2_k101	434,200,950	11,326,367	11,142,962	11,226,892
crystk02	413,952	392,403	317,037	387,906
crystk03	693,726	670,054	531,144	663,014
ms_110	124,651,530	10	10	10
ms_112	145,233,930	10	10	10
m_110	98,622,807	32,247	4,851	32,247
m_112	123,168,087	16,215	12,000	16,215
Maxwell 2D512	8,257,554	7,613,458	7,460,946	7,477,778
Maxwell 2D1024	3,303,0162	273,938	68,479	106,002
KGS 2D1024	2,105,344	1,693,606	1,289,848	1,311,609
KGS 3D96	1,880,064	145,408	39,472	43,008
However, for HDI and CRSD, the number of padded zeros is close to DIA-A (see Table 6). Moreover, similar to DIA-A, they are inspired by DIA. So what's the difference between them. In the following part, we exhibit them.

(1)
Our algorithm includes three sparse storage formats: DIA, BRCSD-I and BRCSD-II, which are corresponding to three different types of diagonal sparse matrices, respectively. However, HDI and CRSD both use the same storage format for all types of diagonal sparse matrices.

(2)
For the first type of diagonal sparse matrices, DIA-A is denoted as the DIA storage format, and the DIA kernel is utilized in our algorithm. In this case, compared to HDI and CRSD, our algorithm does not convert the format, and has the similar kernel execution time with them. For example, for a  matrix of 3 diagonals, assume that its , the time of constructing HDI and CRSD formats is 8.9663 millisecond (ms) and 15.4929 ms, respectively, and the time of executing HDI and CRSD kernels is 0.2243 ms and 0.2102 ms, respectively. The execution time of the DIA kernel is 0.1962 ms. This indicates that our algorithm outperforms HDI and CRSD in this case.

(3)
For the second type of diagonal sparse matrices, DIA-A is displayed in the BRCSD-I storage format, and the BRCSD-I kernel is used in our algorithm. BRCSD-I has the following difference compared to HDI and CRSD.

(3.1) Difference with HDI

i)
BRCSD-I and HDI have different division methods of row pieces, and BRCSD-I has much smaller number of row pieces than HDI. For example, for the  diagonal sparse matrix with 3 diagonals shown in the left figure of Fig. 21, 3 row pieces are obtained according to the division method of row pieces in BRCSD-I. For HDI, the matrix is broken into equally sized groups of rows ().  is a multiple of the warp size (32). Thus,  row pieces are obtained for HDI.

Fig. 21
Download : Download high-res image (144KB)
Download : Download full-size image
Fig. 21. Two samples of the second type of matrices.

ii)
The way of storing data for BRCSD-I and HDI is different. BRCSD-I merges the data of continuous row pieces with the same offsets into one row piece while HDI does not. For example, for the right matrix shown in Fig. 21, BRCSD-I is represented as(3) 
 

HDI represents the matrix by the following arrays:(4) 
 

In addition, the computation complexity of constructing BRCSD-I is smaller than that of constructing HDI. This can be verified by Table 7.


Table 7. Comparison results of HDI and BRCSD-I.

Matrix	Construction time of formats	Execution time of kernels
HDI	BRCSD-I	HDI	BRCSD-I
KGS 2D1024	1030.3066	677.8335	0.6901	0.6217
KGS 3D96	1283.0171	782.3270	0.7453	0.6538
PDE 2D1024	93.5582	59.7293	0.3536	0.2901
PDE 3D1024	216.3984	124.6504	5.0491	3.6675
iii)
BRCSD-I and HDI kernel implementations are different. Although BRCSD-I and HDI kernels both use one thread to compute a row, the BRCSD-I kernel has the following two advantages compared to the HDI kernel: 1) the BRCSD-I kernel avoids the accesses to brcsdI_offsets by utilizing the loop unrolling optimization technique, and thus the accesses to all data are coalesced. In the HDI kernel, accessing the data is partially coalesced because the accesses to hdi_offsets and hackOffsets are not aligned; and 2) as mentioned above, BRCSD-I merges the data of continuous row pieces with the same offsets into one row piece. This is beneficial for utilizing the loop unrolling optimization technique to improve the performance of the BRCSD-I kernel.

To verify the above observations, we take four matrices such as KGS 2D1024, KGS 3D96, PDE 2D1024 and PDE 3D1024 to show the comparison results of HDI and BRCSD-I in Table 7. The time unit is ms. PDE 2D1024 and PDE 3D1024 come from the literature [10], and are a  diagonal sparse matrix with 5 diagonals and a  diagonal sparse matrix with 7 diagonals, respectively. The four matrices are chosen because they belong to the second type of diagonal sparse matrices.

From Table 7, we can discover that for the four test matrices, BRCSD-I not only has much less construction time, but also has less execution time compared to HDI. This verifies our conclusions mentioned above.

(3.2) Difference with CRSD

i)
Similar to HDI, the matrix is broken into equally sized groups of rows () for CRSD, and  is equal to 2 or 4 or 8 [24]. Obviously, its division method of row pieces is different with BRCSD-I. BRCSD-I gets much smaller number of row pieces than CRSD.

ii)
In this case, there are no scatter points for BRCSD-I. For CRSD, a point is regarded as the scatter point only when the point is the only nonzero term of a diagonal in a row piece, and thus scatter points can be encountered in this case. For example, for the second type of diagonal sparse matrices shown in the right figure of Fig. 21, 5 is a scatter point for CRSD.

iii)
The way of storing data for BRCSD-I and CRSD is different. For example, for the right matrix shown in Fig. 21, BRCSD-I are represented in Eq. (3), and CRSD represents the matrix by the following arrays:(5) 
 

Moreover, the computation complexity of constructing BRCSD-I is much smaller than that of constructing CRSD. This can be verified by Table 8.


Table 8. Comparison results of CRSD and BRCSD-I.

Matrix	Construction time of formats	Execution time of kernels
CRSD	BRCSD-I	CRSD	BRCSD-I
KGS 2D1024	1918.4305	677.8335	0.7522	0.6217
KGS 3D96	2172.1483	782.3270	1.0199	0.6538
PDE 2D1024	146.7205	59.7293	0.3162	0.2901
PDE 3D1024	370.7885	124.6504	4.1442	3.6675
iv)
BRCSD-I and CRSD kernel implementations are different, and the BRCSD-I kernel has the following two merits compared to the CRSD kernel: 1) as mentioned above, the accesses to all data in the BRCSD-I kernel are coalesced. However, accessing the data in the CRSD kernel is partially coalesced. For example, the access to x is often not aligned. In addition, after merging the data of continuous row pieces with the same offsets into one row piece, the remainder of the size of row pieces divided by the number of threads per block is usually not equal to zero. This causes some threads to idle and thus decreases the CRSD kernel performance to some extent; and 2) in the CRSD kernel, a separate thread block is used to process the scatter points. Because the scatter points cannot often be enough to make threads in the thread block be fully occupied, a large number of idle threads result in decreasing the performance of the CRSD kernel.

To verify the above observations, we still take KGS 2D1024, KGS 3D96, PDE 2D1024 and PDE 3D1024 to show the comparison results of CRSD and BRCSD-I in Table 8. The time unit is ms. We see that BRCSD-I is much better than CRSD in the time of constructing the format, and has smaller execution time than CRSD. This verifies our above conclusions.

(4)
For the third type of diagonal sparse matrices, DIA-A is denoted as the BRCSD-II storage format, and the BRCSD-II kernel is used in our algorithm. BRCSD-II has the following difference compared to HDI and CRSD.

(4.1) Difference with HDI

i)
BRCSD-II and HDI both divide the matrix into equally sized groups of rows, but the partition size is slightly different. For BRCSD-II, the partition size is generally equal to the number of threads per block (i.e., 256), and for HDI, the partition size is only designated a multiple of warp size.

ii)
BRCSD-II and HDI have some difference in the way of storing data. BRCSD-II merges the data of continuous row pieces with the same offsets into one row piece while HDI does not. For example, for the matrix shown in Fig. 22, BRCSD-II is represented as(6) 
 
  and HDI represents the matrix by the following arrays:(7) 
 
 

Fig. 22
Download : Download high-res image (153KB)
Download : Download full-size image
Fig. 22. A sample of the third type of matrices.

Moreover, the computation complexity of constructing BRCSD-II is slightly less than that of constructing HDI (see Table 9).


Table 9. Comparison results of HDI and BRCSD-II.

Matrix	Construction time of formats	Execution time of kernels
HDI	BRCSD-II	HDI	BRCSD-II
wang3	1.4267	1.3548	0.0093	0.0056
s3dkt3m2	77.0123	71.4339	0.2915	0.2128
kim1	3.3603	3.2248	0.0503	0.0426
nemeth21	5.8537	5.2789	0.0743	0.0630
af_1_k101	524.6214	517.6547	1.6260	1.1214
crystk03	8.7351	7.83354	0.1003	0.0937
iii)
Implementing BRCSD-II and HDI on GPU has some difference. Although BRCSD-II and HDI kernels both use one thread to compute a row, the BRCSD-II kernel has the following two advantages compared to the HDI kernel: 1) the BRCSD-II kernel avoids the accesses to brcsdII_offsets by utilizing the loop unrolling optimization technique, and thus the accesses to all data are coalesced. In the HDI kernel, accessing the data is partially coalesced because the accesses to hdi_offsets and hackOffsets are not aligned; and 2) as mentioned above, BRCSD-II merges the data of continuous row pieces with the same offsets into one row piece. This is beneficial for utilizing the loop unrolling optimization technique to improve the performance of the BRCSD-II kernel.

We take wang3, s3dkt3m2, kim1, nemeth21, af_1_k101, and crystk03 to verify the above observations. The six matrices in Table 3 are chosen because they are the third type of diagonal sparse matrices, and the comparison results of BRCSD-II and HDI are listed in Table 9. The time unit is ms. We observe that BRCSD-II is better than HDI in both the time of constructing the format and the time of executing the kernel. These results verify the above conclusions.

(4.2) Difference with CRSD

i)
BRCSD-II and CRSD both divide the matrix into equally sized groups of rows, but the partition size is different. Moreover, BRCSD-II has much smaller number of row pieces than CRSD. For BRCSD-II, the partition size is generally equal to the number of threads per block (i.e., 256), and for CRSD, the partition size is designated 2 or 4 or 8.

ii)
BRCSD-II and CRSD have different definitions and handling methods of scatter points. For BRCSD-II, a point is regarded as the scatter point only when the point is the only nonzero term of a diagonal in a matrix. However, for CRSD, as long as a point is the only nonzero term of a diagonal in the row piece, the point is regarded as a scatter point. For example, in Fig. 22, only 8 is a scatter point for BRCSD-II while 8, 10 and 11 all are scatter points for CRSD.

For BRCSD-II, scatter points are handled by filling in zeros, and for CRSD, the nonzero items in the row where the scatter points are located are taken out and stored separately. For example, for the matrix in Fig. 22, BRCSD-II is represented in Eq. (6), and CRSD represents the matrix as(8) 
 

iii)
From Eqs. (6) and (8), we can observe that the way of storing data for BRCSD-II and CRSD is significantly different. Moreover, the computation complexity of constructing BRCSD-II is less than that of constructing CRSD (see Table 10).


Table 10. Comparison results of CRSD and BRCSD-II.

Matrix	Construction time of formats	Execution time of kernels
CRSD	BRCSD-II	CRSD	BRCSD-II
wang3	2.1185	1.3548	0.0072	0.0056
s3dkt3m2	199.8022	71.4339	0.2405	0.2128
kim1	5.4964	3.2248	0.0622	0.0426
nemeth21	11.8897	5.2789	0.0932	0.0630
af_1_k101	1659.2945	517.6547	1.6148	1.1214
crystk03	17.9350	7.83354	0.1189	0.0937
iv)
BRCSD-II and CRSD kernel implementations are different, and the BRCSD-II kernel has the following two merits compared to the CRSD kernel: 1) as mentioned above, the accesses to all data in the BRCSD-II kernel are coalesced. However, accessing the data in the CRSD kernel is not fully coalesced. For example, the access to x is often not aligned. In addition, after merging the data of continuous row pieces with the same offsets into one row piece, the remainder of the size of row pieces divided by the number of threads per block is usually not equal to zero. This causes some threads to idle and thus decreases the CRSD kernel performance to some extent; and 2) in the CRSD kernel, a separate thread block is used to process the scatter points. Because the scatter points cannot often be enough to make threads in the thread block be fully occupied, a large number of idle threads result in decreasing the performance of the CRSD kernel.

Here we still take wang3, s3dkt3m2, kim1, nemeth21, af_1_k101, and crystk03 of Table 3 to verify the above observations, and the comparison results of BRCSD-II and CRSD are listed in Table 10. The time unit is ms. We observe that BRCSD-II is much less than CRSD in the time of constructing the format, and has less execution time than CRSD. These results verify our conclusions.

5. Conclusion
In this paper, we present an adaptive sparse matrix-vector multiplication for diagonal sparse matrices on GPU. In this proposed algorithm, the diagonal sparse matrices are divided into three types, and the most suitable sparse storage format and kernel corresponding to it are proposed for each type of matrices. Thus, using a search engine, this algorithm can automatically choose the most appropriate storage format and kernel for any diagonal sparse matrix, and high performance is achieved. The experiments validate the high efficiency of our proposed algorithm.

From Section 4.3, we know that as compared to DIA, BRCSD-I/BRCSD-II has less zero padding and utilizes the loop unrolling optimization technique to improve their performance for the second/third type of matrices. In the following contents, we first take the four matrices in Section 4.3, i.e., KGS 2D1024, KGS 3D96, PDE 2D1024 and PDE 3D1024, to show the effect of reducing zero padding and utilizing loop unrolling in BRCSD-I, as shown in Table 11. The time unit is ms. BRCSD-I-NLP is as same as BRCSD-I except that it doesn't use the loop unrolling. We observe that as compared to DIA, reducing the zero padding can improve the performance of BRCSD-I-NLP. The minimum and maximum execution time ratios of BRCSD-I-NLP to DIA are 1.1094 and 1.2948, respectively, and the average execution time ratio is 1.1803. Furthermore, comparing the execution time of BRCSD-I-NLP and BRCSD-I, we see that the loop unrolling can improve the performance of BRCSD-I. BRCSD-I has less execution time than BRCSD-I-NLP for all four matrices. The minimum and maximum execution time ratios of BRCSD-I-NLP to BRCSD-I are 1.0546 and 1.0986, respectively, and the average execution time ratio is 1.0774.


Table 11. Effect of reducing zero padding and utilizing loop unrolling in BRCSD-I.

Matrix	DIA	BRCSD-I-NLP	BRCSD-I
KGS 2D1024	0.7398	0.6592	0.6217
KGS 3D96	0.7649	0.6895	0.6538
PDE 2D1024	0.3808	0.3187	0.2901
PDE 3D1024	5.2056	4.0204	3.6675
Second, we take the six matrices used in Section 4.3, i.e., wang3, s3dkt3m2, kim1, nemeth21, af_1_k101, and crystk03, to exhibit the effect of decreasing zero padding and utilizing loop unrolling in BRCSD-II, as shown in Table 12. The time unit is ms. BRCSD-II-NLP is as same as BRCSD-II except that it doesn't use the loop unrolling. By comparing the execution time of DIA and BRCSD-II-NLP, we see that decreasing zero padding can improve the performance of BRCSD-II-NLP. The more zero padding is reduced, the better the BRCSD-II-NLP performance is. For example, for af_1_k101, as compared to DIA, the zero padding of BRCSD-II-NLP is reduced from 434,200,950 to 11,226,892, and the reduction ratio is up to 97.41%, and the performance of BRCSD-II-NLP is improved by 93.12%; for crystk03, the reduction ratio of zero padding for BRCSD-II-NLP only reaches 4.42% compared to DIA, and thus the performance of BRCSD-II-NLP improves only 0.73%. Furthermore, comparing BRCSD-II-NLP with BRCSD-II, we observe that the loop unrolling can improve the performance of BRCSD-II. BRCSD-II has less execution time than BRCSD-II-NLP for all six matrices. The minimum and maximum execution time ratios of BRCSD-II-NLP to BRCSD-II are 1.0224 and 1.3571, respectively, and the average execution time ratio is 1.1274.


Table 12. Effect of decreasing zero padding and utilizing loop unrolling in BRCSD-II.

Matrix	DIA	BRCSD-II-NLP	BRCSD-II
wang3	0.0136	0.0076	0.0056
s3dkt3m2	2.0131	0.2345	0.2128
kim1	0.0481	0.0467	0.0426
nemeth21	0.0687	0.0658	0.0630
af_1_k101	18.6152	1.2812	1.1214
crystk03	0.0965	0.0958	0.0937
For our algorithm, the row-piece size equals the thread-block size. If the row-piece size is less than the thread-block size (i.e., the thread-block size is the multiple of the row-piece size), what will the performance change of our algorithm? Assuming that the thread-block size is 256, we set the row-piece size to 8, 16, 32, 64, 128 in that order, and take BRCSD-II to investigate its performance change with different row-piece size. Table 13 shows the performance change of BRCSD-II as the row-piece size increases. The time unit is ms. We observe that when the row-piece size is 8 and 16, the performance of BRCSD-II kernel is worse than that of BRCSD-II kernel with the row-piece size being 256; when the row-piece size is the multiple of warps such as 32, 64, and 128, BRCSD-II kernel has almost the same execution time, and BRCSD-II kernel with the row-piece size being 256 is only slightly better than that with the row-piece size being 32, 64, and 128, respectively. However, the time of constructing BRCSD-II format with the row-piece size being 32, 64, and 128, respectively is more than that of constructing BRCSD-II format with the row-piece size is 256. For example, for s3dkt3m2, the time of constructing BRCSD-II format with the row-piece size being 32, 64, and 128 is 92.8641, 85.0063, and 77.1486, respectively, while the time of constructing BRCSD-II format with the row-piece size being 256 is 71.4339. Therefore, it's a good idea to make the row-piece size equal to thread-block size.


Table 13. Performance change of BRCSD-II with different row-piece size.

Matrix	8	16	32	64	128	256
wang3	0.0066	0.0061	0.0058	0.0057	0.0059	0.0056
s3dkt3m2	0.3226	0.2354	0.2143	0.2189	0.2205	0.2128
kim1	0.0603	0.0511	0.0430	0.0441	0.0449	0.0426
nemeth21	0.0784	0.0722	0.0642	0.0645	0.0651	0.0630
af_1_k101	1.8396	1.5844	1.1297	1.1799	1.1431	1.1214
crystk03	0.1393	0.1012	0.0947	0.0956	0.0945	0.0937
Next, we will further do research in this field and apply the proposed DIA-Adaptive to the practical problems.