Abstract
We study the impact of communication latency on the classical Work Stealing load balancing algorithm. Our paper extends the reference model in which we introduce a latency parameter. By using a theoretical analysis and simulation, we study the overall impact of this latency on the Makespan (maximum completion time). We derive a new expression of the expected running time of a bag of independent tasks scheduled by Work Stealing. This expression enables us to predict under which conditions a given run will yield acceptable performance. For instance, we can easily calibrate the maximal number of processors to use for a given work/platform combination. All our results are validated through simulation on a wide range of parameters.

Previous
Next 
Keywords
Work Stealing

Latency

Makespan analysis

1. Introduction
Work stealing (WS) is a distributed on-line scheduling algorithm proposed for shared-memory multi-cores [5]. With work stealing, each processor maintains a local list of tasks to be executed. When a processor has executed all its local work, it probes others by sending work requests to randomly chosen processors. We say that this processor attempts to “steal” work from others. Despite its apparent simplicity, work stealing has been proven to be very effective for scheduling tasks in shared-memory homogeneous architectures [8]. Today, the research on WS is driven by the question on how to extend the analysis for the characteristics of new computing platforms (distributed memory, large scale, heterogeneity). Notice that beside its theoretical interest, WS has been implemented successfully in several languages and parallel libraries including Cilk [12], [19], TBB (Threading Building Blocks) [26], the PGAS language [10], [22] and the KAAPI run-time system [14].

In this paper, we study the performance of WS in a distributed-memory context. Such an architecture consists of independent processing elements with private local memories linked by an interconnection network. Communication latency is crucial and highly influences the performance of the applications [16]. The impact of scheduling is important since the whole execution can be highly affected by a large communication latency of interconnection networks [17].

Contributions.
In this work, we study how communication latency impacts work stealing. Our work has four main contributions. First, we create a new realistic scheduling model for distributed-memory clusters of  identical processors including latency (denoted by ). Second, we provide an upper bound of the expected makespan to execute a bag of  independent unitary tasks by  processors. This bound is the sum of two terms. The first is the usual lower bound on the best possible load-balancing 
 
; The second term is proportional to 
 
. Third, we develop a lightweight Python simulator for running adequate experiments. Our simulator is developed to be flexible enough to simulate different topologies and applications with different variants of Work Stealing algorithms. Using this simulator, we provide simulation results that challenges this bound. These experiments show that the second additional term has indeed the form 
 
, with . Note that our theoretical upper bound shows that . Finally, we show that our methodology can be easily extended to the case of tasks with precedence. We provide an upper bound of the expected makespan in this case. This bound is also composed of two terms. The first is the usual lower bound 
 
 and the second additional term is proportional to the critical path ()

Our analyses are based on the use of adequate potential functions. While the approach is similar to existing work, and in particular [29], there are two technical difficulties that had to be lifted. First, a natural extension of the potential functions used in [29] does not work as one needs to account for the work in transit. Second, we do not assume that task execution and steal requests are synchronized. This leads to consider timesteps of duration equal to the communication latency.

The rest of the paper is organized as follows. We review the related works in Section 2. We present the independent task model in Section 3. We derive our main technical result in Section 4 in which we obtain a bound on the makespan. We introduce our work stealing simulator in Section 5 that we use to study the limits of our analysis. We show how to extend our analysis to tasks with precedence in Section 6 and conclude in Section 7. Last, the Appendix contains some technical proofs.

2. Related work
Scheduling is the process which aims at determining where and when to execute the tasks of a target parallel application. The applications are represented as directed acyclic graphs where the vertices are the basic operations and the arcs are the dependencies between the tasks [9]. Scheduling is a crucial problem which has been extensively studied under many variants for the successive generations of parallel and distributed systems. The most commonly studied objective is to minimize the makespan (denoted by 
) and the underlying context is usually to consider centralized algorithms. This assumption is not always realistic, especially if we consider distributed memory allocations and an on-line setting.

WS has been studied originally by Blumofe and Leiserson in [8]. They showed that the expected Makespan of a series–parallel precedence graph with  unit tasks on  processors is bounded by 
 
 where  is the length of the critical path of the graph (its depth). This analysis has been improved in Arora et al. [5] using potential functions. The case of varying processor speeds has been studied by Bender and Rabin in [6] where the authors introduced a new policy called high utilization scheduler that extends the homogeneous case. The specific case of tree-shaped computations with a more accurate model has been studied in [28]. However, in all these previous analyses, the precedence graph is constrained to have only one source and an out-degree of at most 2 which does not easily model the basic case of independent tasks. Simulating independent tasks with a binary precedences tree gives a bound of 
 
 since a complete binary tree of  vertices has a depth 
. However, with this approach, the structure of the binary tree dictates which tasks are stolen. More recently, in [29], Tchiboukjian et al. provided the best bound known at this time: 
 
 where .

Acar et al. [1], [2] studied data locality of WS on shared-memory and focus on cache misses. The underlying model assumes that the execution time takes  time units if the instruction incurs a cache miss and  unit otherwise. A steal attempt takes at least  and at most  steps to complete ( multiple steal attempts before succeeding). When a steal succeeds, the thief starts working on the stolen task at the next step. This model is similar to the classical WS model without communication since the thief does not wait several time steps to receive the stolen task.

In complement, [13], [23] provided a theoretical analysis based on a Markovian model using mean field theory. They targeted the expectation of the average response time and showed that the system converges to a deterministic Ordinary Differential Equation. Note that there exist other results that study the steady state performance of WS when the work generation is random including Berenbrink et al. [7], Lueling and Monien [21] and Rudolph et al. [27].

In all these previous theoretical results, communications are not directly addressed (or at least they are taken implicitly into account by the underlying model). WS largely focused on shared memory systems and its performance on modern platforms (distributed-memory systems, hierarchical platform, clusters with explicit communication cost) is not really well understood. The difficulty lies in the problems of communication which become more crucial in modern platforms [4].

Dinan et al. [10] implemented WS on large-scale clusters, and proposed to split each tasks queues into a part accessed asynchronously by local processes and a shared portion synchronized by a lock which can be accessed by any remote processes in order to reduce contention. Multi-core processor based on NUMA (non-uniform memory access) architecture is the mainstream today and such new platforms include accelerators as GPGPU [30]. The authors propose an efficient task management mechanism which is to divide the application into a large number of fine grained tasks which generate large amount of small communications between the CPU and the GPGPUs. However, these data transmissions are very slow. They consider that the transmission time of small data and big data is the same.

Besides the large literature on theoretical works, there exist more practical studies implementing WS libraries where some attempts were provided for taking into account communications: SLAW is a task-based library introduced in [15], combining work-first and help-first scheduling policies focused on locality awareness in PGAS (Partitioned Global Address Space) languages like UPC (Unified Parallel C). It has been extended in HotSLAW, which provides a high level API that abstracts concurrent task management [22]. [20] proposes an asynchronous WS (AsynchWS) strategy which exploits opportunities to overlap communication with local tasks allowing to hide high communication overheads in distributed memory systems. The principle is based on a hierarchical victim selection, also based on PGAS. Perarnau and Sato presented in [25] an experimental evaluation of WS on the scale of ten thousands compute nodes where the communication depends on the distance between the nodes. They investigated in detail the impact of the communication on the performance. In particular, the physical distance between remote nodes is taken into account. Mullet et al. studied in [24] Latency-Hiding, a new WS algorithm that hides the overhead caused by some operations, such as waiting for a request from a client or waiting for a response from a remote machine. The authors refer to this delay as latency which is slightly different that the more general concept we consider in our paper. Agrawal et al. proposed an analysis [3] showing the optimality for task graphs with bounded degrees and developed a library in Cilk++ called Nabbit for executing tasks with arbitrary dependencies, with reasonable block sizes.

3. Work-stealing
In this section, we introduce some formal notations and the WS algorithm.

3.1. Notation and definition
We consider a discrete time model. There are  processors. We denote by 
 the amount of work on processor 
 at time  (for ). A unit of work corresponds to one unit of execution time. We denote the total amount of work on all processors by 
. At  the whole work is on 
. The total amount of work at time  is denoted by 
.

3.2. Work stealing algorithm
Work Stealing is a decentralized list scheduling algorithm where each processor maintains its own local queue or deque of tasks to execute. When a processor  has nothing to execute, it selects another processor  uniformly at random and sends a work request to it. When a processor  receives this request, it answers by either sending some of its work or by a fail response.

•
Latency: All communication takes a time 
 that we call the latency. Fig. 1 presents an example of Work Stealing execution with latency , A work request that is sent at time  by a thief will be received at time  by the victim. The thief will then receive an answer at time . As we consider a discrete-time model, we say that a work request arrives at time  if it arrives between  (not-included) and . This means that at time , this work request is treated. We do not assume that the work requests are synchronized. The number of incoming work requests at time  is denoted by . It is equal to the number of processors sending a work request at time . When a processor  receives a work request from a thief , it sends a part of its work to . This communication takes again  units of time. The processor  receives the work at time . We denote by 
 the amount of work in transit from 
 at time . At end of the communication 
 becomes 0 until another work request arrives.

•
Single work transfer: We assume that a processor can send some work to at most one processor at a time. While the processor sends work to a thief, it replies by a fail response to any other work request. Hence, the work request may fail in the following cases: (1) when the victim does not have enough work, (2) when it is already sending some work to another thief, or (3) when the victim receives more than one work request at the same time. In the latter, the processor picks random thief and send a negative response to the remaining thieves.

•
Steal Threshold: The main goal of WS is to share work between processors in order to balance the load and the speed-up execution. In some cases however it might be beneficial to keep work locally and answer negatively to some work requests. We assume that if the victim has less than  units of work to execute, the work request fails (answering such a work request would increase the makespan as the time to answer a request is  units of time).

•
Work division: Since the tasks are independent, we suppose that the victim sends to the thief half of its work: 

4. Analysis of the completion time
This section contains the main result of the paper which is a bound on the expected makespan. Before presenting the detailed analysis, we first describe its main steps before jumping into the technical analysis.

4.1. General principle
We denote by 
 the makespan (i.e., total execution time). In a WS algorithm, each processor either executes work or tries to steal work. As the round-trip-time of a communication is  and the total amount of work is equal to  and the number of processors is , we have 
 where  is the number of processors. This leads to a straightforward bound of the Makespan: (1)
 
 
Note that the above inequality is not an equality because the execution might end while some processors are still waiting for work.

The key element of our analysis is to obtain a bound on the number of work requests. For that, we use what we call a potential function that represents how the tasks are (un)balanced. We bound the number of work requests by showing that each event involving a steal operation contributes to the decrease of the potential. Our approach is similar to the one of [29] but with one additional key difficulty: communications take  time units. At first, it seems that longer communications should translate linearly into the time taken by work requests but this would neglect the fact that longer communications also reduce the number of work requests.

In order to analyze the impact of , we reconsider the time division as periods of duration . We analyze the system at each time step  for . By abuse of notation, we denote by 
 and 
 the quantities 
 and 
. We denote the total number of incoming work requests in the interval  by 
 and we denote by  the probability that a processor receives one or more requests in the interval  (this function will be computed in the next section). Note that since a steal requests takes at least  units of time to be answered, we have .

The key steps of the analysis are as follows:

1.
First, we will define a potential function  that is such that we can bound the expected decrease of the potential as a function of , the number of work requests in the time interval : This is done in Lemma 1.

2.
Second, we will show that this bound implies that the number of work requests is upper bounded by 
, where 
. This will be done in Lemma 2.

3.
We will then obtain a bound on the Makespan by using Eq. (1).

4.2. Expected decrease of the potential
Our results are based on the analysis of the decrease of the potential. The potential at time-step  is denoted by  and is defined as: (2)
 
We also denote by 
 the contribution of processor  to the potential (for ).

The rational behind the definition of Eq. (2) is as follows. The potential is maximal when all the work is contained in one processor which is the potential function at time  and is equal to 
. The schedule completes when the potential reaches . Up to the multiplicative factor 
, the rest of Eq. (2) is composed in two terms: 
 and 
. These terms serve to measure how unbalanced is the work. The first term is only here to ensure that the potential is never smaller than  (this is a technical condition that ensures that  and that is used in the proof of Lemma 2).

The following lemma shows that  decreases in expectation with . In this lemma, we denote by 
 all the events up to the interval . The notation 
 indicated the conditional expectation of  given all information available at time . We show that it can be upper bounded by a simple function of the potential at time step  and of the number of work requests .

Lemma 1

The expected ratio between  and  knowing 
 is bounded by: 
where the potential is defined as in Eq. (2) and (3)
 
 
 

Sketch of proof

Each event related to a steal request decreases the potential:

•
When a steal arrives at a processor with 
 jobs, approximately 
 jobs remain on this processor while 
 jobs go into a term 
. In Eq. (2), this transforms a 
 into 
, leading to a decrease of potential by a factor  for each successful steal.

•
When some work arrive at a processor, a term 
 of Eq. (2) is transformed into a term 
. Because of the factor , in the potential, this transforms 
 into 
.

The proof of Lemma 1 can be obtained by computing the probability that a given processor receives a work request. This probability depends of the number of work active processors. The full proof is detailed in Appendix A.1.

4.3. Analysis of the makespan
We are now ready to prove the bound on the total completion time 
. To show that, we first obtain a bound on the Number of Work Requests by using Lemma 1. Let us define the constant  as follows: (4)
 
 
where  is defined in Eq. (3).

Lemma 2

Let  denote the potential at time  and let  be the first time step at which the potential reaches . Then, the number of incoming work requests until , 
, satisfies: 
 The constant  is such that .

Sketch of proof

We detail here an informal proof that assumes that the evolution of the potential satisfies  (without the conditional expectation of Lemma 1). This allows us to make a simpler exposition of the main idea of the proof. Because of the stochastic evolution of , the real proof is more technical and requires the use of Martingale arguments and of Jensen’s inequality. It is detailed in Appendix A.2.

Assume that when  steal requests arrive during a time-slot, the expected potential is multiplied by . If  is such that , this shows that if the number of steal requests that arrive per time slots  to  are , ,…, then , which implies that 
. The definition of  in Eq. (4) is a worst case analysis of what would happen if an adversary could choose the . Eq. (4) implies that . This shows that 
.

Lemma 2 provides a bound on the number of work requests before the end of the schedule. By using Eq. (1), this translates almost immediately into a bound on the makespan, as summarized by the following theorem.

Theorem 3

Let 
 be the Makespan of  unit independent tasks scheduled by WS with latency . Then, 
 
 
 
 

The constant  is the same as in Lemma 2. In particular .

Proof

By Lemma 2, the number of incoming work requests until  is bounded by 
, with 
. Moreover by definition, the schedule is finished at time . Thus by Eq. (1) we have 
 
 
 
 
 where we used that 
 for .

5. Experiments results for independent tasks
In the previous section, we proved a new upper bound of the Makespan of WS with an explicit latency. The objective of this section is to study WS experimentally in order to confirm the theoretical results and to refine the constant . To this end we developed an ad-hoc simulator that follows strictly our model. We focus on the case of independent tasks. We start by describing our WS simulator and the considered test configurations. Using the experimental results, we show that our theoretical bound is close to the experimental results. We conclude with a discussion on where would the analysis be made more precise.

5.1. Simulator and configurations
We have used our python discrete event simulator [18] for running adequate experiments. The simulator can run different models of the work stealing algorithm. It models the execution of an application on a platform. An application consists of a list of tasks with or without dependencies, and the platform consists of multiple processors linked by a specific topology. The simulator allows to execute a scenario with a specific task on a specific platform. The simulator is designed to be flexible. It offers various types of applications and various topologies, and its architecture facilitates the development of other types of applications and other topologies for interconnecting the processors. The simulator can be used to study in detail each simulation, by using numerical (execution time, number of steal requests, etc...) or graphical (Gantt chart, real time execution etc...) representations.

For our tests, we configure our simulator in order to follow the model of independent tasks described in Section 3 to schedule  unitary independent tasks on a distributed platform composed of  identical processors. The communication latency between processor is equal to . To ensure reproducibility, the use of the simulator is detailed in [18] and the code is available on github.1

5.2. Validation of the bound and definition of the “overhead ratio”
As seen before, the bound of the expected Makespan is the sum of two terms: The first term is the ratio  which does not depend on the configuration nor on the algorithm; The second term represents the overhead related to work requests. Our analysis bounds the second term to derive our bound on the Makespan. To analyze the validity of our bound, we define what we call the overhead ratio as the ratio between the second term of our theoretical bound (
) and the simulated execution time minus the ratio : for a given simulation, we define (5)
 
 

To study this overhead ratio, we vary the number of unit tasks  between 
 and 
, the number of processors  between 32 and 256 and the latency  between 2 and 500 which is a realistic range of actual systems. Each experimental setting has been reproduced 1000 times in order to compute median or interquartile ranges.

Fig. 2 plots the overhead ratio according to each couple , for different latency values  units of time. The x-axis is  for all values of  and  intervals and the -axis shows the overhead ratio. We use here a BoxPlot graphical method to present the results. BoxPlots provides a good overview and a numerical summary of a data set. The “interquartile range” in the middle part of the plot represents the middle quartiles where 50% of the results are presented. The line inside the box presents the median. The whiskers on either side of the IQR represent the lowest and highest quartiles of the data.

We observe that our upper bound is about 4 to 5.5 times greater than to the one computed by simulation (depending on the range of parameters). The ratio between the two bounds decreases with the number of processors but seems fairly independent of .


Download : Download high-res image (692KB)
Download : Download full-size image
Fig. 3. Evolution of the state of the systems for different number of processors (
). Each column corresponds to a number of processors (,  or ). The first line corresponds to 
 (defined in Eq. (7)). The second line displays the number of idle processors . The third line displays  (defined in Eq. (12)). In all figures, the -axis corresponds to the number of time steps .

5.3. Discussion: where does the overhead ratio come from?
The challenge of this paper is to analyze WS algorithm with an explicit latency. We presented a new analysis which derives a bound on the expected Makespan for a given ,  and . It shows that the expected Makespan is bounded by  plus an additional term bounded by 
 with . As observed in Fig. 2, the constant  is about four to five times larger than the one observed by simulation. A more precise fitting based on simulation results leads to the expression 
 (the value  is an average over all our experiments). We explain below where does the discrepancy between the theoretical bound of  and the experimental result of  come from by looking at the different steps of the proof. Our analysis makes essentially three approximations: (1) The function  is an upper bound on the potential diminution (2) We consider a worst case scenario for the number of steal requests when we define 
; and (3) We bound  by 4.03. We review below the contribution of each approximation to the overhead ratio.

To illustrate our explanations, we run three simulations by using our simulator and report the results in Fig. 3. These three executions have the same number of tasks 
 and latency  and we consider three value of processors: . This figure illustrates the evolution with time of the potential , the number of idle processors and the value of  defined in Section 4.3. Each column represents a value of . The lines represent the results for each metric.

5.3.1. Impact of the bound 
The first step of our analysis is to prove a bound on the decrease potential: We show in Lemma 1 that (6)

This bound is obtained by computing the diminution of the potential in the various cases of the proof of Lemma 1. These various cases make different approximations. First, our bound  is the maximum between the ratio  of Case 1 and the ratio  of Case 2a. Second, we assumed that we do not know when a work requests arrive in the interval. We therefore always took the worst case (arrivals at the end of the intervals). Third, we neglect the diminution of the potential due to working processors (Case 2a).

To see measure the impact of this approximation, we define a theoretical function 
 that would correspond to the potential of the system if the inequality of Eq. (6) was an equality: (7)
 

In Fig. 3-(Line 1), we plot this theoretical function and the real potential function as a function of time step . This figure indicates that the distance between the real potential and the theoretical bound is relatively small at the beginning of the execution, which makes sense since the diminution of the potential is dominated by the diminution related to Case 2a. The two function starts diverging slowly in the middle of the execution and this divergence is accentuated at the end: when the execution is close to finishing, the actual potential decreases much faster that its bound. We believe that this divergence is mostly due to neglecting the diminution of potential due to working processors: At the beginning of the execution when all processors have many tasks to execute, neglecting working processors is negligible; At the end of the execution when the remaining work is small, the potential diminution is greatly affected by the working processors. Our analysis could be tightened by taking a more complex potential function that will take more carefully the impact of working processors.

5.3.2. Evolution of the number of work requests
In our analysis, we study how the potential decreases a function of the number of steal requests . To obtain a bound, we then use a worst-case analysis and define  as the maximal of a function 
: 
. As shown in Fig. 4,  is between 2.8 where  is small to  when  is large. On the second Line 2 of Fig. 3, we depict how the number of idle processors evolve with time. We observe that an execution has essentially three phases: in the beginning, there is a high number of idle processors since the work has to be divided among processors; In the middle of the execution, the number of idle processors is small as everybody is working. In the final execution phase, it increases as finding work becomes harder.

In Fig. 3-(Line 3), we plot  and  as a function of the time step . The figure shows that  is often about 2.8 (because the period where the number of idle processors is low is long). This suggests that our bound 
 is about 1.4 times too high. Being able to capture more precisely how the number of idle processors evolves might lead to a bound that would be around 30% times smaller.

5.3.3. Bound  and impact of the number of processors
In our analysis, we show that  and we bound  by 4.03. In fact, the value of 4.03 corresponds to what happens when  goes to infinity but smaller values of  leads to smaller values of . This explains why in Fig. 2, we observe that the overhead ratio decreases with the number of processors, from around  for  processors to around  to 4.5 for  processors.

In our simulation, we observe that the overhead ratio is between  to . Based on our experimental study of the evolution of the number of work requests with time, we believe that the worst-case analysis  and the bound of 4.03 contributes to a factor of about 1.5 of the overhead ratio. The remaining factor (of about ) is mostly due to the bound  that we obtained in Lemma 1. Refining this lemma, for example by being able to estimate the decrease of potential due to the work execution or by using a different potential definition, would lead to a tighter bound.

6. Extension to a model of tasks with precedence
In this section, we show that the analysis presented in Section 4 can be adapted to the case of tasks with precedence constraints. The main difficulty here is to redefine a new potential function, which we do by combining the ideas of [5], [29] to our model of latency of Section 4. This demonstrates the power of an approach by potential function.

6.1. Model
We consider a model similar to [5], [29] where the workload is composed of  unitary tasks that have precedence constraints represented by a DAG (Directed Acyclic Graph). This DAG has a single source that represents the first task and that is originally located on a given processor. We consider that the scheduling is done as in [5]: each processor maintains a double-ended queue (called deque) of activated tasks. If a processor has one or more task in its deque, it executes the tasks at the head of its deque. This takes one unit of time. After completion, a task might activate ,  or  tasks that are pushed at the end of the deque. The activation tree is a binary tree whose shape depends on the execution of the algorithm. It is a subset of the original DAG and has the same critical path. We define the height of node of this tree as follows. The height of the source as  (i.e., the length of the critical path). The height of another task is equal to the length of its father minus one. We assume that when a processor steals work from another processor, it steals the activated tasks with the largest height. In the case of the DAG of precedence, a processor can only answer positively if it has two or more activated tasks in its deque. In this case, it sends its task that has the largest height.

6.2. Potential function
The potential will depend on the maximal height of the tasks that a processor has in its lists. Denoting by 
 the maximal height of the tasks that processor  has in its deque, we define a quantity 
, that we call the potential work of processor , as: 
 
 For the tasks in transit, we define the potential work in transit 
 
, where  is the height of the task in transit.

The potential is then defined similarly as in Eq. (2): (8)
We are now ready to prove the following lemma, that is an analogue of Lemma 1 for the case of dependent tasks.

Lemma 4

For the case of DAG, the expected ratio between  and  knowing 
 is bounded by: 
where the potential is defined as in Eq. (8) and  is as in Lemma 1, i.e., 
 
 
 
.

Proof

Similarly to the proof of Lemma 1, we study the expected decrease of the potential by distinguishing three cases: the case 
 (work arriving at a thief) and the cases where a processor is or becomes idle correspond to cases 1 and 4 of Lemma 1 and can be analyzed exactly as what was done since Cases 1 and 4 of the proof of Lemma 1 do not depend on the task dependence model. Moreover, the distinction between Case 2 and Case 3 that was done for independent tasks is not necessary for DAGs as there is no steal threshold for DAG. Last, the analysis of Case 2b of Lemma 1 is similar: if a processor does not receive any work request, the potential does not grow.

Hence, in the reminder of the proof, we focus on the only interesting case which what happens when a processor receives one or more work request (Case 2b of the proof of Lemma 1). We distinguish two cases depending on how many tasks are in the deque of processor  when it receives a work request.

1.
If processor  has only one task (say of height ), then it cannot send work. In this case, it will complete its tasks that will activate at most  tasks of height . In such a case, the potential work of processor  will go from 
 
 to at most 
. This shows that 
 

2.
If processor  has two or more tasks, then by construction, it can have at most two tasks of maximal height (say ). In this case, the processor will send one task (of height ) to the thief, and will at the mean time execute the other task. In this case, the maximal height of the task of its deque will be  and the task sent will be of height . This implies that 
 and 
 
. This shows that: 
 

As both  and  are strictly smaller than the  of Eq. (9), the rest of the proof of Lemma 1 can be applied mutatis mutandis to prove Lemma 4.

6.3. Analysis of the makespan
The bound on the expected decrease of the potential of Lemma 4 is the same as the bound of Lemma 1 for independent tasks. Since the proof of Lemma 2 does not depend on the dependence structure of the tasks but only the bound on the expected decrease of Lemma 1, this implies that Lemma 2 also applies for the case of tasks with precedence.

We are now ready to prove the bound on the total completion time 
 for the DAG model that is summarized by the following theorem:

Theorem 5

Let 
 be the Makespan of  unit tasks with a DAG of precedence that has a critical path of  scheduled by WS with latency . Then 
 
 
 The constant  is the same as in Lemma 2. In particular .

Proof

The case of DAG is similar as the independent tasks model the only difference being the expression of the potential at time . In the case of a DAG, the potential at time  is equal to 
 
 
 where  is the critical path. This shows that 
 which implies that 
 

The bounds that we obtained in Theorem 5, Theorem 3 are composed of a term due to the computation of tasks () and an overhead term related to the depth. In case of independent tasks the depth is  but we obtain a slightly better bound due to tasks not being divided below . We actually believe that the bounds on the DAG could be refined when considering non-unit tasks.

7. Conclusion
We presented in this paper a new analysis of Work Stealing algorithm where each communication has a latency of . Our main result was to show that the expected Makespan of a load of  independent tasks on a cluster of  processors is bounded by 
. Our analysis makes use of potential functions whose expected decrease per unit time can be bounded as a function of the number of work requests. We then use this to derive a theoretical upper bound on the expected makespan. We also extend this analysis one step further, by providing a bound on the probability to exceed the bound of the makespan. We showed also that we can extend the same analysis for a model of tasks with precedence.

To assess the tightness of this analysis we developed an ad-hoc simulator. We showed by comparing the theoretical bound and the experimental results that our bound is realistic. We observed moreover that our bound (established on worst case analysis) is four to five times greater than the experimental results and it is stable for all the tested values. By using traces of execution, we quantify the various approximations that are made in the analysis and suggest where the analysis could be made more accurate.

For future work, we intend to study work stealing strategies in complex topologies where the latency can depend on the topology. This work is a first step towards this as it allows a full understanding of the work stealing in the basic, homogeneous, setting.