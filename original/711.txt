Abstract
The recent advancement in the field of electronics has led to development of sensors that capture the image of an area or object in spectral-domain along with spatial information. Due to continuity of spectral domain in hyperspectral images, it is difficult to store, process, analyze or transmit the critical information contained in it. Prediction based compression technique is used to reduce this size by a certain level. It predicts the value of a pixel with some error from previous pixels using a filter and finally, encode that error using variable length encoder. The execution time taken by this technique is very high which can be reduced by high performance computing. In this paper, we designed a mechanism to use high-performance computing techniques in the execution of prediction based image compression algorithms. The average execution time of the RLS-filter based compression algorithm is reduced significantly (by a factor of 29 using 2 nodes with 28 cores each, on PARAM SHIVAY supercomputer) with the proposed technique.

Previous
Next 
Keywords
Multithreading

Multiprocessing

Hyperspectral image compression

RLS filter

Prediction based compression

1. Introduction
High-Performance Computing (HPC) is the demand of the latest technologies like image processing, Internet of Things (IoT), deep learning, machine learning, artificial intelligence, block chain, real-time systems. These technologies require more processing elements (both Central Processing Unit (CPU) and Input/Ouput (I/O)) than traditional ones due to the time complexity associated with it and the availability of more data than it was available in history.

High-Performance Computing (HPC) enables the use of multiple processors/cores/threads for running advanced programs efficiently [35]. It involves implementation of application programs on Grid computers, Cluster Computers, Distributed computers, multi-core computers, Graphics Processing Units (GPUs) [4], and Field Programmable Gate Arrays (FPGAs) [15]. It requires two types of modifications in sequential programming technique, one at the hardware level, i.e., designing hardware to execute more than one process at a time. The second modification is required at the software level, i.e., reorganization of the program such that it can use the maximum processing power of the hardware. Hardware modifications are already going on, and systems with petaflops or 1015floating-point operations per second are available. The focus of most of the researchers is to develop programs that can efficiently and reliably be executed on these systems.

There are two categories [31] of parallel computation: implicit parallelism, and explicit parallelism. Implicit parallelism is processed by the architecture of the compiler, operating system, and hardware that cannot be controlled by the design of the algorithm. Explicit parallelism is carried out by developing an algorithm in such a way that it could take benefit from the available resources [39]. Fig. 1 represents the different forms of parallelism that could be exploited in a broad application. HPC is used by academic institutions, engineers, researchers, military, government agencies, businesses of all sizes, and in the health sector. It primarily focuses on completing tasks faster (performance), and computations on remote devices with high reliability and scalability.

1.1. Hyperspectral Image
Hyperspectral Images (HSI) store detailed information about the spectral properties of any object or area. These images are a set of contiguous bands, each having a fixed range of wavelength denoting reflectance of the object. Hyperspectral sensors capture light reflected from the object and store the reflectance value band-wise. Each band stores and processes some spectral information ranging from 400 to 2500 nm, spread across more than 200 bands. Each pixel in the same band corresponds to the same spectral information but varying spatial information, while each pixel at the same spatial position across the band stores reflectance value of the same object for different wavelengths. Hyperspectral imagery finds its applications in many domains, including agriculture, astronomy, biomedical imaging, Earth observation, track military movements & operations, molecular biology, mineral identification, monitoring natural disasters, identification of disease in the medical industry, physics, and surveillance.

HSIs are large and multi-band data cubes that require large data storage capacities and computational devices for processing and analyzing information. Generally size of HSI exceeds hundreds of MegaBytes (MBs) since each pixel store information of about 12 bit or 16 bit, number of pixels per band range from a few hundred to millions, and number of bands can be 224. For example, calibrated images of standard Consultative Committee for Space Data Systems (CCSDS) [30] dataset have 224 spectral bands, pixels per band typically count to (677 × 512), and each pixel stores 16 bit information. The total size for this standard image is (677 × 512 × 224 16 bits) = 148 MB. Similarly, the multidimensional medical images generated by four-dimensional computed tomography (4D CT), serial block-face electron-microscopy, and magnetic resonance images (MRI) are very large in size. These images need to be transmitted and processed in realtime for tele-medicine applications, or stored for future analysis.

Image compression techniques are used to conquer the problems like high bandwidth & time required for transmission, large disk space required for storage of the large size of HSI. It can reduce the size of HSI data by a factor of 1 to ‘’ , depending on the compression algorithm used. HSI compression is one of the essential steps of HSI processing that is used in every application. Compression makes storage and transmission an easy task by using less number of bits for similar information and removing the redundancy existing in the data. Compression might result in loss of information that could be important for specific applications like object identification, scene classification, anomaly detection, and change detection. Based on the loss of information during the process, compression can be categorized into two algorithms, namely, lossy compression and lossless compression.

1.2. Performance metrics
Performance metrics used to evaluate the proposed algorithm are Mean Absolute Error (MAE), Mean Absolute Percentage Error (MAPE), and Prediction time as a function of a varying number of sub-processes that are executed in parallel. Relative speed-up and work optimality of each method was also calculated using Eq. (1) and Eq. (2) respectively.

(1)
 
(2)
 
 where 
 = Time taken by parallel algorithm on 1 processor, 
 = Time taken by parallel algorithm on  processors,  = number of processors.

1.3. Motivation
In remote sensing, apart from storage and transmission within Earth, HSI compression is mainly processed on satellite, and decompression is mainly processed on the data center, where it has to be processed in real-time. Failing which can result in asynchronous acquisition and transmission of images. Moreover, these algorithms suffer from large execution time due to a large number of pixels on which it has to process. HPC provides a mechanism to reduce the run time of such time-critical algorithms by dividing them into sub-problems and executing them simultaneously. Existing use cases of parallel computing [23] have already proven that satisfactory speedup can be obtained by the use of HPC paradigms on image processing applications.

1.4. Contribution
The significant contributions of this article can be enlisted as:

•
Study of prediction based HSI compression algorithms.

•
Modification of existing prediction based algorithm to remove the dependency among variables.

•
Identification of the optimum number of prediction bands for spectral decorrelation and number of pixels for spatial decorrelation.

•
Parallel implementation on the supercomputer, PARAM SHIVAY (192 nodes, 40 cores per node, 384 GB per node DDR4 RAM).

The rest of this paper is organized as follows. Section 2 provides the literature background of the domain, which serves as a literature review for proposed work. Section 3 describes the proposed method of parallelization. The results of the proposed algorithm are provided in Section 4. Section 5 submits some discussion on results obtained, and Section 6 concludes the article.

2. Related work
The section consists of two sub-sections that discuss existing work in the domain and help in the foundation of proposed work. In the first part, we discuss the theoretical background on the parallel implementation of algorithms based on the processing of high dimensional data. The second part focuses on the prediction based lossless HSI compression technique.

2.1. Parallel programming for high dimensional data
Data collection and information extraction are the two most important steps of prediction based problems. Since data collection is a one-time process, recent developments mainly suggest optimization techniques for improving the accuracy in the information extraction step, including prediction models, neural network models, RNN models, CNN models [14]. Due to the high dimensional structure & size of data, and complexity of these techniques, it takes much time in computation that can be reduced by parallel programming. Parallel computing models have been used for image processing applications previously that permit the use of parallel models like the shared-memory model, distributed memory model, hybrid model and GPU use [25] in HSI compression. Shared Memory Model (SMM) [1] has been used for calculation of local features of an image [26], object identification in multi-spectral imaging [34], face detection using neural network [12], image block representation [42], distributed memory model [1], [22], [32], [38], and hybrid model [7] used for image processing.

2.2. Hyperspectral Image Compression
Traditional compression algorithms designed for 2-D gray scale or Red–Green–Blue (RGB) color images like portable network graphics (PNG) cannot be directly used in the compression of HSIs due to differences in characteristics of HS imagery, explained in Section 1. Generic compression schemes like Burrows–Wheeler transform (BWT), Lempel–Ziv–Welch (LZW)), Deflate, etc., do not provide efficient compression results [3]. These algorithms consider only the spatial redundancy existing in the spectral images, and thus fail to achieve the expected performance. In recent years, researchers have been continuously trying to improve compression methods for hyperspectral imagery utilizing spatial as well as spectral correlations. HSI Compression techniques can be classified into the following major categories: transform-based compression, compressive sensing based compression, tensor-decomposition based compression, vector-quantization based compression, and prediction based compression techniques that exploit spectral correlation existing in HSI along with the spatial correlation[8], [9].

Transform based technique is the most popular 2D image compression technique that has been extended to 3D or HSI compression. It is known as a transform-based technique as it transforms the pixels values into the frequency domain by applying some transformation function to all three dimensions of HSI. There exists some transformation techniques like Discrete Fourier Transform (DFT) [5], Discrete Cosine Transform (DCT) [46], Karhunen–Loeve Transform (KLT) [44], and Discrete Wavelet Transform (DWT) [6] that are used in image compression. It can serve as lossy image compression if floating-point coefficients are converted to integer coefficients for reduced space in storage and secure computation, and lossless compression, if processed without conversion. It can remove both spectral and spatial correlation depending on the domain on which it is applied. The technique can be applied in combination with nearly all other techniques like prediction based, vector quantization, tucker based as well as in compressive sensing based algorithms. Some state-of-the-art algorithms in this field are 3D-DWT [47], KLT, 3D-SPECK [43], 3D-LMBTC [2].

Vector Quantization (VQ) is a data compression technique that takes 3D HSI data cube as input and returns a compressed image. Two significant steps of this method are training (codebook generation) and coding (code vector matching). It represents the pixel values of any spatial position of the first band as the head of an -length vector that consists of pixels of -different bands, where  = total number of bands of HSI. It quantifies the vector instead of performing decorrelation. In [36], a mean-normalized compression method based on VQ is proposed to reduce the dynamic range of vector–vector segmentation. Kamano in [16] proposed the use of two VQs to reduce the size of the codebook to reduce the time complexity of training, following which a fast codebook search method was proposed in [33] to improve the training time efficiency.

The tensor decomposition technique is one of the latest techniques for image compression, which gives high performance compared to traditional methods. Tensor could be considered as an n-Dimensional matrix that can be decomposed easily. In this technique, HSI is stored into 3D tensor(), and one of the tucker decomposition techniques [21] is applied to decompose the 3D tensor() into lower dimension 3D tensor(). Decomposed tensor is then encoded and transmitted through the channel. Some state-of-the-art algorithms NTD+DWT [19], CNN-NTD [24], NTD+DCT [18], of this technique have shown excellent results.

The compressive sensing technique is famous for onboard compression algorithms as it shifts the computation time of encoder to the decoder. It is used in real-time compression as it senses a small chunk of data, compresses it, transmits it to the receiver, and then accepts another chunk. State of the art algorithms for compressed sensing are SHSIR, RLPHCS, OMP, and SSHBCS that show better performance for small bit-rate [49]. The main objective of compressive sensing is to reduce memory usage during computation.

Prediction based compression is an alternative to transform based algorithms with technical and implementational benefits. In this technique, the value of a pixel is predicted after applying some mathematical functions to the previous pixel values. It is developed especially for 3D images, exploits correlation in both spatial and spectral directions, and removes them. Prediction in HSIs is mainly applied on spectral domain with the help of a filter after spatial decorrelation gets completed. Mostly used filter functions to calculate weight matrices are Wiener filter, Recursive Least Square (RLS) filter, and Least Mean Square (LMS) filter. Some start-of-the-art techniques include the Extended-CALIC algorithm proposed in [27] that uses inter-band prediction by extending 2-D CALIC to 3-dimensional images. In [11] LUT (look-up-table) based prediction method was used to find matching pixels from collocated pixels in previous bands. This method was improved by using two-stage LUT in [13]. In [20], a faster prediction method was proposed known as a Fast-Lossless (FL) algorithm that predicts the current band using the previous band and recursively changes the prediction coefficients. The least mean square (LMS) filter is used in the FL algorithm. K-means clustering was used in compression for first-time in [29] that uses the least square filter for calculating the linear prediction coefficient of each cluster individually, and the algorithm was named C-DPCM. An exhaustive brute force method was proposed to improve the compression performance of C-DPCM at the expense of time. It uses adaptive prediction length for the prediction of values of the current band called C-DPCM-APL [28]. RLS filter was first used as a predictor in [40] that finds adaptive step-size of historical values. In [10], an improvised feature of forgetting factor was introduced to reduce the effect of previous pixels on the current predicted pixel value. C-RLS uses a large context window for spatial decorrelation achieving better results. To improve the prediction performance and reduce the run time of the CRLS algorithm, Karaca in [17] proposed a new algorithm named B-CRLS that uses spatial correlation to compress HSI further. In [41], a Fast-RLS-ALP algorithm was proposed to compress HSI on satellite using an RLS filter. Prediction based algorithm is also used in conjunction with other algorithms to improve performance.

It is worth noting that run time of RLS filter based prediction is more, but it has better performance than existing algorithms. In this work, a novel method has been proposed to reduce the running time of RLS based prediction algorithms by executing the algorithms on parallel computers.

3. Proposed method
Parallel programming paradigms including data parallelism & task parallelism have been used to modify the existing RLS filter based prediction algorithms. Vectorization is also considered through programming, which handles instruction level parallelism. The algorithm works in two-phase, namely spatial decorrelation and spectral decorrelation. Spatial correlation in HSIs is available due to high spatial resolution resulting in similar values of a pixel as of neighboring pixels, which is removed by taking the local difference. Spectral correlation is present due to the higher resolution of imaging sensors & narrow, continuous and overlapping spectral bands. In the following section, existing algorithms are discussed along with the parallel versions of respective algorithms.

3.1. Recursive Least Square (RLS)
RLS filter [37] works adaptively as an optimization problem to minimize weighted linear least square cost function by recursively updating the weight matrix. In HSI compression, it predicts the value of a pixel of the th band from a vector of ‘’ collocated pixel values, where  (both inclusive) is the prediction order.

The first step of the RLS algorithm is intra-band prediction, which focuses on spatial decorrelation in the spatial domain. Let  be the current value of a pixel at  position on the th band then, (3)

The local difference matrix of a band ‘’ is calculated using Eq. (3) for all the pixels of that band.  depends on four previous values, as shown in Fig. 2. The same process is repeated for all the bands of HSI, forming a 3-D local difference matrix of size , where  = number of rows or height in HSI,  = number of columns or width in HSI,  = total number of bands in HSI. The local difference matrix is used as an input signal to the second step of the RLS [40] algorithm.

The second step is used to remove spectral decorrelation using the prediction residual. Residual values for each band  represents the difference in original local difference value of pixels from the predicted value of that pixel, and this process is called inter-band prediction. Fig. 3 represents the pixels used for the spectral decorrelation phase of RLS algorithm. In the figure, already predicted green colored pixels of ’’ previous bands () are used to predict red colored pixel of th band.


Download : Download high-res image (109KB)
Download : Download full-size image
Fig. 2. Neighboring pixels used for calculating the local difference.

The computational complexity of Sequential RLS algorithm can be calculated by estimating the steps involved in the most critical phase, i.e., updating weight matrix. It consumes 
 time for each pixel in HSI, where  is the prediction length.

By looking at the structure of given RLS algorithm for HSI compression, it can be observed that it is SIMD-type application, where the same function iterates on  number of bands that can be parallelized by the concept of data parallelism Further, algorithm 1 is examined for task parallelism or instruction parallelism and also converted to SIMD + MIMD-type application. The proposed algorithm is a modified version of [40] that can attain both versions of parallelism using a hybrid model.


Download : Download high-res image (386KB)
Download : Download full-size image
3.2. Conventional Recursive Least Square(A-CRLS)
The conventional RLS algorithm [10] differs from traditional RLS [40] only in the implementation phase. It uses a context window of 24 spatial pixels to perform spatial decorrelation. Fig. 4 represents the context window of the CRLS algorithm. Eq. (3) can be extended to more general form to calculate local difference matrix of CRLS algorithm, as Eqs. (4), (5): (4)
 
(5)
 where, 
 represents reconstructed pixel value,

 represents original pixel value, and  represents local difference value at location  of HSI respectively. In Spectral-domain, prediction order (number of previous bands used for prediction) in the CRLS algorithm is obtained by performing a brute force method on the initial scene. The number of bands giving optimum results for this scene is selected as a prediction order for other scenes of that image. The computational complexity of sequential A-CRLS is explained in the literature [10], which has been significantly reduced by executing it on various processors in this work.

Parallel implementation of A-CRLS

The algorithm is again Single Instruction Multiple Data (SIMD) type and can be converted to the SIMD + Multiple Instruction Multiple Data (MIMD) type application. The modified algorithm can then be implemented on a multi-core and multi-node processor as Algorithm 2 with a slight modification in parameter initialization.

3.3. Recursive Least Square-Adaptive Length Prediction(RLS-ALP)
HSI compression using the RLS-ALP algorithm gives optimal results compared to the traditional RLS algorithm but is more time-consuming. Recursive Least Square-Adaptive Length Prediction (RLS-ALP) finds out optimum prediction order by executing the algorithm multiple times, changing prediction order from 10 to 200 in steps of 10. In the last step, it selects the prediction order giving the best results and executes the model. The computational complexity of sequential RLS-ALP is 
, where  is the number of times RLS algorithm is executed to find the optimal prediction length().

Parallel implementation of RLS-ALP

The algorithm for RLS-ALP can be modified according to the criteria discussed in Algorithm 2. In this case, task parallelism is found to be more critical than data parallelism by executing algorithms with different prediction orders on a different node, and each algorithm is executed in parallel on different cores of a node.

3.4. Fast Recursive Least Square ALP (Fast-RLS-ALP)
A sequential algorithm for Fast Recursive Least Square ALP (FAST-RLS-ALP) is given in [41] that mathematically reduces the execution time of the RLS-ALP algorithm by changing the method to calculate the weight. It is optimal for on-board compression of Band Interleaved (BIL) imagery. Parallel version given in Algorithm 2 is generated from the algorithm already proposed [41]. The computational complexity of sequential FAST-RLS-ALP is significantly reduced to , where  is the prediction length, by replacing the multiplication step in weight calculation with a matrix append operation.

Sequential implementation of algorithms

The sequential algorithms have been optimized through vectorization [48] using the coding paradigm of python. Vectorization is used to run multiple operations from a single instruction accomplishing the goals of SIMD architecture. It operates by utilizing most of the registers available with the system.


Download : Download high-res image (369KB)
Download : Download full-size image
4. Experimental results
Proposed parallel algorithms have been implemented to reduce the running time of algorithms using a parallel programming paradigm, i.e., shared memory parallelism. The following sub-sections contain detailed information about the dataset used, implementation environment, and simulation results.


Table 1. Mean absolute error and mean absolute percentage error of CCSDS dataset.

Algorithm	RLS	CRLS	RLS-ALP	Fast-RLS-
ALP	RLS-parallel	CRLS-
parallel	RLS-ALP-
parallel	Fast-RLS-
ALP-parallel
Image	MAE	MAPE	MAE	MAPE	MAE	MAPE	MAE	MAPE	MAE	MAPE	MAE	MAPE	MAE	MAPE	MAE	MAPE
YSC-0	4.576	0.3754	3.856	0.3522	4.215	0.3627	4.213	0.3626	4.529	0.3752	3.854	0.3524	4.215	0.3621	4.214	0.3628
YSC-3	4.511	0.3666	3.956	0.3501	4.111	0.3589	4.111	0.3589	4.623	0.3822	3.956	0.3503	4.113	0.3594	4.113	0.3594
YSC-10	2.959	0.1863	2.468	0.1498	2.716	0.1662	2.716	0.1661	3.598	0.1866	2.446	0.1494	2.716	0.1652	2.725	0.1669
YSC-11	3.659	0.1654	3.157	0.0794	3.384	0.1251	3.388	0.1251	3.649	0.1655	3.159	0.0799	3.389	0.1256	3.388	0.1251
YSC-18	4.549	0.3258	3.869	0.3549	4.468	0.3179	4.467	0.3176	4.937	0.3118	3.958	0.3552	4.488	0.3178	4.479	0.3179
Average	4.051	0.2839	3.461	0.2573	3.779	0.2661	3.779	0.266	4.267	0.2843	3.475	0.2574	3.784	0.266	3.784	0.2664
YSU-0	7.877	1.005	7.256	0.8125	7.684	0.8956	7.695	0.8958	7.857	1.0051	7.259	0.8125	7.684	0.8956	7.695	0.8957
YSU-3	6.721	0.9554	6.165	0.7568	6.666	0.8995	6.262	0.8991	6.719	0.9555	6.159	0.7566	6.669	0.8999	6.668	0.8998
YSU-10	5.816	0.7373	5.266	0.5967	5.528	0.678	5.527	0.6781	5.816	0.7372	5.268	0.5967	5.529	0.6782	5.521	0.6782
YSU-11	6.252	0.8595	6.149	0.8494	6.591	0.9258	6.591	0.9258	6.282	0.8599	6.159	0.7582	6.592	0.9256	6.592	0.9258
YSU-18	6.713	0.9543	5.891	0.7856	6.289	0.9248	6.289	0.9336	6.722	0.9466	5.895	0.7858	6.345	0.9256	6.289	0.9336
Average	6.676	0.9005	6.145	0.7602	6.552	0.8648	6.473	0.8664	6.679	0.9009	6.148	0.7419	6.654	0.8649	6.553	0.8666
MU	2.992	0.2745	2.156	0.1849	2.491	0.2248	2.491	0.2248	2.991	0.2745	2.159	0.1859	2.056	0.1246	2.061	0.2341
HU	2.599	0.1871	1.975	0.0948	2.056	0.1241	2.057	0.1241	2.589	0.1862	1.985	0.0845	2.165	0.1249	2.058	0.1242
Average	2.795	0.2308	2.065	0.1398	2.273	0.1744	2.274	0.1744	2.79	2.3035	2.072	0.1352	2.111	0.1248	2.059	0.1792
4.1. Dataset
The algorithm was evaluated on the standard CCSDS [30] Hyperspectral dataset available in the public domain. It consists of 12 HSIs produced by the AVIRIS sensor, with the following details. The number of rows is equal to 512 and the number of bands in each image is equal to 224, there are five images of the yellow stone scene that are calibrated and represented by YSC, the number of columns in these images is 677. Each pixel is represented using 16 signed bits, and scene number equal to 0, 3, 10, 11, 18.

Dataset also consists of 5 uncalibrated yellow stone images with scene numbers 0, 3, 10, 11, 18. The number of columns in these images is equal to 680 with the 16 unsigned bits used to represent each pixel, represented by YSU. Scene number 10 of Maine imagery with 680 columns is represented as MU. It is also an uncalibrated image with unsigned 12 bits per pixel information. The last image is scene number 1 of Hawaii represented as HU, the number of columns is equal to 614 and each pixel having information stored using 12 unsigned bits per pixels.

4.2. Implementation environment and metrics
The experiment has been performed on the supercomputer “PARAM-SHIVAY”, with the following configurations: Theoretical Peak Floating-point Performance is 837 TFLOPS, total number of CPU only compute nodes is 192, and total number of cores is 7680.

The implementation is done using Python 3.7 with numpy [45] and scipy library, and parallel programming is achieved using multiprocessing library pre-installed in python 3.7. The application utilizes only a limited number of nodes and memory from hardware as required, but any number of nodes and cores can be pulled anytime. In this section, the term ‘processor’ is used to represent the number of cores used in computation to remove the redundancy with the name ‘core’.

4.3. Results
This sub-section contains the numerical results of the experiment performed on proposed algorithms. The experiment was first carried out on all 12 images by executing the sequential RLS algorithm, followed by RLS-ALP, CRLS, and Fast-RLS-ALP sequential algorithms. The results of MAE (mean absolute error) and MAPE (mean absolute percentage error) were calculated for each algorithm separately. The sequential run-time of each algorithm was calculated by taking an average run-time of 10 consecutive executions.

The process was repeated by changing the number of bands from 4 to 100 in steps of 4 and changing the size of the context window for spatial decorrelation from . The optimal prediction factor was found to be 28, and the context window size does not have much impact on MAE and MAPE.

Parallel algorithms were implemented with varying number of sub-processes and average run-time of 10 consecutive executions of each algorithm, for every sub-process was calculated. The compression ratio of the parallel algorithm is almost similar to the sequential version, so a better metric already defined in Section 1 is used. Table 1 contains the results of MAE and MAPE obtained for CCSDS images. It contains the values for both parallel and sequential algorithms within the scope of this article.

For RLS, CRLS, RLS-ALP algorithm, spatial decorrelation and spectral decorrelation were also calculated separately by different number of processors along with the original method. Fig. 5 shows the results with the help of a bar chart to compare the time taken for spatial and spectral decorrelation. Number of processors = 1 in Fig. 5 represents the runtime of parallel algorithm on one processor. Similar results were obtained for other algorithms, so it is omitted to reduce the space.


Table 2. Speedup, Efficiency and Run-time (in seconds) obtained on multiple processors.

Number of
processors	RLS	CRLS	RLS-ALP	Fast-RLS-ALP
Speedup	Efficiency	Runtime	Speedup	Efficiency	Runtime	Speedup	Efficiency	Runtime	Speedup	Efficiency	Runtime
7	6.2018	0.8859	518.1964	6.0161	0.8594	1727.5847	5.6832	0.8119	564.141	5.4067	0.7724	182.7507
8	6.7739	0.8467	475.6165	6.9854	0.8732	1487.8635	6.359	0.7948	504.1848	5.997	0.7496	163.7579
10	8.2614	0.8261	389.1283	9.9817	0.9982	1041.2377	7.668	0.7668	417.9484	8.573	0.8573	114.8055
14	11.3121	0.8081	284.5069	13.1872	0.9419	788.1372	10.843	0.7745	295.6128	9.125	0.6518	107.5958
16	11.8223	0.7389	271.9258	13.9504	0.8719	745.0197	11.0464	0.6904	290.1091	9.781	0.6113	100.2396
28	18.8516	0.6732	170.6736	19.6491	0.7017	528.9465	17.796	0.707	180.0807	16.083	0.5744	63.0026
32	19.0778	0.5961	168.5835	20.4856	0.6402	507.3477	17.669	0.5521	181.5239	18.1454	0.5671	54.1126
56	22.0086	0.3930	147.9195	28.9475	0.5169	359.0404	19.7148	0.3521	162.9486	17.9807	0.3211	54.9828
The effect of the use of HPC can be visualized in Fig. 6, where the total run-time of different algorithms of prediction-based compression is presented in the form of a graph. It shows the gradual decrease in time when the algorithm is computed in a parallel environment as compared to the sequential one. Two lines representing RLS and RLS-ALP overlap since both have the same computation except the number of previous bands used to predict the values of pixels of a band. However, they differ in terms of numerical values slightly, which can be seen in Table 2, it also contains the values of speedup by the different number of processors. Graphically this parameter is represented in Fig. 7 for speedups obtained by different algorithms, the minimum value obtained is 5.4067 and maximum is 28.9475.


Download : Download high-res image (182KB)
Download : Download full-size image
Fig. 5. Runtime of spatial and spectral decorrelation separately.


Download : Download high-res image (196KB)
Download : Download full-size image
Fig. 6. Total runtime vs number of processors.

Work Optimal Solution:In the HPC environment, a theoretical concept of work optimality is used to check the performance of an algorithm based on number of processing elements consumed. It can be calculated as efficiency using Eq. (2), for each number of sub-processes. The concept is also known as efficiency. Fig. 8 shows the efficiency of proposed parallel algorithms for the different number of processors. It shows the disadvantages of increasing the number of processors to get good speedup as most of the time is wasted as communication overhead. In this experiment, the optimal number of processors is obtained as 10, but if demand is to reduce the time of compression and decompression phase, work optimality has to be sacrificed.


Download : Download high-res image (159KB)
Download : Download full-size image
Fig. 7. Speedup vs number of processors.

5. Discussion
In this experiment, four prediction based HSI compression algorithms are executed in HPC environment. A-CRLS method gives optimum compression results as compared to RLS and RLS-ALP with a drawback of high space and execution time. It can be visualized from the results that the best speedup and efficiency are obtained when A-CRLS is executed using parallel programming paradigms. However, it is noticeably worse in execution time, due to the inherent characteristics of the sequential algorithm (consumes 382% more time than CRLS). We have also executed spatial and spectral decorrelation separately by using a different number of processors. Results obtained in this process represent almost similar behavior for spatial and spectral decorrelation by using the concept of data parallelism.


Table 3. Execution time (in seconds) for Spatial decorrelation by RLS algorithm on multiple processors.

Image	Number of processors
1a	7	8	10	14	16	28	32	56
YSC-0	1031.368	194.183	181.379	149.745	107.651	106.55	68.154	69.419	67.813
YSC-3	1007.378	191.379	170.271	151.823	107.464	105.507	66.532	69.459	72.189
YSC-10	1251.543	189.906	178.938	149.13	103.766	102.786	58.933	57.711	47.137
YSC-11	1015.062	192.227	185.07	148.491	111.501	108.08	66.251	67.534	53.846
YSC-18	1253.905	193.226	169.872	147.683	110.43	106.525	69.355	70.324	51.919
YSU-0	991.658	190.454	168.561	150.08	104.193	105.799	65.561	67.255	52.935
YSU-3	1153.28	191.131	173.382	149.74	105.319	105.702	63.483	57.892	47.833
YSU-10	1136.578	194.429	177.875	148.774	105.312	105.909	68.202	70.993	67.316
YSU-11	1054.316	196.858	180.079	158.328	113.98	110.394	69.763	62.244	48.641
YSU-18	994.232	185.355	171.161	147.254	105.705	101.738	67.254	68.282	67.339
MU	1026.662	193.8798	189.445	154.297	111.221	108.442	67.974	70.823	69.393
HU	926.856	172.478	161.311	137.051	97.5571	96.011	58.2189	52.824	42.805
a
Parallel time on one processor.


Table 4. Execution time (in seconds) for Spectral decorrelation by RLS algorithm on multiple processors.

Image	Number of processors
1a	7	8	10	14	16	28	32	56
YSC-0	2153.269	334.636	313.694	245.728	178.638	164.601	100.6	109.371	99.601
YSC-3	2198.837	320.991	269.313	246.795	177.014	172.305	95.436	96.204	83.669
YSC-10	2188.978	337.872	340.291	236.73	180.283	165.759	107.145	105.785	78.029
YSC-11	2093.89	329.8	292.09	245.651	183.368	170.103	111.059	98.21	83.04
YSC-18	2280.162	348.193	300.953	242.825	179.882	174.634	109.623	108.111	89.005
YSU-0	2168.597	310.328	318.544	231.827	166.28	162.084	98.985	99.56	87.429
YSU-3	2057.414	314.999	281.436	231.624	173.653	161.884	103.644	112.633	90.853
YSU-10	2183.951	342.062	279.009	245.469	172.689	178.707	110.412	102.504	93.265
YSU-11	2158.951	329.174	319.751	246.988	202.914	167.515	112.192	104.668	95.493
YSU-18	2119.385	332.718	285.549	240.301	175.486	169.48	100.341	100.960	89.335
MU	2131.145	331.376	329.675	240.619	178.441	164.221	110.333	102.667	115.59
HU	1990.683	300.702	269.749	222.587	161.336	148.375	98.6325	97.570	80.560
a
Parallel time on one processor.

6. Conclusion and future directions
Parallel programming is used to reduce the run-time of RLS based compression algorithms. In this article, we have proposed a method to reduce the complex structure of prediction-based compression algorithms by using HPC architecture. The very first step is to remove the dependency within the program, which restricts the use of parallel programming. Then, a multi-processing model is used to reduce the run time of prediction algorithms. The program was executed on multi-node system by changing the number of processors in multiple iterations. RLS algorithm is modified to execute spatial and spectral decorrelations separately in different functions. Effect of increasing number of processing elements on execution time of proposed parallel algorithm is discussed in terms of speedup and work optimality.

This method of parallelization can be used in real-time prediction and compression of more complex data. The process can be more optimized by using the concept of hybrid programming, which includes shared memory and distributed memory parallelism together. Moreover, the method can be used to parallelize the compression of medical images used to detect the tumor, cancer using hyperspectral imaging. 