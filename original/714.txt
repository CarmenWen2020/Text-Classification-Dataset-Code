Abstract
Stochastic gradient descent (SGD) is a popular stochastic optimization method in machine learning. Traditional parallel SGD algorithms, e.g., SimuParallel SGD (Zinkevich, 2010), often require all nodes to have the same performance or to consume equal quantities of data. However, these requirements are difficult to satisfy when the parallel SGD algorithms run in a heterogeneous computing environment; low-performance nodes will exert a negative influence on the final result. In this paper, we propose an algorithm called weighted parallel SGD (WP-SGD). WP-SGD combines weighted model parameters from different nodes in the system to produce the final output. WP-SGD makes use of the reduction in standard deviation to compensate for the loss from the inconsistency in performance of nodes in the cluster, which means that WP-SGD does not require that all nodes consume equal quantities of data. We also propose the methods of running two other parallel SGD algorithms combined with WP-SGD in a heterogeneous environment. The experimental results show that WP-SGD significantly outperforms the traditional parallel SGD algorithms on distributed training systems with an unbalanced workload.


Keywords
SGDUnbalanced workload
SimuParallel SGD
Distributed system

1. Introduction
The training process in machine learning can essentially be treated as the solving of the stochastic optimization problem. The objective functions are the mathematical expectation of loss functions, which contain a random variable. The random variables satisfy a known distribution. The machine learning training process can be formalized as (1)
 where  is the loss function,  is the variables,  is the random variable, and  is the probability density function of the distribution .

Because some distributions cannot be presented in the form of a formula, we use the frequency to approximate the product of probability density  and , as a frequency histogram can roughly estimate the curve of a probability density function. Thus, for a dataset, the above formula can be written in the following form: (2)
 
 where  is the number of samples in the dataset, and 
 is the th sample value.

Stochastic gradient descent (SGD) is designed for the following minimization problem: (3)
 
where  is the number of samples in the dataset, and 
 is a convex loss function indexed by  with the model parameters 
. Normally, in the case of regularized risk minimization, 
 is represented by the following formula: (4)
 
where  is a convex function in . It is of note that in the analysis and proof, we treat model parameters, i.e., , as the random variable during the training process.

When 
 is not a strongly convex function, for example a hinge loss, the regularized term would usually guarantee the strongly convexity for 
.

The iteration step for sequential SGD is (5)

Because of its ability to solve machine learning training problems, its small memory footprint, and its robustness against noise, SGD is currently one of the most popular topics [3], [6], [8], [10], [17], [18], [19].

As SGD was increasingly run in parallel computing environments [1], [13], parallel SGD algorithms were developed [11], [29]. However, heterogeneous parallel computing devices, such as GPUs and CPUs or different types of CPU, have different performance. The cluster may contain nodes having different computing performance. At the same time, parallel SGD algorithms suffer from performance inconsistency among the nodes [11]. Therefore, it is necessary to tolerate a higher error rate or to use more time when running parallel SGD algorithms on an unbalanced-workload system.


Download : Download high-res image (210KB)
Download : Download full-size image
In this paper, we propose the following weighted parallel SGD (WP-SGD) for a distributed training system with an unbalanced workload. WP-SGD is given as Algorithm 1. WP-SGD adjusts the weights of model parameters from each node according to the quantity of data consumed by that node. The working pattern of WP-SGD is illustrated in Fig. 1.


Download : Download high-res image (165KB)
Download : Download full-size image
Fig. 1. Working pattern of WP-SGD when the quantities of data differ.

In WP-SGD,  and  is a parameter which is decided during the training process,1  is the learning rate and  is the regularization parameter. We determine the value of the  via experience, data fitting, or analysis of training data and . When  is not a strongly convex function,  is close to the .

WP-SGD is based on SimuParallel SGD [29], which is shown as Algorithm 2. The working pattern of SimuParallel SGD is illustrated in Fig. 2.


Download : Download high-res image (153KB)
Download : Download full-size image
The main bottleneck for SimuParallel SGD in the heterogeneous parallel computing environment is that we need to guarantee that all nodes have trained on equal quantities of data before we average them (Line 5 and Line 11, respectively, in Algorithm 2). This requirement leads to a degradation in performance on the heterogeneous cluster. WP-SGD uses a weighted average operation to break this bottleneck. WP-SGD does not require all nodes to be trained on equal quantities of data and incorporates the delay information into the weights (Line 5, Line 6, and Line 12 with Eq. 6), which allows WP-SGD to run efficiently in a heterogeneous parallel computing environment.


Download : Download high-res image (163KB)
Download : Download full-size image
Fig. 2. Working pattern of SimuParallel SGD with equal quantities of data.

WP-SGD suggests that when the workload is unbalanced within the cluster and there is a delay between the fastest node and the th node, the weight of the model parameters on the th node should be decreased exponentially.

The upper bound of the objective function value calculated by the output of WP-SGD will be less than the upper bound of the objective function value of the output from sequential SGD in the fastest node, when solving problems with more variance (smaller regularization constant) [29], and the system parameters satisfy Eq. (6). (6)
where  is the number of nodes in this system.

What is more, WP-SGD is able to combine traditional synchronous parallel SGD algorithm, i.e., mini-batch SGD, and asynchronous parallel SGD algorithm, i.e., delay SGD.

A numerical experiment on data from KDD Cup 2010 [26] and Real-sim dataset show the following conclusion:

1. The final output of WP-SGD with an unbalanced workload can be nearly equivalent to the output from a system with a perfectly balanced workload with the best performance nodes. In a workload-unbalanced environment, WP-SGD uses less time than workload-balanced SGD.

2. For inappropriate algorithms or synchronous algorithms on the unbalanced workload system, parallel technology brings negative effects.

3. Combining with WP-SGD, traditional synchronous algorithms overcome the system’s workload unbalanced problems.

The key contributions of this paper are as follows:

1. We propose a novel parallel SGD algorithm, WP-SGD, for a distributed training system with unbalanced workloads.

2. We prove that WP-SGD can tolerate a large delay between different nodes. WP-SGD suggests that when there is an increase in the delay between the fastest node and the th node, the weight of the model parameters for the th node should be decreased exponentially.

3. We provide the results of experiments which we conducted using real-world data to demonstrate the advantages of WP-SGD on computing environment with unbalanced workloads.

In the next section, we present related works. In Section 3 we present a basic view of traditional parallel SGD algorithms. In Section 4, we demonstrate the analysis and proof for WP-SGD. In Section 5, we theoretically offer some complementary technologies based on WP-SGD. In Section 6, we present the results of the numerical experiments.

2. Related work
SGD dates back to early work by Robbins and Monro et al. [2], [17]. In recent years, combined with the GPU [1], [13], parallel SGD algorithms have become one of the most powerful weapon for solving machine learning training problems [8], [9], [14]. Parallel SGD algorithms can be roughly classified into two categories, which we call delay SGD algorithms and model average SGD algorithms.

Delay SGD algorithms first appeared in Langford et al.’s work [14]. In a delay SGD algorithm, current model parameters add the gradient of older model parameters in  iterations ( is a random number where 
, in which 
 is a constant). The iteration step for delay SGD algorithms is (7)
In the Hogwild! algorithm [11], under some restrictions, parallel SGD can be implemented in a lock-free style, which is robust to noise [4]. However, these methods lead to the consequence that the convergence speed will be decreased by o(
). To ensure the delay is limited, communication overhead is unavoidable, which hurts performance. The trade-off in delay SGD is between delay, degree of parallelism, and system efficiency:

1. Low-lag SGD algorithms use fewer iteration steps to reach the minimum of the objective function. However, these algorithms limit the number of workers and require a barrier, which is a burden when engineering the system.

2. Lock-free method is efficient for engineering the system, but the convergence speed, which depends on the maximum lag, i.e.  in Eq. (7), is slow.

3. The lower limit of the delay is the maximum number of workers the system can have.

From the point of view of engineering implementation, the implementation of delay SGD algorithms is accomplished with a parameter server. Popular parameter server frameworks include ps-lite in MXNet [5], TensorFlow [1], and Petuum [24]. A method that constricts the delay was offered by Ho et al. [12]. However, if the workers in the parameter server have different performance,  is increased, causing convergence speed to be reduced.

Delay SGD algorithms can be considered as an accelerated version of sequential SGD. Model average SGD algorithms accelerate SGD via the averaging of model parameters. Zinkevich et al. [29] proposed SimuParallel SGD, which has almost no communication overhead. Y. Zhang et al. [28] gave an insightful analysis and proof for this parallel algorithm. However, these methods do not take into account the heterogeneous computing environment. J. Zhang et al. [28] also point out the invalidity of SimuParallel SGD. In fact, the effect of a model average SGD depends primarily on how large the model parameters’ relative standard deviation is, which means it is a trade-off between the parallelism and the applicability for dataset.

From the point of view of engineering implementation, model average SGD algorithms can be implemented in a MapReduce manner [7]. Thus, most of them are running on platforms like Hadoop [22]. If the nodes in the cluster have different performance, the slowest node is the performance bottleneck.

In HPC & AI area, model average SGD algorithm and its variant, batch SGD and decentralized SGD, are the most important parallel and distributed SGD algorithm [15], [21], [23], [25]. However, those algorithms often require all nodes to have the same performance or to consume equal quantities of data. Thus, unbalanced workload is the key performance bottleneck for model average SGD algorithm on HPC. What is more, with the development of heterogeneous parallel computing devices, heterogeneous parallel computing cluster is the main stream high performance computing cluster style. The cluster may contain nodes having different computing performance. Heterogeneous parallel computing cluster exacerbates unbalanced workload problem.

Above parallel SGD algorithms have various superb features. However, all of them lack robustness against an unbalanced workload.

3. Background and main idea
3.1. Notation and definitions

Notation	Definition
The number of samples consumed by the fastest nodes
The delay between the fastest nodes and the th node.Here we define the delay as the gap of the number of data samples between the fast nodes and node .
The th sample value
The label for the th sample
The parameter for the regularization term. For some loss functions, such as hinge loss, it guarantees strongly convexity.
Step length or learning rate for SGD
Variables for objective function. In machine learning, it is the model parameters.
The random variable
The number of samples in the dataset
Loss function
In WP-SGD, the weight for the th node on contracting map rate 
The loss function without a regularization term
The distribution for the random variables
The total number of nodes in a cluster
The contracting map rate for  in SGD
In delay SGD, the delay between the current model parameters and the older model parameters when current model updates itself by the information of the older model parameters.
Maximum number of 
The distribution of the unique fixed point in SimuParallel SGD and WP-SGD, with learning rate 
The distribution of the stochastic gradient descent update after  updates, i.e., 
, with learning rate .
A random variable whose PDF is 
. The output of the th node after 
 iterations.
A random variable. The output of WP-SGD, where the fastest node trained on  samples
The distribution of 
Wasserstein distance between two distributions 
In more average operation SimuParallel SGD and WP-SGD, which are offered at Section 5, the span between two average operations from the view of the fastest nodes
The final output of an algorithm
The mean of the random variable 
The standard deviation of the random variable 
Euclidean distance.
We collect our common notations and definitions in this subsection.

Definition 1 Lipschitz Continuity

A function :  is Lipschitz continuous with constant  with respect to a distance, in this paper, we use Euclidean distance,  if  for all .

Definition 2 Lipschitz Seminorm

Luxburg and Bousquet [16] introduced a seminorm. With minor modification, we use (8)
That is, 
 is the smallest constant for which Lipschitz continuity holds.

In the following, we let 
 as a function of 
 for all occurring data  and for all values of  within a suitably chosen (often compact) domain.  is a constant.

Definition 3 Contracting Map

 is Euclidean distance, for a map , and exists a point 
, if (9)
holds for all  in domain and . We name  as contracting map, 
 is the fixed point and  is contracting map rate.

Definition 4

Relative Standard Deviation of  with respect to 
(10)
As we can see, 
, where 
 is the mean of .

Supertabular 3.1 shows the notations used in this paper and the corresponding definitions.

3.2. Introduction to SGD theory
The core proof idea of SGD’s proof in paper [29] is that with the process of SGD, the PDF of model parameters will converge into a fixed PDF. Thus, all discussions about the change in the training process are about random variables. When we draw a specific realization about model parameters , i.e., a specific machine learning model at  iteration, from model parameters’ PDF, it is an independent process.

Theorem 1, Theorem 2, Theorem 3, and Lemma 1 are key theorems we will use. All four theorems are proved by Zinkevich et al. [29].

Theorem 1

[29, Theorem 11]
Given a cost function  that 
 and 
 are bounded, and a distribution  such that 
 is bounded, then for any point  (11)
 
 
 

Theorem 1 highlights the relationship between the distribution of model parameters and  
 , which is the expected result of SGD when  is equal to .

Theorem 2

[29, Theorem 9]
(12)
where 
 is the distribution of a unique fixed point in SimuParallel SGD. This theorem provides an idea of the bound on the third part of Theorem 1.

Lemma 1

[29, Lemma 30]
(13)
where  is the Euclidean distance.

Theorem 3

[29, Theorem 70]
If 
 is the distribution of the stochastic gradient descent update after  iterations, then (14)
 
 (15)
 
 

The above theorems describe how and why SGD can converge to a minimum. The difference between the value of  using the output  from SGD and the minimum of  is controlled by three factors:

(1) The difference between the expectation of the current distribution of model parameters and the expectation of 

(2) The standard deviation of the distribution of the current model parameters, which is 

(3) The difference between the expected value of  when  satisfies distribution 
 and the minimum value of 

For the sequential SGD, carrying out the algorithm would reduce the first part and the second part. The third part is controlled by  and .

For SimuParallel SGD, the first part and the third part are the same for different nodes. However, 
 can benefit from the averaging operation. SimuParallel SGD uses the gain in the standard deviation to reduce the number of iteration steps needed to reduce the first and second parts. In other words, SimuParallel SGD accelerates SGD.

4. Proof and analysis
4.1. Analysis of WP-SGD
The concept of WP-SGD has two main aspects:

1. Our proposed weight is to compensate for the main loss from the delay between the different nodes. The main loss from the delay is controlled by the exponential term 
.

2. Under the condition that the gain from the standard deviation’s reduction is greater than the loss in the mean’s weighted average from the perspective of the fastest node, the WP-SGD output will outperform the fastest node.

All of the following lemmas, corollaries, and theorems are our contributions.

We focus on the first aspect at the beginning: Corollary 1 and Lemma 3 show how the mean and standard deviation will change by using WP-SGD. Their sum is the upper bound of the relative standard deviation which is shown in Lemma 1.

Lemma 2 is used in the proof of Corollary 1.

Lemma 2

Suppose that 
 are independent distributed random variables over 
. Then if 
 and 
, it is the case that 

In following part of section 4.1, we give theory based on 
 setting.

Corollary 1

The fastest node consumes  data samples. 
 is the distribution of model parameters updated after 
 iterations in node , and 
 is the distribution of the stochastic gradient descent update in WP-SGD. (16)
 

Lemma 3

 is the output of node . Then, if (17)
then the distribution of 
 is 
. It is the case that (18)
 
 
 

Combining Lemma 1, Theorem 1, Corollary 1 whose proof uses Lemma 2, and Lemma 3, we have the following:

Theorem 4

Given a cost function  such that 
 and 
 are bounded, the bound of WP-SGD is (19)
 
 
 
 
 
 

Next, we discuss the second aspect.

It is apparent that there is no guarantee that the output of WP-SGD will be better than the output from the fastest nodes, because from the viewpoint of the best-performing node, the weighted average will damage its gain from contraction of the mean value term. Here, we offer Corollary 2 that defines the conditions under which the output from the fastest nodes will benefit from the normal-performance nodes. In the following, 
 is the Wasserstein distance between two distributions , and the fastest nodes consume  data samples in an unbalanced-workload system.

Corollary 2

For WP-SGD, when (20)
 
 
 the upper bound of the objective function value of WP-SGD is closer to the minimum than is the upper bound of the objective function value of sequential SGD on the fastest nodes.

 is not a prior value. However, Corollary 2 still eliminates the dataset whose 
 and 
 are small. 
 is positively related to the standard deviation of the dataset. The standard deviation of the dataset will influence the values of 
 and 
. In an extreme example, when all samples in the dataset are the same, i.e., SGD degenerates into Gradient Descent, i.e. GD, WP-SGD would be invalid, and this is also the case with SimuParallel SGD.

Most of the time, the standard deviations of real-world datasets, especially high dimension sparsity dataset, are usually large enough. In the case where the 
 and 
 are large enough, under Corollary 3, WP-SGD would be better than the sequential SGD.

Corollary 3

For WP-SGD, when (21)
the upper bound of the objective function value of WP-SGD is closer to the minimum than is the upper bound of the objective function value of sequential SGD on the fastest nodes.

Corollary 3 suggests that WP-SGD can tolerate sufficient delay. As we can see, the robustness of whole system will be stronger as the scale of the cluster increases.

4.2. Analysis and redesign: weight for the dataset and  whose contracting map rate is small
Above weight design using  is to meet the upper bound of SGD algorithm: The comparison between the upper bound shows the superiority between different algorithm. However, in practice, not all SGD processes which use different dataset and loss function reaches the upper bound.

For example, the convergence speed of log loss is faster than hinge loss; the convergence speed of the process which uses small variance dataset is faster than the convergence speed of the process which uses large variance dataset.

Thus, the weight of using  should be used in the condition where the equivalent condition of inequalities is achieved, i.e.  is very close to being a linear function (the proof of Lemma 3 in Zinkevich et al.’s work [29]). This requirement means that  is not a strongly convex function.

The nature of SGD is contracting map. Contracting map rate varies during the iteration process for every iteration in SGD process. When the loss function’s second derivative is larger, or during the process, many of the samples’ directions are parallel to the current model parameters’ direction, the contracting map rate will be smaller. Therefore, from the standpoint of the overall iteration process rather than that of a single iteration, we should redesign a smaller contracting map rate to replace . We denote this new contracting map rate by . Usually,  should be a smaller number when the direction of processing samples is closer to the direction of the current model parameters, i.e., 
, and the second derivative of  is larger.

We can determine the value of the new contracting map parameter via experience, data fitting, or analysis of training data and , as in Fig. 3.

Theorem 5

Incorporating  into Theorem 4
Given a cost function  such that 
 and 
 are bounded, and in view of the overall process, the contracting map rate is , and the bound of WP-SGD is (22)
 
 
 
 
 
 

Corollary 4

Incorporating  into Corollary 3
Given that WP-SGD runs on a dataset having a large standard deviation and a large standard deviation of the fixed point, and in view of the overall process, the contracting map rate of  is , and when (23)
the upper bound of the objective function value of WP-SGD is closer to the minimum than is the upper bound of the objective function value of sequential SGD on the fastest nodes.

5. Running popular parallel SGD algorithms combined with WP-SGD in heterogeneous environments
Current parallel SGD algorithms lack the feature of robustness in heterogeneous environments. However, they are characterized by a number of superb features such as the overlap between communication and computing (delay SGD) and fast convergence speed (model average SGD). It is reasonable to consider combining WP-SGD with these algorithms in order to gain the benefits of their excellent features and the adaptability to unbalanced-workload environments.

5.1. Combining WP-SGD with other model average SGD
Mini-batch SGD that averages the model parameters at each iteration [27] is the most important model average SGD. However, averaging at each iteration operation is expensive, and the mini-batch is more vulnerable to performance differences. There is a compromise parallel SGD algorithm that averages model parameters at a fixed  length. The number of  is from the point of the best performance nodes. Here we offer theoretical analyses of this parallel algorithm and its theoretical performance in unbalanced-workload environments, based on the analyses of WP-SGD.

Deduction 1

Given a cost function  such that 
 and 
 are bounded, we average parameters every  iterations for the fastest node in SimuParallel SGD. Then, the bound of the algorithm is (25)
 
 
 
 
 
 

Deduction 2

Given a cost function  such that 
 and 
 are bounded, we average parameters every  iterations for the fastest node in WP-SGD. Then, the bound of the algorithm is (26)
 
 
 
 
 
 
 

For all nodes with the same performance, the more average the operation, the closer the output model parameters will be to the function minimum. In this case, our consideration should be to balance the cost of operation and the gain from the “better” result. As is well known, not all training datasets’ variances are large enough to get the expected effect. On an unbalanced-workload system, we should also guarantee that 
 
 to ensure overall that the training process is valid.

5.2. Combining WP-SGD with delay SGD
Because of the excellent adaptability on different kinds of datasets and the overlapping of the cost of communication and computing, delay SGD is widely used in machine learning frameworks such as MXNet [5], TensorFlow [1], and Petuum [24]. However, all of these algorithms are designed for a balanced-workload environment. In this section, we offer Algorithm 3, which combines WP-SGD and one kind of delay SGD to make delay SGD algorithms work efficiently in heterogeneous computing environments. Some intermediate variables are defined in the algorithm description. The working pattern of Algorithm 3 is illustrated in Fig. 4.


Download : Download high-res image (241KB)
Download : Download full-size image

Download : Download high-res image (171KB)
Download : Download full-size image
The proof of Algorithm 3 focuses on two main key points: (1) to guarantee that all of 
 to 
 is on one side of the fixed point in the direction of the sample, and (2) to determine the value of the maximum contraction map rate when using this kind of delay SGD. Both of above 2 key points are described in the proof of Lemma 4.

For the first point, when running the ()th update step, we also need to ensure that the first  update steps satisfy the algorithm. The above requirement means that we should be able to find a range in which the projection of the unique fixed point in the current sample direction addressed. With the processing, the range should shrink. We calculate the range of the fixed point based on the latest iteration information at the beginning of each update step, like Fig. 5. We only accept the new model parameters that are on the same side of this range as the older model parameters; otherwise, we abandon these new model parameters and use another sample to recalculate new model parameters. The above operation is determined by the point of this range closest to the old model parameters (in Algorithm 3, this point is denoted 
). These processes are described in  function in Algorithm ??.


Download : Download high-res image (311KB)
Download : Download full-size image
Fig. 4. Working pattern of Algorithm 3 when the quantities of data differ.

For the second point, WP-SGD and Simul Parallel SGD share the same proof frame. In the proof of Simul Parallel SGD, the Lemma 3 in Zinkevich et al.’s work [29] decides the contracting map rate of Simul Parallel SGD. Here, we offer Lemma 4 for Algorithm 3. Using the proof frame of Simul Parallel SGD with following Lemma 4 instead of Lemma 3 in Zinkevich et al.’s work [29], we can find the contracting map rate of Algorithm 3 and finish the whole proof.

The details of Lemma 4’s proof are offered in the Appendix.

Lemma 4

Let 
 
 be a Lipschitz bound on the loss gradient. Then if 
, the Algorithm 3 is a convergence to the fixed point in 
 with Lipschitz constant . 
 is defined as following formula: 
 
. 
 is the maximum delay.

As we discussed in Section 2, the maximum lag the system can tolerate is the maximum number of workers the system can have. When all workers have the same performance, the system will achieve the most efficient working state. In practice, it is very hard to let all nodes in an unbalanced-workload system have the same performance, especially when the clusters consist of different kinds of computing devices. Algorithm 3 is the algorithm designed for this kind of cluster.

5.3. Dalay  model average based WP-SGD
Combining above two algorithms, We propose a mixed parallel SGD algorithm, named as dalay & model average based WP-SGD, which is shown in algorithm 5.


Download : Download high-res image (209KB)
Download : Download full-size image
Based on the analysis of above algorithms, it is easy to gain the conclusion that dalay & model average based WP-SGD can converge into the 
.

6. Numerical experiments
6.1. Platform
We conducted these experiments on a cluster consisting of different types of CPU nodes on Alibaba clouds. The detail information of the server in the cluster is shown in Table 1.


Table 1. The information of nodes in cluster.

# Nodes per
Server	# Server	CPU	Net	Mem
1	1	Intel(R) Xeon(R) CPU
E3-1225 v6 @ 3.30 GHz	1000 Mb/s	31 GB
2	3	Intel(R) Xeon(R) CPU
E5-2640 v2 @ 2.40 GHz	10000 Mb/s	116 GB
1	1	Intel(R) Xeon(R) CPU
E5-2640 v2 @ 2.40 GHz	10000 Mb/s	116 GB
2	4	Intel(R) Xeon(R) CPU
E5-2660 v2 @ 2.20 GHz	10000 Mb/s	113 GB
2	2	Intel(R) Xeon(R) CPU
E5-2680 v2 @ 2.80 GHz	1000 Mb/s	55 GB
6.2. Algorithm and code setting
6.2.1. Delay  model average based WP-SGD
In the code of our experiment, one node uses four threads: three compute&bcast thread and one listening&synchronous thread. We use this setting for Intel(R) Xeon(R) CPU E3-1225 v6 server only have four cores.

In compute&bcast threads, three threads use parameter-server frame asynchronous parallel trains model: computing part consists of one server thread and two worker threads. The work of worker threads is pulling model, computing gradient, and pushing gradient. The work of server threads is receiving gradient, updating model, broadcasting the model.

In listening&synchronous thread, this thread is used to listen to the nodes which finished their workload and give the board cast signal to the server thread.

For we want to make results have good comparability in the different algorithms, we do not use OpenMP to accelerate worker threads. (As we can see, in E5-2640, E5-2660, E5-2680 nodes, we have free cores. If we use full resources, the performance of mini-batch SGD and SimulParallel SGD would be bad, because the performance of E5-2640 is 50% to 70% than other Server.)

In our experiments, we name this algorithm as D&M based WP-SGD.

6.2.2. WP-SGD
In our experiments, each node uses two threads as OpenMP threads to accelerate the process, because in delay&model average based WP-SGD algorithm, each node uses two worker threads to accelerate the computing process. We use this setting because we want all experimental results are comparability.

6.2.3. Simulparallel SGD
In our experiments, each node uses two threads as OpenMP threads to accelerate the process, which is the same as the configuration with WP-SGD.

6.2.4. Mini-batch SGD
In our experiments, each node uses two threads as OpenMP threads to accelerate the process, which is the same as the configuration with WP-SGD.

We use mini-batch SGD as benchmark because mini-batch SGD is the most widely used parallel SGD algorithm in HPC areas to train a machine learning model.

6.2.5. Sequential SGD
In our experiments, we only use one node on Intel(R) Xeon(R) CPU E3-1225 v6 node to measure the performance. We use this algorithm as a benchmark because, as a benchmark, other algorithms show the ability of parallel technologies.

In this experiment, the sequential SGD process uses two threads as OpenMP threads to accelerate the process, which is the same as the configuration with WP-SGD.

6.2.6. Averaging the model parameters directly
As the baseline, we used the nodes’ output from WP-SGD nodes and the outputs created by using the direct averages of the model parameters. We name this algorithm as averaging directly.

We use this algorithm as a benchmark because we want to show that the error method to deal with workload unbalance problems may cause a catastrophe.

In our experiments, each node uses two threads as OpenMP threads to accelerate the process, which is the same as the configuration with WP-SGD.

6.3. Model
We chose hinge loss, which is used to train the support vector machine (SVM) parameters, as our objective function value. Compared with other loss functions, the contraction map rate of hinge loss is much closer to the contraction map rate of the SGD framework, i.e., ().

It is worth noting that our work would be more conspicuous if we use a deep learning model like VGG16 [20] as an experiment benchmark. But, our paper focuses on the correctness and effectiveness of WP-SGD. There are few works on the mathematical properties of deep learning. If we use deep learning model parameters, we are not sure that the reasons for the experiment result are the intricate deep learning network unknown math properties, or the effect of WP-SGD.

6.4. Data
We performed experiments on KDD Cup 2010 (algebra) [26] and real-sim dataset.

In KDD Cup 2010 (algebra) datasets, the dataset’s labels  and the instances in the dataset are binary, sparse features instances. The dataset contains 8,407,752 instances for training and 510,302 instances for testing. Those instances have 20,216,830 dimensions. Most instances have about 20–40 features on average.

In the real-sim dataset, the dataset’s labels  and the instances in the dataset are binary, sparse features instances. The dataset contains 72,309 instances for training, and we randomly selected 1000 samples as test datasets. Those instances have 20,958 dimensions.

6.5. KDD Cup 2010 (algebra) dataset experiments
6.5.1. Configurations:
In all the experiment, we set , . In WP-SGD and D&M based WP-SGD, we use . Because the final output is close to the zero vector, and we wanted to have more iteration steps, the initial values of all model parameters were set to 4. Then, we studied SVM model parameters and calculated the objective function value on the testing data.

In D&M based WP-SGD, different nodes allreduce their model when the fastest node exerts 1000 iterations.

6.5.2. Approach:
For SimulParallel SGD, WP-SGD and D&M based WP-SGD, in order to evaluate the convergence speed and hinge loss of the algorithms on an unbalanced-workload system, we used the following procedure: for the configuration, we trained 20 model parameters, each on an independent random permutation of a part of the whole dataset. During training, the model parameters were stored on disk after  updates of the fastest nodes.

6.5.3. Results:
Fig. 6, Fig. 7 show the objective function value of different algorithms with -axis is the number of iterations and -axis is time.

From the aspect of the epoch, Fig. 7, mini-batch SGD is the fastest algorithm. The output of D&M based WP-SGD is close to mini-batch SGD. SimulParallel SGD, WP-SGD, and sequential SGD share almost the same convergence speed. In detail, in SimulParallel SGD, WP-SGD and sequential SGD, SimulParallel is the fastest algorithm, the output of WP-SGD is close to SimulParallel SGD. As the benchmark, the average model directly algorithm is the worst algorithm.

From the aspect of wall clock time, Fig. 6, D&M based WP-SGD is the best algorithm. Because of the burden of slow nodes, the performances of all synchronous algorithms receive the impacts: the worst algorithm is mini-batch SGD because the cost of communication is expensive. SimulParallel SGD is the worse than WP-SGD.

The above experimental results show that 1. When the system’s workload is balanced, and computing resource is unlimited, the performances of the number of iterations experiment, the synchronous algorithm may be better than the asynchronous algorithm. When the system’s workload is unbalanced, the performances of time-experiment, asynchronous algorithms outperform synchronous algorithms. 2. For inappropriate algorithms or synchronous algorithms on an unbalanced system, parallel technology brings negative effects.

6.6. Real-sim dataset experiments
6.6.1. Configurations:
In the experiment, we set , . Because the final output is close to the zero vector, and we wanted to have more iteration steps, the initial values of all model parameters were set to 100. Other settings are the same as the KDD 2010 experiments.

6.6.2. Approach:
In order to evaluate the convergence speed and hinge loss of the algorithms on an unbalanced-workload system, we used the following procedure: for the configuration, we trained 20 model parameters, each on an independent random permutation of a part of the whole dataset. During training, the model parameters were stored on disk after  updates of the fastest node.

6.6.3. Results:
Fig. 8, Fig. 9 show the objective function value of different algorithms with -axis is the number of iterations and -axis is time.

From the aspect of the epoch, Fig. 9, mini-batch SGD is the fastest algorithm. The output of D&M based WP-SGD is close to mini-batch SGD. SimulParallel SGD, WP-SGD, and sequential SGD share almost the same convergence speed. In detail, in SimulParallel SGD, WP-SGD and sequential SGD, SimulParallel is the fastest algorithm, the output of WP-SGD is close to SimulParallel SGD. As the benchmark, the average model directly is the worst algorithm.

From the aspect of wall clock time, Fig. 8, D&M based WP-SGD is the best algorithm. Because of the burden of slow nodes, the performances of all synchronous algorithms receive the impacts: the worst algorithm is mini-batch SGD because the cost of communication is expensive. SimulParallel SGD is the worse than WP-SGD.

The above experimental results show that 1. When the system’s workload is balanced, and computing resource is unlimited, the performances of the number of iterations experiment, the synchronous algorithm may be better than the asynchronous algorithm. When the system’s workload is unbalanced, the performances of time-experiment, asynchronous algorithms outperform synchronous algorithms. 2. For inappropriate algorithms or synchronous algorithms on an unbalanced system, parallel technology brings negative effects.

6.6.4. Extra experiments
For the detail of Fig. 9 is blurred between sequential SGD, SimuParallel SGD, WP-SGD, and the average model directly algorithm, we also conduct other experiments and zoom in the figure. In this experiment, algorithm settings are the same as real-sim experiments and we use doubled computing resources. We also conduct experiments which only use ten nodes as the benchmark. We think these benchmarks will show more essential characters. Our experimental results are shown in Fig. 10.


Download : Download high-res image (191KB)
Download : Download full-size image
Fig. 10. Using SVM model parameters in different SGD algorithms on a cluster. In this figure, to show results clearly, we only show the key part of whole convergence process.

Fig. 10 shows the objective function value of sequential SGD, SimuParallel SGD, WP-SGD, and averaging the model parameters directly. To make the presentation clear, we only present part of the whole data in Fig. 10. In terms of wall clock time, the model parameters which are obtained from SimuParallel SGD clearly outperformed the ones obtained by other algorithms. The output of WP-SGD was close to the output on a balanced-workload system. Unsurprisingly, averaging the model parameters directly turned out to be the worst algorithm. The above results are consistent with Theorem 4. The convergence speeds of WP-SGD and SimuParallel SGD are the closest. Thus, on an unbalanced-workload system, WP-SGD would obtain a better objective function value in a short time.

7. Conclusion
In this paper, we have proposed WP-SGD, a data-parallel stochastic gradient descent algorithm. WP-SGD inherits the advantages of SimuParallel SGD: little I/O overhead, ideal for MapReduce implementation, superb data locality, and fault tolerance properties. This algorithm also presents strengths in an unbalanced-workload computing environment such as a heterogeneous cluster. We showed in our formula derivation that the upper bound of the objective function value in WP-SGD on an unbalanced-workload system is close to the upper bound of the objective function value in SimuParallel SGD on a balanced-workload system. Our experiments on real-world data showed that the output of WP-SGD was reasonably close to the output on a balanced-workload system.

Appendix.
Lemma 2

Suppose that 
 are independent distributed random variables over 
. Then if 
 and 
, it is the case that 

Proof

By linearity of expectation, if 
 are independent distributed random variables then 
Because and 
it holds following results. (27)
Considering triangle inequality, we have final result. 

Corollary 1

The fastest node consumes  data samples, 
 is the distribution of model parameters updated after 
 iterations in node , and 
 is the distribution of the stochastic gradient descent update in WP-SGD. 
 

Proof

Suppose 
, which is a random variable, is the output of the algorithm, and 
 is the output of each node. Then 
based on Theorem 3, we have following result 
 
Thus, using Lemma 2, 
 
Combining the above with the definition of 
 
we have 
 

Lemma 3

 is the output of node . Then, if 
then the distribution of 
 is 
. It is the case that 
 
 
 

Proof

Combining this with Theorem 3, we obtain 
 
 
 
 
 
 
 
 
 Thus, 
 
 
 

Theorem 4

Given a cost function  such that 
 and 
 are bounded, the bound of WP-SGD is (28)
 
 
 
 
 
 

Proof

Starting from Theorem 1: (29)
 
 
  i.e.  (30)
 
 
 
 
 
 i.e.  (31)
 
 
 

Treat  as the machine learning model gain by WP-SGD, and we can gain (32)
 
 
 

Using Lemma 1, we can gain following formula: (33)
 
 
 

Using Lemma 3 and Corollary 1, to replace the 
, 
 and 
. We can get Theorem 4.  □

Corollary 2

For WP-SGD, when 
 
 
 the upper bound of the objective function value of WP-SGD is closer to the minimum than is the upper bound of the objective function value of sequential SGD on the fastest nodes.

Proof

When we deduce the proof of Simul Parallel SGD (Theorem 11, Fact 28, Lemma 30, Corollary 23, Theorem 69, Theorem 70 in Zinkevich et al.’s work [29] and Lemma 1 in this paper), we would notice that the 
 
 is the upper bound of 
 and 
. In practice, 
 
 does not have practical application value for 
 
 is too large and vague for different datasets and loss function. A more tight upper bound should be described as follows: 
 
 
 
 

Although the distribution of 
, i.e. 
, is unobservable: the algorithm set 
 as a fixed value, the distribution of 
 is determined by the dataset. We can use the distribution of 
 can be calculated by the dataset and loss function. Thus, it is practical valuable for us use the distribution of 
 to rewrite above formula: 
 
 
 
 

When  and 
 we can gain the upper bound of sequential SGD. Thus, we can gain Corollary 2 compared to above upper bounds. We can gain the final conclusion by replacing  with .  □

Corollary 3

For WP-SGD, when 
the upper bound of the objective function value of WP-SGD is closer to the minimum than is the upper bound of the objective function value of sequential SGD on the fastest nodes.

Proof

Notice that 
  decreases as the first part of Theorem 4 decreases. The first part of Theorem 4, i.e. Eq. (34), which also can be written in Eq. (35). (34)
 
 
 
 
 (35)
 
 
 
In addition, the sequential algorithms are a special case in WP-SGD when . Thus, if WP-SGD is better than the sequential algorithm, the first part of Theorem 4 must be less than 
 
 
 
which can be written as 
 
It is apparent that if following inequalities hold, we obtain the result. 
 
and 
 
which means 
We can gain the final conclusion by replacing  with .  □

Deduction 1

Given a cost function  such that 
 and 
 are bounded, we average parameters every  iterations for the fastest node in SimuParallel SGD. Then, the bound of the algorithm is (36)
 
 
 
 
 
 

Proof

Every averaging operation reduces the standard deviation by , and every iteration step reduces the Euclidean distance and part of the standard deviation by . Thus, we obtain Deduction 1.  □

Deduction 2

Given a cost function  such that 
 and 
 are bounded, we average parameters every  iterations for the fastest node in WP-SGD. Then, the bound of the algorithm is (37)
 
 
 
 
 
 
 

Proof

Every averaging operation reduces the variance by 
 
. Every iteration steps reduce the Euclidean distance and part of the variance by . We obtain the final deduction.  □

Lemma 4

Let 
 
 be a Lipschitz bound on the loss gradient. Then if 
 and there exist samples to make the algorithm continue at each iteration, the Algorithm 3 is a convergence to the fixed point in 
 with Lipschitz constant . 
 is defined as following formula: 
 
.

Proof

This proof contains two parts: 1. this iterative procedure has fixed point. 2. Using any initial point, the Algorithm 3 is a convergence to the fixed point in 
 with Lipschitz constant .

For the first part:

When , Algorithm 3 must have a fixed point for Algorithm 3 is degenerated into algorithm traditional sequence SGD algorithm. Traditional sequential SGD must have a fixed point (Lemma 3 in [29]). And we name the fixed point of sequential SGD as . For the sequence of fixed point, we have 
 and following formula is still true. (38)
 
 
 
 Thus, sequential SGD’s fixed point is the Algorithm 3’s fixed point.

For the second part:

Firstly, by gathering terms, we obtain 
 
Define  to be equal to 
 
. Because 
 is convex in 
,  is increasing, and  is Lipschitz continuous with constant 
. 
We break down  into 
 and 
, and 
 is parallel with simple 
, where 
. Thus, 
 
Finally, note that 
 For the vertical dimension, because  function guarantees that 
 are to the same direction, we can gain following formula: 

Thus, 
 
 
 
 
 
 Above requirement is guaranteed by  function in Algorithm 4.

Now, we focus on the dimension parallel to 
. We define 
 (in Algorithm 3, it is 
, and it is the projection of 
 on 
), so we can know that 
This kind of delay SGD must have a fixed point (first part of this proof), and we denote this fixed point by : 
 
 
 
Without loss of generality, assume that 
 for all  is true. Since 
, 
. By Lipschitz continuity, 
Here, we define 
Because of the assumption, we know that 
, and at the beginning, 
, which means that 
. The following operation is to provide a rough idea of the range of . What we care about is the range of  closest to 
, which we denote by 
. 
 is the maximum delay.

A rearranging of the terms yields 
To be able to eliminate the absolute value brackets, we need the terms in the absolute value brackets to be positive. Because 
 if 
it follows that 
 
To satisfy the above terms, we require that 
 
Above requirement is guaranteed by  function in Algorithm 4.

Above requirement can be rewritten as 
 

Note that on the assumption, 
, and so 
It is apparent that 
 is a non-increasing series, which means that 
 
It is apparent that  should satisfy 
 
and from the whole dataset aspect, and  reach the maximum,  should satisfy 
 
and this then implies that 
 
 
