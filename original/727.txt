Abstract
The increase in the number of cores in a single chip brings better capabilities to exploit thread-level parallelism (TLP). However, since power dissipated per area rises at each new node generation, higher temperatures are achieved, speeding up the aging of hardware components, which may provoke undesired system behavior. Considering that many applications do not scale with the number of cores, their execution with the maximum TLP available will not only degrade performance, but also unnecessarily increase temperature, further accelerating aging. Given that, we propose Hebe, a dynamic concurrency throttling approach that learns at run-time the degree of TLP that reduces the aging for OpenMP applications. Hebe is totally transparent, needing no modifications in the original binary code. With a set of extensive experiments (fifteen benchmarks and four multicore platforms), we show that Hebe outperforms state-of-the-art approaches with very close results from the best possible solution given by an exhaustive search.

Previous
Next 
Keywords
Thread-level parallelism

Aging

Optimization

Runtime system

1. Introduction
To satisfy the demand for performance of applications from many domains in big data centers and cloud-based systems, the number of cores in a single chip package has been growing at the same pace as the increasing transistor density. However, considering that power dissipated per area rises at each new node generation (i.e., the well-known end of Dennard Scaling), heat dissipation has become a significant issue when exploiting thread-level parallelism (TLP). Besides the common problems, like cooling, the increase of heat dissipation raises the operating temperature, which influences some of the main causes of aging of hardware components (e.g., negative bias temperature instability - NBTI), shortening their lifetime.

NBTI consists of a vital reliability problem in metal-oxide-silicon (MOS) devices. It refers to the generation of positive oxide charge and interface traps in MOS structures under a combination of elevated temperatures and negative gate voltages [62], [11]. This, in turn, increases the threshold voltage (), which will have adverse effects on current and propagation delay, degrading the device performance [57]. The impact of NBTI on the processor aging has become more significant in modern devices due to the aggressive down-scaling of device geometry and compact device integration. Both strongly affect the operating temperature, intensifying the processor aging [25]. In the end, this increase in the threshold voltage may provoke undesired system behavior (e.g., electromigration, dielectric breakdown, and stress migration [19]) for many critical applications, further increasing the operating expenses. Therefore, controlling the operating temperature is essential to avoid shortening the hardware lifespan.

Given that, when running a parallel application, the processor temperature rises as the number of threads increases, mainly as a result of the increase in power dissipation due to the switching activity in the hardware components (cores and caches). This behavior can be observed in Fig. 1a for the execution of BT kernel from the NAS Parallel Benchmark [5] on an Intel Xeon 32-core machine (retired from our experiments, discussed in Section 3). It shows the average CPU power and temperature for the application execution with a different number of threads (from 2 to 32). As one can observe by comparing Fig. 1a and Fig. 1b, the average NBTI per second of execution (raw numbers got from Eq. (1)) proportionally grows with the temperature rise. Therefore, there is a trade-off between temperature rise, the benefits it brings to lower the total execution time, and the impact they have on aging due to the NBTI [43], [44]. In the end, they are all directly related to how many threads are distributed across the cores in a parallel application.

Fig. 1
Download : Download high-res image (210KB)
Download : Download full-size image
Fig. 1. Behavior of BT kernel from the NAS Parallel Benchmark execution on an Intel Xeon 32-core.

Nevertheless, many applications do not scale as the number of threads increases due to several hardware related reasons: Instruction issue width saturation, off-chip bus saturation, data synchronization, and concurrent shared memory accesses [65], [64], [52], [39]. It means that in many cases, executing a given application in the regular way (i.e., splitting the application into as many threads as possible to use all the available cores) will result in non-optimal use of the available resources, not delivering the best trade-off between performance and temperature and accelerating the impact of NBTI on the aging process. Therefore, by artificially decreasing the number of threads (i.e., thread throttling) for some parallel regions, one can rightly tune the parameters mentioned above to achieve the best performance/temperature ratio and reduce the impact of NBTI on the processor aging. We show that it is possible in Fig. 2 for the execution of Streamcluster application from the Rodinia benchmark suite [17]: the lowest value of NBTI (Fig. 2b) is achieved when running the application with eight threads, which has the best trade-off between performance and temperature (Fig. 2a). Hence, by executing this application with the ideal degree of TLP (e.g., eight threads) instead of with the maximum number of threads (which would be the default behavior), the impact of NBTI on the processor aging is 24% lower.

Fig. 2
Download : Download high-res image (158KB)
Download : Download full-size image
Fig. 2. Behavior of Streamcluster application from the Rodinia Benchmark Suite execution on an Intel Xeon 32-core.

Given the scenario above, we propose Hebe, a transparent aging-aware thread throttling approach for OpenMP Applications. By using an efficient search algorithm, it automatically learns, at run-time, the ideal number of threads for each parallel region aiming to reduce the impact of NBTI on the processor aging. This dynamic capability to adapt is key, since thread balancing will vary depending on intrinsic characteristics of the parallel application at hand (e.g., input set and number of parallel regions), as well as the microarchitecture on which it will execute (e.g., number of cores and instruction-set architecture). Hebe improves previous work [43] by considering the optimization of a more realistic phenomena on the processor aging (i.e. it uses NBTI as the main metric).

We validate Hebe through the execution of fifteen well-known benchmarks on four distinct multicore platforms (AMD and Intel) We compare Hebe to different scenarios: i) the standard way that parallel applications are executed (STD), that is, with the maximum possible number of threads available; ii) a built in feature of OpenMP that dynamically changes the number of threads (OMP_Dynamic); iii) a thermal-aware adaptive energy minimization approach for OpenMP applications proposed by Shafik et al. [60] (TA-OMP). We show that, by using Hebe, the impact of NBTI on the processor aging is up to 80% lower than the one presented by the STD configuration; 87% lower than the OMP_Dynamic; and 91% lower than the TA-OMP approach.

In order to reinforce the need for a tool that specifically optimizes the impact of NBTI on the processor aging, we also compare the results of Hebe when the objective function is changed to optimize performance, energy, or EDP instead of aging. Fig. 3 presents the NBTI of each configuration normalized to Hebe w.r.t. the geometric mean of the entire benchmark set on each multicore processor. With that, we show that the performance, energy, and EDP oriented setup present an NBTI 12%, 12%, and 10% higher than the original objective function, respectively. We also compare Hebe to two well-known approaches that target performance and energy rather than aging: the FDT [65] and the Varuna programming model [61]. Finally, to measure the cost of the learning curve for converging to the ideal number of threads brought by its dynamic adaptation, we also compare Hebe to the solution provided by an exhaustive search. It executes each parallel region with its predefined ideal number of threads, without the learning overheads. From this, we found that the average cost of Hebe to reduce the impact of NBTI on the processor aging is less than 1.0% for most of the benchmarks.

Fig. 3
Download : Download high-res image (102KB)
Download : Download full-size image
Fig. 3. Hebe optimized for NBTI (baseline - black line) versus Hebe set to optimize performance, energy, and EDP considering the geometric mean of the entire benchmark set - values higher than 1.0 mean that Hebe is better. (For interpretation of the colors in the figures, the reader is referred to the web version of this article.)

The remainder of the manuscript is organized as follows. Hebe is described in Section 2. The benchmarks and execution environment used to validate Hebe are described in Section 3. We discuss the results in Section 4. We describe the related work and compare it to Hebe in Section 5, while the final considerations are drawn in Section 6.

2. Hebe: applying dynamic concurrency throttling to mitigate the processor aging
Hebe aims to reduce the impact of NBTI on the processor aging by tuning the degree of TLP of each parallel region of an OpenMP application. The general workflow of Hebe can be observed in Fig. 4: Given an OpenMP application binary (in this example, it has three parallel regions), Hebe first applies a search algorithm over each parallel region during the learning phase (described in Section 2.2) to find the number of threads that delivers the lowest impact of NBTI on the processor aging. Once the search converges to an ideal number of threads for each region (end of search), the stable phase starts. In this phase, each parallel region executes with the number of threads found during the learning phase. In the example of Fig. 4, the search algorithm found that eight is the ideal number of threads to execute the first parallel region. In contrast, 10 and 16 are the best number for the second and third regions, respectively.

Fig. 4
Download : Download high-res image (153KB)
Download : Download full-size image
Fig. 4. Illustration of the learning and stable phase implemented by Hebe.

Considering the structure and organization of Hebe, we start by describing how the NBTI is modeled and how Hebe uses it to guide the search during the learning phase. Then, we describe the search algorithm used during the learning phase. Finally, we discuss how Hebe was implemented into the OpenMP library in order to provide transparency to the end-user.

2.1. Modeling NBTI
NBTI is an aging phenomenon that degrades the electrical characteristics of pMOS and nMOS transistors. It increases the threshold voltage () over the processor's lifetime, which often makes the transistor switches slower [19]. This will affect the delay in the critical path of the processor, incurring timing violations, and errors. The increase in the delay due to the long-term aging (i.e., the increase in the processor's delay's critical path) can be modeled as defined in Equation (1) [35], where  is the supply voltage; d is the duty cycle (which is equivalent to the utilization rate of each core of a processor); T is the processor temperature in K; t is the time; and A and C are constants, 0.856 and 0.0163, respectively [28].(1)

All the input data for Equation (1) can be obtained at runtime directly from the hardware counters: the processor temperature (T) and supply voltage () with the lm-sensors tool, which is available in any version of Linux Operating System; and the duty cycle d of each core can be calculated by considering the ratio of time that each core is under stress, as defined in Equation (2).1(2)

However, the challenge for an automatic and online system is to read the right processor temperature value during its training phase. To better understand it, let us consider the CPU temperature behavior for the BT-NAS benchmark execution with a different number of threads on two Intel Xeon processors, shown in Fig. 5. At the beginning of the execution, which usually corresponds to the learning phase (as shown in Fig. 4), the CPU temperature is the same regardless of the number of threads - since temperature variations are not instantaneous. However, after some time, as the application executes, the CPU temperature changes according to the number of threads that was defined earlier. Therefore, decisions taken during the learning phase based on the current CPU temperature will very likely lead to a non-ideal solution, since there is a significant period of time for the temperature to correspond to the current number of active threads.

Fig. 5
Download : Download high-res image (268KB)
Download : Download full-size image
Fig. 5. Behavior of CPU temperature when running the BT-NAS benchmark on two Intel processors with a different number of threads. X-axis has been cut to provide a better data visualization.

Hence, to ensure that the search algorithm (discussed in Section 2.2) can converge to an ideal solution, it is necessary to use data that reflect the CPU temperature behavior and, consequently, the value of . Given that, many authors have already shown that the CPU temperature is mainly affected by the power dissipation [45] - which is instantaneous and will proportionally correspond to future temperature changes. Therefore, considering that power consumption (i) is instantaneous; (ii) can be easily read through hardware counters in off-the-shelf processors; and (iii) reflects the CPU temperature; any optimization in the power consumption will result in a significant impact on the . Because of that, we use this information during the learning phase (as will be discussed in Section 2.2).

2.2. Learning algorithm
In this section, we discuss how Hebe converges to a number of threads that deliver the best tradeoff between temperature and execution time for a parallel region, aiming to reduce the effects of NBTI on the processor aging.

Hebe is given an application A with m independent parallel regions . Each parallel region in R can be executed by at most n distinct cores . We denote by  the set of parallel regions in an application A. The optimization problem we are interested in seeks an assignment of parallel regions R (of a given application A) to a subset of threads/cores in C. We denote by 
⁎
 the set of all subsets of threads/cores in C, that is, 
⁎
. A feasible assignment can be defined as a function 
⁎
 that assigns a subset of threads/cores to parallel regions. For the best of our knowledge, there is no runtime API capable of providing the power dissipation separated per-core without incurring in greater overhead to the learning algorithm. Based on that, we assume that all subsets in 
⁎
 of n distinct cores perform similarly concerning the execution time and power dissipation. The power dissipation is obtained directly from the hardware counters present in modern processors. In the case of Intel processors, we use the Running Average Power Limit (RAPL) library, while the Application Power Management library is used for AMD processors [26]. Furthermore, it is possible to create and use a per-core power model using per-core performance counters at a negligible overhead [23].

Therefore, we denote by  the execution time of executing parallel regions on α distinct cores (threads). Likewise, we denote by the power dissipation of running the parallel region R on α cores (threads). The trade-off between execution time and power dissipation is defined as a score function over a valid assignment ϕ. Function is defined as the distance between and , where α represents the size of any valid subset in . For the ease of presentation, we define such trade-off distance of assignment ϕ as = . An optimized assignment ϕ consists of finding a valid assignment ϕ that leads to minimum value to trade-off function . In other words, it strives to find a balance between performance (i.e., running time of a given ) and power dissipation of α chosen cores in C. We say an assignment ϕ is optimized if it is minimum with respect to . Formally,

Algorithm 1 receives as an input a set of cores C, a set of parallel regions R and three parameters: (i) α describing the initial number of threads given to a region , (ii) β representing the increasing factor of number of threads given to in an iteration, and (iii) the maximum number of available cores n. Each parallel region keeps track of its current state in the algorithm's execution. The procedure starts increasing exponentially (with respect to β) the number of threads α given to (lines 4-7), while minimizing score function = . Upon reaching a local maximum with respect to function , the algorithm performs a modified hill-climbing search (lines 8-23) in the interval where the algorithm has found a minimum value for .

Algorithm 1
Download : Download high-res image (133KB)
Download : Download full-size image
Algorithm 1. Hebe: Search Algorithm.

In order to avoid minimum locals and plateaus during the process, and so wrongly converging to an incorrect number of threads, the search algorithm uses lateral movement. This movement is performed before selecting the best number of threads by testing a neighboring configuration at another point in the search space that has not yet been tested. When Hebe converges to the best number of threads for a particular parallel region, it begins to monitor the behavior of such region. If there is any change in the workload, which in this work we consider a variation of 20%, the search algorithm starts its execution again. The algorithm converges to a minimum value in at most iterations.

Because finding the number of threads that delivers the best outcome in any metric to execute a parallel region is a convex optimization problem, it means that there will be one specific number of threads that is ideal for a given metric and parallel region. Hill Climbing algorithms are very suitable for this kind of problems and are also known for having low complexity and being fast, which is essential to reduce the technique overhead (since it is executed at run-time). Moreover, other authors have already shown that when hill-climbing is used along with another approach to guide the search, in most cases such algorithms will reach a near-ideal solution, escaping from the local minima and plateaus [66]. On top of that, as the search algorithm implemented by Hebe learns towards the best number of threads during application execution, all the computation done in the search phase is not wasted (i.e., it is used by the application), reducing the overhead of Hebe.

2.3. Implementation and transparency
Hebe works with any OpenMP application that exploits parallelism through parallel loops but does not influence in the execution of OpenMP applications implemented with other directives (e.g., sections or tasks). OpenMP is a parallel programming interface for shared memory in C/C++ and FORTRAN. It consists of a set of compiler directives, library functions, and environment variables that eases the developer burden of creating and managing threads in the code. Therefore, extracting parallelism using OpenMP usually requires less effort when compared to other APIs (e.g., PThreads and MPI), making it more appealing to software developers. That is the reason why it is widely used and there are many benchmarks and applications implemented with OpenMP: NAS Parallel Benchmark [59], SPEComp [4], Parboil [63], Rodinia [17], PARSEC [9], hydrobench [67], OpenLB [29], GROMACS [1], LULESH [32], etc.

We implemented Hebe inside the GNU OpenMP library (

Image 1
), an Offloading and Multi-Processing Run-time Library, which provides all the functionalities of OpenMP.
Image 1
is dynamically linked to OpenMP applications, so any modifications in its code are entirely transparent to user applications. Hence, to use Hebe, the user simply has to replace the original OpenMP
Image 1
with Hebe's
Image 1
. This new library includes all original OpenMP functionalities plus the new functions of Hebe. When the environment variable
Image 2
is set to
Image 3
in the Linux Operating System, the thread management system of Hebe is used instead of the OpenMP original functions. If it is not set, Hebe would not influence the execution of that OpenMP application (i.e. the application executes with the original OpenMP functions). Hence, there is no need to make any modifications in the OS (e.g., package installation, kernel recompilation, or superuser permission). In this way, any existing binary code can benefit from Hebe without any modifications to the need for recompilation.
In order to better understand how Hebe works, let us consider the regular way for parallelizing an iterative application with OpenMP and the respective main functions implemented by

Image 1
, shown in Fig. 6. When the program starts executing, the
Image 4
function is called, which initializes all the environment variables used by OpenMP during the application execution. Then, when the application reaches the OpenMP directive
Image 5
(used to indicate a parallel region), functions to create and define the number of threads are called (e.g.,
Image 6
). So, whenever the application executes this directive, the number of threads to run the parallel region is set. At the end of the parallel region, the
Image 7
function is responsible for joining and finalizing the parallel region environment. Finally, when the application ends, the
Image 8
function is called to finalize the OpenMP environment.
Fig. 6
Download : Download high-res image (85KB)
Download : Download full-size image
Fig. 6. OpenMP code example and the respective libgomp functions.

Given the organization of the OpenMP library, the functionalities of Hebe were split into four functions:

•
Image 9
is responsible for recognizing if Hebe is active. If so, it initializes the data structures, libraries and variables used by Hebe. This function was implemented inside the original
Image 4
function.
•
Image 10
sets the number of threads that execute each parallel region based on the current state of the search algorithm (described in Section 2.2). It also initializes the hardware counters for collecting data from the execution environment of the current parallel region. We have implemented this function inside the
Image 11
, and when Hebe is active, it replaces the original
Image 6
function.
•
Image 12
is executed after the parallel region to get its behavior regarding the optimization metric (as defined in Section 2.2). It performs one step of the search algorithm and, according to this algorithm, defines the number of threads that will be used for the execution of the respective parallel region in the next iteration. This function was inserted into the
Image 7
function, and when Hebe is active it replaces the original OpenMP functionality.
•
Image 13
concludes and destroys Hebe environment at the end of application execution. We have implemented it inside the original
Image 8
function.
3. Methodology
3.1. Benchmarks
Fifteen well-known applications written in C/C++ and already parallelized with OpenMP from assorted suites were chosen:

•
Six kernels from the NAS Parallel Benchmark [5]: Block Tri-diagonal solver (BT-NAS), Conjugate Gradient (CG-NAS), discrete 3D fast Fourier Transform (FT-NAS), Lower-Upper Gauss-Seidel solver (LU-NAS), Scalar Penta-diagonal solver (SP-NAS), and Unstructured Adaptive mesh (UA-NAS). As the original version of NAS is written in FORTRAN, we use its OpenMP-C version [59].

•
Three applications from the Rodinia benchmark suite [17]: LU Decomposition (LUD), Speckle Reducing Anisotropic Diffusion (SRAD), and Streamcluster (SC).

•
Five applications from different domains: fast Fourier transform (FFT) – calculates the discrete Fourier transform of a given sequence [49]; HPCG – measures the performance of basic operations; Jacobi (JA) method iteration – computes the solutions of a diagonally dominant system of linear equations [51]; n-body (NB) - computes a simulation of a dynamical system of particles [8]; and Poisson (PO) - computes an approximate solution to the Poisson equation in a rectangular region [51].

•
One real application: Livermore Unstructured Lagrangian Explicit Shock Hydrodynamics (LULESH2.0 - LL), a real application widely used by a variety of computer simulations of science and engineering problems that require hydrodynamic modeling [31]. It is one of the five challenge problems implemented by the Lawrence Livermore National Laboratory (LLNL) in the DARPA Ubiquitous High-Performance Computing program and has been studied as a proxy application in Department of Energy's co-design efforts to reach the exascale era.

We have chosen these benchmarks because they have the scalability limited by different issues (i.e., main bottlenecks that prevent significant speedups as the number of threads increase), as discussed in [65], [64], [52]: Data-synchronization: the number of synchronization operations performed to ensure data integrity during the execution of a parallel region [65]. The following applications fit in this group: NB and SP-NAS. Shared memory accesses: the number of accesses to memory regions that are more distant from the processor so threads can communicate [64]. The applications in this class are BT-NAS, LU-NAS, LUD, PO, and SC. Off-chip bus saturation: applications that operate on huge amounts of data that are private to each thread and have to be continuously fetched from the main memory [65]. The following applications are classified into this class: FFT, HPCG, JA, and LL. Issue-width saturation: when two or more threads are mapped to the same core exploiting SMT, they may compete for the same resources, leading to the non-optimal use of the core. This may delay the execution of a parallel region [52] by increase the unbalancing between threads. This class comprises the following applications: CG-NAS, FT-NAS, SRAD, and UA-NAS.

Because the chosen benchmarks have their scalability limited by distinct issues, they also cover a wide range of TLP behaviors, as shown in Fig. 7. We measured TLP as defined by the authors in [10]: the average amount of concurrency exhibited by the program during its execution when at least one core is active and expressed in Equation (3). is the fraction of time that i cores are concurrently running different threads, n is the number of cores, and is the non-idle time fraction. The closer this value is to 1.0 (normalized to the total number of cores available), the more TLP is available: NB has the lowest while the FT-NAS benchmark presents the highest TLP.(3)

Fig. 7
Download : Download high-res image (147KB)
Download : Download full-size image
Fig. 7. TLP available for each benchmark - normalized w.r.t. the maximum number of threads in each processor.

3.2. Execution environment
The experiments were performed on four multicore platforms (Table 1). All the architectures used Linux Kernel v.4.15. We compiled the applications with gcc/g++ 9.2, using the optimization flag -O3, and OpenMP 5.0. In order to prevent the processor frequency from influencing the results of the different execution scenarios, we have set the DVFS governor to performance, in which the frequency is set to the maximum allowed. Furthermore, all executions consider the environment variable GOMP_CPU_AFFINITY set to bind each thread to a specific CPU. The results presented next are the average of ten executions for each configuration with (standard deviation lower than 0.5%).


Table 1. Main characteristics of each processor.



We compare Hebe with the following scenarios:

•
Regular execution of OpenMP Applications (OMP REGULAR): the application executes with the maximum number of threads available in the system, with no adaptation at runtime. This is the standard behavior of any OpenMP application without user intervention;

•
OpenMP Dynamic (OMP_Dynamic): a built-in feature of OpenMP that dynamically adjusts the number of threads of each parallel region, aiming to make the best use of system resources, such as memory and processor;

•
TA-OMP: a thermal-aware adaptive energy minimization approach for OpenMP applications, proposed by Shafik et al. [60]. In it, the user annotates the code with a power budget for each parallel region. Then, the runtime system (implemented within the OpenMP library) uses this power budget to control the degree of TLP, the thread affinity, and DVFS as the application executes. We have faithfully implemented this approach inside the OpenMP library, as defined in [60]. For the experiments, we have considered four different values for power budget concerning the thermal design power (TDP) of each processor: 25%, 50%, 75%, and 100% (e.g., in the AMD 7 2700 processor, we have executed each application with the following power budgets: 26.2 W, 52.5 W, 78.7 W, and 105 W);

•
Exhaustive Search: It is the execution of each parallel region with the number of threads that delivers the lowest NBTI value (Equation (1)), without the influence of the search algorithm. The ideal number of threads was obtained through an exhaustive execution of each parallel region of each application with 1 to n threads, where n is the maximum number supported by hardware. Then, we pre-configured the application to use the right amount of threads during execution;

•
Two popular approaches for thread throttling: Varuna-PM, by faithfully implementing its programming model (as defined in [61]) and applying it to our benchmarks; and FDT, where the number of threads is defined based on the contention for locks and memory bandwidth. We have implemented the FDT mechanism in C language and inserted their functions into the OpenMP codes, as defined in [65]. Due to the lack of some hardware counters in the AMD Ryzen, we have only evaluated FDT in the Intel Machine. Although Varuna-PM and FDT do not aim to improve the processor lifetime, this comparison will show how they behave with respect to this specific metric. We also changed the original Hebe search algorithm to improve (i) performance, (ii) energy consumption, and (iii) energy-delay product (EDP) rather than aging to analyze whether it is necessary or not to have a specific tool that optimizes aging.

4. Results
In this section, we present the results achieved by Hebe and compare them to the approaches defined in the previous section. Hence, we start by discussing the convergence of the search algorithm implemented by Hebe in Section 4.1. Then, we discuss in Section 4.2 what is the outcome of converging to an ideal or near-ideal number of threads for each parallel region by comparing the results of Hebe to the OpenMP regular execution, the OpenMP dynamic feature, and the thermal-aware approach proposed by Shafik et al. [60]. The overhead of the learning algorithm implemented by Hebe is discussed in Section 4.3. Finally, in Section 4.4, we show the need to develop a thread-throttling tool to mitigate the impact of NBTI on the processor aging.

4.1. Convergence of Hebe
Thanks to its run-time analysis, Hebe can detect either the best degree of TLP exploitation to reduce the impact of NBTI on the processor aging or get close to the ideal value in most times.

As a first example, let us consider the applications in which the limiting factor for the lack of scalability is data-synchronization (NB and SP-NAS). For this type of application, the higher the number of threads, the prolonged the synchronization time, which eventually may worsen performance and rise temperature as more threads execute concurrently, accelerating the impact of NBTI on the processor aging. We depict this behavior in Fig. 8 for the NB execution on the AMD 16-core system. It shows, for each number of threads, the execution time (primary y-axis) split into two parts: the time to execute the parallel region and to synchronize; and the trade-off between temperature and performance used by Hebe during the learning process (secondary y-axis). Lower values are better (zero in this metric would represent 0 seconds to execute at 0 degrees). As one can see, the execution time reduces when the number of threads increases from 1 to 4. However, from this point on, the synchronization takes more time than the code execution of the parallel region. Hence, the trade-off between temperature and performance worsens, which means that temperature grows at a higher rate than performance, increasing the NBTI. By selecting the right number of threads before the curve changes, Hebe achieved the best performance/temperature results, and therefore, the lower NBTI value, as shown in Figs. 8a and 8b.

Fig. 8
Download : Download high-res image (216KB)
Download : Download full-size image
Fig. 8. Scalability behavior of the NB benchmark on the AMD 16-core system.

For applications that operate on huge amounts of private data and have to be continuously fetched from the main memory (e.g., FFT, HPCG, JA, and LL), there is also a sweet spot point in terms of saturation [40]. Let us consider the execution of the third parallel region of the FFT benchmark on the Intel 32-core system. Fig. 9a shows that when it runs with more than 14 threads, the off-chip bus saturates (100% of utilization - y-axis, left side, solid line).2 From this point on, there are no further improvements in the trade-off between performance and temperature (represented by a dashed line in the graph, y-axis, right side), which once more results in an increase in the NBTI (Fig. 9b). By being capable of converging to this number of threads, Hebe delivered the lowest value of NBTI.

Fig. 9
Download : Download high-res image (235KB)
Download : Download full-size image
Fig. 9. Scalability behavior of the third parallel region of FFT benchmark on the Intel 32-core system.

In applications with high communication demands among the threads (BT-NAS, LU-NAS, LUD, PO, and SC), there is an optimal point where the shared memory accesses do not overcome the reductions in the NBTI achieved by the parallelism exploitation. We show this behavior for the execution of the second parallel region of the LUD benchmark (which has two regions) on the Intel 32-core system in Fig. 10. Fig. 10a shows that when the last-level cache (LLC) miss rate from all threads starts to increase significantly (after sixteen threads - primary y-axis), no further improvements in the trade-off between the execution time and processor temperature are achieved (secondary y-axis). Therefore, by converging to the ideal number of threads, Hebe reached the lowest value of NBTI, as shown in Fig. 10b.

Fig. 10
Download : Download high-res image (246KB)
Download : Download full-size image
Fig. 10. Scalability behavior of the second parallel region of LUD benchmark on the Intel 32-core system.

Hebe also converges to either the ideal or near-ideal number of threads for applications that are negatively influenced by the issue-width saturation [58]. Examples of benchmarks in this class are CG-NAS, FT-NAS, SRAD, and UA-NAS. To understand how Hebe behaves in such applications, let us discuss the behavior of the second parallel region of the SRAD benchmark on the Intel 32-core system (Fig. 11). In this case, the ideal number of threads is 16. As can be noted in Fig. 11a, when the number of threads increases from 16 to 17, the number of cycles that the threads spend without issuing any instruction increases abruptly. Therefore, the trade-off between performance and temperature increases. Once more, by avoiding the excessive increment in the number of threads and finding the ideal one (Fig. 11b), Hebe reduced the NBTI value in 17% compared to the OMP regular execution (32 threads).

Fig. 11
Download : Download high-res image (237KB)
Download : Download full-size image
Fig. 11. Scalability behavior of the second parallel region of SRAD benchmark on the Intel 32-core system.

4.2. NBTI evaluation
Fig. 12, Fig. 13 present the results for the entire benchmark set along with their geometric mean (Gmean) considering the four multicore systems. Fig. 12a compares Hebe to the regular execution of OpenMP applications (OMP Regular, represented by the black line), while Fig. 12b compares Hebe to the OMP_Dynamic (also represented by the black line). Moreover, Fig. 13 depicts the comparison between Hebe and TA-OMP when considering different power-budgets w.r.t. the TDP of each processor (e.g., 25%, 50%, 75%, and 100%). Results are normalized according to the setup to be compared (OMP Regular, OMP_Dynamic, or the power budget set when running the TA-OMP approach [60]), so values below 1 mean that Hebe is better.

Fig. 12
Download : Download high-res image (370KB)
Download : Download full-size image
Fig. 12. Hebe versus OMP Regular and OMP_Dynamic: lower than 1.0 means that Hebe is better.

Fig. 13
Download : Download high-res image (648KB)
Download : Download full-size image
Fig. 13. Hebe versus TA-OMP with different power budgets on each multicore processor: lower than 1.0 means that Hebe is better.

Hebe versus OMP Regular: as observed in Fig. 12a, Hebe is capable of reducing the impact of NBTI on the processor aging in most cases due to the reasons discussed in Section 4.1. In the best case, by converging to an ideal degree of TLP to execute the NB benchmark, Hebe reduces the NBTI value (Equation (1)) by up to 80% in the Intel 32-core system. In very specific scenarios where the design space exploration is limited (e.g., the best result is achieved with a number of threads close to the number of cores, and therefore, there is a significant overhead due to the learning phase), it presents similar or even worse results as the baseline. As an example, Hebe is 44% worse for the FT-NAS application on the Intel 32-core system. However, when considering the geometric mean of the entire benchmark set and all processors, Hebe provided a reduction of 21% in the NBTI value.

Hebe versus OMP_Dynamic: Hebe outperforms the OpenMP feature in most cases, as shown in Fig. 12b. Instead of using a search algorithm to converge for a near-optimal number of threads for each parallel region in particular, in this specific implementation, the OMP_Dynamic considers the workload of the last 15 minutes of execution to define the number of threads [16]. Because of that, it was not capable of getting near to the optimal number of threads, being worse than the OMP Regular execution in many cases. Considering the best case of Hebe, the NBTI value is reduced by 87% for the PO benchmark on the AMD 16-core system. In the overall (geometric mean of the entire benchmark set and processors), Hebe achieved an NBTI value 42% lower than the OMP_Dynamic feature.

Hebe versus TA-OMP [60]: Differently from the search employed by Hebe, which exponentially increases the number of threads in the first phase of the learning, TA-OMP increases the number of threads by one at each iteration, spending more time for converging to a solution. This strategy strongly affects the overhead of the learning phase on processors with a high number of cores because more iterations will be executed with a non-ideal number of threads. Furthermore, applications with high degree of TLP will also be affected by the strategy employed during the learning phase. However, even though TA-OMP is capable of converging to an ideal configuration of threads and provide better results than Hebe in particular cases (e.g., where the design space exploration is limited – AMD 16-core system, and when the application presents low degree of TLP, i.e., JA and FFT), it requires the user knowledge of the power budget to minimize the processor temperature with minimal impact on the performance. As one can observe in Fig. 13, by just varying the power budget, the NBTI result is different. Therefore, because of the intrinsic characteristics of TA-OMP, Hebe delivered lower NBTI values in most cases, regardless of the power budget set (Fig. 13). Considering the best case for Hebe, the NBTI is 91% lower than TA-OMP (NB benchmark on the Intel 24-core system – Fig. 13c). When the geometric mean of the entire benchmark set and processors is considered, Hebe presented an NBTI value 53% lower than the TA-OMP with power-budget set to 25% of the TDP; 43% lower when set to 50%, 41% lower when set to 75%, and an NBTI 34% lower than TA-OMP when the power budget is set to 100% of the TDP.

4.3. Learning overhead of Hebe
Because Hebe learns as the application executes, there is an overhead imposed by the search algorithm. The overhead can be measured by the difference between Hebe and the Exhaustive Search solution and is originated from two situations:

•
The execution of the search algorithm itself, which comprises: (i) the overhead for calling the functions implemented by Hebe (as described in Section 2); (ii) the cost for reading the hardware counters at each step of the learning algorithm; and (iii) the time to evaluate the current value of the optimization metric and decide the next step of the search algorithm. We have found that this overhead is less than 2% of the total overhead of Hebe.

•
The execution of a given parallel region with a number of threads that is not ideal, while the search is trying different possibilities to converge. This is the most significant overhead of Hebe, representing more than 98% of its total cost.

As can be observed in Fig. 14, the overhead of using Hebe is almost negligible for most of the benchmarks. However, for a few cases, where the lowest value of NBTI is achieved with the highest degree of TLP or a number near it, Hebe presented its worst results (e.g., CG-NAS, PO, and FT-NAS executing on the Intel 32-core system). In those cases, Hebe added extra overhead (learning phase) to converge to a configuration already used by the OMP Regular execution.

Fig. 14
Download : Download high-res image (147KB)
Download : Download full-size image
Fig. 14. Hebe vs. Exhaustive Search.

To better understand the worst cases of Hebe, we discuss its behavior when converging to a solution for the third parallel region of the FT-NAS benchmark on the Intel 32-core system (Fig. 15). Through the exhaustive search, we found that running the region with 32 threads provides the lowest NBTI value while the execution with a low degree of TLP (e.g., 2, 4, 6, 8) significantly increases it (Fig. 15a). Hence, as the search algorithm implemented by Hebe starts with the number of threads equal to two, and exponentially increases this number before performing a local search between candidates (see Section 2.2), there is a significant increase in the NBTI for the first iterations of the learning phase.3 This behavior can be observed in Fig. 15b, where the red bars show how much worse the NBTI value (normalized to the ideal one found by the exhaustive search) was for each step of the learning algorithm during its convergence. It also shows, for each iteration, the number of threads that Hebe used during the search before converging to the solution (32 threads).

Fig. 15
Download : Download high-res image (181KB)
Download : Download full-size image
Fig. 15. Behavior of the NBTI for the third parallel region of FT-NAS benchmark. NBTI is normalized to the best solution found by the exhaustive search.

4.4. The need for a tool to reduce the impact of NBTI on the processor aging
Besides comparing Hebe with the previously approaches (OMP Regular, OMP_Dynamic, TA-OMP [60], and Exhaustive Search), we show in this Section that well-known thread-throttling tools that target performance, energy, or energy-delay product (EDP) cannot be simply used to optimize the NBTI value.

Fig. 16 depicts the NBTI results achieved by Hebe compared to the ones obtained by Varuna-PM and FDT. For most of the cases, Hebe presented better results than both strategies, regardless of the multicore system. When considering the geometric mean of the entire benchmark set and processors, Hebe delivered an NBTI value 71% lower than Varuna-PM and 45% lower than FDT. The main reasons behind the behavior of each approach are discussed below.

Fig. 16
Download : Download high-res image (297KB)
Download : Download full-size image
Fig. 16. Hebe versus Varuna-PM and FDT: lower than 1.0 means that Hebe is better.

Hebe versus Varuna-PM: Varuna was developed to be used in different kinds of applications (e.g., big data and ones that are recursively implemented), since it creates as many threads as possible (we executed each application with 1566 threads - numbers taken from [61]). As a particular example, let us consider the NB benchmark, which has its scalability limited by data-synchronization, the greater the number of threads, the greater the time the threads spend synchronizing. This increases the execution time and rises the processor temperature. It is important to emphasize that the results achieved with Varuna-PM do not consider the improvements provided by its analytic engine and manager system. Nevertheless, even if such components of Varuna-PM could improve performance by 15% (values taken from [61]), it would not be enough to provide similar trade-offs between performance and processor temperature as Hebe, in most cases.

Hebe versus FDT: FDT ignores fundamental hardware characteristics that are correlated with the parallel application behavior: it assumes that bandwidth requirement increases linearly with the number of threads, ignoring cache contention and data-sharing between the threads. Moreover, FDT does not consider the effects of the SMT. Because of such reasons, FDT converges to a non-ideal number of threads in many times even when considering performance, which is its main purpose. Moreover, as its training phase executes each parallel region in single threaded, it leads to a higher overhead for applications that present a medium or high degree of TLP (e.g., FT-NAS and CG-NAS). However, FDT and Hebe presented similar results in few applications that are in the group of scalability issues that FDT handles, such as FFT (off-chip bus saturation).

We have also changed the learning algorithm of Hebe to optimize performance, energy, or the trade-off between both metrics represented by the EDP – energy-delay product. With this, we show that the degree of TLP exploitation that delivers the best execution time, energy, or EDP is not necessarily the same that reduces aging. Fig. 17 presents the results for each benchmark and processor. We use Hebe that optimizes for NBTI as baseline, so values higher than 1.0 mean how much Hebe reduces the impact of NBTI than the versions tuned for performance, energy, or EDP.

Fig. 17
Download : Download high-res image (557KB)
Download : Download full-size image
Fig. 17. Hebe optimized for NBTI (baseline - black line) versus Hebe set to optimize performance, energy, and EDP considering the benchmark set - values higher than 1.0 mean that Hebe is better.

Summarizing the evaluation performed in this section, one can note the need of developing such approach and specifically target the reduction of the impact of NBTI on the processor aging. In the overall (geometric mean of all benchmarks and processors), the NBTI value of Hebe was: 71% lower than with Varuna-PM, 45% lower than with FDT, 12% lower than with Hebe-Perf, 12% than with Hebe-Energy, and 10% lower than the execution with Hebe optimized for EDP.

5. Related work
We have split this section into three main parts: techniques to reduce aging at different levels are discussed in Section 5.1; approaches for thread throttling (but that do not target aging) in Section 5.2; and how Hebe advances correlates with them in Section 5.3.

5.1. Reducing aging
5.1.1. Microarchitecture and DVFS
Namaki-Shoushtari et al. [46] present ARGO, an aging-aware technique for allocation of register files (RFs) on GPGPUs to uniformly distribute the heat throughout them, aiming to reduce the hardware aging. Bartolini et al. [7] propose a distributed and self-calibrating model-predictive controller, which performs the thermal management of a single-chip on multicore architectures. The model was improved in [12] to select the ideal CPU operating frequency to reduce the operating temperature. Zoni and Fornaciari [69] explore the power-gating technique to reduce the hardware aging in network-on-chip buffers. Reghenzani et al. [55] present a data-driven controller based on a constrained extremum-seeking algorithm to optimize the resource allocation under specific thermal constraints.

Chantem et al. [14] propose a DVFS control policy to maximize performance while considering a peak temperature constraint. Bao et al. [6] present a temperature-aware DVFS technique that exploits the available slack times by reducing the frequency at which processors operate. Hanumaiah and Vrudhila [27] propose a DVFS technique to determine the CPU frequency and voltage that guarantee tasks completion within a deadline while thermal constraints are satisfied. Lee et al. [36] present a processor temperature predictor at different CPU frequencies and efficiently scale the frequency to reduce the temperature. Recently, Khdr et al. [34] propose a DVFS-based boosting technique for reducing both long- and short-term aging effects that are induced by higher temperatures.

5.1.2. Compiler
Hasieh et al. [30] propose a static thermal management technique at the compiler level to reduce the temperature of the register file for very-long instructions word (VLIW) architectures. Yang and Orailoglu [68] present a compiler-directed register shuffling strategy to attain power density control through exploiting the extant spatial slack associated with register file accesses. Sabry et al. [56] present a thermal-aware compilation flow for System-on-Chip architectures to reduce the hotspots and uniformly distribute the heat in the register file. Given that a considerable fraction of execution time of an application is spent to execute NOP (No Operation) instructions, Firouzi et al. [21] propose two methods, software- and hardware-based, to optimize NOP instructions assignment to minimize NBTI effect on the processors.

Considering the thermal and stress variation across the hardware components, Ahmed et al. [2] propose a wearout-aware compiler-directed register assignment approach that aims to distribute the stress induced wearout throughout the register file. Rahimi et al. [53] present a VLIW reallocation strategy for reducing the aging of GPGPU architectures by distributing the stress of instructions throughout VLIW slots. The model is enhanced in [41]. Considering the aging of data cache memory, Oehlert et al. [47] propose a compiler-based optimization to mitigate the aging effects on caches using SRAM memory cells.

5.1.3. Scheduling
Many works were developed aiming to reduce the aging of hardware components of MPSoCs architectures. Coskun et al. [20] present a proactive MPSoC thermal management technique that predicts the processor temperature and adjusts the task allocation to minimize the impact of thermal hotspots and temperature variations. Mulas et al. [45] propose a thermal balancing policy that exploits task migration to maintain the cores temperatures within a predefined range for MPSoCs architectures. Chantem et al. [15] present a solution for assigning and scheduling tasks on an MPSoC architecture to reduce the processor aging.

Considering multi/many-core architectures, Powell et al. [24] propose Heat-and-Run, an approach that performs thread migration on multiprocessor cores to migrate threads away from overheated cores to non-heated cores. Marongiu et al. [42] propose a workload re-allocation strategy for reducing the processor aging when running OpenMP applications. Ge et al. [22] present a distributed task migration for thermal management in many-core systems, in which tasks are distributed to each processor according to their heat dissipation capabilities. Corbetta and W. Bornaciari [19] explore the impact of different instruction allocation strategies on the processor aging. Cho et al. [18] present a spatiotemporal power multiplexing method that exploits the thermal capacity of materials to redistribute the heat by varying the location of the active cores on a many-core system. Paterna et al. [48] propose an adaptive task allocation strategy that exploits runtime information about variability and aging of the cores to increase the processor lifetime. Khdr et al. [33] present a DTM technique for migrating tasks to balance temperatures between the cores and reduce processor aging for on-chip systems.

5.2. Thread throttling
In this section, approaches that are capable to adapt the number of threads to optimize parallel applications but do not target aging are discussed. Suleman et al. propose the FDT framework [65], which can adapt the number of threads considering contention for locks and memory bandwidth. Thread Tailor [37] dynamically adjusts the number of threads to optimize some specific parts of the system, such as cache and memory footprint. Li et al. [38] propose a library to save energy with no performance loss for hybrid MPI/OpenMP applications. Thread Reinforcer [50] consists of a framework that executes an application binary multiple times with a different number of threads for a short period while searching for the ideal configuration. Then, the application is re-executed with such configuration. Chadha et al. [13] propose LIMO, a dynamic system that monitors the application as it executes and adapt the degree of TLP accordingly.

Raman et al. [54] present Parcae, a framework that comprises a compiler (that applies multiple parallelizing transforms on sequential code) and a run-time system (that monitors the application execution to determine the best degree of TLP) to optimize performance of parallel applications. Sridharan et al. [61] present Varuna, a monitor system that intercepts thread and task creation from PThreads, TBB, and Prometheus libraries, and create a pool of tasks to optimize their degree of parallelism. F. Alessi et al. [3] propose an extension designed for energy management of OpenMP applications. Shafik et al. [60] present a thermal-aware approach for OpenMP applications, where the programmer needs to insert code annotations in the sequential and parallel parts of the code to enable temperature minimization. Although it aims to distributed the temperature among the cores, it does not focus on the mitigation of processor aging. Aurora [40] is a tool built into the OpenMP library to transparently optimize the performance, energy, or EDP of OpenMP applications.

5.3. Our contributions
Even though many different strategies (i.e., at RTL, DVFS, compiler) have been proposed to minimize the effects of NBTI and other phenomenas on the processor aging by reducing the processor temperature, as discussed in Sections 5.1 and 5.1.2, none of them present any kind of adaptation of TLP exploitation targeting temperature and aging. The approaches presented in Section 5.1.3 aim to reduce the processor aging of parallel and distributed applications by changing the way how tasks/threads are scheduled in the cores rather than tuning the number of threads that are running at a given moment. Therefore, compared to these works, Hebe is orthogonal and can be used with them to further reduce aging.

On the other hand, when compared to strategies that perform adaptation of TLP exploitation (listed in Section 5.2), Hebe is the only one that was built specifically to optimize the processor lifetime. On top of that, it is transparent to the user (no changes in the source nor binary codes are necessary) and is totally automatic, being able to adapt to workload changes even during execution. For that, the programmer only has to enable Hebe through the use of one environment variable in the Linux OS. However, because of this high level of transparency, Hebe is limited to OpenMP applications only.

6. Concluding remarks
In this manuscript, we have presented Hebe. It is an approach for OpenMP applications that reduces the impact of NBTI on the processor aging by finding the degree of TLP that offers the best trade-off between performance and processor temperature. Hebe optimizes an OpenMP application binary by only setting an environment variable in Linux OS without any code transformation or recompilation. Through an extensive set of experiments, we have shown that Hebe can reduce the impact of NBTI on the aging and increase the processor lifetime with a negligible overhead in most cases. As a future work, we will enhance Hebe to consider the execution of parallel applications on heterogeneous platforms and programming models.