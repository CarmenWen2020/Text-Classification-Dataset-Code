Abstract
Information Centric Networking (ICN) provides caching strategies to improve network performance based on consumer demands from the intermediate routers. It reduces the load on content server, network traffic, and improves end-to-end delay. The content requesters use an Interest packet containing the name of data to express their needs. If such Interest packets are routed efficiently, the end to end delay and throughput of the network could be improved further. This paper describes an efficient method of forwarding Interest packets to retrieve the requested content at the earliest possible time. Here the data source is found and considered as a single player game with content requester as its start state and location of the desired content as final or goal state. The Monte Carlo Tree Search (MCTS) algorithm is used for constructing the path from content requester to concerned data source. For performance evaluation, the proposed scheme is integrated with Leave Copy Down (LCD) and Leave Copy Everywhere (LCE), Cache Less for More (CL4M), and Probability based caching (ProbCache) In ns-3 simulation environment (ndnSim), all these are evaluated in terms of content search latency, server hit ratio, network load, overhead and throughput. Simulation observation reveals that the integration of MCTS significantly improves performance in regard to experimental parameters.

Previous
Next 
Keywords
Information Centric Network (ICN)

Content centric network (CCN)

Routing

Forwarding

MCTS

1. Introduction
Future networking is going to be too demanding in terms of access delay and throughput [7]. End users would be interested in obtaining the desired content securely without bothering about the service provider in such challenging situations. The Information Centric Networking (ICN) is proposed [30] to address such demands of future users. The ICN reduces network traffic and content delivery latency by delivering content (Data) from any data store in the network instead of relying only on data producers. To do so, it suggests unique naming contents and consumer search for data chunk by name. Hence, any node with the content request may serve the requester. There are several architectures defined for ICN. Few of such architecture are Translating Relaying Internet Architecture integrating Active Directories (TRIAD, 2000) [6], Data-Oriented Network Architecture (DONA, 2006) [4], [16], Network of Information (netInf, 2013) [10], and Named Data Network (NDN, 2014) [14], [31]. Along with unique content naming [3] and caching [33] data in the intermediate routers, routing based on content names are some other areas of importance in ICN. Both caching and routing are centered on the content names rather than the IP addresses. Following such identifying conventions, nodes or Content Routers (CR) in the network that owns certain contents can satisfy any content request raised by various consumers.

The work described in this paper follows the NDN architecture [20] where a consumer generates an ‘Interest’ packet containing the name of the desired data (or content) [8], [9]. The ‘Interest’ packet is forwarded toward the content store for serving. Routing in ICN involves two activities: the forwarding of Interest packets to the content store and the forwarding of data chunk toward the requester. To forward Interest packets, every CR maintains a Forward Information Base (FIB) table. The Open Shortest Path First for NDN (OSPFN) [28] and Named-Data Link State Routing (NLSR) [20] focus on building routing information and filling the FIB table. However, few protocols [2], [25], [27] aim at forwarding Interest packets to the nearest cache efficiently without bothering about FIB creation. Although it works as an overlay protocol, the second category has a significant impact on finding content in less time with minimum overhead. The focus of this paper is to suggest an efficient ‘Interest’ packet forwarding mechanism. The protocol does not explore the preparation of FIB and hence it works as an overlay routing technique. The contributions of the proposed work may be listed as follows:

•
An improved Interest packet forwarding technique for faster content retrieval in ICN is presented in this work.

•
Uses of Monte Carlo Tree Search (MCTS) algorithm for finding the closest source (Cache) of searched content.

•
Mathematical modeling of the proposed scheme for better interpretation and justification is presented.

•
Detailed analysis of the proposed scheme with comparative performance evaluation through simulation in ns-3 based ndnSim is demonstrated.

Forwarding Interest packets in the right direction toward the content cache helps retrieve data with shorter delay and minimized overhead. The task for finding the CR holding in the desired content may be treated as a gaming system which considering consumer as the start state and cached location as the goal. The MCTS is proven as an efficient approach to find the goal state in a single player gaming system. Finding the best CR for requested content has more resemblance to a single player game. Moreover, as the MCTS uses existing knowledge of content stores, this algorithm is a deep learning approach and hence ignites search performance. That is why we model the content searching as a single player game and try to find the goal, (i.e., the searched content’s cache location) using the MCTS algorithm. Novelty of the presented work lies in the fact that MCTS has not been extended so far to ICN routing to the best of our knowledge.

2. Related work
ICN’s routing mechanisms are mostly extended from Open Shortest Path First (OSPF) and Link State Routing (LSR) algorithms by integrating content centric network paradigm. Among the available routing mechanisms, OSPFN [28] proposed by Lan Wang et al. is the base of attempting toward routing in ICN. To support name based routing, the authors extend OSPF to disseminate name prefixes and compute paths to such named contents. It is deployed in an NDN testbed to demonstrate the basic functionality and multipath routing. The Named-data Link State Routing protocol (NLSR) proposed by AKM Mahmudul Hoque et al.  [20] is a variation of the LSR protocol designed for NDN. It differs from IP based interpretation in the following ways: First, it uses a hierarchical naming method, second, it implements a trust model for security. The third difference is the use of the ChronoSync [34] protocol for routing updates. Finally, it integrates multipath routing. It has also been deployed on the testbed of NDN since 2014.

Vince Lehman et al. in [18], proposed a routing protocol based on Hyperbolic Routing (forwarding) (HR). It uses a greedy approach to find the content. Interest packets carry coordinates of the probable content location. Based on the coordinates, every node forwards Interest packet through the shortest path. Tracking coordinates of the possible destination of the content is quite tricky except for actual producer. However, ICN focuses on getting content from caches rather than a real producer. Work described in [27] by Ivan Voitalov et al. is a scalable routing with an addressing scheme. To incorporate the addressing, geographic coordinates of nodes are taken into account. The proposed approach efficiently accomplishes its operation when ICN is deployed as an overlay network. It means that the protocol relies on the underlying TCP/IP. However, the assumption about size of the FIB to be equal to the number of adjacencies that a node has is impractical. Also, the network topology cannot be arbitrary and should follow the Internet like topology to implement the said scheme. Bitan Banerjee et al. [2] suggest the characteristic time based routing for ICN. It focuses on the expected duration of likelihood for content in a specific store. The content requests are directed to such locations based on characteristics time. The authors have tried to use existing routing strategies such as Dijkstra’s shortest path algorithm for efficient routing. However, determining the probability of content availability or the characteristic time is a challenging task. Unless this period is designed optimally, the protocol will miss the cache and generate more massive overhead.

With the advent of Machine Learning (ML) and Deep Learning (DL), researchers integrate more models into various ICN activities. A study on the applicability of Convolutional Neural Network (CNN), Recurrent Neural Network (RNN), and Reinforcement Learning (RL) with ICN is carried out in [15]. A Deep-Learning-based Content Popularity Prediction (DLCPP) is proposed in [19] to predict content popularity. It creates a distributed and reconfigurable DL network of SDN switches for prediction. Deep Q-Networks (DQN), based on Q-leaning, is adopted to provide a caching solution in [29]. The authors have considered an SDN IoT application for the deployment of the protocol. The effectiveness of the proposed solution is verified through simulation. The DeepCache framework for content caching in ICN is described in [21] through the RNN model to boost cache performance. A Long Short-Term Memory (LSTM) encoding is the heart of this approach. But these researches do not explore routing or forwarding. Mostly they integrate AI models with caching process. Hence, there is a necessity to examine the use of such techniques to routing in ICN.

Despite being a paramount concern, efficient forwarding of Interest packet is less investigated [2], [18]. Because of efficient selection path to the content, it significantly improves content access time and network overhead. The work [2], [18], [34] emphasizes this idea by forwarding Interest packets to the nearby content cache. If DL techniques are integrated, efficiency would be tremendously high. This fact motivates us to carry out this research. Here DL technique by integrating MCTS [8] algorithm is applied to locate the closest content store to serve an Interest packet. The MCTS is used for constructing the path from the content requester to the concerned data source. Due to the use of the MCTS with existing knowledge of content stores, this algorithm is considered by deep learning approach, as stated in [13]. Another reason for considering MCTS as deep learning is the backpropagation for computing two critical parameters. E.g., the average scope of visiting a CR for locating content using cache and the server hit ratio as an occurrence of a cache miss. Moreover, the algorithm’s satisfaction is determined by a threshold that is dynamically updated from apriori knowledge. This knowledge is built up over time and reflects the current status of the network.

3. Proposed methodology
The research question addressed in this paper is “how to find the nearest cache store of a requested content in ICN”. The problem is modeled as a single player game [11] with the content requester as start state and content store as the goal state. The MCTS is used in many games solving, including single player [32] game. In this work, the MCTS is used to construct the content requester’s path to the concerned data store.

3.1. System model
We model the network as a graph denoted by , where  represents a set of vertices (all CR nodes),  is a set of directed links. A CR may have number of outgoing interfaces (B) with cost  and collectively denoted by the . There are N consumers in the network that may demand M unique contents. While serving a consumer’s request, the data may be served from any source near the consumer. This source may be the producer of the data or any cache that is maintained in a CR. This primary goal of work is to forward the Interest packet to a CR that incurs minimum data retrieval cost. The cost of hop in the path is considered to choose a path for the requested content store. Moreover, the Interest packet reaches the content store by the time; it may be purged due to aging. Hence, the time to live value of the content must be taken into consideration during path selection. We assume that all CRs that cache data are characterized by various parameters like load on it, size of cache, and queue length of all its incoming interfaces. These parameters together decide the cost of using a CR as data source, considering all CRs are identical for same content.

3.2. Monte carlo tree search mechanism
CRs are considered as states and edges connecting two states are modeled as an action. The search of the CR that can serve the interest of the consumer initiates by consumer. Hence, the consumer node is considered as the root of the tree. The root forms the initial state of the model denoted by . From a given node, including the root, there are many outgoing interfaces through which the Interest packet can move. Every possible edge is called probable actions. Let  be the set of actions at a given state . The tree that corresponds to this state has  child nodes, corresponds to possible next state  that is a result of selecting an action . Every tree node stores a value  and a visit count . A path from a leaf node defines an action policy . The parameters and have special significance to the model. The signifies the quality of decision. A higher value of the parameter indicates a better decision. It is calculated from the tradeoff between the cost of communication and the server hit ratio (server hit ratio is defined later in this section). The parameter signifies the association of a path with a CR and gives an idea of predicted utilization. It balances the choices of a path by lowering the reward of data sources and overused paths. Every CR that serves content to an interest packet for a given state and an action is rewarded with an amount . The total reward is the sum of all rewards from the leaf to the root node. A detailed procedure is given in Algorithm 1.


Algorithm: 1 MCTS
Input: Set of probable actions ()
Output: target CR having requested content.
Step 1: Root is constructed by the consumer of the data who generates the interest packet.
Step 2: Repeat until
 a. Select an action from the and perform a
 transition in , the possible next step
 b. Repeat step (a) for every action in till named
 data in interest packet is found, or server (producer of
 the data) is reached.
Step 3: Stop
The term is called the computational budget and it decides the level of the search tree. From a node, a child (i.e., an action) is selected based on a selection strategy. Although many selection strategies exist for MCTS, we have used the single player upper confidence bound, because finding the best CR for requested content has more resemblance to a single player game.

3.3. Formulation of the objective function
In the context of ICN, a server hit is said to occur when the data requested by a consumer is not available in any of the intermediate CRs. In that case, the content request is served from the actual producer of the data. Server hit is not desirable as it increases network load and the end to end delay. In this section, an objective function is designed to model the server hit. Minimizing the server hit is the primary concern of the objective function. The parameters given in Table 1 are used in the computation.

Moreover, the following statements are valid and applicable to the modeling of the objective function Using the above parameters and conditions, the objective function to minimize server hit is defined as follows: (1)Eq. (1) is the ratio of server hits to the total content requests raised in the network. For the satisfactory realization of Eq. (1), the following constraints are imposed. (2)Constraint through Eq. (2) ensures that for every content request d, one path having one edge is selected. No multiple paths are assumed here. (3)The constraint represented by Eq. (3) states that one edge is associated with one path. (4)Eq. (4) signifies that anycast candidate path uses the same set of links to the same data store. With the stated goal and constraints in the next subsection, the Single player Monte Carlo Tree Search (MCTS) algorithm is described.


Table 1. Parameter for devising the objective function.

Parameter	Description
Outgoing interface
Candidate out interface
All content requests
Requests that hits the server
Anycast request for a content
Data request by a single consumer
Data requested by more than one consumer
Network edges
Total outgoing interfaces request the same data
Candidate path for data cache or producer
Interest packet id for data d
Source of path p may be producer or CR
The consumer of the data on path p
3.4. Single player Monte Carlo Tree Search (MCTS) algorithm
The algorithm comprises four steps, and the process begins at the content requester (the start state), and it tries to find the best children based on the need for the problem (in our case, the best content source). Once the content is found (a goal state), the game halts. All steps are explained below.

Selection step: In this step, a favorable child is selected based on some children’s essential characteristics. We have considered the time to live value of the content, load on the link, and the load on the children node for selecting the best next node. Every node has the load information of its children acquired through information interchange. This step uses exploitation to choose the best path and exploration to deal with less promising moves.

Playout step: All the unexplored branches or children of the current node are examined to explore the possibility of finding the searched content. Different available branches are selected in a uniformly distributed random way. So, such a quasi-random based selection process takes the heuristic knowledge [23], [24], [27] acquired by the node. In our approach, we have taken the maximum hits for the content in the selected branch.

Expansion step: Till this step, the probable nodes for building a tree are appropriately selected. The expansion step decides on which stored node of the tree, the new child to be connected. This step expands the tree one step toward the goal of the game. As depicted in [26], a similar approach, designed for one child per play expansion, is adopted in our work. This approach suggests that a node not present in the tree and encountered first is selected for expansion.

Backpropagation step: This step is used to update two parameters the average scope in visits () and the server hit ratio . The backpropagation helps in the propagating result of playout back to the root. An average of the playout results is used for backpropagation.

The above four steps are executed based on a satisfaction threshold of a previous knowledge-awareness defined at the process’s initiation. The expansion of each branch leads to reaching close to the desired data store. As mentioned in the expansion step, highest average score of a node is determined by Eq. (5) to select the next path. Suppose, is a node and is children of p, the next move is chosen so that the following formula’s value is maximized. (5)where,

: the confidence bound

: number of times a node I is visited

: results achieved so far for the selected child node

: expected results

: constant

Eq. (5) takes into account some previous knowledge acquired by the CR in order to serve the content. This knowledge is built up over time and reflects the current status of the network. Hence, it gives a close approximation of the content’s probable location and has a high probability of selecting the correct route.

4. Performance analysis
The simulation compares the performance of LCE [12], LCD [17], CL4M [5], and ProbCache [22] with and without integration of the MCST algorithm. A copy of the requested content is stored in every CR along the delivery path. The LCD suggests that if there is a cache hit, the content is replicated at the cache of the one hop downstream CR toward the requester. The work reported in [5], the CL4M (Cache Less for More), introduces the betweenness centrality of nodes to cache contents. It emphasizes minimizing places of caching content to avoid repeated caching. The content is accessible to many consumers as it is cached in a CR of high betweenness centrality. The ProbCache [22] is a caching mechanism backed by probability of retrieval request. It considers on-path caching capacity and estimated traffic per unit time to calculate the desired probability of expected content requests based on data popularity. This protocol suggests caching the most popular content in the intermediate codes. Unlike LCE, the CL4M caches content only in a subset of CRs along with the delivery. The ProbCahe approximates the caching by selecting content based on probability of future access. This analysis aims to examine how efficiently the MCTS algorithm can route Interest packets to the nearest appropriate cache. That is why the performance is evaluated with and without integrating the MCTS in all four protocols.

4.1. The setup
The proposed caching protocol is simulated in the ns-3 based NDN simulator, ndnSIM [1]. By default, the ndnSIM implements LCE with LRU as a caching and replacement policy respectively. A set of 9000–15000 unique contents with a content size of 2500 Bytes (i.e., K 5000) is modeled. As described earlier in this paper, the content request follows Zipf distribution, where the skewness parameter ranges from 0.1 to 1.2. The cache is considered empty initially, and the capacity ranges from 50 to 150, with an average of 70 chunks. All simulations are executed for 550 seconds, and applications are started after 10 seconds of the simulation. For the simulation, two topologies are used. One is a random topology with 400 nodes and another topology similar to the US26 network with 26 CRs, as depicted in Fig. 1. We believe that the real network like simulation topology will interpret the observation as in a testbed environment.

4.2. Simulation results
The experiments aim to compare the results of various performance parameters with and without the proposed MCTS single player model. The content requests obey Poisson’s distribution with a mean arrival rate of requests per unit time and their lifetime exponentially distributed with mean . The traffic load is expressed in . Any changes in the parameters above are stated accordingly during the discussion of the individual results.

a. Content Search Time (CST): The CST indicates how quickly content is located and delivered to the requester. A lower CST is a desirable property for any content centric network. This section presents the said parameter for random and US26 topology.

i. Random topology: CST for random topology against cache size and popularity skewness is in Fig. 2 and Fig. 3, respectively. The observed parameter decreases with the increase of both cache size and skewness. The algorithm with the integration of MCTS outperforms the one without MCTS. Examination of varying cache size shows a reduction of latency in the range of 10%–30% in case of LCD and LCE, 12%–25% for CL4M, and ProbCache with MCTS integration compared to their original version. Observation of latency vs. skewness demonstrates an improvement of 8%–15% in LCD and around 7%–24% for LCE, 5%–19% for CL4M, and 5%–17% for ProbCache.

ii. US26 topology: The CST against cache size and skewness in US26 topology is presented in Fig. 4 and Fig. 5, respectively. With the increase of cache size and skewness, the search time gets decreased when MCTS is integrated with the LCD and LCE. This reduction is ranged from 12%–25% for LCD and LCE and around CL4M and ProbCache. As a whole LCE+MCTS shows the best performance, and LCD shows the worst. The CL4M and ProbCache are performing between the other two. Moreover, the functioning of CL4M+MCTS is close to LCE+MCTS.


Download : Download high-res image (258KB)
Download : Download full-size image
Fig. 2. Content search time analysis against cache size for random topology comprises 250 nodes. The cache size varies between 50 to 150 contents per cache, and the content count is 12000. With this storing capacity, every content could be accommodated in the cache.


Download : Download high-res image (289KB)
Download : Download full-size image
Fig. 3. Content search time analysis against popularity for random topology comprises of 300 nodes. The cache size is 120 contents per cache, and the content count is 12000.

Analysis of CST results: Observation shows that the MCTS integrated version of LCE and CL4M shows the lowest latency in searching content. In LCE, contents are located densely and MCTS can find the goal state in a closer code. Moreover, the reason for the superior performance with MCTS lies in the use of pre-prepared tree of the probable locations of content. Based on the information acquired by the node it constructs the search tree locally for the content search. As a consequence, the content is searched in lesser time. The MCTS based approach also has the option of exploring the alternate paths in case of failure in one path due to the evacuation of the content from the expected location. The tree constructed in a node works as a lookup table for the desired content.


Download : Download high-res image (263KB)
Download : Download full-size image
Fig. 4. Content search time analysis against cache size for US26 topology with 26 nodes. The content count is 2000. With this storing capacity, every content could be accommodated in the cache.


Download : Download high-res image (267KB)
Download : Download full-size image
Fig. 5. Content search time analysis against popularity for US26 topology with 26 nodes. The content count is 2000, and the cache size is 60 chunks with 2500B content size. Total nodes in the random topology are 300.

b. Server Hit Ratio (SHR): The SHR is defined as the rate of accessing the content produces for serving user requests. The ICN emphasizes on serving user requests from intermediate caches rather than accessing the content producer. Hence, minimum possible SHR is a desirable property of ICN. If underlying packet forwarding approach rightly forwards the Interest packet, then user requests need not be reached the server provided at least one copy of the content is available in the cache. In this subsection, SHR is plotted for a random topology against simulation time (Fig. 6a) and content request arrival rate (Fig. 6b).

The graphs in Fig. 9(a) depict that the SHR decreases for all protocols over the progress of simulation time. However, relative decrease is higher in protocols with MCTS integration. For example, in case of LCD+MCTS, the SHR is decreased by 15% compared to LCD without MCST. Similarly, the decrease in LCE+MCTS is around 20% (max.) compared to LCE without MCTS. Similarly, the CL4M and ProbCache have shown an improvement around 17%. The SHR is shown against content request arrival rate for a random topology in Fig. 9(b). The SHR increases with the increase in content request arrival rate. Relatively, LCE+MCTS shows best performance with lowest SHR. Specifically, LCE+MCTS shows an improvement up to 10% compared to LCE. On the other hand, LCD+MCTS shows up to 16% lower hit than LCD without integration of MCTS. The CL4M performs close to LCE and ProbCache have comparatively higher hit than LCE and CL4M.


Download : Download high-res image (299KB)
Download : Download full-size image
Fig. 6a. Server hit over simulation time within a duration of 550 s. Total simulated contents are 9000. The cache size is 60 chunks with 2500B content size. Total nodes deployed in the random topology are 300. The skewness parameter is taken as 0.7.


Download : Download high-res image (315KB)
Download : Download full-size image
Fig. 6b. Server hit over content request within a duration of 550 s. Total simulated contents are 9000. The cache size is 60 chunks with 250B content size. Total nodes deployed in the random topology are 300. The skewness parameter is taken as 0.7.

Analysis of SHR results: A content request hits the server if the content is not available in any of the intermediate routers or the interest packet is not correctly forwarded to the appropriate cache. As the caching algorithm does not take care of the forwarding of Interest packet, the underlying network tries to send it to the best possible cache store. On the other hand, any caching approach with the integration of MCTS helps find the appropriate cache of the requested content. Hence, the MCTS integrated version performs better than its original counterpart.

c. Network load

The traversal cost of data packets (chunks) is considered for network load evaluation. It is measured in terms of amount of data propagated through network over time. The observed information is plotted against simulation time and depicted in Fig. 7. As the simulation time increases, the load on the network decreases. At the beginning of the simulation, caches are empty and hence the data is mostly delivered from producer of the data and thus network load is high. However, with the progress of the simulation, caches are filled with content and hence, content requests are served from intermediate CRs. It significantly reduces the distance traversed by the data packets and subsequently, the network load drops. As and when contents are cached, the load decreases due to the intermediate store’s availability of contents. Protocol wise, the ProbCache and LCD have comparatively higher load than CL4M and LCE when we consider the MCTS version. Nearly at 50% elapse of the simulation time, all four protocols with MCTS show considerable improvement. Protocols without the integration of the proposed technique perform with higher load as shown in the graphs.

Network overhead is computed as a load on the network due to cache miss in the destination CR directed by the MCTS algorithm. The results are shown in Fig. 8 as a percentage change in load for the increased content request. For network overhead computation, we record the distance traversed by every Interest packet for those propagations for which the Interest packets are unsuccessful in acquiring the data. The requests that produce a cache miss impose the extra load on the network as it needs to traverse till the content router, but content is not present. The fraction of changes in load due to the need for content replacement is plotted in the given graph. With the increase of content requests, the percentage increase in overhead (with respect to without cache miss) is plotted in graphs. In this plotting we have considered only MCTS integrated versions of all four protocols. Observation shows mixed behavior among the protocols. Up to around 30 requests, CL4M shows the best performance and LCD shows worst. With that range of request arrival, LCE and ProbCache show moderate performance in comparison. But, after increasing request rate, behavior alters. For higher request rate above 40, LCE shows the best behavior. Better performance of LCE is due to dense caching of contents in the network. For ProbCache, the probability estimation misses and hence cache miss occurs and shows higher overhead.

e. Network Throughput

The throughput is measured as the ratio of the total number of content request raised and the total servings made from the intermediate CRs and presented in Fig. 9. The said parameter is examined for varying content request arrival rate. We have taken only MCTS integrated version to observe this parameter. Throughput decreases with the increase in content request. LCE shows highest value up to 76% whereas the LCD depicts the lowest performance in the range up to 56%. On the other hand, the CL4M shows higher throughput around 78% with higher request arrival rate. But with lower request rate the performance is around 60% approximately. On the contrary, ProbCahce shows better performance with lower arrival rates and drops with a higher rate. With the increase in request rate, more cache miss occurs in the ProbCache protocol. In LCE, contents are kept almost everywhere and LCD keeps minimum number of copies. The CL4M stores content only in node with higher centrality betweenness and a lower cache miss occurs.

5. Conclusion
The work presented in this paper is a path selection approach to forward Interest packets with an objective to retrieve the requested content at the earliest possible time. The output of MCTS algorithm executed in CRs of the network assists in finding the closest destination cache. A reliable naming scheme is assumed to be implemented for content identification. The schemes presented are based on the heuristic knowledge of the previous search and current status of the network components. For the simulation, a random topology of 300 nodes and the US26 network topology of 26 nodes aredesigned ndnSim. Four protocols, LCE, LCD, ProbCache and CLM4 are implemented by integrating the MCTS algorithm. Performance is compared with and without the integration of MCTS into these four methods. The examined parameters include content search latency, server hit ratio, network load and overhead. Observation shows that MCTS integrated version of all four protocols outperforms their base variations.