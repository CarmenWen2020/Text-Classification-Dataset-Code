Abstract
Identifying long pairwise maximal common substrings among a large set of sequences is a frequently used construct in computational biology, with applications in DNA sequence clustering and assembly. Due to errors made by sequencers, algorithms that can accommodate a small number of differences are of particular interest. Formally, let  be a collection of  sequences of total length ,  be a length threshold, and  be a mismatch threshold. The goal is to identify and report all -mismatch maximal common substrings of length at least  over all pairs of strings in . Heuristics based on seed-and-extend style filtering techniques are often employed in such applications. However, such methods cannot provide any provably efficient run time guarantees. To this end, we present a sequential algorithm with an expected run time of , where  is the output size. We then present a distributed memory parallel algorithm with an expected run time of  using  expected rounds of global communications, under some realistic assumptions, where  is the number of processors. Finally, we demonstrate the performance and scalability of our algorithms using experiments on large high throughput sequencing data.


Keywords
Approximate sequence matching
String algorithms
Suffix trees
Hamming distance
Parallel algorithms

1. Introduction
Sequence matching algorithms are at the core of many applications in computational biology. Next Generation Sequencing (NGS) [15] instruments sequence hundreds of millions of short reads, that are typically randomly sampled from one or multiple genomes. Deciphering pairwise alignments between the reads is often the first step in many applications. For example, one may be interested in finding all pairs of reads that have a sufficiently long overlap, such as suffix/prefix overlap (for genomic or metagenomic assembly [20]), or substring overlap (for read compression [8], finding RNA sequences containing common exons [9], [17], etc.). Much of modern-day high-throughput sequencing is carried out using Illumina sequencers, which have a small error rate ( 1%–2%) and predominantly () substitution errors [16]. Thus, algorithms that tolerate a small number of mismatch errors can yield the same solution as the much more expensive alignment computations. Motivated by such applications, we formulate the following all-pair -mismatch maximal common substrings problem:

Problem 1

Given a collection 
 of  sequences with 
, a length threshold , and a mismatch threshold , report all -mismatch maximal common substrings of length  between all pairs of sequences in .

A pair of two equal length substrings 
 and 
 is a -mismatch common substring if the hamming distance between them is . Also, they are a -mismatch maximal common substring if neither 
 and 
, nor 
 and 
 are a -mismatch common substring pair.

In this paper, we present efficient solutions for this problem in both sequential as well as parallel settings. Our sequential algorithm runs in 
 expected time, where  is the output size. Our distributed memory parallel algorithm runs in 
 expected time using 
 expected communication rounds, where  is the number of processors. Here we make a reasonable assumption that the number of occurrence of any -long substring across all sequences in  is . Under this assumption, our algorithm enforces an effective partitioning of a series of modified suffix trees to localize processing within each processor. We demonstrate the scalability and performance of our parallel algorithm using genomic datasets ranging in size from 18 million to over 270 million reads, on up to 1024 processor cores.

Related work.
To solve such problems in practice, seed-and-extend style filtering approaches are often employed. The underlying principle is: if two sequences have a -mismatch common substring of length , then they must have an exact common substring of length at least 
 
. Therefore, using a fast hashing technique, all pairs of sequences that have a -length common substring are identified. Then, by exhaustively checking all such candidate pairs, the final output is generated. In the sequential setting, the filtering heuristics can be broadly classified under three categories: suffix filtering [12], [23], spaced seeds filtering [3], and substring filtering [21]. In case of parallel heuristic methods, filtering based methods mainly focused on the corresponding applications. For example, [11] and [19] proposed suffix tree based parallel clustering of EST data. Clearly, a filtering-based algorithm cannot provide any run time guarantees and often times the candidate pairs generated can be overwhelmingly larger than the final output size. Recently, [22] published the first and only known sub-quadratic exact sequential algorithm for this problem that includes insertions and deletions along with mismatches. Work done on accelerating pairwise edit distance estimations among  sequences using sequence alignment can also be applied to this problem [18]. However, for a large number of short sequences (with low error rate, mostly mismatches), all pair sequence alignment is impractical because of its quadratic time complexity.

This paper is organized as follows. In Section 2, we introduce notations and data structures used in our algorithm. Due to the absence of a provably efficient sequential algorithm for this problem, we first design such an algorithm and present it in Section 3. In Section 4, we describe the parallel algorithm in detail and prove the claimed bounds on expected time and communication rounds. In Section 5, we discuss the implementation details of the parallel algorithm. Finally in Section 6, we discuss the results of our implementation on genomic and gene expression datasets.

2. Notation and preliminaries
Let  denote the alphabet of the sequences in . Throughout this paper, both  and  are assumed to be constants. Let 
 be the concatenation of all sequences in , separated by special characters 
. Here each 
 is a unique special symbol and is lexicographically larger than all characters in . Clearly, there exists a one to one mapping between the positions in  (except the 
 positions) and the positions in the sequences in . Let  denote the identifier of the sequence corresponding to the position  in . We use  to denote the longest common prefix of strings  and , and 
 to denote their longest common prefix while permitting at most  mismatches. The reverse of a string  is denoted by 
⃖
. Therefore, 
⃖
. We now briefly review some standard data structures that will be used in our algorithms.

2.1. Suffix tree, suffix array and LCP data structures
Denote the suffix of  starting at position  by , the prefix of  ending at  by , and the substring of  starting at position  and ending at position  by . The generalized suffix tree of  (equivalently, the suffix tree of ), denoted by , is a lexicographic arrangement of all suffixes of  as a compact trie [14], [24]. The  consists of exactly  leaves, and at most  internal nodes, all of which have at least two child nodes each. The edges are labeled with substrings of . Let  refer to the concatenation of edge labels on the path from root to node . If  is a leaf node, then  is a unique suffix of  (equivalently a unique suffix of a unique string in ) and vice versa. For any node  in ,  is the number of its ancestors and  is the length of its path.

The suffix array [13], , is such that  is the starting position of the suffix corresponding to the th left most leaf in the suffix tree of , i.e., the starting position of the th lexicographically smallest suffix of . The inverse suffix array  is such that , if . The Longest Common Prefix array  is such that, for  In other words,  is the  of the lowest common ancestor of the th and th leaves in the suffix tree. There exist optimal sequential algorithms for constructing all these data structures in  space and time [10].

All operations on  required for our purpose can be simulated using , , and a range minimum query (RMQ) data structure [6] over the  array. A node  in  can be uniquely represented by an interval , where  and  are respectively the leftmost and rightmost indexes of ’s leaves in .  is the minimum value in  (can be computed in constant time using an RMQ). Similarly, the longest common prefix of any two suffixes can also be computed in  time. Finally, the -mismatch longest common prefix of any two suffixes  and  can be computed in  time as follows: let , then for any , 
 Based on these terminologies, we redefine Problem 1 as follows.

Problem 2

Given ,  and , report all tuples , such that

1.

2.

3.

The first and the second constraints ensure length threshold and left maximality (thereby right maximality) conditions, whereas the third constraint ensures that the suffixes belong to two different sequences.

3. Our sequential algorithm
3.1. The exact match case
As a warm up, we first show how to solve the exact match case () in optimal  worst case time. First create the , then identify all nodes  in  such that  and . Such nodes are termed as marked nodes. Clearly, a pair of suffixes satisfies condition (1) specified in Problem 2 iff their corresponding leaves are under the same marked node. This allows us to process the suffixes under each marked node  independently as follows: let 
 denote the set of starting positions of the suffixes of  corresponding to the leaves in the subtree of . That is, 
. Then,

1.
Partition 
 into (at most)  buckets, such that for each , there is a unique bucket containing all suffixes with previous character . All suffixes that have a $ symbol as the previous character are put together in a special bucket. Note that a pair  satisfies condition (2) specified in Problem 2 and hence a valid output pair, only if both  are  are not in the same bucket, or if both of them are in the special bucket.

2.
Sort all suffixes w.r.t. the identifier of the corresponding sequence (i.e., ). Therefore, within each bucket, all suffixes from the same sequence appear contiguously.

3.
For each , report all answers of the from  as follows: scan every bucket, except the bucket in which  belongs to, unless  is in the special bucket. Then output  as an answer, where  is not an entry in the contiguous chunk of all suffixes from .

The construction of  and Step (1) takes  time. Step (2) over all marked nodes can also be implemented in  time via integer sorting. By noting down the sizes of each chunk during Step (2), we can implement Step (3) in time proportional to the sizes of input and output. By combining all, the total time complexity is , i.e.,  for constant sized alphabet.

3.2. The -mismatch case
Standard string data structures such as suffix trees and suffix arrays can directly support only exact matching. To enable finding approximate matches, we present the following novel approach: We transform the approximate matching problem over given strings into an equivalent exact matching problem over a set of carefully crafted inexact copies of the suffixes of the original strings. Such inexact copies of the suffixes will be termed as modified suffixes. Such a solution would be obvious if we could generate modified suffixes corresponding to all possible ways of accommodating  mismatches from the original suffixes. However, this strategy is combinatorially explosive. The key to our algorithm is the specification of a bounded set of modified suffixes that nevertheless are sufficient to compute all -mismatch maximal common substrings. Details follow.

Definition 1

Let  be a special symbol not in . A -modified suffix is a suffix of  with  of its characters replaced by .

Let  be a set of positions. Then, 
 denotes the -modified suffix obtained by replacing  positions in the suffix  as specified by . For example, let , , then  and 
.

Our algorithm consists of two main phases. In the first phase, we create a collection of sets of -modified suffixes. In the second phase, we independently process each set and extract the answers. The first phase takes 
 time, where as the second phase takes 
 time, where  is the height of . It is known that the expected value of  is  [4]. Therefore, by combining the time complexities of both phases with  replaced by , we obtain the expected run time as claimed. We now describe these phases in detail.

3.2.1. Details of phase-
This phase is recursive (with levels of recursion starting from  up to ), such that at each level , we create a collection of sets of -modified suffixes (denoted by 
) from the sets in the previous level. At level , there is only a single set 
, the set of all suffixes of . See Fig. 1 for an illustration. To generate the sets at level , we take each set 
 at level  and do the following:

•
Create a compact trie of all strings in 

•
For each internal node  in the trie, create a set consisting of the strings corresponding to the leaves in the subtree of , but with their th character replaced by . Here  is . Those strings with their -th character is a 
 symbol are not included.

Property 1

All modified suffixes within the same set (at any level) have  symbols at the same positions and share a common prefix at least until the last occurrence of .

Property 2

For any pair  of positions, there will be exactly one set at level , such that it contains -modified suffixes of  and  with  symbols at the first  positions in which they differ. Therefore, the  of those -modified suffixes is equal to the 
 of  and .

We have the following result about the sizes of these sets.

Lemma 1

No set is of size more than  and the sum of sizes of all sets at a particular level  is 
, where  is the height of .

Proof

The first statement follows easily from our construction procedure and the second statement can be proved via induction. Let 
 be the sum of sizes of all sets at level . Clearly, the base case, 
, is true. The sum of sizes of sets at level  generated from 
 is at most 
 the height of the compact trie over the strings in 
. The height of the compact trie is , because if we remove the common prefix of all strings in 
, they are essentially suffixes of . By putting these together, we have 
.  □

Space and Time Analysis: We now show that Phase-1 can be implemented in  space and 
 time in expectation. Consider the step where we generate sets from 
. The lexicographic ordering between any two -modified suffixes in 
 can be determined in constant time. i.e., by simply checking the lexicographic ordering between those suffixes obtained by deleting their common prefix up to the last occurrence of . Therefore, suffixes can be sorted using any comparison based sorting algorithm. After sorting, the  between two successive strings can be computed in constant time. Using the sorted order and  values, we can construct the compact trie using standard techniques in the construction of suffix tree [5]. Finally, the new sets can be generated in time proportional to their size. In summary, the time complexity for a particular 
 is 
. Overall time complexity is 
By replacing  by , we bound the expect run time of Phase-1 by 
.

We generate the sets in pre-order of the corresponding node in the recursion tree. As soon as a set (at level ) is generated, we immediately pass it to Phase-2, extract the necessary information and discard it from the working space. Also, any set at level  is also deleted after all -level sets in its subtree are processed. This way, at any point of time in the execution of the algorithm, we need to maintain only  sets, corresponding to the sets in a root to leaf path in the recursion tree. Since the size of each set is at most  (Lemma 1), we can bound the working space also by , assuming .

3.2.2. Details of phase-2
In this phase, we seek to process each set 
 created by Phase-1 independently and generate the answers in time linear to the total size of all sets and output. i.e., 
. We first present a simple 
 time approach. Following are the key steps.

1.
Create a compact trie over all -modified suffixes in 
. Then identify the marked nodes as before. Recall that a marked node has a , where as the  of its parent is .

2.
Let  be the set of  positions corresponding to modifications in the -modified suffixes in 
. Clearly, if the leaves corresponding to two modified suffixes (say 
 and 
) are in the same subtree of a marked node, then their 
 is . If  and , then report  as an answer.

The trie can be created in time linear to the size of 
. Note that the key step in the creation of a trie is the sorting of -modified suffixes. To do it efficiently, we map each -modified suffix to the lexicographic rank of the suffix obtained by removing all characters (from left) until its last  symbol. Using this as the key, the -modified suffixes can be sorted via a linear time integer sorting. The second step of extracting answers can also be implemented using the exact same procedure described in Section 3.1. However, the problem with this approach is that, a pair  can get reported more than once, although only once per set. In the worst case, an answer can get reported 
 times. The resulting time complexity is therefore 
.

3.2.3. Improving the run time complexity via bucketing
To achieve the claimed 
 run time, we need to ensure that each output  is reported exactly once. For this, we exploit Property 2 as follows: while processing a pair of two -modified suffixes 
 and 
 under the subtree of some marked node, report  as an answer iff

1.
.

2.

3.

From Property 2, for a pair , there will be only one pair of -modified suffixes satisfying this condition (1). The following is unique to that pair:  for all . Therefore, the task can be executed efficiently by processing the set of -modified suffixes in the subtree of each marked node  as follows:

1.
Partition them into (at most)  buckets based on the previous character as in Section 3.1.

2.
Partition the -modified suffixes in each bucket into (at most) 
 sub-buckets based on the sequence of  characters that were originally at the positions in . Each sub-bucket is therefore associated with a unique string of length : the (previous) character corresponding to the bucket in which it belongs to, followed by the sequence of  characters at the positions in .

3.
Within each sub-bucket, sort the -modified suffixes based on the identifier of the sequence to which it belongs.

4.
Finally, for each 
, we visit each sub-bucket and find answers of the form  as follows: let 
 be the sequence of  characters corresponding to a sub-bucket. If 
 or  is some 
 symbol and 
 for , then for all entries 
 in the sub-bucket with , report  as an answer. Notice that the entries within a sub-bucket are sorted according to the sequence identifier. Therefore, all entries with  comes together as a contiguous chunk, which can be easily skipped.

Analysis.
The overall time for implementing the first three steps is 
 and final step takes 
 time. Therefore, total time complexity is 
, assuming  and  are constants.

Theorem 1

Problem 1 can be solved in  space and 
 expected time, assuming  and  are constants.

Note: If the hamming distance between two reads is , then our algorithm will not output them as an answer. To capture such answers, we run the algorithm for all numbers of mismatches starting from  up to . The run-time remains the same.

4. Our parallel algorithm
In this section, we show how to extend our ideas to obtain a provably efficient parallel algorithm. We assume that the input set of strings , or equivalently their concatenated string , is distributed across the  processors such that each processor has the same number of total characters. Note that a maximal common substring with -mismatches is essentially a concatenation of  maximal exact matches separated by  mismatch positions in between. Among the various ways the  mismatches can be positioned in a substring of length , the shortest maximal exact match occurs when the  mismatch positions are uniformly distributed. This leads us to the following observation.

Observation 1

If two sequences 
 have a -mismatch maximal common substring of length , then within that region, there is a maximal exact match of length 
 
.

In other words, a -mismatch maximal substring of length  contains at least one maximal exact match of length . Depending upon the position of that match among the  matching segments, we categorize answers into different types: An answer  is of type- () if the th exact match segment is of length . Fig. 2 illustrates the different types of output for .

We present our algorithm in terms of the generalized suffix tree () of  (i.e., a suffix tree of ). However, in the actual implementation, the required operations are equivalently carried out using  and the  data structures. While the  view adds greatly to presentation clarity, its implementation using , , and  leads to efficient implementation. Using the recent result by Flick and Aluru [7], we construct these data structures in  time, where  is the time for parallel sorting of  numbers using  processors. Flick and Aluru’s [7] algorithm is based on a non trivial adaptation of the classic prefix doubling algorithm by Manber and Mayers [13]. After this step, each processor gets a contiguous chunk of size roughly  of each array, i.e., for , the th processor 
 gets the chunk corresponding to the range . Additionally, we maintain all the above data structures for the reverse of , denoted by 
⃖
. The respective data structures for 
⃖
 are denoted by 
 and 
.


Download : Download high-res image (42KB)
Download : Download full-size image
Fig. 2. Illustration of output types for .

Using a distributed , our algorithm first identifies all the set of suffixes having the initial exact match of  characters. It accomplishes this by selecting all the internal nodes  of  such that  and the  of ’s parent is  (Section 4.1). After re-distributing the chosen internal nodes across  processors, we process the nodes independently (Section 4.2) and generate the suffix pairs satisfying the three constraints (Sections 4.3 Generating type-, 4.4 Generating type-). Since  is stored in a distributed fashion, independently processing these internal nodes requires information about suffixes that are not stored locally. To retrieve the requisite information, we use rounds of all-to-all communication. The key reason for using this strategy is that independent processing of the suffix tree nodes localizes the computation and hence provides good performance by minimizing the communication overheads.

4.1. Load balancing via re-distribution
After construction of the data structures as mentioned above, the next step is re-distribution, where we partition the  and distribute subtrees of the  to the  processors. We term a  node  as primary if  and . Alternatively, the first node of a root to leaf path that has a  is a primary node. See Fig. 3 for an illustration. We assign each primary node, along with all suffixes corresponding to the leaves in its subtree, to a single processor, in such a way that the total number of suffixes associated is approximately the same across all processors. We make the following reasonable assumption:  is sufficiently long in practice, such that the frequency of occurrence of any -long substring across all sequences in  is  (infact  for any constant  works). Under this assumption, we can easily perform re-distribution in such a way that the total number of suffixes associated with any processor is . Note that the redistribution amounts to merely adjusting the boundaries according to which the sorted arrays are partitioned across the processors.

Each primary node  is processed by the processor assigned to it. We seek to achieve the following: Let  and  be two suffixes under a primary node , which is assigned to processor , and  be the length of their . Then, if  and  correspond to a maximal exact matching segment (which is clearly of length ) of an answer 
, pair 
 will be reported as an answer while processing the subtree rooted at  by . We now present the details.


Download : Download high-res image (94KB)
Download : Download full-size image
Fig. 3. Re-distribution of the primary nodes of .

4.2. Processing of a primary node
We present an overview of our strategy using a small example. Let  be a set of three strings 
 and 
. Let 
, 
 and 
 be the strings ,  and  respectively. Given the input set , all -mismatch maximal common substring pairs of length at least 13 are desired. The expected output pairs for this example are 
 and 
. Clearly, the first pair is a type- output, while the second pair is a type- output. Based on  Observation 1, the -mismatch maximal common substring pairs should have a maximal exact match of at least  characters. In the case of our example, this exact match is CAGGTACA and it occurs in suffixes 
, 
 and 
. Therefore, there exists a primary node  in the generalized suffix tree of , which includes the three suffixes and its  is .

After selecting the primary node , the first output pair 
 can be generated from  by matching the modified suffixes 
 and 
, where 
. In case of the second output pair 
, the modified position should be to the left of the initial exact match. This case can be handled by matching the modified suffixes 
⃖
 and 
⃖
, where 
. This example shows the differences in the way modified suffixes can be used to generate different types of output pairs.

We now illustrate the procedure to process a primary node  using Fig. 4. The procedure is recursive and is similar to that in Section 3.2.1, except that the set 
 at the root denotes the set of all suffixes corresponding to the leaves in the subtree of . Recall that any string within a set at level  is a suffix in 
 with  of its characters replaced by . All modified suffixes within a set share a common prefix of length at least up to the last position of modification. Also, by a straightforward generalization of Property 2, for any two suffixes 
 and for any level , there will be a unique set containing two modified suffixes 
 and 
 such that  is the set of first  positions in which  and  differ, i.e., 

From now onwards, we use the following terminology. The collection of sets at level  is termed an order- universe of the initial set 
, and its total size is 
 (refer to Lemma 1). Also, each set at level  is termed a partition of the order- universe of 
. In subsequent sections, we show how to process the partitions and generate answers. Specifically, type- answers are generated while processing the partitions at level , whereas answers of any other type, say , are generated while processing the partitions at level . We start with the simpler case of type-.

4.3. Generating type- answers
The partitions at level  are processed independently of each other. To process any partition 
, first create a compact trie of all modified suffixes within 
. Then, identify each node in the trie whose  is , whereas the  of its parent is . We call such nodes as secondary nodes. Then, for any two modified suffixes 
 and 
 corresponding to the leaves in the subtree of a secondary node , 
. Therefore, to check if  is an answer, it is enough to check if  and . To do this efficiently, we apply the bucketing strategy described in Section 3.2.3 over the set of suffixes corresponding to each secondary node .

4.4. Generating type- answers for 
We first present the key intuition behind our algorithm for generating type- answers. Recall that in a type- answer 
 with  and  as the starting positions of the th exact match common segment, the  of  and  should be at least . In other-words,  and  must be suffixes corresponding to the leaves in the subtree of some primary node. Therefore, while processing a primary node , we are looking for  pairs with the following constraints:

•

•
Let 
⃖
⃖
 Then, 

The type- answer corresponding to the above constraints is 
. In order to efficiently generate these pairs, we process each partition 
 as follows. We use  to denote the corresponding set of  positions of modifications.

1.
Create a compact trie of all modified suffixes in 
. For each internal node  in the trie (we denote the  by ), we apply steps 2 and 3 described below.

2.
Let 
 be the set of modified suffixes corresponding to the leaves in the subtree rooted at . Create another set 
 of reverse prefixes w.r.t. 
 as follows: 
⃖

3.
Process each such 
 as follows:

(a)
Create an order- universe of 
 and process each partition  in it as described in steps 3b to 3d.

(b)
Create a compact trie of  and identify the secondary nodes within it. A node is secondary if its  is 
, where as the  of its parent is 
, where 
. Let 
 be the positions of modifications of the modified suffixes in .

(c)
Note that if 
⃖
 and 
⃖
 correspond to the leaves in the subtree of a secondary node,  and , then clearly 
 is an answer, where 
 is the  of 
⃖
 and 
⃖
. Using an exactly similar bucketing technique described in Section 4.3, we can process  in time proportional to  and the number of answers generated from .

(d)
Finally, we note the following key point: an answer 
 may get generated from multiple partitions. However, in exactly one partition, the positions specified in  and 
 correspond to mismatches (from Property 2). Therefore, to avoid reporting the same answer 
, we make sure that positions in  and 
 are mismatch positions. We also check that  to ensure right maximality.

We now prove the correctness of the above procedure with respect to a fixed type- answer 
 with  and  being the starting positions of the th maximal exact common substring which is sufficiently long (i.e., length ). As mentioned in Section 4.2, there exists a partition 
 where the positions of modifications (i.e., ) are the first  mismatch positions between  and . While creating the compact trie of this particular partition 
, there will be a node  from which the modified suffixes 
 and 
 diverge, i.e., , where 
 and this ensures the right maximality of the match. With the condition , we are imposing the th mismatch at these positions. Now, while creating the th order universe out of 
, there will again be a unique partition (say ) such that the modifications (denoted by 
) are at the first  positions in which 
⃖
 and 
⃖
 differ. Finally, we report the answer 
 uniquely while processing this specific partition.

Note that an answer can belong to multiple types. Therefore while processing and reporting a type  answer, we simply check if it belongs to another type 
. If so, we do not output it as a type- answer.

4.5. Bounding the run time
After the initial construction of the  and its re-distribution, the remainder of our algorithm localizes computations within each processor. This is the key reason for devising the complex strategy outlined, which should provide good performance by minimizing the communication overhead. However, as a preparatory step to execute the primary nodes assigned to a processor, it needs to create the data structures , , etc. for the suffixes in the underlying subtree. While these can be locally computed, the additional time for doing so can be saved by deducing them from the corresponding globally distributed arrays (, , etc.) for the set of strings . We rely on rounds of all-to-all communication to achieve this. In this section, we prove that the expected computation time is 
. In the next section, we bound the expected number of all-to-all communication rounds to 
 and the expected communication cost to 
. Adding together the computation and communication costs, we obtain the claimed run time of 
 
 in expectation.

The key steps involved in our parallel algorithm are (i) recursive construction of sets of modified suffixes (partitions), and (ii) processing of some of these partitions to generate the outputs. The first part involves the construction of compact tries, whereas the second part involves bucketing, followed by sorting of modified suffixes w.r.t.  and scanning the buckets to generate the outputs. The construction of compact tries consists of two key steps: (i) arrange the modified suffixes in their lexicographic order, and (ii) compute the  between every pair of consecutive modified suffixes. The topology of the trie can be inferred in additional linear time [5].

Note that in our case, the given set of modified suffixes have a common prefix at least up to the last position (say ) of modification. Therefore, they can be sorted with respect to the rank (inverse suffix array value) of the corresponding suffix obtained after removing the first  characters. The pertinent inverse suffix array values can be obtained by querying on  (the required communication cost will be bounded in the next section). With such a reduction, the task of (modified) suffix sorting can be reduced to integer sorting, for which standard linear time algorithms can be used. Computing the  values also requires a linear number of queries on the distributed arrays. In summary, the compact tries needed throughout the execution of the algorithm can be constructed locally in time linear in the number of modified suffixes involved.

We now bound the total time for construction of the compact tries on any processor. Let  be a primary node of size . Let  and 
 be the heights of the suffix trees of  and 
⃖
, respectively. Then, the total of the number of modified suffixes and modified reverse prefixes generated while processing  is at most (refer to Lemma 1) 

Therefore, using our linear time trie construction procedure, the time for construction of all compact tries w.r.t. node  is 
. Since the sum of subtree sizes of all primary nodes assigned to a processor is bounded by , the worst case run time on any processor is 
.

The second part in processing a set involves bucketing followed by sorting and scanning. Here also, we use linear time integer sorting algorithms. The time for scanning and generating output pairs takes time linear to the size of the sets and the number of outputs generated. However, we may encounter the same answer while processing multiple sets (at most 
 in number), although we report it only once. Therefore, we bound the total time by 
. By replacing  and 
 by , the expected height of a suffix tree of a string of length  [4], we obtain the claimed run time.

4.6. Bounding the communication costs
Since the  is distributed, the construction of the compact suffix tries required gathering relevant portions of the global data structures in a collective operation involving all the  processors. As mentioned in Section 4.5, the recursive construction of sets of modified suffixes and their compact tries require queries on the distributed array data structures. To answer such queries efficiently, we rely on all-to-all communication. Here we make the following standard assumptions on available memory and bandwidth: The number of elements that can be communicated in and out of a processor is at most  in a single round of collective communication. Given this assumption, we claim that the total number such rounds required to satisfy all  queries is 
 in the expected case.

First, we bound the communication costs w.r.t. the number of outgoing elements from a processor. The number of  queries required by a processor is equal to the number of modified suffixes (or modified reverse prefixes) it handles, which is 
 in expectation. As per our assumption, the number of elements that can be accommodated in a round is only . Therefore, the number of rounds of collective communication required is 
 in expectation. Bounding the communication costs w.r.t. the number of incoming elements to a processor is slightly tricky. We use the following key result [4]: The expected length of the longest repeat within a string of length  is . In other words, let 
, then the expected value of  is . Also, the  of any two -modified suffixes is . In order to bound the communication costs w.r.t. the number of incoming  queries, it is sufficient to prove the following.

Lemma 2

For any fixed , the number of  queries is 
 in expectation.

Proof

Recall our recursive algorithm for creating partitions. While processing some node  (with  being ) in a trie, an  will be queried iff there exists a modified suffix (with starting position ) in the subtree of , such that . The number of distinct such ’s is bounded by the maximum possible value of , which is . Additionally, the number of modified suffixes with a fixed starting position  is 
. By putting all together, the number of  queries is 
. By substituting  for  and , we obtain the claimed bound.  □

Using similar arguments, the number of 
 queries for any fixed  can also be bounded by 
 in expectation. Therefore, the number of rounds of all-to-all communications needed is 
 in expectation.

Note that  elements are communicated between the  processors during each round of communication. Assuming that it takes a constant time to transfer an element from one processor to another, the total time taken by the communication rounds is bounded by 
 
 in expectation.

4.7. Space complexity
 is implemented as distributed , , , 
, 
 and 
 arrays. All of these arrays require  space in each processor. Local  and 
 data structures take  space, while  and 
 arrays take  space per processor.

If all the tries constructed, described in Section 4.4, are processed simultaneously, then the space required to store all of the tries is bounded by runtime complexity, as derived in Section 4.5 without the  term i.e., 
. The term for  is not included because the output pairs need not be retained in main memory and can be written out to standard output or disk storage.

However, if we process the tries in multiple batches, with total size of  per batch in a processor, then the space complexity analysis follows the communication bounds discussed in Section 4.6. In this case, the space required is bounded by  per processor per batch with a total of 
 batches. This also helps us to derive a bound on the number of batches required when the space available per processor is limited. If the space available per processor is bounded by , then the algorithm requires 
 batches of size  each.

5. Implementation details
We implemented our algorithm using C++ and MPI. We use block-wise distribution for the distributed SA and LCP arrays. We refer to the elements located within a processor as its ‘local elements’ or ‘local array’.

5.1. Representation of  by distributed data structures
We represent the generalized suffix tree () of all the suffixes of the strings 
 using the following distributed data structures.

1.
Distributed , , and  w.r.t. .

2.
Distributed 
, 
 and 
 w.r.t. 
⃖

3.
Local  and 
 data structures to answer range minimum queries for the local  and 
 arrays respectively, in every processor.

4.
To enable distributed range minimum queries, we maintain in every processor an array of size , , where  has the minimum value of processor ’s local . Similarly 
 is constructed for 
 array.

For  and  arrays, we use 64-bit integers. The  array is implemented as a byte array because its entries in the worst-case are limited by the length of the reads, and high throughput sequencing reads have short lengths that can fit in a byte.

5.2. Representation of compact suffix tries
As described in Section 4.2, -modified suffix sets are generated by a recursive construction of compact suffix tries. We represent a compact suffix trie by two arrays: (a) the sorted order of the suffixes that constitute the compact trie, and (b) LCP array, which contains the lengths of the longest common prefixes between every pair of consecutive suffixes.

5.3. Representation of internal nodes
An internal node  can be represented with the tuple  for both the  and the compact suffix tries constructed at the lower levels of recursion. For an internal node  in the , . For an internal node in the suffix trie generated for a set 
,  is the length of the ( - )-modified suffix matched thus far.

To save space, we use the following representation: , where . The advantage of this representation is that we can use a 32-bit integer for , even when ’s size exceeds the limit of unsigned 32-bit integers. This is because for most datasets, the largest of the primary nodes includes only few tens of million suffixes (less than the size limit of a 32-bit integer). Also, both  and  are bounded by the maximum length of the input sequence, and hence, we use a byte each for both  and . In total, each internal node takes only 14 bytes in this representation.

5.4. Construction of distributed 
After constructing  and  using [7], we construct the  array corresponding to  by an all-to-all communication of the pairs  for each  entry. We then construct local  table (an implementation of [6]) in each processor, corresponding to the local  array. We then construct  by gathering all processor  minimums. We also build a local RMQ table on the  array.

We repeat the above steps to construct 
, 
, 
, 
 and 
 w.r.t. 
⃖
. Not including the communication costs to construct SA and LCP, only a constant number of collective communication operations is required.

5.5. Selection of primary nodes
We select the regions of interest in  by scanning the  array to select those regions where  values are . All primary node leaves should be from one of these regions. If a selected region straddles two processors, this region is shifted to the lower ranked processor so that any such region is completely contained within a processor. As noted earlier in Section 4.1, our assumption about the distribution of -length prefixes will limit the region size per processor to . Note that only  and  arrays are shifted. , , 
, and 
 tables remain distributed as they were constructed.

In order to identify all the internal nodes from the selected regions, we use the all-nearest-smaller-values (ANSV) solution [2] with respect to the  array. Left-to-right and right-to-left ANSV solutions of the  array produce the left most () and the right most () indexes of the internal nodes respectively. After collecting the internal node tuples generated by the ANSV solution, we sort and remove the duplicate entries.

5.6. Batch processing of ’s internal nodes
After selecting the internal nodes of , we process these nodes in batches for two reasons. One, the memory available per processor is not large enough to accommodate all the suffix tries constructed. Two, the number of elements that can be communicated in a single all-to-all operation is limited in many MPI implementations, and hence the number of elements that can be queried and answered in distributed ISA and range minimum queries is limited. This limit depends upon the MPI implementation.

We process the internal nodes in batches of size . We choose  to be more than the size of the largest primary node. We partition the nodes assigned to a processor into batches of size at least  and at most . Note that even if some processors complete all their batches earlier than others, they have to participate in the collective operations to answer the ISA and range minimum queries addressed to them.

In order to achieve better load balancing among batches, the batches are expected to have approximately uniform sizes across all the  processors. As described in Section 5.5, internal node tuples are identified by the ANSV solution and the duplicate entries are eliminated by sorting. Internal nodes are added to the batches in this final sorted order. There are two ways to sort these tuples — either by  or by . We found that sorting by  first produces more uniform batch sizes across processors compared to sorting by , and hence better run-time performance.

5.7. Construction of compact tries
Given an input set of suffixes 
, construction of the compact trie is accomplished by building the corresponding SA and LCP arrays.  queries are used to rank the suffixes in the trie’s SA, while range minimum query () results are used to construct the trie’s LCP array. We only describe these queries in relation to a 
 set generated during a rightward extension, i.e., modified suffix sets in generating type- outputs. In case of 
 being a part of order- universe while processing 
 (Section 4.4), we follow a similar procedure except that instead of  and , we use 
 and 
.

5.7.1. SA of the compact trie
Note that 
 suffix sets are generated under the context of some internal node . After generating 
 from an internal node , the rank of these suffixes is queried from the distributed . We preform one distributed  query for all suffixes of 
 at the current recursion level of the current batch as follows.

1.
An all-to-all communication, where each processor sends all its 
 suffixes in the current batch as queries to the processors owning the  entries.

2.
An all-to-all communication, where each processor returns the  values back to the processor from which the queries were received.

After receiving the  query results, we construct the trie’s SA, and sort the SA entries based on their  rank. Since there is a constant number of all-to-all communications required for a round of suffix trie construction, communication complexity described in Section 4.6 remains valid.

5.7.2. LCP of the compact trie
After constructing the SA w.r.t. 
’s trie, we construct the LCP array by distributed range minimum queries as below.

1.
Partition the entries of all the suffix arrays constructed in the previous step into  parts, each part corresponding to the processor owning the  values of the SA entries.

2.
An All-to-all communication to send the range minimum queries (i.e.,  indexes of interest).

3.
An All-to-all communication to send/receive the range minimum query results. For a trie,  results sent from a processor include the following:

(a)
The length of longest common prefixes between two consecutive suffixes of the trie’s SA.

(b)
The minimum value between the left most (corresp. right most)  entry in the queried processor and the left most (corresp. right most) suffix of the trie’s SA entry in that processor.

In distributed range minimum queries, the above results are sent for all the tries generated while processing the current batch.
Based on the  results and the local  tables, the LCP values between two consecutive suffixes in the SA corresponding to the trie for 
 can be computed. The number of elements queried and answered is bounded in a similar manner to the ISA queries, and hence communication complexity remains the same.

5.8. Generation of maximal common substring pairs
After we construct the suffix tries using the procedure described in Section 5.7, we generate the valid pairs as given in Sections 4.3 Generating type-, 4.4 Generating type- using the -modified suffix sets. However, in order to find which one of the  buckets a (modified) suffix belongs to, we query the distributed array  to identify the preceding character. Hence, maximal common substring pair generation adds an extra round of all-to-all communication to answer the queries against .

6. Experimental results
We ran our experiments on an Intel Xeon Infiniband Cluster. Each node has a 2.7 GHz -core Intel Xeon  processor with  GB of main memory and is running RHEL7.6 operating system. The nodes of the cluster are interconnected with EDR ( Gbps) InfiniBand. Experiments were conducted on up to 64 nodes using 16 cores per node, totaling 1,024 cores. We evaluated our algorithm on three different datasets, detailed in Table 1. Dataset D2 (NCBI SRA Accession Number SRR2984882) consists of 18.4 million reads, sampled from the genome of Yeast (S. cerevisiae). Datasets D1 (Accession Number SRR2891093) and D3 (Accession Number SRR3169276) are human RNA-Seq datasets, which are randomly sampled from expressed portions of the genome (RNA sequences produced by the genome). The whole genome dataset is sampled uniformly at random over the entire length of the genome. While the sampling is uniformly random over RNA sequences too, the frequency of each RNA sequence itself is proportional to the expression of the corresponding gene. Hence, these datasets constitute a highly non-uniform sampling of the underlying genomic space. The datasets also cover three different Illumina NGS sequencers — HiSeq 2000, HiSeq 2500 and the desktop sequencer NextSeq 500.


Table 1. Datasets used for experiments.

D1	D2	D3
Accession	SRR2891093	SRR2984882	SRR3169276
Type	RNA	DNA	RNA
Organism	H. sapiens	S. cerevisiae	H. sapiens
Source	HEK293T	WGS	Stem cells
Sequencer	NextSeq 500	HiSeq 2000	HiSeq 2500
No. of Reads	60,100,561	18,415,332	272,462,716
Read length	75	101	151
Total size	4.507 Gbp	1.860 Gbp	38.417 Gbp
Dataset Sizes after pre-processing
D1	D2	D3
No. of Reads	21,682,850	8,263,882	116,295,542
Data size	1.626 Gbp	0.835 Gbp	17.560 Gbp
Input size	3.154 Gbp	1.535 Gbp	34.318 Gbp
Reads may be redundant in the sense that a read may be fully contained in another. Clearly, such a contained read shares potentially the same overlaps with other reads as the read containing it. Hence, including the contained reads in the input is not useful. It also significantly increases the output size and run-time without adding any additional value. Hence, we developed a pre-processing algorithm to eliminate all redundant reads. Also, RNA-Seq reads are characterized by consecutive A’s at the end, covering the poly-A tail of the mRNA molecule. Common substrings that contain these have no biological relevance, hence we trim the poly-A tail from the reads to avoid generating spurious output pairs.

Note that the input size is approximately twice that of the dataset size. This is because DNA is double stranded, with the two strands being reverse complements (under ,  substitution) of each other. The input for each sequence consists of any one of its strands. To correctly infer a maximal common substring between two sequences given as complementary strands, the input must take both forms of the sequence into account, thus doubling its size. However, after removing duplicates that were introduced by the addition of reverse complement sequences, the input size is slightly less than that of the dataset size.


Table 2. Runtime in seconds vs No. of cores for D1 with  and .

No. of cores		
Runtime (sec)	Relative speedup	Runtime (sec)	Relative speedup
64	947.81	1.00X	12005.6	1.00X
128	587.38	1.62X	7304.82	1.64X
256	280.21	3.38X	3428.42	3.50X
512	154.15	6.14X	1953.64	6.18X
1024	114.24	8.29X	1285.06	9.34X
No. of cores		
Runtime (sec)	Relative speedup	Runtime (sec)	Relative speedup
64	849.78	1.00X	10463.70	1.00X
128	532.09	1.59X	6828.73	1.53X
256	254.72	3.34X	2975.68	3.52X
512	141.88	5.98X	1699.56	6.16X
1024	102.05	8.32X	1097.10	9.53X
No. of cores		
Runtime (sec)	Relative speedup	Runtime (sec)	Relative speedup
64	792.48	1.00X	9229.78	1.00X
128	423.32	1.87X	4766.11	1.93X
256	236.61	3.35X	2639.18	3.49X
512	124.49	6.36X	1388.58	6.64X
1024	91.02	8.71X	991.36	9.31X
The value of  depends upon the application, error rate of the sequencing machine, and the read length. To demonstrate the scalability of our proposed algorithm, we ran experiments with multiple values of  and . For dataset D1, we ran experiments with ,  and . For the longer read datasets D2 and D3, we used  and  respectively. For all the chosen values of , we found that the largest subtree under an internal node at depth  or greater is significantly smaller than , validating the assumption used in deriving the run-time complexity. Another interesting observation is that between 15% and 35% of the leaves do not belong to any primary nodes (i.e., they are in the subtrees of internal nodes with  less than ). These are not considered for processing by the algorithm, hence eliminated.

The run-times for dataset D1 as a function of the number of processor cores are shown in Table 2. To illustrate the run-time behavior of the algorithm, the times are shown for  and , and for three different values of . The run-time grows exponentially with , and this behavior is reflected in the significant increase in run-time for  over . However, given the improving error rates of sequencing machines, applications typically use very small values of . As the value of  increases, the number of -mismatch maximal common substrings that satisfy the length criterion () decreases, thus reducing the run-time.

The run-times for all parameter choices of  and , as the number of processor cores is varied from  to 1,024, demonstrate reasonably good, but not perfect, scaling. To better illustrate the scaling, the relative speedups when compared to the run-time on  cores are also shown. For , the relative speedup on 1,024 cores ranged from 8.29X to 8.71X. This slightly improves to –X, for .

A similar set of experimental observations for dataset D2 with  and  are shown in Table 3. The reason for less than ideal speedup is due to the imbalance in the work assigned to the processor cores. While we make sure that every batch has approximately same size in our batch-wise processing, the distribution of internal node sizes is not uniform. We profiled the runs of datasets D1 (with ) and D2 (with ) using the hpctoolkit software [1]. hpctoolkit uses the CPU TIME timer on Linux to profile the code and sampling at a frequency of 200 samples per second per process. Using the profiler results, we compute, among the total CPU time, the total time spent by all processes waiting for the results of their distributed  and  queries (Table 4). This time can serve as a proxy for the imbalance in the work assigned to a processor. As the number of cores increases, we note that there is significant increase in the waiting time, while the total non-waiting time remains approximately the same except for  with 1024 cores. In this case, the additional time is due to the communication costs during the construction of .


Table 3. Runtime in seconds vs. No. of cores for D2 with  and .

No. of cores		
Runtime (sec)	Relative speedup	Runtime (sec)	Relative speedup
64	624.67	1.00X	11506.40	1.00X
128	359.86	1.73X	6449.24	1.78X
256	189.89	3.29X	3688.41	3.11X
512	108.76	5.74X	2082.66	5.52X
1024	87.72	7.12X	1580.13	7.28X
No. of cores		
Runtime (sec)	Relative speedup	Runtime (sec)	Relative speedup
64	593.01	1.00X	10305.10	1.00X
128	341.07	1.74X	5750.79	1.79X
256	186.95	3.17X	3328.68	3.09X
512	103.22	5.74X	1887.19	5.46X
1024	86.85	6.82X	1526.13	6.75X
Even though the input size for D1 is larger than D2, Fig. 5 shows that the ratio of time taken for  to the time taken for  is smaller for D1 compared to D2. For D1,  takes 10–12X longer than , while it is 17–20X longer for D2. This is due to difference in coverage of the underlying sampled space of the genome by the reads, in the respective datasets. D2 is a whole genome sequencing dataset for Yeast with a genome length of 
 base pairs. Coverage of the genome by the reads, defined as the ratio of the total length of the reads () to the length of the genome, is 68X.

While it is not customary to define coverage for RNA-Seq datasets as it varies for each expressed portion depending on the rate of expression, we can use the average of these individual coverages as an indicator of the depth of sampling. The total coding region within the human genome (
) is 2%, which is sampled in RNA-seq datasets. This leads to an average coverage of 24X coverage for dataset D1. The lower the coverage, the fewer the number of overlaps between input sequences. Therefore, the height of  is likely to be smaller, and hence, faster run-time. This particular difference in the datasets also has an effect on speedup, i.e., D1 exhibits better scaling than D2.


Download : Download high-res image (312KB)
Download : Download full-size image
Fig. 5. Runtime vs No. of cores for datasets D1 and D2 with  and  respectively.


Table 4. Profiling for datasets D1 and D2 with  and . Total CPU time, query waiting time and the non-waiting time are presented in micro seconds.

D1 with 
No. of cores		
Total CPU time  (μ)	Query waiting time  (μ)	Non-waiting time  (μ)	Total CPU time  (μ)	Query waiting time  (μ)	Non-waiting time  (μ)
64	5.9 
6.9 
5.2 
7.5
2.0 
5.5 
128	7.0 
2.0 
4.9 
8.3
2.8 
5.4 
256	7.7 
2.7 
5.0 
9.5
4.0 
5.5 
512	8.6 
3.1 
5.4 
1.0
4.9 
5.5 
1024	1.5 
8.6 
6.4 
1.7
1.1 
5.7 
D2 with 
No. of cores		
Total CPU time  (μ)	Query waiting time  (μ)	Non-waiting time  (μ)	Total CPU time  (μ)	Query waiting time  (μ)	Non-waiting time  (μ)
64	4.3
1.0
3.2 
8.0 
3.4
4.6 
128	4.6
1.4
3.1 
9.8 
5.2
4.6 
256	5.5
2.1
3.4 
1.1 
7.1
4.6 
512	6.7
2.8
3.8 
1.4 
9.2
4.7 
1024	2.2
1.7
4.9 
2.1 
1.6
4.8 
For , the run-times are too long to record observations on fewer cores, underscoring the importance of the parallel algorithm. Table 5 shows the run times on 1,024 cores for datasets D1 and D2. As we can see from the third column in Table 5, the ratio of run-times for  and  is roughly in line with the increase from  to . This reflects the exponential increase in run-time as a function of .


Table 5. Runtime for  and  with  and .

Dataset		Time (sec)	
 
D1	15	18535.0	14.42
D1	17	14898.2	13.57
D1	19	12207.5	12.37
D2	20	38241.7	24.20
D2	22	33502.4	21.29
In order to push the limits of our algorithm, we ran dataset D3 containing over  million reads with longer read length of , whose total size is an order of magnitude larger than D1. For , we were able to process such a large dataset in under 1.5 h (50001.29s) on 1,024 cores.

7. Conclusion
Approximate sequence matching algorithms are of significant interest in computational biology as replacement for quadratic alignment-based algorithms, particularly as high-throughput sequencers are producing large-scale datasets. In this paper, we presented a parallel algorithm for finding -mismatch, all-pair maximal common substrings between a large set of strings. While the only sub-quadratic sequential algorithm to solve this problem is the one recently proposed by [22], there is no parallel algorithm to solve this problem to date. Our work achieves an expected parallel run-time complexity of 
 
, where  is the number of such reported maximal common substrings. We note that for  this reflects the best possible run-time for computing exact all-pair maximal common substrings, while the run-time degrades slowly by a factor of  for each additional error tolerated.

We also present our algorithm for the practical distributed memory model of parallel computation, and demonstrate its performance on real, large-scale datasets. While the scaling results are constrained by the size of the parallel computer available to us, we see no difficulty for the algorithm to scale beyond the 1,024 cores it is demonstrated on. The algorithm is useful in identifying overlaps between Illumina sequencer reads which typically contain a small rate of substitution errors. As these sequencers account for over 90% of DNA and RNA sequencing worldwide, the algorithm could have significant impact. We are currently exploring the use of this algorithm for applications in genome mapping and assembly.