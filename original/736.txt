Abstract
We analyze time and energy performance of distributed computations in heterogeneous systems with hierarchical memory. Different levels of memory hierarchy have different time and energy efficiency. Core memory may be too small to hold whole load to be processed, while computations using external storage are expensive in time and energy. In order to avoid the costs of processing the load in the external memory, it is allowed that the load is distributed to the worker processors in multiple installments. A minimum energy solution is found by use of mixed integer linear programming under a limit on schedule length. Two types of fast heuristics with several variants are also examined. The trade-off between the criteria of processing time and energy is studied. Key features of optimum solutions are analyzed. It is shown that holding machines in a diverse set of energy modes and limited use of the out-of-core memory can be beneficial for the time and energy performance. The proposed scheduling algorithms are evaluated in the terms of solution quality and runtimes.

Keywords
Performance modeling and prediction
Energy-aware systems
Storage hierarchies
Time–energy trade-off
Divisible load theory

1. Introduction
Energy consumption is among the biggest limitations in running big data centers and supercomputing installations. In this paper we study the trade-off between time performance and energy cost which arises in processing divisible loads on heterogeneous systems with hierarchical memory. Divisible loads are data-parallel applications which can be divided into parts of arbitrary sizes, and the parts can be processed independently in parallel. Divisible load theory (DLT) has been first used in the late 80s to analyze distributed computations on clusters of workstations [1] or chains of intelligent sensors [10]. DLT provides methods of analyzing performance of a broad class of distributed applications operating on big data volumes [6], [11], [29].

Contemporary computer systems have hierarchical memory organization. CPU registers are at the top of the hierarchy. They have the shortest access time, but are very limited in the number. The next level of memory hierarchy are processor caches. The main memory, here by convention referred to as RAM, has much bigger size but is slower. The following levels of memory hierarchy are based on external and networked storage: distributed memory caches, SSDs, HDDs, NAS, tapes, optical media, etc., and are usually managed by software stacks. Thus, while going down the memory hierarchy the size of available storage grows, but the access time increases. Current computer systems do not impose strict limitations confining computations to certain memory levels. It is possible to spill code and data structures from the higher to the lower memory levels by use of, e.g., virtual memory, networked caches, etc. However, there is a significant difference in performance: external storage can be by more than an order of magnitude slower than the RAM [13]. Storage software stack is also less energy-efficient [36]. Conversely, prudent usage of the external memory may be beneficial, e.g., to avoid excessive communication or to use fewer machines. Since the external and the networked storage usually is managed by software while the higher memory levels are controlled by hardware, we assume a hierarchical memory system, but we reduce the above hierarchy to just two types of memory: (1) the core (alternatively: main) memory comprising registers, caches, RAM, and (2) out-of-core memory comprising all types of external storage.

One of the key elements in DLT is the load scattering method. In the single-installment approach each worker processor receives load at most once. A disadvantage of the single-installment method is that load chunks tend to be big which (i) delays starting the computations, (ii) increases the chance of using out-of-core memory. Therefore, in the multi-installment load scattering each processor may receive more than one chunk of load to process. Consequently, computations may start earlier and momentary memory footprint is smaller. Hence, we assume multi-installment processing.

Attaining energy economy of high-performance computations is a complex challenge. For example, it is possible to run the job parts on slower but more energy-efficient CPUs. Thus, it is possible to reduce energy consumption by exploiting system heterogeneity and by exercising alternative scheduling strategies. A natural trade-off arises because a user may choose faster computation at higher energy cost, or vice versa, wait a bit and pay less in energy. This trade-off will be the subject of this paper. The main contributions of this paper can be summarized as follows:

•
Multi-installment distribution of the load in heterogeneous system with hierarchical memory is explicitly planned to allow omitting memory limitations.

•
The energy cost and computation time models of hierarchical memory system supported by experimental data are used.

•
The computation scheduling problem is solved to optimality by mixed integer linear programming (MIP).

•
The trade-off between time and energy optimization is analyzed. Complicated landscape of such a trade-off is shown.

•
It is demonstrated that the interactions between the machine parameters can be very intricate.

•
The trade-off between starting more machines and using the less efficient out-of core memory of fewer machines is analyzed.

•
The trade-off between using deeper vs. the shallower machine suspension modes is studied.

•
Since the optimum schedules are computationally hard to obtain, fast heuristics are proposed as alternative solution methods.

•
The MIP and the heuristics are evaluated in runtime vs. solution quality trade-off.

Further organization of this work is the following. In the next section we review related works. In Section 3 models of time and energy performance specific for systems with hierarchical memory are introduced. The related scheduling problem is formulated in Section 4. In Section 5 algorithms solving the problem are proposed. In Section 6 performance of heterogeneous systems and the optimum solution features will be studied. Since the problem is bicriterial the trade-off between time and energy will be analyzed. Section 7 is dedicated to the evaluation of the proposed algorithms. Section 8 summarizes results of this work. Key notations are collected in Table 3.

2. Related work
Time and energy efficiency in parallel processing is intensively studied and a wealth of results on this subject exists, see the surveys [17], [25], [26], [28], [34], [37], [44]. The problem of attaining time–energy efficiency has been attacked on several, not mutually exclusive, levels of abstraction: (i) on hardware of computing platform level [2], [21], [27], [31], (ii) algorithm level [7], [14], [18], (iii) runtime environment level [5], [19], [22], [41], (iv) scheduling and management at a data center level [3], [15], [42]. We assume application level scheduling by the runtime environment in a close cooperation with the computing platform. On the one hand, our results give hints to the platform on the set of maintained active machines and their energy-saving modes. On the other hand, the algorithms in the runtime environment do their best with the provided heterogeneous computing platform.

As already mentioned our application is data-parallel and divisible load theory (DLT) will be used as the performance and scheduling model. A general introduction to DLT and a review of its many applications can be found in [6], [11], [16], [29]. Divisible load processing problems of similar nature have been studied, e.g., in [4], [8], [9], [12], [13], [20], [23], [30], [33], [35]. In [8] a genetic algorithm was applied to select the right set of processors for single-installment communication in heterogeneous system. In this submission we assume multi-installment load scattering. Moreover, we take into account energy cost and the impact of hierarchical memory system. Multi-installment scattering of the load to heterogeneous processors was studied in [33]. Two heuristics were proposed assuming that the sequence of communications with the machines is a known repetitive pattern. We will allow for any sequence of communications, and what is more, the sequence need not be repetitive.

Processing divisible load with non-linear complexity algorithms was studied in [4]. It was demonstrated that the classic DLT approach based on load equipartition is not well suited for non-linear complexity algorithms unless clever data partitioning methods are applied. How this can be achieved was demonstrated for vector outer-product and for matrix multiplication. Our case is slightly different because the nonlinear execution time is a result of memory hierarchy not the data processing algorithm itself. Consequently, simple load partitioning algorithms are sufficient.

Energy use in DLT applications has been considered, e.g., in [32] to assign measurement workload in a wireless sensor network. The residual battery energies were used to determine workload partition, resulting in longer lifetime of the whole network. Energy may be considered as a special type of cost. Scheduling with monetary cost has been considered in [9], [35]. Cost minimizing heuristics have been proposed for single-installment case. In [30] a polynomial-time algorithm has been proposed to build time-cost trade-off when communication, computation times, and costs are proportional to the size of the load and all communication links have the same speed.

Publication [20] is the first work studying a flat (non-hierarchical) memory model. A heuristic has been proposed for the case with communication times proportional to the size of the load. In [13] hierarchical memory systems have been analyzed with respect to time performance. Energy in processing divisible loads on homogeneous flat memory systems were subject of [12]. Interrelations between system parameters were represented by maps of equal energy consumption similar to weather maps. Our current submission extends paper [23] studying divisible load processing in homogeneous systems with single-installment load scattering. This submission differs from [23] in considering heterogeneous systems, multi-installment load scattering, proposing a set of fast heuristics for heterogeneous systems, studying the impact of heterogeneity on the time and energy performance, analyzing the runtime-solution quality trade-off existing in the set of proposed algorithms. Let us note that the problem we are considering is NP-hard [40].

3. Time and energy performance model
The time and the energy required for the computations on a load chunk depend on the chunk size . An important determinant is how  is big compared to the size of the main memory. In order to verify the relationship between computing time, consumed energy, and the size of the load chunk a number of computational tests have been conducted. Example results are collected in Fig. 1. Three example platforms and three applications (image edge detection, quicksort, search for a string in a text block) are reported upon. In Fig. 1 dependence of computing time and energy on load size  is shown. The dashed lines represent best linear regression fit into the measured data. Note that the vertical axes in Fig. 1 are logarithmic, and hence, in this coordinate system the linear functions are not straight lines. It can be verified that both runtime and energy consumption increase significantly faster with problem size when out-of-core memory is used. Interestingly, usually power consumption of the out-of-core computations is lower than in-core. However, the speed is by far lower, and hence, the overall energy consumption increases much faster with work size . The point of switching from one dependence to the other is smaller than the hardware RAM size because some memory is reserved by the operating system and runtime environments.


Download : Download high-res image (524KB)
Download : Download full-size image
Fig. 1. Time and energy dependence on load size. Edge detection: (a) time, (b) energy; quicksort: (c) time, (d) energy; string search: (e) time, (f) energy, vs. problem sizes. Logarithmic vertical axes. Continuous lines are fit using linear regression.

It can be observed that the dependence of computing time and consumed energy on chunk size can be represented by piecewise-linear functions. For example, the time of processing load of size  on machine 
 is (1)

Component 
 corresponds with computations in core with rate 
 (reciprocal of speed). The second component represents out-of-core computations. Functions 
 have two properties:
 and 
, for , where 
 is the size of the main memory on machine 
 available to the application (not necessarily the whole hardware RAM). Beyond 
 the machine starts using out-of-core memory. The energy consumed in the computations is determined by an analogous function (2)
satisfying conditions 
. For memory size 
, both 
 and 
 satisfy: (3)

Let us observe that since out-of-core processing time and energy increase much faster than in-core, we have: 
 
, for all machines 
. Values of these coefficients can be obtained by use of linear regression on intervals of load sizes . The values in Table 1 have been obtained for the measurements shown in Fig. 1. Only 
s are given in Table 1, because 
s can be calculated via the available RAM formula (3) presented above: 
. Note that coefficients 
 depend on both the machine and the application.


Table 1. Example time and energy function parameters.

Case: machine, application	
[s/MB]	[s/MB]	[s]	[J/MB]	[J/MB]
1. Pentium IV@2.8 GHz, 1 GB RAM@266 MHz, HDD Caviar WD400,FreeBSD 9.0, image edge detection	0.0392	5.9943	−4208.2	4.0509	355.2
2. i5@3.2 GHz, 8 GB RAM@1.6 GHz, Ubuntu 14.04LTS, Seagate Sti1000dm003 quicksort	0.0136	0.3331	−2404.8	0.8925	20.46
3. AMD A8-7670K@3.6 GHz, 8 GB RAM@1.6 GHz, Ubuntu 14.04LTS Seagate Sti1000dm003, string search	0.0058	0.4151	−2928.1	0.3717	21.77
4. Problem formulation
We assume that computations are performed in a single-level tree system with root 
 (a.k.a. master, server, originator) and worker machines (computers, processors) 
 in the leaves. The machines can be in one of four states:

1.
idle - consuming power 
,

2.
starting - which takes time 
 and power 
,

3.
networking — busy-waiting or receiving the load, using power 
,

4.
computing - when the received load is processed.

Communications and computations proceed according to the following scheme (see Fig. 2). Initially volume  of load is held by the originator 
 which is in the networking state, while machines 
 are idle. 
 is not taking part in processing the load. The role of 
 is scheduling of the computations by sending load chunks to the chosen processors. If 
 is capable of processing some load in parallel with communications, then this capability may be represented as an additional worker processor. 
 activates the machines which takes energy (4)
on machine 
. Note that not all machines have to be used. For example, some computer which is too slow, or is consuming too much power, may be kept idle. Let us observe that idle state energy should not be ignored because, unless completely disconnected from electric network, idle machines still contribute to the costs that the system owner must bear. The originator activates machines just-in-time which means that completion of the starting operation coincides with the beginning of receiving the load to process. The duration and energy cost of sending a wake up signal is negligible and starting some machine 
 can be performed in parallel with some other machine 
 communicating with 
. In the following, terms “installment”, “load chunk”, “communication” will be used interchangeably. Transferring  units of load to 
 takes time (5)
where 
 (e.g. in seconds) is a fixed overhead also called communication startup time, 
 is communication rate (in seconds per byte). The communication with machine 
 draws power 
. Then, the energy cost of communication is (6)
Note that Eq. (6) represents the networking energy in the interval of communication with 
 consumed in both the interconnect (e.g. routers, the interconnect software) and in 
. The computation starts after the entire load chunk is received. The load is distributed in a multi-installment manner and a processor may receive more than one load chunk. 
 sends the load to the worker processors one at the time, i.e. the load is distributed sequentially. Let 
 be the sequence of the communications, where 
 is the index of the machine receiving the load in the th communication from 
 and  is the number of communications. As discussed in Section 3, time 
 and energy 
 required for the computations on load chunk of size  on machine 
 are given by Eqs. (1), (2), respectively. We assume that the size of produced results is small and the time of returning the results to 
 is very short compared to the whole schedule length. Hence, the result return operation need not be explicitly scheduled (still, result return can be tackled in DLT, see [6], [11], [29]). After processing the received load chunk a machine is busy-waiting to receive another load chunk. The busy-waiting is energetically equivalent with networking and 
 consumes power 
 in this state.


Download : Download high-res image (168KB)
Download : Download full-size image
Fig. 2. Multi-installment schedule. 
 denotes the last chunk sent to machine 
.

Let 
 be the set of worker processors. A scheduling algorithm for our problem must determine:

•
subset 
 of machines participating in the computation,

•
sequence  of load distribution communications between 
 and worker machines in 
,

•
the load chunk sizes,

for minimum energy  and length . This problem is effectively bicriterial. Multiple criteria problems can be handled in various ways [38]. In order to deliver the relationship between  and , we will minimize one criterion — energy , for a constrained value of the other criterion — schedule length .

Let us compare our scheduling model with the existing approaches. There are papers, e.g. [20], assuming hard memory limits. It means that instances with 
 are infeasible. A border between core and out-of-core memory cannot be incontrovertibly inserted in such a model. In our case, by Eqs. (1), (2), a feasible solution always exists, albeit possibly with bad performance. It is still possible to apply the very basic DLT approach [6] assuming that computing time, and energy as a kind of cost, are proportional to the assigned load size. Unfortunately, in this model energy can be very far from reality in a hierarchical memory system. Table 2 gives an example of the disparity of these two approaches. In Table 2 
 is the energy consumed in the computations according to the proportional cost model, 
 is the same type of energy but calculated according to Eq. (2). System parameters and the formulae used to calculate 
 are given in Table 2. It can be concluded that the earlier existing approaches cannot be easily adjusted to our situation, or are significantly inconsistent with reality.


Table 2. Difference between hierarchical memory and proportional cost models.

 [MB]	10 000	12 000	14 000	16 000	18 000
 
 [MB]	5714	6857	8000	9143	10 286
 
 [MB]	4286	5143	6000	6857	7714
 [J]	4000	4800	5600	6400	7200
 [J]	4000	4800	16 400	41 886	72 000
, 
 s/MB, 
 s/MB, 
 s, 
 J/MB, 
 J/MB, 
 J,  MB, 
 s/MB, 
.


Table 3. MIP notations.

Variables
The amount of load sent to 
 in the th communication;
Sum of all forms of the consumed energy;
Energy consumed by the originator;
Total energy consumed in idle waiting before starting the processors and later in busy-waiting;
Energy consumed by machine 
 in the computations on the th load chunk;
Total energy consumed in the networking;
Total energy consumed in the computations;
Total energy consumed in starting the processors;
The time until starting machine 
;
The total time when machine 
 is busy-waiting;
A variable equal to the product 
;
The time when the th communication begins;
The duration of computation on the th load chunk on machine 
;
A binary variable equal 1 if 
, 0 if 
;
A binary variable equal 1 if machine 
 receives load in the th communication, 0 otherwise;
A binary variable equal 1 if machine 
 received some load in the th or earlier communication, 0 otherwise;
Constants
Parameters of machine 
 piecewise-linear computing time function;
Communication rate of machine 
 (inverse of bandwidth);
Parameters of machine 
 piecewise-linear energy function;
Size of the set of available machines
Number of communications
Fixed communication overhead of machine 
;
Idle state power of machine 
;
Networking power of machine 
;
Starting state (power up) power of machine 
;
Startup time from idle of machine 
;
Schedule length;
Size of load to process;
Big constants.
5. Solution methods
In this section we introduce two types of strategies for load distribution. Firstly, we introduce two groups of fast heuristics which distribute load iteratively, choose the sequence of communications and regulate load chunk sizes according to some simple rules. Secondly, a method of constructing an optimum multi-installment schedule is presented which sequences communications and sizes load chunks using mixed integer linear programming (MIP). The MIP finds an optimal solution using both core and out-of-core memory, while the heuristics avoid using out-of-core memory.

5.1. Fast heuristics
Our heuristic algorithms are defined by the processor sorting rule () and the load chunk sizing algorithm. Beyond these two components, the mode of operation is the following.

Initially all processors are idle. The originator starts the first idle machine on the  list and sends it a load chunk. The processors are activated until exhausting idle machines or until receiving a request for new load from some ready processor. Then, the originator sends load chunks to the ready processors first. A processor is ready at some moment  if it has already been started, received and processed its load chunk by . Let 
 be the set of processors ready at . The originator ranks online processors in 
 according to  and sends a load chunk to the processor on the topmost position. Consequently, processor 
 preferred by  may receive a new load chunk earlier than some other processor 
 which joined 
 before 
. Let us also note that further idle processors will be started when no processors are ready (because they are computing and the originator is not communicating with them). This procedure is repeated until exhausting load .

5.1.1. Processor sorting rules (PSR)
The considered processor priority rules order the processors according to: k1 — non-decreasing 
, k2 — non-decreasing 
, l2 — non-decreasing 
, a1 — non-decreasing 
, a2 — non-decreasing 
, b2 — non-decreasing 
, C — non-decreasing 
, S — non-decreasing 
, PI — non-decreasing 
, PN — non-decreasing 
, PS — non-decreasing 
, O — non-decreasing 
, RAM — non-increasing 
, Rnd — order the processors randomly. The Rnd rule is introduced as a reference method, to verify the utility of the other rules. Now we are ready to introduce the load chunk sizing algorithms.

5.1.2. Simple static chunk (SSC)
This algorithm assumes that load chunk sizes are equal to the size of available RAM of the machine, i.e. 
. Thus, SSC avoids using out-of-core memory. A disadvantage of SSC is lack of load balancing in the final stage of computation. It means that there are no precautions against uneven computation completion times of the processors. For example, the slowest processor may receive the last load chunk when all other processors already finished computations. Consequently at the end of the schedule many processors may be idle while only a few processors strive with unnecessarily big load chunks. A better solution would be to split the last chunk between the idle processors. However, predicting which processors will be used in the last iteration is hard because the algorithm is running online in a heterogeneous system.

5.1.3. Guided self-scheduling adaptation (GSS)
This algorithm extends a loop scheduling algorithm for homogeneous systems [11]. Let 
 be the size of the load remaining to be distributed and 
 be the processor about to receive the th load chunk. The size of the chunk is calculated as 
By referring to the remaining load 
, load chunk sizes decrease in the course of the schedule. Assuming that the initial size of the load is greater than memory size (
), the algorithm starts with load chunk sizes of RAM size. When 
, GSS gradually decreases chunk sizes and in this way equalizes the spread of machine completion times. With decreasing 
 the size of 
 tends to zero, and GSS could send an excessive number of tiny load chunks. Therefore, GSS does not use chunk sizes smaller than some fixed size, by convention denoted as 1 in the above equation. This can be the size of the processed data structures or the size which sufficiently amortizes fixed overheads in processing one load chunk. In the further considerations we assume that it is 1 MB.

Computational complexity of the above heuristics is 
 and  for SSC and GSS, respectively. Terms 
 are upper bounds on the number of load chunks for SSC and GSS, respectively. Both methods use some PSR and the  component is a result of applying, e.g., processor priority queue and enforcing the PSR. Though complexity of the algorithms depends on , they are very fast in practice as will be shown in Section 7.

It is possible to apply all the above s and choose the best result. Such heuristics will be referred to as super-SSC, or super-GSS, in the following text. Note that all s can be checked in the off-line mode (by simulation) only. Some studies demonstrate [24] that combining many simple methods is a lightweight method of improving solution quality. Thus, super-SSC and super-GSS serve the purpose of evaluating how much potential of solution quality improvement resides in the s.

Let us now proceed to the technical matters of time and energy calculation. Since values of 
 cannot be determined in the analytical way for a heterogeneous system, they are found a posteriori for some schedule , e.g., obtained by simulation or from runtime logs. Given schedule , the consumed energy is: (7)
where: 
 is the energy consumed by the originator, 
 is the energy consumed by 
 in the initial idle state, 
 is the energy consumed by machine 
 while starting, 
 is the energy consumed by 
 while processing the assigned load, 
 is the energy consumed by 
 in networking and busy-waiting. Since the originator can only communicate or busy-wait, 
. The 
 energy in the idle state is 
, where 
 is the time before 
 begins waking up (see Fig. 2). If some machine is not used in the computation, then 
. The energy consumed in starting this machine is 
. If 
 does not participate in the computation, then 
. The energy consumed in computations on 
 can be calculated as: 
In the busy-waiting and communications 
 which takes part in the computation draws power 
. Hence, (8)
 A machine which was not started has 
.

5.2. Mixed integer linear program
In this section we formulate the problem as a mixed integer linear program (MIP). Both divisible load scheduling [40], and mixed integer linear programming in general, are NP-hard. This means that according to the current state of knowledge (unless P  NP), to solve these problems to optimality exponential runtime algorithms are required. In our case, the worst-case computational complexity grows exponentially in the number of processors  and the number of installments . However, for reasonable problem sizes MIPs can be solved fairly well by modern solvers. Thus, utility of MIPs must be assessed on the practical basis rather than by the worst-case pessimistic estimation. This will be subject of Section 7. The notations used in the following linear program are collected in Table 3.

Given schedule length limit , the minimum-energy schedule can be calculated by solving the following mixed integer linear program: (9)
subject to: (10)
 (11)
 (12)
 (13)
 (14)
 (15)
 (16)
 (17)
 (18)
 (19)
 (20)
 (21)
 (22)
 (23)
 (24)
 (25)
 (26)
 (27)
 (28)
(29)
(30)
(31)

Let us note that by use of decision variables 
 any subset of the  machines and any communication sequence of length  can be achieved. In the above MIP total energy usage is minimized by Eq. (9). Eqs. (10)–(15) define components of the energy consumption. In particular, by (11) energy spent in the computations is a sum of the energy parts 
 spent on computing on load chunks  on machines 
. The dependence of energy parts 
 on load chunk sizes 
 is defined by inequalities (12). Since 
 is minimized, (12) guarantees that 
. Since 
, term 
 in (12) makes the constraint more restrictive when 
 and helps easier solving the MIP. In (13) energy 
 is the sum of the energy 
 used while waiting before starting the processors and energy 
 consumed later in the busy-waiting. In (14), 
 only if processor 
 is used in some installment. Consequently, energy cost 
 of starting 
 is paid once, and only if 
 is indeed activated. Energy cost of networking is calculated in (15).

By Eq. (16) only one machine may receive some load from the originator in installment . Inequality (17) guarantees that a machine is not receiving any load if it is not chosen to take part in the th communication. By (18) all work is executed.

If a machine is chosen to take part in the computation, then by inequality (19) there is always enough time for starting the machine. The th communication fits in time interval 
 by (20). Let us remind that 
. Hence, constraints (21) ensure that if some machine 
 receives the th and th load chunks, then there is enough time between 
 and 
 to transfer the load to 
 and process it. Constraints in (22) serve the purpose of linearizing product 
, that is, they guarantee that 
. Such a product cannot be directly used in a MIP because it would change the formulation into a quadratic programming problem. However, it is possible to substitute 
 with additional variable 
 and constraints (22). Constraints (23) guarantee that processing load chunk  on 
 lasts for time 
. Let us observe that for 
 (23) guarantee that 
 which implies 
, and vice versa, for 
, 
 and 
. The two big constants 
 in (23) have been chosen to avoid using arbitrarily large numbers, which are hard to operate upon for MIP solvers, and still make the formulation as tight as possible. Since for any feasible solution 
, the first big constant has been set to 
 which guarantees that the first inequality in (23) is not binding if 
. The 
 constant, has been set to 
 which guarantees that 
 for 
 is not binding because 
 because 
.

Inequalities (24) guarantee that computations on each machine finish before the end of the schedule. Eqs. (25) allow to calculate the length of busy-waiting intervals on machines 
 (used in (13)). Inequalities (26), (27) serve the purpose of calculating time 
 which is the idle time before activating machine 
 (Fig. 2). Note that 
 and the inequalities in (26) are binding only if 
 which happens if the th communication is the first message sent to 
. Thus, 
 where  is the first load chunk sent to 
. Inequalities (27) are a boundary case of (26).

The constraints in (28)–(31) define the trigger variables 
, used in (14), (26), (27), (25) such that 
s are equal to 0 before the first message sent to 
 and equal to 1 from the first message sent to 
 on.

For practical matters, let us note that the big constant , used in constraints (21), (22), (26), (27) can be substituted with  if (9)–(31) is solved for some known value of . MIP (9)–(31) can be reformulated to calculate minimum schedule length , or to minimize  subject to a limit on the usage of energy . In the former case ( is minimized), constraints (10)–(15) are removed, while the big constant  can be set to some upper bound on the schedule length. For example, 
. In the latter case (minimization of  subject to ),  is again minimized while equation (9) becomes a constraint.

6. System performance modeling
Parameters of heterogeneous systems on various divisible applications may differ significantly (cf. Table 1). Therefore, rather than discussing particular numbers we will analyze tendencies which appear in many test instances. The behavior of optimum problem solutions will be demonstrated on the example data shown in Table 4. The values in Table 4 have been generated pseudo-randomly. Generating test instances is described in more detail in the next section. Unless said to be otherwise  GB. The time–energy functions presented in this section were obtained by first finding the minimum schedule length 
 for the given  as described at the end of Section 5.2, and then MIP (9)–(31) was used to calculate minimum energy consumption for test values of  starting with 
 and increasing with a step of 1s. Let us note that in the following discussion trade-offs are not Pareto-fronts because dominated solutions are also shown. More formally, function 
 of minimum energy consumption vs. schedule length , number of installments , and set ’ of used processors will be examined. We will use terms “trade-off” or “time–energy function” for brevity when referring to 
. Gurobi version 7.5.2 with at most 0.2% optimality gap has been applied as the MIP solver.


Table 4. Test system parameters.

 [s/MB]	0.11168	0.12261	0.14105	0.07070	0.05656	0.11436	0.10254	0.08598	0.10227	0.08740
 [s/MB]	3.07840	0.87647	4.88723	0.90572	2.12593	1.58645	2.12614	2.23513	1.15140	1.39684
 [s]	−5227.37	−5642.63	−11272.18	−3623.98	−10913.84	−11577.97	−12467.41	−10427.70	−7262.05	−7199.29
 [J/MB]	15.729	16.569	17.631	10.552	9.427	14.662	15.079	13.027	16.495	13.656
 [J/MB]	377.024	106.704	473.569	117.549	281.207	149.552	309.572	310.694	208.662	194.871
 [J]	−636602	−674663	−1082852	−464368	−1433370	−1060913	−1814371	−1444280	−1330178	−9963201
 [s/MB]	0.007	0.002	0.004	0.009	0.003	0.005	0.003	0.002	0.004	0.009
 [s]	62	80	3.4	1.1	2.3	1.2	65	5.1	75	82
 [W]	4	6	88	71	95	70	2	93	5	4
 [W]	99	90	96	93	113	113	108	120	111	95
 [W]	98	108	115	96	99	114	120	91	93	108
 [s]	0.62	0.8	0.034	0.011	0.023	0.012	0.65	0.051	0.75	0.82
 [MB]	1762	7485	2375	4340	5274	7865	6161	4852	6922	5498
In Fig. 3, Fig. 4 dependence of total energy usage on schedule length  for different numbers of load chunks  is shown. In Fig. 3 two lower bounds are also shown: a diagonal line representing energy usage if all processors remained idle (denoted “idle”), and a horizontal line of energy usage if all work were done in RAM on an ideal processor with all the best parameters of the given processors and no other processors were present (denoted “ideal 1”). The following phenomena can be observed in Fig. 3, Fig. 4: Time–energy functions are not convex and may have many local optima, “cliffs” in energy usage separate intervals of monotonous time–energy dependencies. The existence of many local optima and non-monotonic nature of the relationships make optimization efforts hard. The “cliffs” appear when schedule length  is sufficiently large to switch off some machine (this will be discussed in the following text, see Fig. 6). The intervals of decreasing  with increasing , e.g. at  for  in Fig. 4, represent the opportunity of shifting the load from faster but more energy-intensive machines to the slower but more energy-economic ones. The fact that energy consumption increases with schedule length is a result of busy-waiting and idle processors. We will return to the impact of idle processors power in the following (Fig. 9). It can be observed in Fig. 3, Fig. 4 that with increasing number of load chunks  performance of multi-installment processing improves, however, the returns are diminishing. This is intuitively expected because more installments mean smaller load chunks, shorter communications, earlier starting of the computations, and using less the out-of-core memory. The minimum number of chunks which allow processing the whole load in RAM is  here, but the processors with the biggest main memory are not necessarily the fastest or best in the energy efficiency sense. Thus, if schedule is long enough, it may be still profitable to use out-of-core memory even if RAM size is sufficient (this will be discussed in the following, cf. Fig. 7). It can be verified in Fig. 4 that using more than  installments does not give substantial performance gains. Thus, a small multiple of the minimum number of chunks which allow fitting the load in core memory was sufficient here.


Download : Download high-res image (167KB)
Download : Download full-size image
Fig. 3. Energy vs. time for changing number of installments . Logarithmic axes.


Download : Download high-res image (316KB)
Download : Download full-size image
Fig. 4. Closeup on energy vs. time for changing number of installments .

In Fig. 5 distribution of the load between the processors is shown with changing schedule length  for  installments. Loads 
 obtained by the machines in all messages are shown. In Fig. 5 dependence of energy  on schedule length  is also shown to allow coordinating the “cliffs” in energy usage with the changes of load distribution. Fig. 5 serves the purpose of illustrating how complex processor load distribution interplay can be. It can be verified in Fig. 5 that steep reductions in energy usage coincide with removing processors from computation (load assignments become zero). The case of machines 
 in Fig. 5 (shown as continuous lines) is illustrative. At  
 is switched-off and its load is taken over by 
 which is needed as a faster machine. But then at  the schedule is long enough to give up 
 and use a slower but more energy-efficient 
. As longer schedules are allowed ( grows) machine 
 gives its load to the other, more energy-efficient machines, and is switched off at . This situation is repeated twice more: 
 is switched on again at  to take the load of 
, switched off at , and switched on once more at  to substitute 
, and off at . Thus, machine 
 which is comparatively slow but more energy-efficient than the other machines acts as a substitute for the faster 
.


Download : Download high-res image (380KB)
Download : Download full-size image
Fig. 5. Distribution of the load between machines and energy consumption vs. schedule length . The left axis is for load sizes, the right axis is for energy . Number of installments: .

In Fig. 6 changes of energy usage with schedule length are shown for increasing sets of available machines. This means that from the machines in Table 4, subset 
 were available for the computation, where . Labels added at the relationships are indices of the used processors. For better clarity indices of machines absent in the optimum working set vs. changing  are listed in Table 5. It can be observed that adding machine 
 to the working set was profitable and the minimum energy use point moved to shorter schedules. When schedules get shorter (see Table 5), additional machines are switched on and energy usage instantly increases as a direct result of the starting energy cost. Observe pivotal role of 
 which joins and leaves the processor working set (cf. Table 5). It is worth observing that in this heterogeneous system the trajectory of increasing energy  with shortening schedule length  can be different depending on the actual set of available machines. For example, for the set of  machines the energy used can be bigger than (because of the cost of holding more machines), smaller than (because the schedule is shorter), or in-between (a subset of the extra machines work) the energies consumed by smaller sets of  machines (see Fig. 6 in  range [300,400]). Thus, a bigger set of machines gives an advantage of choosing more energy-efficient subset of processors, but also a disadvantage of holding machines which are available but not engaged in the computation.


Download : Download high-res image (297KB)
Download : Download full-size image
Fig. 6. Energy vs. time for changing set of machines at the number of installments .


Table 5. Processors omitted in the best schedules vs. schedule length , for  machines.

End of  range	292	313	338	346	377
Omitted	none	2	2,9	1,2,9	2,7,9
End of  range	406	431	507	688	799
Omitted	1,2,7,9	2,7,9,10	1,2,7,9,10	1,2,6,7,9,10	1,2,4,6,7,9,10
Let us consider now the utility of using out-of-core memory. Even if the number of load chunks  is sufficient to process the whole load in the core (here ), it may be profitable to use some out-of-core memory. This may allow for fewer communications or for starting fewer processors. In Fig. 7 usage of the out-of-core memory is shown for various schedule lengths  and installment numbers . The fraction 
 is shown along the vertical axis. In other words, the relative excess of load chunk sizes 
 over RAM size 
 is shown. For  installments and  it is possible to process three load chunks in main memory of one processor while the remaining load is processed out of core in another processor, which allows to avoid starting other machines. Thus, in this case out-of-core memory usage persists though it seemed unnecessary. However, it can be seen in Fig. 7 that indeed with increasing number of installments  the use of the out-of-core memory ceases. The above observations can be rephrased as confirming presence of a trade-off between the cost of starting more processors, executing more communications or using the less efficient out-of-core memory. Since relative excess of load chunk sizes over RAM size is low, Fig. 7 also shows that taking advantage of out-of-core memory requires good parameter data (benchmarking) and dealing with the cost of MIP solver. The computational costs will be further studied in Section 7.


Download : Download high-res image (242KB)
Download : Download full-size image
Fig. 7. Out-of-core memory usage vs. time  for changing number of installments .

In Fig. 8 average sizes of installments are shown for different combinations of processor set size  and the number of installments . The averages have been taken over schedule lengths  from the minimum schedule length 
 to the minimum-energy 
 schedule length (e.g. for  for all the points  with a step of 1s, see Fig. 6). Each point in Fig. 8 represent an average from at least 110 samples. The set of used processors for different values of  is chosen as for Fig. 6. A common pattern in the shape of the relationships can be observed in Fig. 8: The installment sizes grow slowly until reaching a maximum, and after the maximum, the installments sizes quickly fall. Beginning with small chunks allow to start the computations quickly by avoiding lengthy data transfers. The messages after the sizes maximum are shorter to allow balancing the load on the machines which receive some work as the last ones.


Download : Download high-res image (318KB)
Download : Download full-size image
Fig. 8. Average chunks sizes vs. installment number for selected numbers of installments  and machines .

Let us return to the impact of idle processors power 
 on the time–energy trade-off. Even idle processors consume energy which is signified by the lower bound “idle” in Fig. 3. In Fig. 9 the time–energy trade-off is shown for the previous instance with various 
 settings. The original instance is shown as “mixed”. The same instance with machines starting from hibernation to disk (cf. mode S4 of ACPI [39]) are denoted “HDD” (
 s, 
 W, see the next section for details of test instance generation). The setting corresponding to starting from a suspension to RAM (
 s, 
 W) are denoted “RAM”. The case for cold-starting (as if the machines were disconnected from electric power, 
 s) is shown as “cold”. Finally, the same instance with all processors idle power changed to 
, but retaining their original startup times is an optimistic lower bound (LB) on possible idle power vs. startup time cases. Hence, the LB line shows potential for possible improvements by modifications of energy-saving modes. It can be observed that the lower the idle power 
 is, the wider the range (both in time and in energy) between the shortest and the lowest-energy schedules. The more realistic cases, where 
 is bigger, have narrower options for optimizing the energy usage. Suspending machines to RAM allows for only marginally shorter schedules, but the energy cost only increases with  as 
s are not significantly smaller than the power consumed in communication or computation. The cold – and HDD – start do not dominate the original case (mixed) because of long, and consequently, energy-costly startups. Therefore, short schedules, even though compute- and power-intensive, can be more effective than long schedules with many machines mostly in low energy mode. Furthermore, using a mix of machines – some in shallower and some in deeper suspension (mixed case) – is advantageous because it allows for a quick start of computations using the machines in shallower suspension, while simultaneously activating the machines in a deeper sleep mode. This conclusion is supported by a statistically significant correlation between the position in communication sequence and parameters 
 (e.g. machines with smaller 
 receive load earlier), and lack of strong correlation with other parameters. Let us observe that in this example, keeping the machines in a mixed set of active states rather than cold allows to construct schedules shorter by 25% and using 20% less energy. In general, keeping machines in idle state should be avoided (which is the strategy of many cloud infrastructure providers).


Download : Download high-res image (328KB)
Download : Download full-size image
Fig. 9. Energy vs. time for various idle state power 
 configurations at  machines, and  installments.

7. Algorithm performance comparison
In this section we compare performance of our algorithms. Quality of the schedules and the time taken to find them will be evaluated. Quality of the schedules is measured by schedule length  and energy usage . In general, there is no unique way to compare performance of two algorithms constructing solutions as a trade-off between two criteria [43]. Moreover, it is not possible to reduce such trade-offs to one dimension (i.e. a single numerical score) without losing some information [43]. In the case of MIPs, it would require considering three-dimensional trade-off: between the solving algorithm runtime, the obtained schedule length  and the used energy . Though conceptually possible, it would be unwieldy. The fast heuristics introduced in Section 5.1, however, construct only a single solution for a given instance, not a trade-off between  and . This restricts options for algorithm performance comparison. Hence, in the further discussion we will reduce algorithm performance examination to just two reference points of practical importance in the time–energy trade-off: the shortest schedules and the lowest energy schedules. Let us remind that the minimum schedule length will be denoted as 
 and the energy cost of the lowest energy schedule as 
. Solution quality will be measured either as a distance from the shortest known schedule length 
, or as the distance from the lowest known energy usage 
. The algorithm runtime to construct the schedules, how this runtime is traded for solution quality, and sources of solution inefficiency will be discussed in the following.

To this end, a set of instances have been generated pseudorandomly. The numbers of machines were . For each number  a set of 30 test instances has been generated, with 
 MB, 
 J/MB, 
 J/MB, 
 s/MB, 
 s/MB, 
 s/MB, 
E-6 s, 
 W, where  means that the value was drawn from a uniform distribution in range . Values 
 were calculated so that the two linear components of execution time and energy consumption, respectively, intersect at 
 (cf. Section 3). With probability 0.5 a machine was chosen to have a short startup (e.g. because it is suspended to RAM), otherwise it had a long startup (as if starting from hibernation to HDD). In the former case, 
 W and 
 s. In the latter case, 
 W and 
 s. Hence, idle power 
 and wakeup time 
 have been correlated in this way that machines with low 
 need more time to wake up, and vice versa, short wakeup time 
 is possible in shallower suspension mode using higher power. Unless stated to be otherwise  has been assumed for the MIP model (9)–(31). Gurobi version 7.5.2 has been used as the MIP-solver using 6 parallel threads and runtime limit of 1200 s. Two versions of the model have been solved: with MIP optimality gap set to 0.2% and with the gap set to 10%. The experiments have been conducted on a PC computer with Intel i7@2.8 GHz and Windows 7.

Algorithm runtime quartiles Q1, Q2 (median),Q3 vs. changing number of machines  are shown in Fig. 10. For the algorithms based on MIP (9)–(31) runtimes for finding minimum schedule lengths 
 with 0.2% MIP gap (denoted as “MIP T”) and 10% MIP gap (MIP T10%) are shown. The runtimes for finding the lowest energy schedules are similar and have been omitted to avoid cluttering the picture. As it can be seen the runtime cost of solving the MIP model quickly increases and the median runtime reaches the 1200 s limit at  for MIP gap 0.2%. Relaxing the optimality requirements (MIP T10%) helps, but still median runtime reached the time limit around  processors. Were it not for the limit of 1200 s, it should be expected the MIP solver runtime would continue exponential growth. Thus, the MIP model can be used for moderate size instances. Conversely, the CSS and GSS heuristics are by far faster and distribute the load to  machines in less than 10 s. The super-SSC and super-GSS (denoted SuSSC, SuGSS, respectively) are by an order of magnitude slower than SSC, GSS because they execute the heuristics with all processor sorting rules (PSRs). The dispersion of the heuristics’ runtimes is also much smaller and for all PSRs quartiles Q1, Q2, Q3 overlap in the picture. Now we should examine how these runtime costs are exchanged for the solution quality.


Download : Download high-res image (270KB)
Download : Download full-size image
Fig. 10. Algorithm runtimes vs. number of machines . Logarithmic axes.

The trade-off between solution quality and algorithm runtime cost is shown in Fig. 11. Computational complexity of solving the problem depends on , and in the case the of heuristic algorithms also on . In order to avoid concealing the algorithm runtime vs. solution quality relationship by this dependence of computational complexity on , these parameters have been set to  MB. In the figure boxes represent algorithm runtimes (horizontally) and relative distance from the best obtained solution (vertically) on a population of 30 test instances. The boxes span from quartile Q1 to Q3 in time (horizontally). Quality span is represented analogously along the vertical dimension. Medians (Q2) of runtime and quality are also marked. There are 14 PSRs of the fast heuristics SSC and GSS. Statistical analysis (ANOVA) revealed that neither in the runtime nor in the solution quality has any sorting rule a statistically sound advantage for the considered . Thus, to avoid cluttering the picture the results of all the sorting rules are put together in boxes distinguishing only SSC and GSS methods (denoted “All SSC”, “All GSS”, respectively). The results for the super-CSS and super-GSS methods, which choose the best result among all processor sorting rules, are shown as SuSSC and SuGSS, respectively. It can be seen in Fig. 11 that the solutions constructed by solving the MIP model (9)–(31) are always the best with respect to the solution quality. But this guarantee of quality comes at the cost of the runtime, the highest among all the studied methods. Relaxing the MIP gap to 10% (MIP 10%), helps with respect to the runtime with only minor loss in solution quality. However, as shown in Fig. 10, this approach has limited scalability because at  also the relaxed MIP model exceeds the 1200 s time limit. The heuristic solutions are on average 1.7 – 3 times worse in schedule length  (Fig. 11a) and 1.5 – 2.3 times worse on energy  criterion (Fig. 11b). Conversely, heuristic methods are by 4 – 5 orders of magnitude faster than solving the MIP model. It can be observed that GSS algorithm is better than SSC, but it is slightly slower (approx. 50% longer runtimes). The super-SSC improves solution quality compared to the original SSC methods, but it is still worse both in solution quality and runtime (when referring to the medians) than the original GSS methods. The super-GSS is only marginally better in solution quality than the original GSS methods. It can be concluded that the GSS methods dominate other heuristics.


Download : Download high-res image (341KB)
Download : Download full-size image
Fig. 11. Solution quality vs. algorithm runtime for (a) schedule length criterion, (b) energy criterion. Logarithmic runtime axes.  machines.


Download : Download high-res image (515KB)
Download : Download full-size image
Fig. 12. Time and energy quality among the heuristics vs. processor number . (a) SSC-heuristics schedule length, (b) SSC-heuristics energy; (c) GSS-heuristics schedule length, (d) GSS-heuristics energy. Lines show quality medians of the worst, , and the best heuristics. Logarithmic axes.

Let us now analyze the sources of heuristic algorithm inefficiency with respect to solution quality. In Table 6 quartiles Q1, Q2, Q3 of the fractions of the schedule length spent in busy-waiting or idle are shown. The fractions were collected over a population of 30 instances with  compared to the minimum length reference solution. In the case of heuristic algorithms all processor sorting rules were considered. This statistic has been collected for all available machines (even if some of them were not used), and separately, only for the machines which indeed took part in the computation. It can be seen that the solutions from the MIP model have very little idle time. The results for all machines and for the activated machines do not differ much. This signifies that MIP schedules almost always use all available processors, idle times are short if any, communications and computations are very well coordinated to avoid idle times. These quality results come at twofold price: computational complexity of solving the MIPs, and benchmarking the application and the platform to obtain precise data used in the model. Conversely, SSC quite often has long idle time. In particular, values 1 mean that some processors are not activated at all. This is confirmed by the fact that overall amount of idle time in SSC schedules decreases if only the active processors are considered. Thus, SSC has a potential for improvement by tuning the set of used processors. GSS schedules involve almost all processors which is signified by equal values both when all machines, and if only the active ones are taken into consideration.


Table 6. Idle time fraction in schedule length.

Quartile	All machines	Only active machines
MIP	SSC	GSS	MIP	SSC	GSS
Q1	0.0001	0.4736	0.3530	0.0001	0.2507	0.3530
Q2	0.0326	1	0.5342	0.0313	0.4505	0.5342
Q3	0.1124	1	0.7177	0.1061	0.6825	0.7177
In Fig. 12 evaluation of heuristic solutions quality is extended to bigger processor numbers . Relative distance from the lower bound is shown along the vertical axes. The lower bound of schedule length is (32)
where 
, 
, 
, 
, are minimum machine startup time, minimum communication overhead, minimum RAM size and communication rate in the processor set, respectively. In (32) it is assumed that the shortest possible communication is done and then, whole load  is processed on all machines in parallel. The energy lower bound is (33)
where 
 is minimum machine starting power, 
 is minimum networking power, and 
 is the minimum energy per load unit in the processor set. In (33) it is assumed that the least energy-costly machine startup is executed, all the load is transferred over the least energy-consuming communication link and the load is processed on the most energy-efficient processor. In Fig. 12 quartiles Q1, Q2, Q3 for the population of all the method solutions are shown. Moreover, the medians (Q2) of the best, the worst, and Rnd processor sorting rules are shown as lines. It can be seen that the median of all methods does not differ much from the performance of Rnd order. Some methods slightly distinguish themselves both in positive and in the negative sense. For example, the order of increasing in-core computing rates  (that is decreasing computing speeds) allows for a bit shorter schedules (Figs. 12a, 12c), and consequently more energy-efficient solutions among the CSS-heuristics (Fig. 12b). For these cases sorting rule  improves schedule length and energy quality measures by roughly 30% related to the Rnd median quality. Depending on the number of processors, results of  are in the lowest 10%–30% of the results population of Rnd sorting rule. Similar observations can be done for  rule (Fig. 12d). It can be concluded that the quality of solutions generated by the heuristics is similar, but PSRs ,  have some modest advantage.

8. Conclusions
In this paper time and energy performance of processing data-parallel computations in heterogeneous systems with hierarchical memory has been studied. Hierarchical memory subsystems incur complex dependence of the running time and energy consumption on the size of solved problem. These dependencies have been represented by piecewise-linear functions. The computation scheduling problem has been rendered as an optimization problem consisting in selecting the set of activated processors, the sequence of processor communications, and sizing the load chunks. Two approaches have been proposed: solving a MIP formulation, or applying fast heuristics. The results obtained indicate that due to the existence of idle processors which still consume some power, there are sharp local minima in the energy vs. schedule length functions. Hence, short schedules, even though compute- and power-intensive, can be more effective than long schedules with some machines in low-energy computing mode and some other in the idle mode. Moreover, holding machines in a diverse set of energy modes is advantageous because the machines in shallow suspension can quickly start the computations, while simultaneously starting the machines in deeper suspension modes. It has been also established that limited use of the out-of-core memory may be beneficial by limiting communications or activating fewer machines. The performance of the scheduling algorithms is determined by the solutions’ quality and the runtimes. The schedules obtained by solving an MIP are almost always the best, but this dominance comes at cost: MIP runtime and the need for information on the model parameters. The fast heuristics proposed here build solutions approx. 3 times worse with respect to solution quality, but in 4–5 orders of magnitude shorter time while requiring less detailed information on the application and system parameters. Among the fast heuristics, GSS methods offer good trade-off between solution quality and runtime, while sending the load to the fastest or the least-energy consuming processors is moderately advantageous.