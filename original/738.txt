Abstract
With the advent of Internet of Things (IoT) connecting billions of mobile and stationary devices to serve real-time applications, cloud computing paradigms face some significant challenges such as high latency and jitter, non-supportive location-awareness and mobility, and non-adaptive communication types. To address these challenges, edge computing paradigms, namely Fog Computing (FC), Mobile Edge Computing (MEC) and Cloudlet, have emerged to shift the digital services from centralized cloud computing to computing at edges. In this article, we analyze cloud and edge computing paradigms from features and pillars perspectives to identify the key motivators of the transitions from one type of virtualized computing paradigm to another one. We then focus on computing and network virtualization techniques as the essence of all these paradigms, and delineate why virtualization features, resource richness and application requirements are the primary factors for the selection of virtualization types in IoT frameworks. Based on these features, we compare the state-of-the-art research studies in the IoT domain. We finally investigate the deployment of virtualized computing and networking resources from performance perspective in an edge-cloud environment, followed by mapping of the existing work to the provided taxonomy for this research domain. The lessons from the reviewed are that the selection of virtualization technique, placement and migration of virtualized resources rely on the requirements of IoT services (i.e., latency, scalability, mobility, multi-tenancy, privacy, and security). As a result, there is a need for prioritizing the requirements, integrating different virtualization techniques, and exploiting a hierarchical edge-cloud architecture.

Previous
Next 
Keywords
Mobile Edge Computing

Fog Computing

Classical virtualization

Network function Virtualization (NFV)

1. Introduction
Cloud computing is the delivery of centralized and virtualized computing, storage, services, and application resources over the Internet. Cloud Computing decouples services from underlying infrastructure and eliminates upfront cost and complexity of managing IT infrastructure [42]. However, cloud computing is not able to serve real-time Internet of Things(IoT) applications because centralized cloud infrastructure is usually located at a few fixed places, which are away from users; moreover, IoT applications require ultra-low latency, low jitter, high-demand bandwidth, mobility services, to name a few [189].

The above-mentioned requirements of IoT applications can be met to some extent through paradigms that are fundamentally created based on a cloud computing infrastructure. Spanning Cloud Computing (SCC) [141], [229] is a cloud-based delivery model in which an application requiring a large pool of computing resources is deployed over multiple cloud datacenters to achieve better performance, higher availability and lower monetary cost [67], [68], [139]. SCC is a double-edged sword cloud-based paradigm since it imposes several overheads such as management and orchestration of resources and requires a security mechanism to protect decentralized resources across different cloud datacenters. Cloud-based Content Delivery Network (CCDN) [242] is another cloud-based paradigm that takes the advantage of Geo-distributed and pay-as-you-go model of cloud platforms and provides content-delivery-as-a-service. Similar to the traditional CDNs, CCDNs such as Amazon CloudFront [16], Google CDN [88] and Azure CDN [23] enhance Quality of Experience (QoE) through delivery of content by replicating them in the vicinity of the content requester [201], [272]. Although both SCC and CCDNs move data processing and storage closer to users and improve performance metrics, they are far behind the requirements of IoT applications.

To meet the requirements of IoT applications, there is a need for cloud-based services in proximity of data source (e.g., IoT devices/applications) to process, analyze and filter data for taking proper actions. The extension of computing services from centralized cloud-based paradigms to the edge of network is called edge computing [113], [199], [263] that boosts the overall efficiency of infrastructures by achieving ultra-low latency, reducing backhaul load, supporting mobility services, and increasing service resilience.

Edge computing paradigms consist of connected resource-constrained devices such as smartphones, wearable gadgets, single-board computers (SBCs), network devices (switches, routers) and resources with moderate capabilities such as base stations (BSs), LANs, Cloudlets [204], to name a few [144]. Devices generate and collect a huge amount of data that is pre-processed and analyzed initially at the edge of a network. This data can then be transferred to a centralized cloud for extracting deep knowledge using different Machine Learning (ML) and Optimization (convex or Bayesian) approaches [149], [222]. Thus, edge computing is complementary to cloud computing. Mobile Edge Computing (MEC) [1], [6], [143] and Fog Computing (FC) [162], [165] are the most prominent paradigms of edge computing to harness the power of distributed edge resources to support IoT applications such as smart houses, grids, retails, farming, connected cars and healthcare.

Like cloud computing, the fundamental technology of edge computing paradigms is resource virtualization that decouples hardware resources from software in order to run multiple tenants on the same hardware [12], [39], [157], [158]. Nevertheless, edge computing has distinctive differences with cloud computing in terms of location awareness, mobility-based services, resource- constrained and heterogeneous devices and wide-spread distribution of devices. These differences imply that heavyweight virtualization (running virtual machine monitor (VMM) directly on either hardware or OS) [85], [233] cannot be applicable to all use cases of IoT applications that may leverage edge computing. This constraint leads to the deployment of lightweight virtualization techniques such as containers [208] and unikernels [135] at the edge devices with limited and moderate resource capabilities. These techniques can be scaled down to edge resources as they require less CPU cycles, smaller memory footprint size, less instantiation time and more agility in mobility. The pure usage of one technique may not be performance wise effective [159], [160] and a combination of techniques can be utilized based on the virtualization features, resource capabilities and application requirements.

In addition to the virtualization of computing resources, there is a strong tendency to virtualize networking resources to meet the requirements of IoT applications and to cope with the dynamism of a network status with the help of Software Defined Network (SDN) framework [117]. Network Function Virtualization (NFV) [168] decouples service/network functions (SFs/NFs) such as firewalls, load balancers, Intrusion Detection system (IDS) from underlying network hardware and converts them into Virtualized Network Functions (VNFs). VNFs run on classical VMs, containers and unikernels to execute Service Function Chains (SFCs) with direct and causal dependencies. Due to the existence of such dependencies, it is essential to design algorithms for NFV placement and migration at the edge-cloud environments. The proposed algorithms for classical VMs are not directly applicable since the classical VMs run users’ requests without any dependencies.

Related Surveys: With respect to the management of computing and network resources in an edge-cloud environment, researchers have surveyed and analyzed different aspects of FC [261], MEC [72], [133] and cloudlet [204] from definitions, architecture, resource management and security perspectives. Some studies briefly discuss the concept of different edge computing paradigms, compare them based on several features and identify their roles in IoT applications [184], [234], [259], [259]. They mostly focus on the requirements and configuration of edge devices, which make them suitable to large scale IoT applications.

Several researchers have conducted surveys on architecture and resource management in edge computing. Hong et al. [100] discussed infrastructures, architectures and algorithms for resource discovery and management in FC. Yousefpour et al. [261] have delineated different edge computing paradigms in details from definition and usage perspective. The authors briefly have discussed FC from different aspects of performance and security. Another study [7] has provided an exhaustive details of Service Placement Problem (SPP) in the context of FC. Similarly, Mouradian et al. [162] discussed applications specific architectures and algorithms for resource sharing, task scheduling and load balancing in FC. Naha et al. [166] studied different architectures of FC in details and review resource allocation and scheduling, fault tolerance and simulation tools in FC environments. Some researchers have analyzed different aspects of security in edge computing paradigms. A recent survey in [199] focused on a detailed analysis of different threat models that impede the integrity of edge computing paradigms. The authors in [170] delineated security and privacy challenges in FC and analyzed promising solutions to tackle these challenges. In another study [114], the authors argued security challenges of applications and potential solutions in the FC context.

There are several valuable survey papers on MEC, as a subset of FC, from different aspects. Mach et al. [133] have reviewed different architectures of MEC and substantially focused on computation offloading from user’s perspective. Abbas et al. [1] have presented a survey on MEC based on an investigation of the related technologies. They discussed deployment of different applications in the MEC context from performance and security perspectives. Mao et al. [143] have provided an extensive survey with focal point on joint radio-and-computational resource management. Wang et al. [248] have presented a survey on MEC that covers the convergence of computing, caching, and communication techniques in MEC. The authors in [245] have surveyed research efforts on service migration in MEC and briefly summarized the three hosting technologies (virtual machine, container and agent) for mobile applications.

Several survey studies in the NFV context have been conducted in terms of architectures, management and security. Bonfim et al. [39] have provided a systematic literature review on the integrated architecture of SDN/NFV and NFV deployment scenarios. Mijumbi et al. [150] have explained the relationship between NFV, SDN, and cloud infrastructures, NFV standards and currently implemented NFV projects. In [253], the authors have analyzed different strategies of NFV placement as well as their cons and pros. A comprehensive survey paper [260] covered many NFV aspect varying form NFV architectures, NFV standards, to different algorithms for NFV placement, scheduling and migration. The authors in [122], [257] have debated security threats in the NFV infrastructures and the existing solutions for the identified threats. Table 1 summarizes a comparison between our work and the state-of-the-art efforts in the context of virtualization at the edge computing.

Paper contribution and organization: Although the above-discussed survey papers are inspiring, none of them specifically investigates the research issues in virtualization of computing and networking resources at the edge network. Also neither do they discuss to leverage virtualization to support micro-services in IoT frameworks to meet the requirements of IoT applications. Although these studies indicated reasons to leverage edge computing, nor do they present the reason behind shifting from cloud computing and its derivates (i.e., SCC and CCDN) to the edge computing paradigm from more than 20 features and 4 pillars. Motivated by these research gaps, we aimed to cover four specific subjects. (1) Investigating the reason behind moving from cloud computing paradigms (CC, SCC, and CCDN) to the edge computing paradigms (MEC and FC). (2) Discussing different virtualization techniques and their capability to deploy in IoT frameworks, and mapping the state-of-the-art studies with respect to the selected factors of virtualization techniques. (3) Delineating placement and migration of virtualized computing and networking resources at the edge network and mapping up-to-date studies to provide taxonomies in terms of virtualization type, cost function, objective function, solutions, and so on. (4) Identifying the potential research direction for virtualization of computing and networking resources in IoT frameworks that include application, gateway and hardware layers as discussed later. The indicated subjects shape the structure of the paper as shown in Fig. 1, in which the topics in the middle boxes in respect to computing and networking virtualization can be read independently.


Table 1. Contribution of the survey papers on virtualization of the cloud-edge infrastructure.

Work	Context	Computing Virt.	Networking Virt.	Objectives
Salaht et al. [200]	Fog and edge
(service placement problem)	No	No	Investigating optimization algorithms for services placement
Hong and Varghese [100]	Fog/edge computing (resource management)	No	No	Investigating architectures, infrastructure, and algorithms for managing resources
Ren et al. [196]	All edge paradigms (Architecture and characteristics)	Partial	Partial	Discussing offloading computation, caching, security, and privacy
Mouradian et al. [162]	Fog
(Architecture and algorithm)	No	No	Discussing application-agnostic/specific architecture and algorithms for resource sharing, task scheduling, offloading, load balancing
Dolui and Datta [62]	All edge paradigms (Implementation)	No	No	Implementation of different fog computing
Khan et al. [114]	Fog Computing (security)	No	No	Investigating security issues and possible solutions in fog computing
Naha et al. [165]	Fog (Architecture and application requirements)	No	No	Discussing the requirements of infrastructure, platform, and applications
Capra et al. [44]	Edge computing (hardware requirements)	No	No	Discussing hardware requirements for IoT
Roman et al. [199]	Mobile edge (security)	No	No	Discussing threats and security issues in FC, MEC, and MCC
Yi et al. [259]	Fog (Concepts)	No	No	Discussing essential concepts in terms of virtualization, security and privacy, QoS, computation offloading, resource management, and so on
Ni et al. [170]	Fog (Architecture, concepts, security)	No	No	Discussing architecture and role of fog computing in IoT frameworks as well as its security and privacy issues
Abbas et al. [1]	MEC (architecture and application)	No	No	Presenting different architectures and applications deployment in the MEC context
Mach et al. [133]	MEC (Architecture and computation offloading)	Partial	Partial	Discussing different MEC architecture and computation offloading in MEC servers
Mao et al. [143]	MEC (communication)	No	No	Providing a focus on joint radio and communication resource management, and discussing issues in terms of mobility, energy, caching, mobility, and privacy in the context of MEC servers
Beck et al. [29]	MEC (applications deployment)	No	No	Analysing opportunities and limitation of applications deployment from technical perspective in MEC servers
Wang et al. [245]	MEC (service migration)	Partial	No	Discussing live migration for datacenters and handover in cellular networks
Bonfim et al. [39]	Edge (architecture)	No	Partial	Investigating in-depth review of NFV/SDN architectures and synthesizing their design
Our Work	Edge (virtualization)	Yes	Yes	Investigating virtualization of computing and networking resources in edge computing from features and performance perspective
Virt: virtualization.


Download : Download high-res image (194KB)
Download : Download full-size image
Fig. 1. The organization of the paper.

The remainder of this paper is organized as follows. Section 2 introduces different cloud and edge computing paradigms, followed by the features and pillars analysis in Section 3 to understand the reasons behind moving from cloud to edge computing paradigms. Section 4 reviews and compares different virtualization techniques and identifies the selection factors of these techniques for IoT frameworks. Placement and migration of virtualized computing resources at the edge computing are discussed in Section 5. Section 6 discusses deployment and management of virtualized networking resources in the context of edge computing. Section 7 outlines the potential directions for future research followed by a conclusion in Section 8.


Download : Download high-res image (91KB)
Download : Download full-size image
Fig. 2. The evolution of computing utilities.

2. Cloud and edge computing paradigms
The study of distributed computing can be traced back to the early 1960s, when cluster computing emerged in the client server era, as shown in Fig. 2. The second milestone in the context of computing paradigms was in the early 2000s known as the cloud computing era. Then, the concept of cloud computing has been extended to the edge computing as refereed to the edge computing era. In the following, we discuss the last two computing eras and the reasons behind the transition between these two computing paradigms.

2.1. Cloud computing paradigms
Cloud Computing (CC) provides virtualized infrastructures, platforms, and software services over the Internet [42]. Infrastructure as a Service (IaaS) allows users to access computing, storage as a standalone VM. Platform as Service (PaaS) supports customers to develop, manage and run applications without dealing with software configuration and maintenance related issues to those applications. Software as service (SaaS) hosts applications and facilitates them with the needed infrastructures. All these cloud service models are identical in elasticity, on-demand provisioning and pay-as-you-go model [75]. Cloud computing supports flexibility in infrastructure scalability, resource diversity, level of access control and security features. It also provides efficiency in data and applications access, data security and Quality of Service (QoS), eliminates capital expenditure and reduces operation costs.

However, being no exception, cloud computing imposes high latency and jitter on applications due to sending and receiving data across wide area network (WAN) [94]. Second, it makes vendor lock-in as a major impediment to users who are vulnerable to any surge in services price, and changes to regulations and policies imposed by cloud providers [18]. Last but not the least, cloud computing supports limited control for users to manage and monitor their services. To overcome such limitations, users resort to Spanning Cloud Computing (SCC).

Spanning Cloud Computing (SCC) supports multiple datacenters deployment and extricates users from drawbacks of relying on a single datacenter, and explicitly or implicitly puts together the merits of different cloud providers [229], [235]. SCC enables users to avert vendor lock-in which may lead to users’ business becomes more resilient to distributed denial of service (DDoS) attacks. This is because if one datacenter goes down, users can utilize others. SCC also allows users to have a larger set of datacenters selection, which leads to latency and jitter reduction [249]. Furthermore, SCC offers users to have competitive options in terms of performance [193], [235] and monetary cost [142], [142].

Despite these benefits, we believe that SCC is a double-edged sword and suffers from the complexity of managing and monitoring services which require specific tools and software that may result in more challenges in cost management. It also potentially is a threat from security perspective due to revealing more access points to hackers. In summary, SCC enhances some performance metrics (e.g., reduction in latency, jitter and monetary cost) but it has still a big gap to meet stringent latency, low jitter and mobility services for IoT applications.

Cloud-based Content Delivery Network (CCDN) brings more and closer cloud-based servers to users to enhance redundancy, performance and flexibility [201], [242]. CDNs reduce response time and jitter through these edge servers in Point-of-Presence (PoP) locations and reduce traffic on a core network via finding the best route to send requests to edge servers. Hence, they improve response time further in comparison to SCC but they still cannot meet the requirements of IoT applications.

Therefore, despite improvements in some features through the transition from cloud computing to SCC and then to CCDNs, all cloud-based paradigms are generally deficient in the required capabilities to serve IoT applications’ overwhelming demand for handling and processing a tremendous amount of data in near real time. To fulfill the requirements of IoT applications, there is a need of edge computing ecosystem to support the processing of big data in the vicinity of the data sources.

2.2. Edge computing paradigms
In contrast to cloud computing, edge computing offers constrained and decentralized infrastructure resources, while bringing the required resources closer to data sources/edge and averting the requirements of sending data to a centralized cloud. Thus, edge computing reduces latency, jitter and load on a network core and improves security through storing data in on-premises infrastructure. The definition and location of edge are a controversial issue among researchers [261]. Some of those believe that edge is IoT-connected devices with a limited resource capabilities that process the collected data. The other researchers consider edge as a concept that moves processing to data sources. We believe that the edge of a network and its location highly depend on the context of applications deployment. An edge is a logical border and it can be changed as the data consumer and data generator/provider are varied. For instance, in the context of connected vehicles, a car is an edge where the location of data collection and process are the same, while in the context of MEC, radio access network (RAN) is an edge computing that processes a user’s data. At a larger scale, a server in CCDN is an edge that process a user’s requests. In the following, we discuss different paradigms of edge computing.

Mobile Cloud Computing(MCC) becomes popular as the proliferation of mobile devices necessitate to efficiently manage constrained resources. The nature of such resources attributes to the synergy between cloud computing and mobile devices as Mobile Cloud Computing (MCC) [72]. MCC overcomes several limitations from users’ perspective like battery life, computation power, memory limitation especially for running data-intensive mobile-based applications [261]. Nevertheless, the deployment of cellular communication to transfer data to/from the network core on a long distance imposes high latency and jitter, as well as overhead on core network. Instead, the viable solution is to compute, analyze, and filter data close to data source, which paves the way towards Fog computing (FC) [261] and Mobile Edge computing (MEC) [133].

Fog Computing (FC) [40] pushes the resources down to IoT devices by enabling data processing, analyzing, managing and filtering at the edge of a network. FC allows computing and storage resources to scatter along the path between IoT devices and cloud computing depending on the required QoS. Thus, the notion of FC is broad and includes any devices with or without computing, storage and network capabilities. These devices vary from single board computers (SBCs), wireless/wired access points, network devices (switches, gateways), base stations, LAN networks, to any resource rich nodes. Fig. 3 shows hierarchical architecture of FC, where cloud is in the top, devices with very low richness are at the bottom, and fog nodes or Mobile Edge Computing (MEC) servers are in the middle. The trend of data management in fog ecosystem is of data collection via sensors and sending it to SCCs to act accordingly by actuators. For instance, in an agricultural automated irrigation system, sensors installed in agricultural farm collect data in terms of temperature, soil humidity and wind speed and send them to SCBs for analysis and taking proper actions, e.g., irrigating for 20 minutes. This trend makes FC suitable for IoT applications where mobility, low latency, location awareness requirements cannot be supported by cloud computing paradigms. This trend of data processing also makes FC not to have continuous connection of Internet since the updated data can be sent to cloud later. However, the data with high volume requiring heavy processing should be sent to a cloud to make long term and established decisions based on the available data.

Mobile/Multi-access Edge Computing1 is an extension of MCC and enables cloud-based resources and services in the proximity of users. MEC is a subset of FC and provides virtualized computing resources with moderate capabilities in close proximity of the resource constrained mobile devices. These resources are termed MEC for which servers are co-located with base station (Fig. 4) and deployed at a stationary location such as a train station, stadium, shopping center or at a mobile location like car. The synergy between 5G and MEC supports significantly faster data (up to 10 Gbps), ultra-low latency, and more connected world. The evolution of Radio Access Network (RAN)2 architecture leads to different MEC paradigms. For 1G and 2G cellular networks, RAN had all-in-one box including analogue, digital and power functions located in a dedicated room with all facilities such as air conditioning and power supply. From 3G onwards, the remote radio head/unit (RRH)3 was separated from BBUs4 through fiber cable or microwave and it is called D-RAN [93]. D-RAN includes a Base Transceiver Station (BTS) with a local dedicated BBU, which is associated with one cell. In D-RAN, handover is time consuming due to long distance between BBUs, and management cost is high as the number of users increases [13], [49], [54]. Thus, D-RAN cannot meet the requirements of IoT applications.


Download : Download high-res image (264KB)
Download : Download full-size image
Fig. 3. Fog Architecture.


Download : Download high-res image (199KB)
Download : Download full-size image
Fig. 4. Simple Architecture of Mobile Edge Computing (MEC).


Download : Download high-res image (444KB)
Download : Download full-size image
Fig. 5. Architecture of H-CRAN (left) and F-RAN(right), F-UEs: Fog User Equipments, F-AP: Fog Access Point.


Table 2. Different Paradigms of Mobile Edge Computing (MEC).

Feature	D-RAN	C-RAN	H-CRAN	F-RAN
Architecture	NA	Centralized	Centralized	Centralized, Decentralized
Latency	Very high	Very high	High	Very low
Fronthaul load	Low	Very high	High	Very low
CAPEX/OPEX costs	Extreme high	High	Very high	Medium
Resource deployment	BBU, RRH	BBU,RRH	BBU, RRH HPN	BBU, RRH, Fog-node
Data and Control plane separation	No	No	Yes	Yes
Computing virtualization	No	Yes	Yes	Yes/No
Network virtualization	No	No	Yes	Yes/No
Resource proximity	Far	Close	Close	Very close
Cellular network utilization	3G-4G	3G-4G	4G	5G
To address such issues, C-RAN [49] offers virtualized BBUs to decrease OPEX and CAPEX costs [238], and centralized BBUs to improve network performance and bandwidth rate. However, C-RAN shifting computing processing into a centralized BBU demands an interconnection fronthaul links with high bandwidth, which confines it to serve IOT applications. To cope with such drawback, a heterogeneous C-RAN (H-CRAN) [126], [183] decouples data and control planes through High Power Nodes (HPNs)5 to reduce fronthaul load and increase data transmission rate. Like C-RAN, H-CRAN cannot meet the requirements of IoT applications because it still exploits centralized resources that impose a heavy load on fronthaul links with the increment of users.

To further mitigate load on fronthaul links, processing and signaling resources are moved adjacent to users through UEs and network devices such as RRHs. This design leads to distributed and centralized Fog/F-RAN [28]. In the distributed F-RAN, some computation, storage and resource management functions are drifted from BBU to UEs, while in the centralized one resources are controlled and managed with exploitation of SDN and NFV. Thus, F-RAN reduces latency further and makes resources management easier compared to D-/C-RAN and H-CRAN.

As discussed above, the aim of the changes in the architecture of edge computing paradigm is to reduce latency, jitter and load on network fronthaul and backhaul. This makes edge computing paradigms more suitable for IoT applications. As depicted in Fig. 4, Fig. 5, apart from D-RAN, the architecture of all MEC paradigms consists of three layers. Terminal layer consists of mobile devices that host applications including compute-intensive (e.g., Augmented Reality), latency-sensitive (e.g., driverless cars, tactile internet applications), and bandwidth-intensive (e.g., mobile big data analytics). As can be seen, the main difference between MEC paradigms is in access and network layers. In C-RAN control, caching and communication are centralized in network layer (Fig. 4), while in H-CRAN control is centralized in access layer to reduce load on fronthaul link (Fig. 5, left-side). In F-RAN, storage and communication are centralized in network layer and distributed in access and terminal layers (Fig. 5, right-side).

With respect to the architecture of different MEC paradigms, we compare them from main features perspective as summarized in Table 2. F-RAN exposes resources at the edge of a network and it thus has the lowest latency, and H-CRAN follows the next. D-RAN and C-RAN impose the highest latency to users/devices, which depends on the distance between users and centralized resources/RAN in C-RAN/D-RAN. C-RAN increases load on fronthaul link more than H-CRAN, which in turn, raises it more than F-RAN in which the load reduces further through exploitation of edge network. Aside from D-RAN, all other MEC types leverage computing virtualization to enhance the efficiency of resource utilization. Due to the separation of data and control plane, H-CRAN and F-RAN can exploit network virtualization. Therefore, considering the features listed in the above table, F-RAN is the most suitable candidate to handle IoT applications.

Miscellaneous Edge Computing Approaches: The aim of all the discussed edge computing paradigms is to meet the requirements of IoT applications. In addition to the discussed edge paradigms, Cloudlet [204] is a type of edge computing that is located between edge devices and cloud computing in the proximity of users to address the drawbacks of MCC with exploitation of a trusted and resource-rich server in one hop away from users/edge devices (e.g., community places). Mobile ad-hoc Cloud Computing (MACC) is another type of edge computing and includes mobile edge devices which shares resources in a temporary and dynamic network via transport and routing protocols [101]. It is a decentralized infrastructure and suits to environments with lack of continuous internet connectivity [261]. Compared to FC and MEC, MACC consists of only mobile devices, making it more decentralized [261]. There are also some buzzwords such as mist computing [188] and follow me cloud [220] that more and less aim at closing resources to users. Liu et al. [128] presented open-source edge computing projects in academia and industry and draw a comparison between open-source tools for these projects.

3. Features analysis of cloud- and edge-based paradigms
In this section, we zoom out from details of each specific computing paradigm and analyze them with respect to their features and pillars as depicted in Fig. 6.

3.1. Feature-based comparison
To understand the objective of edge-cloud landscape as a whole, we investigate the following features.

(1) Resource capacity and type: It relates to amount and type of computation, communication and storage resources. As application providers move more towards edge-based paradigms, the capacity and type of resources reduce.6 It is worth noting that SCC is an exception that offers different types of resources from different cloud providers. The limitation of resources means application providers need to make a compromise between computation and data transfer time as illustrated in Fig. 7. Thus, real-time and short-term decisions are taken at the edge of a network; otherwise data is sent to cloud-based infrastructure.

(2) Decentralized topology: It refers to the arrangement of nodes (every of things) and connection in networks. More movement towards edge computing, more decentralized and distributed in network topology. This feature allows devices/ resources to dynamically join and leave network in edge computing compared to those in cloud-based paradigms. In fact, from users perspective, network topology of cloud-based paradigms is a black-box, although a logical network topology in SCC can be defined such as a tree topology in which master and slave nodes respectively are at the root and non-root level of a tree. In edge-based paradigms, network topology can be a complex graph, where MACC is the most dynamic in network topology due to including only mobile devices with the highest degree of decentralization [261]. The network topology has significant effects on performance metrics such as execution delay required to offload an application to edge computing resources.

(3) Latency and jitter: Latency is the most important reason behind the deployment of an application at network edge [40], [47]. As applications are more shifted from cloud-based to edge-based paradigms, they incur less latency and jitter. Cloud-based paradigms impose several hundreds of milliseconds latency which depend on the distance between client and cloud, cloud datacenter provider, route of the network traffic, among others [155]. Deployment of SCC can reduce latency 80% and even further through CCDNs [249]. Even several tens of milliseconds latency are still high for real-time applications, which tolerate at most hundreds of nanoseconds or less than ten milliseconds. High jitter has a negative influence on performance of real-time applications especially beyond 100ms [250]. Edge-based paradigms reduce jitter since they avoid sending data over WAN network.

(4) Intelligence: Full life-cycle data service consists of the following sub-services [273]. Data pre-processing includes aggregation, filtering, and cleaning raw data. Data analysis provides decision making based on optimization approaches and statistical models. Data distribution and execution policy executes predefined rules based on analyzed data locally in network edge or sent it to a cloud. The first sub-service is usually managed in edge devices, while the two later sub-services are conducted either in a network edge or a cloud. The selection of each paradigm depends on the required response time, resources capability, and the size of data leveraged. Thus, as the capability of the resources increase the intelligence strengthen raises from edge- to cloud-based paradigms as shown in Fig. 6

(5) Availability and reliability: As long as we move towards edge-based paradigms, availability and reliability decrease as depicted in Fig. 6. This is because components of a system co-operatively provide a service. It should be noted that SCC is an exception in this respect because replication of computing and data across multiple-datacenters increase availability [140]. MACC has the lowest availability among edge computing due to freely attached and detached mobile nodes to ad hoc networks.

(6) Security and privacy: Moving from cloud- to edge-based paradigms increases privacy and reduces security. This is because (i) edge-based paradigms eliminate data access through third party especially in FC due to storing data under control of users, and (ii) cloud-based paradigms are more strengthen in physical cyber-security and deployment of centralized and heavy security mechanisms. Due to the nature of edge computing, it is required to design flexible, lightweight security mechanisms so that attacks and failure to a certain extent should be handled and recovered in the least amount of time. Classical trust-based security mechanisms such as policy- and SLA verification-based [112] are not an effective way to utilize a large number of heterogeneous edge devices. Thus, deployment of security mechanism with less authentication principles (e.g., white-listing functions [78]) can be a viable solution.

(7) Resource utilization: To understand the usability of edge computing for a specific application, we need to define performance metrics in terms of resource utilization (processing, storage, bandwidth, I/O, energy) and throughput [20]. As we move more towards the edge computing, these performance metrics are more critical from users perspective due to tight constraint on processing power, storage and RAM capacity, as well as battery lifetime. Among these, battery life is the most important concern from user perspective in the context of edge computing compared to the cloud computing that exploits both brown and green energy [146], [255]. To relieve this concern, the solutions can be varied from computing offloading strategies, lightweight design OS for mobile devices, hardware modules (e.g., CPU and GPU) to spread workloads across edge-cloud computing [53], [232]. Nevertheless, cloud and edge computing are common in many of these performance metrics that are evaluated mostly via simulators. Since these simulators mostly consider many assumptions (which are not compatible with real environment), it would be better to evaluate these parameters in a real implementation for edge computing. This evaluation is more in accordance with the real-time data generation in edge computing required for on-line action to a particulate event. Interested readers are referred to more details in respect to the evaluation and definition of performance metrics for cloud and edge computing in [21].

3.2. Pillars-based comparison
We define pillars7 as the essence of edge-based paradigms in terms of constraints and objectives. The pillars are as follow.

(1) Constrained-resource devices: These devices include a range of sensors, actuators, gateways, routers and servers with medium capabilities (e.g., MEC server). They process data closer to data sources, reduce data transfer time and provide intelligence to react to the environmental events. However, such devices restrain to deploy heavy virtualization techniques and data processing. Thus, the former lack mandates to use lighter virtualization techniques, and the latter one necessitates a synergy between edge and cloud paradigms.

(2) Ultra-low end-to-end (E2E) latency: This is one of the essential pillars that inspires users to make the resources close to data sources. This requirement is a necessity for services such as car-to-car communication, remote operations and factory automation. The following ways can reduce E2E latency. (i) Enhancement in cellular communication: 5G makes major advance on bandwidth capacity (100 Mbps–10 Gbps), improved connectivity (to more than 100 billion devices), reduction in energy consumption (1000 times lower) and mobility support (for users/devices with speed of at most 500 km/h).8,9 (ii) Move resources closer to users: As distance between edge devices to edge computing is shorter, E2E latency is less. It is worth noting that E2E latency depends on the required time for sending data to/from edge devices and processing task/data in edge server [133]. Thus, FC and MEC are the most fitting to reduce E2E latency. (iii) Offloading tasks to edge computing: As we will discuss later, this is one of the most vital strategy to reduce E2E latency by optimizing the deployment of under-utilized servers, and less congested links between edge device and edge server.

(3) Mobility Services: This is another pillar that motivates users to leverage edge computing paradigms. Mobility can be one direction or mutual, where in the former one device is mobile and other is fixed as in MEC [72], [133] and Cloudlet [204], while in the latter both devices are mobile as observed in the context of FC such as Vehicle to Vehicle (V2V) communication [264]. Mobility can be considered for computation, data and users. In one-direction mobility, most data and computation are transferred from a less capable device to a more capable one to speed up computation and meet QoS. Whilst in mutual mobility, users/devices make close to each other or relay on MEC to speed up procuring data required. Moreover, in mutual mobility resource discovery is more challenging since each side is mobile and required specific QoS. Mobility services must provide stable connectivity between mobile devices through handover/handoff [91], [92] that is initiated via network or users based on factors such as application type, required bandwidth, delay preferences, power consumption, network load and estimated data rates [72], [133]. The handover operation is abstracted in the form of a VM, which is placed close to mobile devices. Obviously, this operation is more complex for mutual mobility since VM placement is affected by two mutual mobile devices as required for V2V wireless communication [17].

(4) Virtualization: Virtualization10 is the essence of edge-cloud environments and abstracts computing and network resources [225]. Virtualization plays a key role in CAPEX and OPEX reduction, resources allocation to the new services and reclaiming them as no longer needed. In contrast to the cloud data centers, the characteristics of edge computing impose several challenges to virtualized computing and network resources. (i) Resource-constrained edge devices/fog nodes mandate to deploy lightweight virtualization techniques in terms of memory footprint. (ii) Mobility of edge devices/users implies to deploy agile virtualization techniques in terms of memory footprint and instantiation time to easily migrate VMs from one node to another. (iii) The nature of edge devices distribution and wireless and wired communication make a requirement of global management and control of resources.

We notice that virtualization techniques have been well studied and developed for the well-rich resources (e.g., cloud data centers). Nonetheless, as summarized in Table 4, most edge paradigms support computing and networking virtualization, and these virtualization techniques should be tailored for them to make compatible with their indicated limitations. Due to high mobility of devices, VM migration in edge computing should be revised to make it agile for time critical applications. Networking virtualization providing via SDN and NFV framework [39] needs to be efficient in terms of bandwidth, energy, and performance, and also compatible with legacy devices. Such virtualization hides the complexity of IoT platform and makes efficient IoT big data analytic in the collaborative cloud-edge paradigms. The rest of this work discusses virtualization of computing and networking resources at the edge network for IoT applications (see Table 3).


Download : Download high-res image (222KB)
Download : Download full-size image
Fig. 8. A comparison of computing virtualization techniques from architecture perspective in both cloud and edge computing.


Table 3. A comparison of features and pillars of cloud-based paradigms. Pillars are bold.

Feature	Cloud Computing(CC)	Spanning Cloud Computing(SCC)	Cloud-based Content Delivery Network(CCDN)
Proponent	Cloud providers	NA	Usenet
Architecture	Centralized	Graph-based	Graph, Hierarchical
Usage type	General	General	Content Delivery
Standardization	NIST, ITU	CSCC, ETSI	IEEE
Use-case	General	General	Content-delivery
Network connectivity	WAN	WAN	WAN, LAN
Resources type	DC	Mulitple-DCs	MD＋DC
Resource capacity	High	Very High	High
Application(s) support	Computation-intensive	Computation-intensive	Data-intensive
Mobility support	No	No	No
Virtualization support	Yes	Yes	Yes
Proximity	Very Far	Far	Relatively far
Ultra-low latency	No	No	No
Location awareness	No	No	No
Security granularity	DC	DC	Participant data servers
NA: Not Applicable, DC:DataCenter, WAN: Wide Area Network, MD: Mobile Devices, ED: Edge Devices, ETSI:European Telecommunications Standards Institute, CSCC: Cloud Standards Customer Council, NIST: National Institutes of Standards and Technology, ITU: International Telecommunication Union.


Table 4. A comparison of features and pillars of edge-based paradigms. Pillars are bold.

Feature	Mobile Cloud Computing (MCC)	Mobile Edge
Computing (MEC)	Fog Computing (FC)
CloudLet	MAC	
Proponent	NIST	NA	Nokia, IBM	Cisco
Architecture	MD＋CloudLet＋DC	Distributed	Distributed	Decentralized/Hierarchical
Usage type	Mobile Devices	Mobile Devices	Mobile Devices	General
Standardization	NIST	NA	ETSI, 3GPP, ITU	OPenFog Consortium, IEEE
Usecase	Mobile applications	Disaster relief	mission-critical	IoT applications
Network connectivity	WAN	Bluetooth, Wi-Fi, Cellular	WAN, cellular	All communication types
Resources type	MD＋DC	MDs	RAN＋MD＋DC	Any devices
Resource capacity	Limited	Moderate	Moderate	Very limited
Application(s) support	Moderate Computation	Very low computation	Moderate computation	Low computation
Mobility support	Yes	Yes	Yes	Yes
Virtualization support	Yes	No	Yes	Yes
Proximity	Close	Very close	Close	Very close
Ultra-low latency	No	No	Yes	Yes
Location awareness	No	Yes	Yes	Yes
Security granularity	Mobile device, DC	Mobile device	DC, RAN	ED, network between ED and DC
3GPP: 3rd Generation Partnership Project.

4. Virtualization of computing resources
The concept of virtualization came into play in mid 1960s [85], when IBM required to run multiple tasks on a mainframe in parallel. Virtualisation allows to partition a single physical server into several virtual machines (VMs) in hardware level [215] (e.g., Xen [251] and KVM [121]). Hardware-level virtualization, dominated in cloud computing, cannot effectively be leveraged in edge paradigms. This is because these paradigms aim at delivering low latency, bandwidth efficient and resilient services to IoT applications through constrained-resource edge devices. Thus, it is required to scale down virtualization infrastructure in OS-level [215] (e.g., Docker11 and LXC12) featured in low initialization time, small memory footprint, and image size. Merely usage of lightweight (i.e., OS-level) virtualization cannot meet the requirements of IoT applications as shown in Fig. 9. Thus, the decision on which virtualization technique to use depends on, as depicted in Fig. 9, features of virtualization techniques, device capabilities and application requirements. We discuss these factors in the following sub-sections.

4.1. Computing virtualization in edge computing
This section discusses virtualization techniques and provides a detailed comparison between them.

Hypervisor-based Virtualization provides hardware level isolation in two types [211] as depicted in Fig. 8. Type 1 hypervisor runs directly on the server/hardware without loading an underlying OS, while Type 2 hypervisor is installed on top of the underlying OS with the same purpose of Type 1. Type 1 is more efficient in security and latency due to the underlying OS elimination compared to Type 2 although it introduces more latency and vulnerability to security threats. In both types, each VM requires Bins/Libs,13 and guest OS. Thus, hypervisor-based virtualization is subject to large image size and slow instantiation time.

Container-based Virtualization is an OS level isolation and overcomes constraints of the hypervisor-based virtualization through a container engine in which host OS kernel is shared among processes [24] (Fig. 8). Thus, container-based virtualization reduces the image size (several MB) and the instantiation time (several seconds) of a container, making it a lightweight virtualization that utilizes resources more efficiently than its counterpart. Containers offer near native computing and network I/O performance [71].

Sharing host OS confronts containers with two major issues. (i) denial of service: If one application consumes most of the OS resources, then other applications are deprived from the bare minimum resources required for continued operations. (ii) exploiting kernel: If an attacker is able to take control of a host OS, it can access all applications running on the host. Thus, containers compromise security for performance. Furthermore, a lack of full interoperability is another weakness of containers and impedes them to fully operate across different clouds. For instance, OpenShift [194], Red Hat’s container-as-a-service platform, only works with the Kubernetes orchestrator [119] offered by Google.

Unikernel [135] is an executable lightweight virtualization image that runs on a hypervisor without requiring a separate OS as depicted in Fig. 8. It includes application code that executes a process and the required OS libraries for that application. Thus, unikernels are a single-purpose approach with smaller memory footprint and lower instantiation time. They also shrink the attack surface due to including used drivers, necessary I/O routines and the needed support libraries. Hence, they improve security features. With respect to performance, they run entirely in the privileged mode of CPU to avoid a context switch across the user-kernel boundary. However, some researchers criticize no clear definition behind unikernel memory footprint and library components included. Migrages14 and ClickOS15 are examples of unikernel.

A comparison of virtualization techniques: As shown in Fig. 8, the main difference between hypervisor- and container-based VMs is OS sharing. The same difference exists between containers and unikernels, but unikernels, in contrast to hypervisor-based VMs, include only libraries needed for an application. Thus, we make the following two remarks. (1) Unikernels are smaller in image size than containers, following by hypervisor-based VMs; As a consequence, unikernels and containers are more agile/portable to be moved while VMs are not. In practice, there is no significant difference in image size for VMs and containers that host data intensive applications. This is due to the size of OS kernel (about 10 MBs) is very less than the size of data (several GBs) hosted by VM and container. Therefore, the migration time of an individual VM or container is roughly same, but this time can be significant if a bulk of VMs or containers are migrated. (2) A container has the lowest isolation level due to exploiting sharing OS kernel, while other techniques provide better isolation level for VMs.


Download : Download high-res image (165KB)
Download : Download full-size image
Fig. 10. Properties of different virtualization techniques .

Fig. 8 also shows that unikernels are different in allocation of the required library components of OS to each application. The minimization of library components number in unikernels might help them to be safer than hypervisor-based VMs. Some references believe that unikernels are a shortened version of hypervisor-based VMs [136], [175]. In practice, with respect to the benefits of hypervisor- and container-based virtualization techniques, a better way is to have agility of containers and safety of VMs via hardware enforced isolation. In-Process Memory Isolation Extension (IMIX) [76] and Intel Software Guard Extensions (SGX) [102] are a set of security related code that is built into modern CPU to encrypt a portion of memory to protect VMs from accessing each other.

Fig. 10 compares virtualization techniques based on the following properties. A smaller image size requires less resources to be hosted and less time to migrate. The image size of a VMs, containers and unikernels typically reaches to hundreds MBs, tens MBs and several MBs respectively [159]. A shorter instantiation time brings less latency and it thus improves Quality of Experience (QoE) from user’s perspective. This value for VMs, containers and unikernels is several seconds or even minutes, hundreds milliseconds and tens milliseconds respectively [159]. A low powered hardware resources require a lighter virtualization technique for hosting a VM and less bandwidth for VM migration if needed. Safety relates to the isolation level of the address spaces used by VMs running on the same hardware. A VM technique can perfectly achieve this goal if it does not allow penetration of security threats through co-located VM [266]. The capability of storing persistent data is another criterion that makes VM more suitable in comparison to lightweight virtualization techniques.


Table 5. Edge devices with the capability of virtualization techniques.

Edge devices type	Virtualization technique	Examples of devices
Dumb devices	NA	Sensors, actuator
Low-capability devices	Lightweight	Raspberry Pi, SBCs, Camera [107], [157]
Medium-capability devices	Most lightweight	Cloudlets [204]
High-capability devices	Most heavyweight	MEC servers [72], [133]
4.2. Richness of resources
As shown in Fig. 9, the second factor that impacts the selection of virtualization technique is richness of resources. In contrast to the cloud computing, edge computing exploits constrained-resource devices with low richness of CPU, RAM and storage. With respect to the capability of resources, as summarized in Table 5, we classify the edge devices as below.

(1) Dumb devices lack computing resources to be virtualized and it is thus exploited in the form of bare metal. For example, sensors and actuators collect data from environments based on their functionality.

(2) Smart devices with low-capability resources. They are labeled as single-board computers (SBCs)16– such as Raspberry Pi (RPi)– with scarce resources in terms of CPU, memory, storage and network [192]. It is efficient to deploy lightweight virtualization (either container/unikernel or both) for this type of device, which depends on the requirements of IoT applications. Unikernels are an elite choice to deploy in these devices to process and analyze data close to data sources (e.g., augmented smart house or vehicles) by using Machine Learning (ML) models. Container-based virtualization can be deployed in such devices with almost negligible effects on performance compared to native performance running on bare metal without any virtualization [157]. A combination of both virtualization techniques can be utilized in a cluster of SBCs [106] to achieve multi-tenant (provided by containers) and high safety (provided by unikernels located in containers).

(3) Smart device with medium capability resources. This type of devices is expected to support a limited number of applications requiring moderate resources. These devices are deployed either in closed (e.g., shopping malls) or open (e.g., stadium) environments to serve either stationary or mobile devices. It may be efficient to deploy hypervisor based virtualization for serving stationary users but it should be tailored for mobile users running time critical applications that require a smooth handover defined as switching radio communication from the serving cell such as small cell base stations (SCeNBs) to another one. Since communication between two devices is not available during handover, hypervisor based virtualization should be improved in initial VM provisioning and migration. VM handoff [91], [92] might be a viable solution where a base image is stored in the device and only the overlay of VM (the difference between the VM in the source and destination devices) is migrated. Cloudlets [204] and MEC servers can use such virtualization technique.

(4) Smart device with high-capability resources. These devices are resource rich hardware and handle delay tolerant applications on the order of a few seconds or longer. A set of such servers are utilized in Cloud-RAN (C-RAN) [49] that takes the advantage of hypervisor-based VMs in which Docker-based containers run. The combination of both techniques enables C-RAN (i) to host heavy applications or a bunch of them relating to the same third-party demanding a high degree of safety via VMs, and (ii) to offer mechanisms for packaging and agile movement in MEC collaboration space.

4.3. Iot application requirements
As depicted in Fig. 9, the third factor that affects the selection of virtualization technique is the requirements of IoT applications. There are numerous IoT applications that provide three main services by deploying in the edge computing [38]. Enhanced Mobile Broadband (eMBB) service is suitable for applications that require high data rates across a wide range of coverage area. Ultra Reliable Low Latency Communications (URLLC) service is appropriate for applications that need mission critical communications. Massive Machine Type Communications (mMTC) service suits for applications that connect to a large number of devices. The power of edge paradigms supports these services, where each service includes different IoT applications with various requirements.

Fig. 11 shows that latency and scalability respectively are top priorities for URLLC and mMTC services, while both requirements are the top priority for eMBB services. The importance of remaining requirements depends on the type of the application. Mobility feature is the common requirement for all IoT services and has essential effect to consider deployment of lightweight virtualization. Privacy and security can be important for applications that process sensitive and personal information (e-health application), and show track of users (e.g., analyzing energy consumption of users via smart grid application). This requirement inclines the use of heavyweight virtualization. Multi-tenancy requirement is critical for processing sensitive and personal data (e.g., e-health and smart city applications) and non-critical single controlled data (e.g., autonomous vehicles or smart grid). Hence, multi-tenancy implies the use of virtualization technique with higher isolation.

Solely deployment of IoT applications at the edge network cannot meet the stringent requirements, discussed above, for the critical IoT services. To achieve such requirements, it is vital to optimize resource management to reduce response time, and energy consumption, and to improve resource utilization. We believe that resource management in edge environment is more complicated compared to the one in cloud computing due to distributed resources and hierarchical levels in richness of resources. Recently, for example, some studies exploited AI and blockchain to optimize performance metrics and energy consumption in the edge-based healthcare applications [84], [217], [231], [232]. For more details about resources management to meet QoS for particular application, readers are referred to [81], [100].

Therefore, based on the prioritized requirements, richness of resources and features of different virtualization types, we can select a virtualization technique to abstract resources. Relying on a single virtualization technique cannot meet all the requirements of an application and a combination of virtualization techniques can be envisioned [106], [158]. For example, a combination of hypervisor based VM and unikernels is suitable for applications that need multi-tenancy. A combination of container based virtualization and unikernels is appropriate for applications with a need of high mobility.

4.4. Virtualization of IoT platforms
The aim of edge cloud computing is to facilitate IoT applications to meet their needs through efficient utilization of edge devices, computing and network resources. To this end, virtualization allows to share devices, resources and micro services/functions between IoT applications. Micro services provide transformation from monolithic cloud computing to the set of functions [95], which, in turn, to create large, complex and horizontally scalable IoT applications. As discussed above, lightweight virtualization, specially containers, is the most promising candidate for hosting micro services within IoT devices and share them between IoT applications in a particular domain such as smart houses.

Fig. 12 illustrates a simplified schema of IoT framework virtualization, which consists of three layers. Applications layer represents micro services built for a particular IoT application or shared between IoT applications. As an example, applications AP1 and AP3 share micro-service F1. Virtualized resources layer represents IoT Gateway that connects IoT devices with the application layer via, for example, Docker API. The hardware layer that includes IoT devices and fog nodes with richer resources. IoT gateways can be envisioned including application services (orchestrator (e.g., Kubernetes,17 Docker Swarm,18 and Mesos19), database, and web server), and underlying software components that virtualize IoT devices. IoT gateways, as summarized in Table 6, mostly leverage Docker container to instantiate fast process with small footprint and to provide high density multi-tenancy.

IOT gateway design should expose several features to effectively manage dynamic IoT environment [161]. Scalability represents dynamic joining and leaving of IoT devices simultaneously being connected to an IoT gateway. Interoperability of an IoT gateway makes possible data transformation via different protocols such as HTTP, MQTT and COAP [61] to support interactions between different applications and systems. Isolation and multi-tenancy of the IoT gateway provided by lightweight virtualization techniques facilitate IoT applications to share micro services and improve resources utilization [161], [172]. However, some of the reviewed studies did not meet this feature for the designed IoT gateway [58], [60], [89]. Table 6 shows virtualization techniques exploited in the IoT frameworks.

5. Performance oriented virtualization optimization techniques in edge computing
5.1. VM placement
From a user’s perspective, one of the imperative issues relating to a fog node is to select VM(s) to offload applications to speed up the process of computation and save energy consumption at edge devices. The fundamental questions with respect to the computation offloading are: when, where, and which parts of an application should be deployed at VM(s) in fog nodes?

Basically, two ways can be envisioned to offload an application: full and partial [133]. Full (resp. partial) offloading refers to offloading all (resp. some) parts of an application that depends on two primary factors. The first factor is whether an application can be divided into several parts for migration from edge nodes/devices to fog nodes. If so, which parts have one- or double-sided dependencies and which parts have not [143]. Independent parts can be offloaded in the same or different fog nodes and simultaneously processed in parallel. Otherwise, all dependent parts are offloaded to the same fog nodes. The second factor is the amount of data to be transferred into a fog node, which is known beforehand (e.g., image processing and sound recognition) or is unknown (e.g., augmented reality(AR)).

A decision about when an application should be partially or fully offloaded depends on mono or multi- objective function, where the former optimizes applications offloading based on a single objective function and the later one based on several objective functions [1]. As shown in Fig. 13, the imperative objective functions are execution time, energy consumption, resource utilization, and cost. Heterogeneity, mobility, and geographical location of edge resources make differentiate and complicate the optimization of these metrics in comparison with the ones in the context of cloud. As depicted in Fig. Fig. 14, execution time is the duration time for offloading data to a fog node, processing time at it, and duration time for sending the processed data back. Thus, the amount of data load on network links and the capability fog nodes are determining factors in whether an application should be offloaded. Energy consumption is the required energy to send data to a fog node and send back the results from it, which leads to save battery power on computation due to performing it on fog nodes. A trade-off between energy consumption on computation and data transfer determines whether to conduct computation offloading. Resource utilization relates to maximize the number of micro-services served over fog nodes [100]. Cost is defined for users and service provides, and it consists of communication cost

to offload the components of applications and computation cost to process the task at fog nodes. The main difference between cost optimization in cloud and edge computing is that fog node might be possessed by different service providers. This complicates cost optimization in the edge context from user perspective although provides more available options to exploit cheaper services.


Table 6. Virtualization techniques leveraged in IoT applications/frameworks.

Work	IoT application	Virt. techniques	Hardware resources	Objective
Morabito et al. [161]	IoT gateway	Container	SBC	An implementation of scalable and energy-efficient IoT Gateway with isolation and multi-tenancy features
Novo et al. [172]	Transport, agriculture	Container	RPi (model B)	Providing the capabilities of cellular networks to constrained networks while enabling the connectivity between wireless sensor networks and cellular networks
Li et al. [127]	General	Hardware-,OS-level	Sensor devices	Design a virtualization enabled fog computing for IoT
R. Morabito [157]	NA	Container	RPi	Performance and energy evaluation of lightweight virtualization instances on SBCs
Lee et al. [123]	NA	Container	RPi3	Network performance evaluation of container-based virtualization, which is lower than that of naive Linux
Morabito et al. [159]	Smart City, V2V	Container, Unikernel	RPi3	Applicability of lightweight virtualization techniques in smart city and vehicle communication platforms
Roca et al. [198]	General	NS	Sensor, router	Define Fog Function Virtualization(FFV) concept to provide flexibility and adaptability to run different applications on edge devices
Kakakhel et al. [107]	NS	Container	SBC	Evaluation of container-based live migration for stateful applications
Morabito et al. [160]	General	Container	RPi3	Performance evaluation of container-based virtualization for clustered IoT devices
Noronha et al. [171]	General	Container
(LXC)	micro-processor	Performance evaluation of LXC on broad range of embedded micro-processors
Jang et al. [104]	Video analytics	Container	Camera	Video analytics at the edge computing to adapt cameras to environment changes
Ogawa et al. [174]	Smart city,	Container	Camera	IoT devices virtualization to share them between IoT applications to improve utilization of computing resources.
Harjula et al. [95]	General	Container	RPi3	A virtualized and decentralized nanoservice-based model
Celesti et al. [46]	General	Container
(LCV)	RPi	Evaluation the overhead of container virtualization in an IoT device
Puliafito et al. [190]	General	Container	RPi3	Evaluation of container-based virtualization for cold and live migration techniques
Samaniego and Deters [202]	Smart-house	Unikernel	RPi2	Abstracting the complexity of IoT devices from user’s perspective and creating many-to-many relations between users and devices
Saurez et al. [205]	Vehicular traffic	Container	Relion servers	Handling proactive migration of components of the application between fog nodes
Bellavista and Zanni [32]	Smart connected vehicles	Container	RPi1	Creating scalable and flexible fog nodes over constrained-resource devices
Chang et al. [47]	Video surveillance, 3D indoor localization	Container
(LXC)	Smart phones, RPi	Bringing the capabilities of OpenStack to the edge devices
A. Krylovskiy [118]	General	Container	RPi2, RPi B＋	Evaluation the overheads of containerized IoT gateways in terms of throughput and latency
Hong et al. [99]	Image processing	Container	RPi	Implementation of a fog computing platform and studying an efficient modules deployment on fog devices to maximize number of satisfied requests
Salikhov et al. [115]	Smart houses	Unikernel	Sensors network	Implementation of a prototype platform for running multiple concurrent applications over a network of sensors
Kaur et al. [110]	General	Container	NA	Architecture implementation for tasks selection and scheduling at edge network to optimize energy consumption leveraging a container migration technique
Karhula et al. [109]	General	Container	RPi3	Evaluation of a multi-container IoT gateway in terms of delay and the number of events in comparison to the non-containerized gateway
Gonalves et al. [86]	General	VM	Cloudlet	A simulation-based evaluation of proactive VM placement and migration based on mobility prediction
Dolui and Kiraly [63]	General	Container	ARM Cortex A53	Evaluation of a multi-container and micro service-based framework for IoT gateway using base image hierarchies and image layering to optimize in/cross-container performance optimization
Alam et al. [11]	Smart home	Container	RPi3	Implementation of a modular and decentralized for IoT architecture using container and micro-services
Petrolo et al. [186]	General	Container	RPi2	Implementation of a semantic-based gateway for cloud of things, acting as an interface for users as well
Virt: Virtualization, NA: Not Applicable, NS: Not Specified, RPi: Raspberry Pi, V2V: vehicle to vehicle

After taking decision on when and which part(s) of an application to be offloaded, VM allocation must be performed in a single or multiple fog nodes to process a task. For non-partitioned applications, VM allocation in a single fog node relies on the availability of VM resources and the required execution time for an application. If VM resources are not enough, a fog node should ideally delegate to the request to another fog node or to a centralized cloud based on the objective functions and constraints. For partitioned applications, VMs allocation, consequently, the optimization of objective function becomes more complicated as multiple fog nodes host workload [131], [241]. Irrespective of partitioned or non-partitioned applications, this optimization problem in the edge context is NP-hard [200]. Approximation (as in [22], [221], heuristic [41]), and meta heuristic (i.e,Ant Colony [65], Genetic Algorithms [98], Particle Swarm Optimization [111]) are lightweight solutions to optimize objective functions in edge paradigms, which can be evaluated via simulator (CloudSim [43] and iFogSim [90]), analytical tools (Matlab, IBM CPLEX), and test bed environments (Grid’5000 [26] and Adhoc testbed [77]). Table 7 summarizes the objectives of IoT applications deployed in the virtualized edge computing. Note that the mobility in this table refers to the mobility of devices or users.


Table 7. Deployment of IoT applications in the virtualized edge computing.

Work	Virt. type	Mobility	Hardware resources	Solution	Objective functions
[58]	VM
Container	No	Intel Atom N2600	Heuristic	Maximizing E2E performance
[270]	VM	No	MEC server	Concrete
Heuristic	Minimizing the average of data transfer among fog nodes
Minimizing the latency for storing and retrieving data
[14]	NS	No	Small cell	Heuristic	Minimizing the energy efficiency
[265]	NA	No	FN	Heuristic	Minimizing execution time
[147]	VM	No	FN	Heuristic	Reduction in latency
[99]	Container	No	RPi	Heuristic	Maximizing throughput
[205]	Container	Yes	Penguin Relion 1752	Heuristic	Maximizing resource utilization and minimizing latency
[151]	NS	No	FN(300-1400MIPS, 256 MB-2 GB RAM)	Concrete	Maximizing resource utilization in terms of number of services deployed
[213]	NS	No	Sense actuate, and process devices	Heuristic	Maximizing resource utilization
[212]	NS	No	Sense actuate, and process devices	Meta-heuristic	Maximizing resource utilization
[8]	NS	No	Computer desktop, sensors	Concrete	Optimal placement of services in the edge context
[34]	NS	No	FN(2-8 GB RAM,  MIPS)	Concrete (ILP)	Minimizing the overall latency
[59]	NS	No	FN(2-16cores CPU, 8 GB RAM)	Heuristic	Minimizing monetary cost, energy and runtime
[79]	NS	No	FN	Heuristic	Minimizing monetary cost of servers and bandwidth through different heuristic algorithms
[156]	NS	No	320 FN	Heuristic	Maximizing resource utilization in term of computing and network
[223]	NS	No	Sensors	Heuristic	Maximizing network utilization
[27]	NS	No	Cloudlet, BS, smart devices	Concrete	Minimizing monetary cost using ILP
[45]	VM	No	1vCPU, 2 GB RAM	Concrete	Minimizing E2E latency using ILP
[246]	NS	No	NA	Approximation	Maximizing resource utilization
[15]	NS	No	RPi	Heuristic	Optimizing E2E latency
[64]	Container	No	ARM A8, 256 MB RAM	Heuristic	Optimizing resource utilization
[258]	VM	No	FN(a quad-core CPU, 4 GB RAM)	Heuristic	Minimizing execution time
[137]	NS	No	NS	Heuristic	Maximizing resource utilization
[19]	VM	No	Server(2,4 cores, 8-64 GB RAM)	Meta-heuristic	Optimizing multi-objective functions in term of cost, latency, user footprint and support
[83]	NS	No	Dual core ARM CPU, 64 GB SD	Meta-heuristic	Minimizing the execution time
[25]	NS	Yes	Server	Heuristic	Minimizing the operation cost
[179]	VM	Yes	Smart phone, small cell	Heuristic	Minimizing the cost placement and migration of application’s component
[227]	VM	No	Small cell	Concrete	Maximizing resources utilization in terms of Fog nodes
[97]	NS	No	FN(1-2 GHz, 1-2 MB RAM)	Concrete	Minimizing network cost using ILP
[116]	NS	No	FN	Heuristic	Maximizing the utilization of computing resources
[180]	VM	Yes	MEC server	Heuristic	Minimizing latency under constrained budget
[219]	NS	Yes	Cloudlet	Concrete	Minimizing latency using ILP
[214]	VM	No	Cloudlet	Heuristic	Maximizing the utilization of fog resources
[244]	Container	Yes	FN	Heuristic	Reducing delay of IoT applications
[262]	NS	No	FN	Heuristic	Reducing delay for IoT applications
[228]	NS	No	Desktop computer	Heuristic	Minimizing executing time for mobile services via convex optimization
[4]	VM	Yes	User base	Heuristic	Minimizing the response time and maximizing throughput
Virt: Virtualization, NA: Not Applicable, NS: Not Specified, RPi: Raspberry Pi, BS: Base Station, FN: Fog Node, MIPS: Million instructions per Second, SD: Secure Card, MEC: Mobile Edge Server.

Smart devices include smart phones, tablets, smart glasses, and so on.

5.2. VM migration
VM migration refers to moving one VM from a physical node to another one to improve performance and support users/ devices mobility [5], [33]. The process20 of VM migration includes (i) transferring data from VM’s memory to a target physical machine, (ii) sending the state of the resources (CPU, RAM and storage) to a destination node, and (iii) suspending VM on a source node and resuming it on a destination node. This process of VM migration is conducted through the following techniques.

5.2.1. VM migration techniques in edge-cloud context
Fig. 15 depicts different techniques of VM migration [74]. In cold migration, a VM initially is stopped, then the whole disk is transferred, and finally the VM is resumed. Live migration guarantees the running services on the source node while most of the state of VM is being transferred [9]. Live migration consists of two techniques [9] (see Fig. 15). The pre-copy migration initially transfers almost all memory pages (e.g., CPU state, registers and optionally, non-pageable memory) to a destination node before stopping a VM for the final state transfer. This technique iteratively transfers the modified memory pages until for a pre-defined number of iteration and then stops VM in order to transfer the last modified memory pages. After that, a VM resumes on a destination node. This technique is utilized by hypervisor such as KVM [121], Xen [251], and VMware [74]. A similar approach is reflected to as VM teleportation in VirtualBox [237]. The post-copy migration is quietly the opposite of the pre-copy migration. Post-copy migration technique stops a VM and transfers the execution of the VM to the destination node so that the VM can resume there. Then, this technique copies the memory pages as page faults happens.

The performance metrics for cold and live migration are migration and down time [91]. The former is the time to complete migration process, whilst the latter one is the time during which a VM is not running and responsive. Clearly, the down time equals to the migration time due to suspending a VM before its state transfers, making this technique unsuitable to both cloud and fog computing. The down time of pre-copy migration is less than its migration time since a VM is stopped after freezing most of the memory pages. Post-copy migration copies the memory pages once as is done in the cold migration and in the copy phase of the pre-copy migration. However, it incurs shorter down time though its performance is degraded by pages fault, making it unsuitable technique for fog computing. Table 8 compares different migration techniques, where down time reduces from cold to live (i.e., first pre-copy then post-copy), and the responsiveness of a VM during migration, conversely, increases from live to cold migration. This is because in cold migration a disk and RAM data are transferred [189], whilst in live migration only RAM data is transferred. Live migration techniques are applicable in the context of cloud computing, especially within datacenters where shared disks are exploited through storage area network (SAN) or network attached storage (NAS) devices [52], [167]. Amongst Google, AWS, and Amazon, only Google currently supports live migration.

The migration technique discussed above are no longer efficient for fog computing because they either incurs high down time or degrade response time due to probably frequent pages faults as happens in post-copy migration. Hence, there is a need of virtualization technique(s) with lower down time and more efficient in performance and energy with relatively low resource requirements in hardware. To achieve this purpose, it should be considered several aspects of edge computing that are not valid in cloud computing. (i) Fog nodes span through a WAN, where there is no shared disk and lower bandwidth compared to a LAN within cloud datacenters. Thus, it is effective to minimize transferred data during VM migration. (ii) Most fog nodes share temporal data and typically transfer the analyzed data, and it thus is not supposed to store persistent data in disks. Hence, in edge computing, typically the state of the RAM should be transferred. (iii) Due to running time-critical applications in edge computing, it is essential to reduce the total time required to transfer a VM from a source to a destination node.


Table 8. Comparison among different VM migration techniques.

VM migration technique	Performance metrics	Down time	Type of transmitted data	Context applicability
Cold Migration	Down time	High	Disk	NA
Pre-copy	Down time	Low	Disk, RAM	Cloud
Post-copy	Down time	Very Low	Disk, RAM	Cloud
Handoff	Completion time	Very Low	Overlay contents	Edge
VM handoff [204] mitigates the drawbacks of live migration and leverages VM synthesis to seamlessly transfer a VM between two nodes. VM synthesis speeds up VM provisioning in the destination node by dividing VM into a base VM (e.g., Linux VM image) that is pre-populated in the destination edge node and the overlay VM that is only transferred from source edge node to destination one as needed. Thus, in contrast to cold and live migration, VM handoff reduces the completion time21 defined as the total duration time from the commence of VM hand-off in a source edge node until a VM resume in the destination edge node. This is because the size of overlay VM – the difference between base VM and the desired custom VM [92] – is small. The size of the overlay VM can be further reduced by exploitation of compression and de-duplication techniques in the pipelined stages [91]. However, the completion time of a VM handoff is relatively high and it is required a lighter virtualization technique to be more agile. Ma et al. [132] tailored container virtualization as a complementary technique to a VM handoff by reduction in a file system transfer size in order to accelerate VM migration.22 To make a VM handoff more effective, it is required to perform optimization in (i) the selection of compression algorithms and de-duplication methods, and (ii) the number of VM overlay creation based on the bandwidth networks between two edge nodes/servers, and availability of computing resources for creating VM overlay in a source edge node [91]. In addition to reduction in the size of VM overlay, multi-path TCP [48], [191] is another solution to speed up VM migration, masks the effect of IP changes over WAN, and enhances connection connectivity between two nodes. With respect to VM migration in edge computing, migration policy and strategy should be considered. Migration policy relates to when a user’s VM should be migrated, in which user’s speed, direction and position are important. Migration strategy corresponds to where a user’s VM should be moved. Shortest distance and lowest congested link between edge devices and fog node are the simple strategy to speed-up VM migration.

5.2.2. VM migration optimization in edge computing
We discuss VM migration in the following aspects as depicted in Fig. 16.

VM migration benefits. In contrast to the traditional VM migration in cloud to reduce energy consumption and enhance resource utilization, VM migration in the edge context brings the following benefits. The first benefit is end to end (E2E) latency reduction which has already been discussed and depicted in Fig. 14. The latency in steps (i) and (iii) can be optimized through VM migration, whilst the one in step (ii) can be optimized through well-allocated VM to task where the availability and capability of resources should be taken into consideration. The second benefit is TCP throughput maximization defined as the rate of data successfully delivered over a TCP connection and affected by TCP windows size and round trip latency factors. The increment in the TCP window size results in higher TCP throughput, which requires more memory for buffering on an edge node. Due to the limited resources in the edge nodes, this solution may not be effective to increase TCP throughput. The second factor can be affected by the physical distance between the edge devices and the edge nodes increases. The more distance, the more deployed network hops to transfer data, which degrades the TCP throughput. However, like E2E latency, TCP throughput can be improved through VM migration from a high to less congested edge node at close distance to edge devices.

VM migration cost. VM migration imposes cost in terms of migration time and degradation of resources (Network, CPU, RAM and disk) utilization. Migration time is directly affected by the VM migration technique and the bandwidth provisioning between the source and destination edge nodes, and the amount of transferred data (i.e., memory and disk state). As an example, in pre-copying live migration, the amount of data is the memory size of a VM and the average number of dirty memory page yielded in each time slot. The second dimension of the VM migration cost is degrading the resource utilization because VM migration acts as an I/O operation. Thus, VM migration degenerates I/O applications and it should be performed on the edge nodes that process computing-intensive applications or I/O applications with low I/O footprints.

VM migration path. As discussed, VM migration is a viable solution to react to device mobility in order to reduce E2E latency and improve TCP throughput. However, VM migration is not an effective way in some cases when the migration cost outweighs the migration gain. Instead, providing a new optimized path with less congested traffic [163] between the edge node and VM deployed in the edge nodes to send and receive data can be an efficient way from performance perspective [30], [187]. Thus, VM migration along with an optimized path between edge nodes and fog nodes can improve the target performance metrics. This goal can be further enhanced through finding an optimized VM migration path based on the users/devices movement prediction [73]. Table 9 summarizes the exploited virtualization techniques in the context of edge computing.


Table 9. VM migration in the edge context.

Work	Virt. type	Virt. techniques	Hardware resources	Objective/Cost
Puliafito et al. [189]	Container	Cold
live	RPi3	(i) Evaluation of throughput and RTT
(ii) Evaluation of down time, page faults and transferred data
Kakakhel et al. [108]	Container	Cold
live	Cloudlet
(1 vCore, 1 GB RAM)	(i) Evaluation of throughput and RTT
(ii) Evaluation of migration, down time and pages fault
Bittencourt et al. [37]	VM
Container	Handoff	Cloudlet	(i) Propose a general architecture to support VM migration
(ii) NA
Saurez et al. [205]	Container	Live	Fog node
(6 cores, 48 GB RAM)	(i) Proposing migration APIs, discovery resources
(ii) Migration time, discovery time for fog nodes
Tang et al. [224]	VM
Container	Handoff	Fog Node	(i)Latency and power consumption reduction
(ii) Migration time
Lopes and Higashino [129]	VM
Container	Live
Handoff	Fog nodes	(i) Proposing a simulator for VM migration in FC
(ii) Migration time measurement for VM/container in FC
Qiu et al. [191]	Container
LXC	Live	Cloudlet
1 vCPU, 1 GB RAM	(i)Reduction in migration time and improve resilience of migration process via multi-path TCP
Filiposka et al. [73]	VM	Handoff	Cloudlet
(1–2 cores, 1-2 GB RAM)	(i) A simulation-based evaluation for VM migration for mobile users with speed of 1-2mh
(ii) NA
Gonalves et al. [87]	VM	Proactive	Cloudlet
( 2800 mi, 8 GB RAM)	(i) A simulation based VM migration to reduce migration times and unavailability VM
(ii) Reduction in migration time
Zhang et al. [268]	VM	Live
(pre-copy)	NA	(i) A numerical evaluation of reduction in network traffic via VM migration for mobile users
(ii) Migration time
Majeed et al. [2]	Container	Live
(Stateless)	Cloudlet
(2cCPU, 64GB RAM)	(i) Investigation of container-based offloading across devices, fog nodes, and cloud based on ML approaches using resources utilization
(ii) NA
Chaufournier et al. [48]	VM	Live
(Pre-copy)	Cloudlet
(8core, 64 GB RAM)	(i) VM migration improvements in terms of throughput, migration time using multi-path TCP
(ii) Migration time
Ma et al. [132]	Container	Handoff	Cloudlet
(3vCore 16 GB RAM)	(i) Reducing in completion time of container migration via potential reduction in file system transfer size
(ii) Down time, transferred data and page faults
Teka et al. [226]	VM	Live	Cloudlet
(7cores, 16 GB RAM)	(i) Using multi-path TCP to reduce TCP connection establishment for VM migration, and RTT and throughput measurement
(ii) Migration time and down time
Machen et al. [134]	VM
Container
(LXC)	Live	MEC server
(2vCore, 16 GB RAM)	(i) A comparison between KVM and LXC in migration time, down time for several applications
(ii) Migration time
6. Virtualization of network resource
Due to the highly dynamic feature of edge paradigms, providing network functions (e.g., firewalls, caches, proxies, intrusion detectors and WAN accelerators) through adding, removing and upgrading network hardware is a tedious task and imposes heavy operational (OPEX) and capital (CAPEX) expenditure costs [39]. Network Function Virtualization (NFV) [153], [154] solves this issue by moving network functions from network hardware appliances to software hosted in network devices and servers as Virtual Network Functions (VNFs). VNFs execute a set of network functions ranging from security (firewalls, intrusion detection systems), performance improvements (caches, proxies, traffic accelerators), traffic shaping (load balancer, rate limiters), to name a few. Connectivity, control and management of VNFs is in charge of Software Defined Network (SDN), which shapes network traffic without dealing with network hardware by introducing separation between control plane and data plane.23 The control plane software runs on either standard server or network devices and handles packets received by the network devices based on the network rules and policies. The data plane, represented by virtualized hardware (i.e., VNF), forwards and receives packets based on the mandated rules by the control plane. Thus, NFV and SDN are complementary technologies, where NFV provides basic networking functions through hardware virtualization and SDN manages them for specific use programmatically defined and modified. It should be noted that SDN can be deployed in purely hardware network devices (e.g., switches) to flexibly control network flow [206]. However, the core similarity of SDN and NFV is to abstract network functions on the beneath network hardware resources for network applications as illustrated in Fig. 17. SDN abstracts the separation of control data and forwarding data, and NFV abstracts the data plane from the hardware infrastructure. The difference between SDN and NFV returns to how they separate functions and abstract resources. SDN abstracts networking resources (e.g., switches or routers) through moving control decision to a virtual network control plane to define source and destination of network flows, whilst NFV virtualizes physical resources beneath a hypervisor to expand/shrink a network without adding/removing network devices. In the following, we discuss integration of SDN and NFV.

6.1. Architecture of SDN and NFV integration
SDN and NFV integration as anabstract layer sits between application layer and hardware layer and consists of the following components as depicted in Fig. 17.

Control Plane: SDN controller/NOS provides a centralized view of a network to steer the network flow/traffic by using OpenDaylight [177] software that exposes northbound API to the network applications to collect and analyze information about the requirements of the network applications. OpenDaylight supports different southbound protocols such as OpenFlow [148], OVSDB [176], and Network Configuration Protocol (NETCONF) [69] to allow connection between SDN controller and data plane (i.e., virtualized switch (OpenVswitch (OVS) [176]) to steer network flow/traffic.

Data Plane: This component consists of virtualized SDN switches (Open vSwitch) and Virtual Network Function (VNF) that includes firewalls, domain name system (DNS), caching or network address translation (NAT) on VMs instead of carrying out by proprietary or dedicated hardware. These VNFs run standard server, switches, storage devices or even cloud computing infrastructure with the help of Xen, KVM, VMware hypervisor for computing resources and VXLAN [239] and NVGRE [173] for networking resources.

NFV management and orchestration: This component includes the following sub-components in Fig. 17. (i) virtualized infrastructure management (VIM) orchestrates allocation, deallocation, upgrade of NFV infrastructure and optimization of their use. (ii) Virtual Network Function Manager (VNFM) manages instantiation, configuration, start, stop, scaling in/out, updating, upgrading, suspension, termination of VNFs in Point of Presence (PoP) infrastructure. (iii) NFV orchestrator (NFVO) serves resource and service orchestration. The former orchestration provides co-ordination, authorization, release and engage of NFVI resources (e.g., a fixed set of virtual CPU, a certain amount of memory and disk space) within PoP infrastructure for a given VNF. The latter one manages the relationship between VNFs and provides on-fly scaling in/out, terminating, updating, upgrading and managing of network services topology. Nokia as one of the leading vendor offers CloudBand Application Manager (CBAM) [138] as VNF Manager and CloudBand Network Director as NFV orchestrator. A list of open source projects implementing ESTI NFV MANO framework is in [150].

Network Application: This includes service/virtual functions (SFc/VFs) (e.g., firewalls, deep packet inspections (DPIs) and virus scanners in the context of network protection system [35]) that defines a specific treatment to the received packet and runs on the virtualized hardware (i.e., VNF creation). A partially or totally ordered of SFs (called service function chain (SFC)) must be applied to traffic network to achieve the required network services/policies. A network policy, as a flow, mandates packets to be traversed according to their associated SFC. For example, a network protection application as a network service, might provide three service functions like firewall, DPI and virus scanner as a SFC. In summary, SCF abstracts a network service in terms of the required SFs and the order in which they should be executed. Virtualization at network level, discussed above, reduces operational and expenditure cost via multi-tenancy, and makes flexible, fast and elastic demands of network services through on the fly adding, updating and removing virtual functions without dealing with network hardware appliances. These advantages are necessities for IoT applications that support context aware, ultra reliable and user specific network services. This mandates a well-defined SDN and NFV architectural design, well selected virtualization types hosting VNF and performance-oriented optimization algorithms for VNF management. The first requirement relates to NFV MANO and readers are referred to [39] and the last two requirements we discuss in the following sub-sections.

6.2. NFV virtualization techniques
Virtual Network Function (VNF) executes a particular computing- or bandwidth-intensive network function. VNFs are mainly distinguished in the underlying hardware virtualization and the network functions they perform. They generally exploit three virtualization techniques as shown in Fig. 18.

Hypervisor based VNF runs on the richness capable hardware and Cloud4NFV [216] and OPNFV [178] platforms, for example, move both data and control planes to the virtual resources. This technique is high in resource consumption and slow in movement and deployment. To combat such limitation, Container-based VNF allows a network function to be packaged within a lightweight container engine, e.g., Docker. This makes container-based NFV frameworks (e.g., Glasgow Network Functions (GNF) [56]) to be agile in movement and deployment. Unikernel-based VNF is a specialized hypervisor based VM and optimized for middlebox processing to execute specific network functions. ClickOS [145], as an example, incurs extensive changes in Xen’s I/O to significantly accelerate networking in the middleboxes running in VMs. This design requires several performance related features to be considered. It should support flexibility to run network functions on different operating systems, provide isolation in terms of performance (CPU and memory) and security to achieve multi-tenancy and guarantee scalability in the exploitation of the resources proportional to the workload.

Selection of x-based VNF (x represents different virtualization techniques) is not a black and white choice since each virtualization technique is the best in some quantitative features, not all (see Fig. 10). Unikernel-based virtualization is the best in instantiation time and image size, whilst container-based virtualization outperforms other techniques in memory usage [56]. Network throughput, the rate of successful delivery over a communication channel, is another quantitative feature in which unikernel performs best [145]. Idel RTT is the delay that arises from virtualization layer, where hypervisor based VMs act the worst mainly due to copying the packet from hypervisor to VMs; unikernels are the best if packets forwarding is optimized [197]. Furthermore, other features listed in Table 10 should be considered in the deployment of x-based VNF ClickOS and the art of NFV. Hypervisor based virtualization provides flexibility to run different guest OS on the same platform in the cost of direct access VMs to NIC devices. This thus complicates live VNF migration. In contrast, containers are inflexible because they run on the same OS, imposing restriction on network operators to run them from different vendors. Hypervisor based virtualization requires alignment of dependencies between guest OS and inter VM networking solution, while containers and unikernels allow running the VNF process directly in a host OS. Thus, unikernels and containers are the best in service agility. Open source communities support hypervisor- and container-based virtualization well, while unikernels do not. Hypervisor-based virtualization has the highest degree in application compatibility, while container-based virtualization has compatibility in Linux, windows and Mac. Table 10 summarizes the features of different x-based VNF, where the higher rank indicates better performance in the associated feature. For example, unikernel with rank 1 is better than hypervisor-based VM with rank 3 in terms of instantiation time.


Table 10. Comparison between different x-based VNF (x is hypervisor, container and unikernels) in quantitative features (i.e., the first five features — instantiation time, image size, …, idle RTT) and non-quantitative features (i.e., flexibility, …, application capability). The higher the rank, the better x-based VNF in terms of features.

Features	Instantiation time	Image
size	Memory usage	Network throughput	Idel RTT	Flexibility	Service agility	Management framework	Application compatibility
Rank 1	Unikernel	Unikernel	Container	Container	Container	Unikernel	Container	Hypervisor	Hypervisor
Rank 2	Container	Container	Unikernel	Unikernel	UNikernel	Hypervisor	Unikernel	Container	Container
Rank 3	Hypervisor	Hypervisor	Hypervisor	Hypervisor	Hypervisor	Container	Hypervisor	Unikernel	Unikernel

Download : Download high-res image (68KB)
Download : Download full-size image
Fig. 18. Different types of VNF .


Download : Download high-res image (50KB)
Download : Download full-size image
Fig. 19. Differential characteristics between VNF and classical VM .


Download : Download high-res image (67KB)
Download : Download full-size image
Fig. 20. Differential characteristics for VNF placement in cloud and edge paradigms.

6.3. VNF management in cloud- VS. edge-based paradigms
In addition to the selection of virtualization technique, VNF orchestration and management is another imperative issue in NFV-enabled networks. We investigate it from the following perspective.

VNF vs. classical VM. As illustrated in Fig. 19, VNFs and classical VMs have the following essential differences. First, VNFs must run task in a sequence in the form of SFC, while classical VMs do not. A SFC determines a partially/totally ordered sequence for network traffic/flow that must be traversed through VNFs. This ordered sequence can be liner- or graph-based shapes [35]. In the liner shape, all service functions must be serially conducted, while in the graph based shape they can be executed in parallel as long as the sequence among service functions in SFC is preserved. SFC shapes can be either static, or dynamic in which the number and shape of service functions vary with time and load [35]. The dynamism of the SFC shape makes indispensable virtualization of service functions in network appliances. Second, VNFs are differentiated in functionality, the required resources and their interrelationships, whilst classical VMs are usually the same in functionality. For example, firewall and NAT functions need lightweight resources, while IDS function requires intensive computing resources. This makes a challenging issue in the synergy of cloud and edge paradigms due to leveraging devices with a variety of resource capabilities. Therefore, due to these important differences, the designed algorithms for classical VM placement and management are not applicable in the NFV enabled networks and revisiting such algorithms is a must.

VNF Placement in cloud- vs. edge-based: As shown in Fig. 20, the following factors impact the management and placement of VNF in the edge-cloud environment.

(1) Resource richness: Due to resource constrained edge devices, essential factors for VNF deployment in PoP locations are instantiation time, memory usage, and idle delay [56]. These requirements can be met through container based virtualization. It is noteworthy that unikernels cannot be deployed in edge devices (e.g., customer premises equipment and home routers) lacking support of hardware virtualization since unikernels are built and modified on top of hypervisor. Furthermore, this resource constraint results in different objective functions and QoS constraints for edge paradigms compared to the ones for cloud paradigms as discussed in the next section.

(2) Network/substrate topology: This refers to an organization of hardware servers and their connection through physical links. Network topology and bandwidth are two essential aspects of differences between two paradigms. Cloud-based datacenters follow a centralized and well-defined network topology (e.g., fat-tree [10]) with high bandwidth connection between servers, while edge-based paradigms usually exploit irregular topology with constrained bandwidth. Thus, computation optimization is more important than the bandwidth optimization for cloud-based paradigms, while these two optimization problems become an imperative trade-off for edge-based paradigms in the context of VNF placement. This results in expanding/compressing SFs across servers within cloud datacenters/edge computing devices. This is because the more SFC is split, the need for high richness server decreases and the demand for bandwidth increases.

(3) User/device mobility: Shifting from a host-centric to a data-centric model provides computational resources to end users to support device/user mobility. MEC, as an example of edge paradigms providing such model, supports context-aware network services for which VNF migration is a necessity to guarantee the required QoS. In contrast, the cloud-based paradigms do not support such applications and VNF migration thus does not happen. In both cloud and edge paradigms, VNF migration might happen in order to reduce operational cost and maintain acceptable end-to-end flow performance as discussed later.

Therefore, as mentioned above, the synergy of cloud and edge computing creates an extra level of complexity for VNF management across distributed resources with a variety of capabilities. A key challenge relating to this issue is how to make a balance approach in resource utilization of both edge- and cloud-based paradigms for NFV placement. As an example, VNF deployment in edge computing improves response time while reduces availability, and vice versa as if it is located in cloud computing. Where to place VNFs for a given set of PoP locations and how to steer network traffics across them (i.e., SFC provisioning) mandate to specifically define objective functions and QoS requirements.

6.4. VNF placement and migration in edge-cloud environments
NFV provides network services in the form of a software-based virtualization entity as VNF. VNFs are logically connected and executed based on SFCs (represented user’s requests). One essential issue relating to VNFs and SFCs is how to manage them in an edge-cloud environment so that performance and cost are optimized while the required SLAs for SFCs are preserved. This issue arises four questions. (i) Where to initiate a VNF for a given PoP locations across an edge-cloud environment? (ii) How many resources such as CPU, network bandwidth, RAM capacity, and probably storage space should be allocated to a VNF? (iii) How to steer SFCs among determined VNFs based on the predefined order between SFCs? (iv) When VNF should be migrated and consolidated in reaction to changes in user’s requests (SFCs) and resources statues? The first three questions define a static VNF placement problem and the last one results in a dynamic one.

Input entities of VNF placement: To solve static and dynamic VNF placement problems, three input entities, as shown in Fig. 21, should be taken into consideration. (1) Network topology: The edge-cloud environment has a hierarchical or graph-based architecture, where lower layers are located at the proximity of users with limited bandwidth while higher layers are located far away from users with higher bandwidth. (2) SCF topology: It can be a simple line, or bifurcated in the real scenario for network applications [35]. This creates a critical issue in how to split SFC topology to steer data flow between VNFs. Obviously, this issue can be eliminated if the whole of a SFC is located in a physical server. (3) Service tasks: They are different in requiring resources allocated to VNFs based on their functionality. For example, NAT, as a service task, requires light-weight computing resource, but quick in response since it remaps one IP address into another by changing network address information in the IP header of packet. While an IDS as a service task needs intensive-computing resources due to processing data with high rate.


Download : Download high-res image (61KB)
Download : Download full-size image
Fig. 21. Input entities of VNF placement .


Download : Download high-res image (63KB)
Download : Download full-size image
Fig. 22. Cost function of VNF placement .

Cost function of VNF Placement: With respect to the input entities, VNF placement optimization consists of costs depicted in Fig. 22. (1) Resource cost can be translated into the amount of computational and communication resources used, the number of servers deployed, or monetary cost if it can be applicable to resources usage. (2) Delay is the overall processing and communication [124], [152] time to process a service task. Processing time includes the time needed for the VM hosting VNF to apply network operations on the arriving packet and depends on the capacity of computational server/device. The communication time consists of transmission and propagation time. The former is the time to transmit a packet over a link and depends on the packet size and link bandwidth. The latter is the time to be required for transmitting signal data from the source to destination nodes and measured based on the distance only. Propagation time can be ignored if the packet size is large. All these delays in all layers of an edge-cloud environment should be jointly optimized especially for URLLC services.

Considering input entities and cost functions discussed above, two optimization problems can be defined. (1) The first one is resource usage and end-to-end latency of service chain (simply response time) optimization problem along with a QoS like a specific rate of SLA violation. This optimization problem can be reduced to the conflicting optimization problems due to different capabilities of resources spread across the edge-cloud environments. As an example, a trade-off between response time and availability, where an VNF deployed in the edge computing resulting in low latency, and in the cloud leading to high availability. Thus, it is not possible to optimize both latency and availability through VNF deployment either only in edge or only in cloud computing. (2) The second optimization problem is to make decisions on when and where network traffic should be steered. This optimization problem is formulated in the form of Integer Linear Programming (ILP) problems [70], [254], [266]. They have high complexity in computational time, making them an unsuitable solution for large scale edge-cloud environments. To tackle such problem, most researches make effort to propose greedy solutions to provide fast decision on VNF placement and management [195], [196].

Due to the dynamism of edge-cloud environment and changes to SFCs, the initial decision on VNF placement cannot be well cost-efficient. Hence, it is necessary to re-optimize the optimization problem in each time slot, periodical time, or based on the unsatisfactory in a specific SAL (e.g., SFCs rejection lower than a specific percentage). To cope with this issue, it is required to design algorithms for horizontal and vertical scaling of VNFs [230]. A VNF is vertically scaled up through allocation of more resources, and scaled down through releasing redundant resources. A VNF is horizontally scaled up/down through adding/removing resources. VNF migration is orthogonal to scaling down/up and creates reconfiguration costs for VNF movement between physical servers and re-steering SFCs among VNFs. The reconfiguration degrades response time and causes temporal service interruption and thus a dynamic VNF placement requires to carefully consider the migration cost of VNFs. Table 11 maps the provided taxonomy to the state-of-the-art studies in networking virtualization.


Table 11. Comparison of VNF deployment in edge-cloud environment.

Work	Domain	Virt. type	NFV OP	SFC	Cost function	Solution
J. Son, R. Buyya [218]	Edge-cloud	NS	NFV-P	Yes	E2E latency	Sim.- Heuristic
Toosi et al. [230]	Cloud	VM	NFV-M	Yes	Resource cost	Sim.- Heuristic
Bhamare et al. [36]	Edge-cloud	VM	NFV-P	Yes	Delay cost	Imp.(EC2)- ILP and Heuristic
Li et al. [125]	Edge-cloud	NA	NFV-P	Yes	Resource cost	Sim.- ILP and Heuristic
Xie et al. [252]	Edge	Container	NFV-P	Yes	Resource cost(bandwidth)	Sim. - ILP and Online
Ghaznavi et al. [80]	Cloud	Unikernel	NFV-P	Yes	Resource Cost(CPU)	Sim. - ILP and Heuristic
Brogi et al. [41]	Edge-cloud	NA	NFV-P	Yes	Resource/Delay costs	Sim. - Heuristic
Wang et al. [240]	Edge	NA	NFV-P	Yes	Resource-cost	Sim. - ILP and Hungarian
Yang et al. [256]	Edge	VM	NFV-P	Yes	Resource-cost	Sim. - Online
Drxler et al.[66]	All	NS	NFV-P	Yes	Resource-cost	Sim. -MILP-Heuristic
Cziva et al. [55]	Edge-cloud	NS	NFV-P/M	Yes	E2E latency	Sim. - Optimal stopping theory [185]
Chen et al. [51]	Edge	NS	NFV-P	No	Resource-cost	Sim- Genetic and Heuristic
Chemodanov et al. [50]	Edge-cloud	NA	VNF-P/M	Yes	Resource cost(bandwidth)	Sim. - ILP and Heuristic
Zhang et al. [267]	Edge-cloud	NA	NFV-P	Yes	Resource cost	Sim. - ILP and Heuristic
Nguyen et al. [169]	Edge-cloud	NA	NFV-P	No	Resource-cost	Sim. - non-convex IP and Markov approximation
Zhang et al. [269]	Edge-cloud	NA	NFV-P	No	Resource/Delay costs	Sim - Heuristic
Santa et al. [203]	Edge	VM	NFV-M	No	NA	Impl.
Behravesh et al. [31]	Edge-cloud	NA	NFV-P	Yes	Resource cost	Sim. - MILP
Ren et al. [195]	Edge	VM	NFV-P	Yes	Resource/Delay costs	Imp. - Approximate and Heuristic
Virt: Virtualization, OP: Operation (P: Placement, M: Migration), NA: Not Applicable, Sim: Simulation, Impl: Implementation.

7. Future research directions
Based on this review, this section identifies a few key research directions in virtualization of computing and network resources in edge-cloud environment.

Selection and Deployment of Computing Resources Virtualization: Hypervisor-based, container-based, and unikernels are the three main virtualization techniques [225]. The selection of one of these techniques and making it adapt to different use-case scenarios in the context of edge computing is not an easy task. We should take three factors into consideration. The first factor is the virtualization properties such as instantiation time, image size, memory footprint and live migration support. The second factor is the requirements of IOT applications [165] mainly response time, mobility, scalability and multi-tenancy. The last factor is the capability of the resources [261] in terms of memory, storage, computing and networks. A wise consideration of these factors can help a well-designed and deployment of virtualization techniques although we should compromise between, for example, security of hypervisor-based and agility of container-based virtualization.

Mobility Services in Edge Computing Paradigms: Beyond ultra-low latency and high bandwidth demands, mobility is the most distinction factor in resource management in edge computing compared to cloud computing. Mobility of devices either being transported by itself, human or another carrier, is one of the essential features that influences the selection and deployment of lightweight virtualization techniques (i.e., unikernel and container) which are more agile [135], [208]. The deployment of such lightweight techniques cannot satisfy users/devices with high speed (e.g., cars and trains), and user movements pattern and acceleration of VM transmission should be considered. We classify user movements into planed (i.e., social events), regular-based (i.e., workplace) and unpredictable movements. For the planned movement, fog nodes can simply be provided in the event placement to serve users (e.g., sports stadiums) [128]. For the regular-based movement, the historical and statistical prediction of user/device movements [86], [164] leveraging ML or other optimization approaches, e.g., Bayesian, can be exploited. For the last movement type, cumulative distribution patterns of users’ movements is effective to find proper paths for a large number of users rather than for a particular user. In respect to VM transmission, the VM overlay technique [91], [92] using effective compression algorithms is a way to migrate VM for entities with high speed. Overlay VM is the difference between a base VM and the current deployed VM.

Most of the reviewed studies, as listed in Table 9, Table 11, consider only the mobility of devices/users for VM placement and migration. This is valid in the context of MEC due to the fixed MEC servers, while in fog computing the location of both edge devices and fog nodes can change over time [264]. This makes VM placement and migration more challenging because the resource discovery is required and VM migration happens not only due to performance degradation but also because of disappearing a fog node serving mobile devices. Thus, it is required to design mobility-aware methods/algorithms for offloading the tasks from mobile devices to VMs in MEC servers or in fog nodes based on network coverage, network load and the acceptable execution delay.

Virtual Network Function (VNF) Placement and Deployment: The emergence of SDN and NFV decouples hardware properties from network functionality (e.g., firewall, proxy server, and IDS) and deploys them upon generic computing resources under a central control [39]. SDN-/NFV-enabled network reduces monetary cost and improves E2E latency in the edge computing paradigms [256]. Nevertheless, the deployment of VNFs increases the utilization of resources in a central cloud and reduces E2E latency in the edge computing. To make a balance, it is required to make decisions on where (edge or central cloud) to create VNF and which VNF should serve the requests/service functions so that applications receive acceptable E2E response time and service functions violation. One solution for such balance can be the binary selection of either edge or cloud computing to serve a SFC, which suits to the two layered edge-cloud computing architecture (e.g., MEC).

Nonetheless, for the multi-layer edge-cloud architecture (e.g., FC), deployment VNFs and steering SFCs among servers is more challenging issue in comparison to the ones in the two layered architecture. Steering SFCs across servers in these architectures can be considered as a spectrum. From one extreme side of spectrum, the whole SFC is put in a server to achieve high utilization of resources while increases response time, and from another extreme side, a SFC can be spread across servers that raises bandwidth consumption and reduces response time. Hence, there is a need of a balance between these two sides based on the type of applications. Real-time applications are sensitive to response time and it is a need of compact SFCs in a server. For semi-/non-real time applications, users have more freedom to scatter SFCs across servers. Furthermore, behaviors of users have a determinative rule in VNFs placement, where a user’s service consumption and mobility respectively specify which server should host the VNF and when the VNF should be migrated from one server to another. The network dynamism is another factor that may influence the QoS requirements in the long term, which necessitates to recompute VNF placement and migration. Optimization of VNF placement and migration along with steering SFCs can be formulated in Integer Linear Programming (ILP) problem, which yields high time complexity [41], [269]. Thus, the exploitation of efficient online greedy algorithms is an effective solution to cope with such complexity, which requires theoretical and experimental evaluations.

VNF Virtualization and Management: The existing research on the NFV framework in the edge-cloud environment relates to VNFs placement with SLA satisfaction and NFV/SDN architecture for Wi-Fi networks, wireless networks and Virtualized Customers Premises Equipment (vCPE) [39]. However, virtualization generally degrades the performance of a system and thus there is a venue to evaluate the impact of virtualization on VNF performance in terms of process sharing, TCP/UDP throughput, packet loss and packet delay [209]. Furthermore, complications arise when multiple different service providers cooperation is required. Traditional NFV and SDN approaches are suitable for single-owner networks and data centers where all network routes and devices are controlled by the same entity. In contrast, routing network packets through a third-party network may be complicated or unacceptable due to trust and performance issues. Therefore, flexible and secure cross-network resource hand-off protocols and software implementations must be developed in order to support dynamic service migration for highly mobile clients. Lastly, implementing a virtualized edge infrastructure hosted by telecommunication providers that is shared between multiple service providers on demand would be potentially useful. Such infrastructure can enable moving services closer to a large number of clients based on the current or the predicted load. Modern virtualization techniques are highly suitable to this type of resource sharing between multiple independent tenants.

Synergy of IoT, AI, Blockchain, and Edge Computing: The impact of emerging technologies (IoT, AI, and Blockchain) are essential on the computing at the network edge in the case of data processing, data storage and management. IoT transfers a world of connected devices and servers into a world of data in the context of edge and fog computing. Artificial Intelligence (AI) allows to process data at the edge of network to make real-time actions without processing data at the core network (cloud servers). This reduces data transfer to the cloud and provides better response time. Blockchain records data in a distributed ledger/database. Due to distributed and resource-constrained devices at the edge network, the impact of these technologies is different with the one on the cloud computing investigated by [84]. The connectivity and richness of IoT devices and fog servers affect the AI (Machine Learning (ML) + Deep Learning (DL)) and blockchain at the edge computing. Since the model training of AI is computing-intensive, we need to deploy lightweight model training or move the processing into fog servers instead of edge devices. Another solution is to leverage distributed and federated model training across edge devices [247]. This solution, however, brings communication overhead, where the edge computing faces with limited bandwidth. In respect to each of these solutions, it might be effective to reduce dimension of data with the help of AI [82] since computation time of a model training is directly proportional to the data dimension and instances/records. With these steps, we approach to have data analytics at the edge computing as discussed more later. Integrating blockchain and edge computing enhances integrity, security, privacy, processing, tracking and monitoring of data via peer-to-peer network with the help of consensus mechanism such as Prof of Work (PoW) and Prof of Stack (PoS) network [57]. Nevertheless, this technology is not simply to integrate with edge computing due to heavy processing for consensus protocol and storing a copy of each block on all devices [130]. Moreover, this technology needs more energy and bandwidth [120]. Thus, to take the advantage of blockchain, it is required to implement a lightweight blockchain across edge devices and edge servers so that distributed ledgers are placed in the more powerful servers in terms of storage. Private and public blockchain, respectively running on a local subnet and across subnets of edge computing, is another solution to reduce storage and bandwidth consumption.

Synergy of Virtualization, Machine Learning, and IoT Data Analytic: Due to the massive amount of data at the edge computing, centralized data processing in a cloud computing infrastructure raises several interesting issues in terms of high bandwidth demand and high latency [271]. However, the decentralized data processing in the edge computing with the help of ML techniques copes with such issues [3]. We can well imagine (i) IoT big data is processed only in a cloud or edge, which respectively suits to delay tolerant and delay sensitive applications, and (ii) both cloud and edge computing are jointly exploited to analyze IoT big data. ML and IoT data analytics meet management and control of virtualized resources in order to have their global status [207]. VIM, MANO and SDN, already discussed, expose such status and control the available virtual resources for IoT big data analytic in the cloud-edge environment. ML techniques can leverage randomized and distributed algorithms [207] to combat the high dimensional big data in terms of features and samples number generated by IoT devices. Randomized algorithms are not suitable to IoT platforms since these algorithms iteratively run on centralized data. However, distributed algorithms enable the processing of data locally in each computing devices/node in an edge; the results of the locally processed data contribute to global decision making. This arrangement increases the communication between nodes for distributed algorithms deployment and management. To enable IoT big data analytic in a cloud-edge platform, it would be effective to (i) map different stages of big data analytic (data acquisition, feature extraction, models processing, uploading and feedback and information extraction [273]) to cloud-edge layers based on the richness of resources, (ii) design distributed algorithms with low communication requirements and convergence in a reasonable iterations and (iii) leverage algorithms that enable the utilization of online sample data instead of offline. Online distributed algorithms are thus the best fit in the context of IoT big data analytic because online and distributed features of algorithms respectively allow to process the new arrival of the data generated by edge devices and analyze data in itself constrained-resource devices.

Data Reduction Techniques in Edge Computing: Due to resource-constrained edge devices for processing the expected huge amount of IoT data, data reduction/cleaning is an essential process to reduce storage cost, decrease energy consumption, prevent I/O and bandwidth bottleneck. Data cleaning, namely, is to remove missing values, redundant values and outliers so that data integrity and reliability are maintained. In addition, data can be reduced with simple, but very effective approaches, such as sampling, filtering, summarizing, and compression [103]. However, these typical data reduction approaches directly are not applicable in the edge environment due to distribution nature of resources and online/time-series data generation. Thus, low-complex distributed data cleaning algorithms would be appropriate for such environment in which the generated data are transferred to the edge/fog severs to apply different data reduction techniques [243]. These techniques can be raw data reduction via Pearson coefficient metrics, filter data redundancy via K-nearest neighbor clustering, and more complicated and effective approaches [182], [236]. To cope with the processing of online data generated in IoT devices, it is essential to apply data reduction approaches with least delay and re-configuration. Traditional approaches such as aggregation techniques (e.g., summarizing) and event/policy-based events are not suitable for reducing online data. This is because these approaches require a posterior dataset or need an engine to pass specific data without actual data reduction [181]. To figure out this issue, the effective way is to streamify data with the help of caching and Perceptually Important Points (PIP) techniques in which the accuracy of forwarded data and forwarding delay should be balanced [181]. Solely relaying on indicated solutions to address the reduction of distributed and online data in edge computing might not be efficient. Instead, it is effective to leverage centralized data reduction approaches [210] in the edge devices to remove dirty data, and then the cleaned data should be transferred into the edge servers to apply ML and DL to reduce dimensions and instances of data [82]. Thus, a pipeline data reduction approach should be designed throughout the edge-cloud ecosystem to achieve raw data reduction, packet reduction, and event reduction [96].

8. Conclusions
The edge computing paradigms move cloud-based services closer to the edge of network hosting IoT applications requiring ultra-low latency and jitter, mobility and location awareness services and high demand bandwidth. The edge computing paradigms consist of a variety of resources with different capabilities in richness, where virtualization is the essence of their infrastructure. In this paper, we first analyzed different paradigms of cloud and edge computing and investigated the reasons behind transition from one paradigm to another based on the requirements of IoT applications. More specifically, we have identified the objectives, features and pillars for both edge and cloud paradigms to understand which paradigm is more suitable to a particular IoT application. Second, we discussed different virtualization techniques for computing and networking resources and explained which virtualization technique could be appropriate based on the richness of resources and the requirements of IoT applications. We then investigated the classical VMs and Virtual Network Function (VNF) from performance perspective in an edge-cloud environment. We finally map the existing studies to the provided taxonomy for each research subject in this paper. Based on the lessons learned from the reviewed studied, we have identified the future research directions and the potential starting point to tackle the challenges regarding virtualization of edge computing paradigms. The challenges include the determination of virtualization type, placement and migration virtualized computing and network resources in the edge-cloud environment adapted to the requirements of IoT applications deployment.