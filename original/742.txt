Abstract
Modern GPGPU supports executing multiple tasks with different run time characteristics and resource utilization. Having an efficient execution and resource management policy has been shown to be a critical performance factor when handling the concurrent execution of tasks with different run time behavior. Previous policies either assign equal resources to disparate tasks or allocate resources based on static or standalone behavior profiling. Treating tasks equally cannot efficiently utilize the system resources, while the standalone profiling ignores the correlated impact when running tasks concurrently and could hint incorrect task behavior. This paper addresses the above drawbacks and proposes a heterogeneity aware Selective Bypassing and Mapping (SBM) to manage both computing and cache resources for multiple tasks in a fine-grain manner. The light-weight run time profiling of SBM properly characterizes the disparate behavior of the concurrently executed multiple tasks, and selectively applies suited cache management and workgroup mapping policies to each task. When compared with the previous coarse-grained policies, SBM can achieve an average of 138% and up to 895% performance enhancement. When compared with the state-of-art fine-grained policy, SBM can achieve an average of 58% and up to 378% performance enhancement.

Previous
Next 
Keywords
Manycore architectures

Dynamic scheduling

GPGPU

OpenCL

Heterogeneous applications

1. Introduction
GPGPUs (General Purpose Graphics Processing Units) are emerging as throughput processors to enable superior performance for a wide variety of applications. Applications with massive parallelism are initiated as enormous threads that will be dispatched and executed concurrently by the processing engines in a GPGPU. Fig. 1(a) illustrate an example of AMD GCN (Graphics Core Next) architecture [2] that consists of multiple Compute Units with private L1 cache, and shared L2 cache. Due to the fast scaling of GPGPU system sizes, modern GPGPUs support concurrent execution of multiple tasks to leverage the task-level parallelism and enhance the utilization of computing resources. For example, both Hyper-Q from NVIDIA [7], [12] and Asynchronous Compute Engines (ACE) from AMD [2], [4] can dispatch multiple tasks for execution simultaneously to exploit the massive parallel computing capability of computing engines. The computation resources, including computation units and memory, will be shared by all the dispatched tasks. Since the parallel tasks would share the resources during the execution, it has been demonstrated that applying improper resource management policy when concurrently executing tasks with different run time behavior could lead to performance degradation [13], [16], [24], [25], [27]. This issue is mainly caused by the mismatch between the disparate behavior of the tasks and the inefficient resource management policy. The above issue happens when a GPU is shared by different parallel tasks. As shown in Fig. 1(b), an application contains multiple parallel tasks where each task consists of workgroups that follow specific execution behavior. Each workgroup has parallel work-items that can be executed concurrently. This organization of parallel tasks and terminologies follow the same convention in OpenCL [20] and will be used in the rest of this paper. Since these tasks are implemented for different functions in an application, the workgroups from these tasks would behave differently and have various demands for memory and computation resource. Although the work-items in a workgroup would behave similarly, the characteristics between workgroups from distinct tasks could be significantly different. In this paper, we use a heterogeneous application to refer to the application that has multiple tasks with disparate execution behavior and resource demand. Note that, without loss of generality, the concurrent tasks could also be dispatched from different applications using the architecture support of modern GPGPUs, such as Hyper-Q [7], [12] and ACE [2], [4].

Fig. 2 illustrates an example of executing multiple workgroups from two tasks. The two tasks, matrix multiplication (mm) and histogram (hist), are adopted from AMD SDK [3]. The task hist is Cache Friendly and could benefit from having more cache space, while the task mm is Computation Intensive and needs more computing engines to overlap the memory latencies by computations [14], [17]. The two characteristics (Cache Friendliness and Computation Intensity) will be explained in more detail in Section 2.2. In Fig. 2(a), both tasks (mm and hist) are allocated equal number of Compute Units (CUs) and the same access privilege to the memory (L1 cache, L2 cache, and memory). Although modern GPGPUs can support concurrent execution of workgroups from different applications [2], [4], [7], [12], the current workgroup mapping policy is not aware of the execution behavior and memory access patterns of each workgroup. Fig. 2(b) applies a heterogeneity aware scheme which assigns more computing engines to mm and bypasses the data access through L1 and L2 caches. Having more computing engines could increase the throughput of the Computation Intensive mm, while bypassing the data accesses would retrieve the data with low transaction overhead. The less-contended caches are also benefiting the Cache Friendly hist. The experiment in Section 6 shows that the heterogeneity aware scheme in Fig. 2(b) can enhance the performance significantly when compared to Fig. 2(a).


Download : Download high-res image (214KB)
Download : Download full-size image
Fig. 1. (a) Block diagram of Compute Units and caches in GCN GPGPU architecture. (b) Heterogeneous application execution model. An application consists of tasks, while a task consists of workgroups. Each workgroup consists of a number of work-items.


Download : Download high-res image (287KB)
Download : Download full-size image
Fig. 2. (a) Equally assign computation and memory resources to different tasks. Task demands are not considered in this scheme. (b) Assign computation and memory resources according to task characteristics.

Several previous works have been proposed to handle execution and resource management for the concurrent tasks with disparate behaviors. However, the prior approaches still pose shortages from different design aspects. First, there is a lack of insight to the heterogeneity among the concurrently executed workgroups. The works in [14], [17], [18], [22] focused on resource assignment by assuming all the workgroups dispatched to a GPU are homogeneous and have the same execution behavior, and tend to apply the same computation and cache management scheme to all the workgroups. Second, the resources are not allocated proportional to the observed heterogeneity at run time. The studies in [13], [24] are aware of the heterogeneity between tasks within an application; however, the resource management schemes still tried to identify a common coarse-grained policy that equally allocates computation resources to these different tasks. In this work, we consider a policy as coarse-grained when the policy treats all the concurrently dispatched tasks equally and applies the same execution and resource management scheme to all the tasks. On the other hand, a fine-grained policy considers the disparate characteristics of different tasks and allocates resources accordingly. The authors in [16] statically profiled workgroups of tasks at compile time but did not take into account the change of task characteristics impacted by the other co-executed tasks. Third, the previous management schemes [13], [15], [24], [25], [27] did not consider both computing and memory resources together. The work in [25] focused only on the computing resources and issued more instructions per cycle for the more Computation Intensive task. The diverse demand to the memory resources that could possibly be raised from different tasks was not considered.

This paper addresses the above issues with heterogeneity aware Selective Bypassing and Mapping (SBM) to manage both computing and cache resources for heterogeneous applications in a fine-grained manner. The main contributions can be summarized as follows: First, we perform comprehensive studies on crucial design factors, including patterns of data accesses, demand for computing engines, and the correlated performance for heterogeneous applications within a GPGPU. The results reveal the insight of architectural impact and design concerns when supporting heterogeneous applications.

Second, this paper proposes Selective Bypassing and Mapping (SBM) scheme to efficiently manage the resource usage with proper workgroup assignment policies based on the run time characteristics of tasks in an application. SBM profiles the heterogeneous application at run time, while considering both computation and caches/memory resources demand for each of the concurrently dispatched tasks. After that, SBM applies the most suited resource management scheme.

Third, this paper conducts thorough experiments on various heterogeneous applications. We show that the coarse-grained approaches that apply fixed resource management schemes such as [13], [15], [24] for memory or computation resources cannot properly allocate resources for different demands from tasks and would degrade the system performance. We also demonstrate that the fine-grained approaches which conduct static profiling without considering impact of concurrently executed tasks, such as EWM [16], can lead to inefficient resource management scheme.

This paper is organized as follows. Section 2 introduces the background of this paper. Section 3 elaborates the proposed Selective Bypassing and Mapping (SBM) scheme. Section 4 discusses the architecture enhancement to support the proposed SBM scheme. Section 5 illustrates the modeling of the heterogeneous application execution. Section 6 shows the performance enhancement and discusses the experimental results. Section 6 concludes paper.

2. Background
2.1. Software application and hardware architecture
This paper uses AMD GCN (Graphics Core Next) architecture [2] as the main target platform. OpenCL API is used as the parallel execution model. An application is initiated as tasks in a multi-dimensional domain where each basic element within the domain is called a work-item. A work-item is implemented as a thread.1 As illustrated in Fig. 1(b), the total number of work-items is mapped onto an N-dimensional grid (ND-Range). A workgroup contains multiple work-items and will be dispatched for execution by the Compute Units in a GPGPU. Fig. 1(a) illustrates reference architecture of GPGPU, which is composed of multiple Compute Units. Each Compute Unit contains four SIMD (Single Instruction Multiple Data) processing engines, a private memory, and L1 cache. L2 cache is shared between the Compute Units. All of the workgroups are mapped to the Compute Units equally, regardless of their Computation Intensity. According to the current GCN architecture [2], all of the workgroups’ memory accesses have the same privilege to access the caches/memory.

2.2. Characteristics of applications
In general, an application can be characterized by three fundamental properties listed in Table 1, namely cache friendliness, computation intensity, and heterogeneity. Cache friendliness specifies how well an application can benefit by having more cache space [14], [17]. A Cache Friendly application contains tasks that pose well data reuse and can effectively utilize the cache space. Computation intensity reflects how the computation parallelism of an application impacts the overall performance. The third property, heterogeneity, describes an application containing parallel tasks with disparate run time characteristics.


Table 1. Characteristics of applications.

Properties	Characteristics
Cache friendliness	An application that poses well data reuse and can effectively utilize the cache space.
Computation intensity	An application that can achieve superior performance after exposing high computation parallelism.
Heterogeneity	An application that contains workgroups with disparate run time characteristics and resource requirements.

Download : Download high-res image (173KB)
Download : Download full-size image
Fig. 3. Normalized IPC versus TLP of the three types of tasks behaviors (conv, dct and sf). IPC is normalized to the maximum IPC (22.96) in the three tasks. The performance is indicated by the normalized Instructions Per Cycle (IPC). Thread Level Parallelism (TLP) represents the number of workgroups mapped to each Compute Unit from a task.

This paper focuses on the resource management scheme for heterogeneous applications that have workgroups with disparate execution behavior and resource usage patterns. The threads from the same task should have similar execution properties while the threads from different tasks would pose different characteristics. The management for the system resources for a heterogeneous application has become a performance critical yet non-trivial design concern [16], [24], [25]. To properly manage a heterogeneous application in modern GPGPU, the execution and resource management scheme should (1) be aware of the properties of tasks in an application, and (2) allocate resources, including both computation and memory, according to the disparate requirements from tasks in a fine-grained manner. The actual properties of the parallel tasks are determined by the correlated effects from different levels of cache friendliness, computation intensity, and heterogeneity, as well as the utilization of available resources.

Fig. 3 illustrates three types of tasks having three different properties namely Computation Intensive Non-Saturating, Computation Intensive Saturating and Computation Unfriendly which were obtained after profiling dct, sf and conv respectively. The -axis tracks the performance which is the IPC normalized to the maximum IPC of the three tasks (22.96) while the -axis indicates the TLP (Thread Level Parallelism) of tasks. In this paper, we use IPC (Instructions Per Cycle) as the main performance indicator, while TLP represents the number of workgroups mapped to each Compute Unit from an application. Note that in this paper, the size of the workgroups (i.e. number of threads within a workgroup) is fixed for all the tasks within an application. In other words, more workgroups imply higher TLP.

The first type of tasks in Fig. 3 is Computation Intensive Non-Saturating [27] where the performance (i.e. IPC) would be enhanced with increasing TLP. The IPC of a task will keep increasing as long as there are available Compute Units that can be exploited by more outstanding workgroups since there are few memory instructions in this type and adding more threads does not cause cache contention. The second type of tasks is referred as a Computation Intensive Saturating [27]. The IPC of this task also increases with TLP in the beginning. However, the maximum IPC would saturate even after allocating more workgroups in the computation units. The IPC cannot be further enhanced mainly due to the structural contention among the outstanding tasks on the shared resources. The third type of tasks behavior happens when the structural contention is too severe and starts degrading the task performance when applying higher TLP. This type of task is then referred as a Computation Unfriendly task. The performance saturates or drops at a certain TLP level because of the structural contention like L1 cache contention. L1 cache becomes full and adding more threads to the Compute Unit initiates more cache accesses that cause thrashing and degrades the overall performance for Computation Unfriendly tasks while it saturates the performance of the Computation Intensive Saturating tasks. The performance saturates or drops sooner in the tasks that have more memory instructions and less computation instructions. The three computation intensity characteristics are listed in Table 2.


Table 2. Computation intensity characteristics.

Properties	Characteristics
Computation intensive non-saturating	The performance is increasing and does not saturate with high TLP
Computation intensive saturating	The performance is increasing but will saturate with high TLP
Computation unfriendly	The performance is decreasing with high TLP
2.3. Previous works to handle heterogeneous applications
Previous works have proposed solutions to enhance the performance by managing the execution and resources according to tasks properties. Some works focused on homogeneous applications with similar execution behavior. By labeling GPGPU applications to different types, the works in [14], [17] provided a TLP (Thread level parallelism) aware policy to manage the accesses from GPGPU to the shared cache of a CPU–GPGPU heterogeneous architecture. The work in [17] proved that bypassing the shared cache of the Cache Unfriendly GPU task would enhance the performance of the Cache Friendly CPU task. Authors in [18], [22] aimed to manage resources by categorizing applications as computation- or data-intensive. These works applied coarse-grained classification of GPGPU applications types without considering the heterogeneity within GPGPU applications.

Some works [13], [24], [27] are aware of the heterogeneity in an application; however, the resource management schemes still treat the heterogeneous application with a common policy. The resources, such as the number of computation engines or memory accesses, are evenly distributed to tasks without considering the disparate characteristics and different demands for resources. Xu et al. [27] proposed a dynamic resource sharing algorithm, Wang et al. [25] proposed a fine-grained context switching mechanism. Approaches [25], [27] allocate more computation resources to the more Computation Intensive tasks. However, the disparate memory requests from tasks were not taken into account. Liang et al. [28] have used a performance aware cache bypassing and partitioning for the tasks but still map equal number of workgroups from each task to each Compute Unit. They have not considered the computation resource requirements of the tasks and assumed that it is equal. Additionally, their partitioning scheme did not consider mapping the tasks into separate Compute Units which is beneficial when there is a race between the tasks on the computation engines.

The work in [16] used a static scheme to profile each task individually. The execution scheme then managed the execution based on the behavior of each task. But the approach in [16] did not consider the heterogeneity between tasks and the run time impact between tasks with different characteristics.

2.4. Impact of concurrently executing tasks
One of the key features that was mostly ignored by previous works is the impact of concurrently executing tasks. The characteristics of a task in an application could be impacted by the other task and behave differently from the case when running the task alone. Fig. 4 uses an example to illustrate the change in the task characteristics. We use different labels for cases when a task is under the impact of another task. For example, fw_(alone) denotes that the profiling of fw task is done without the effect of any other task, while fw_(mm) denotes that the profiling of fw is done under the effect of mm. Profiling is done by running each task for a profiling period under different TLP levels and measuring the IPC. The length of the profiling period used is 40 k cycles and will be discussed in more detail in Section 6.5.


Download : Download high-res image (155KB)
Download : Download full-size image
Fig. 4. Profiling the execution of two different tasks (fw and mm) concurrently and alone. IPC is normalized to the maximum IPC (12.63) in the four cases.

Fig. 4 shows that profiling the tasks separately (fw_(alone) and mm_(alone)) is different from profiling the tasks under the effect of each other (fw_(mm) and mm_(fw)). In standalone profiling, fw alone (fw_(alone)) appears to be more Computation Intensive than mm alone (mm_(alone)). fw_(alone) has an enhanced performance (higher IPC) over mm_(alone). However, when running these tasks together in the same application, both tasks (mm_(fw) and fw_(mm)) are equally Computation Intensive. Both of the tasks fw and mm (fw_(mm) and mm_(fw)) have the same performance enhancements (IPC). The race on the computation and memory resources appears clear and reflects the precise tasks behavior when executing them together.

This means that standalone profiling could introduce an inappropriate indication to the Computation Intensive task in an application. This observation is a key concern when performing proper resource management. For this particular case (fw and mm), our proposed SBM could successfully track the behavior of tasks in a heterogeneous application and attains 21.9% performance enhancement over EWM [16] which profiles the tasks in a standalone manner.

3. Selective bypassing and mapping for heterogeneous applications
3.1. Policies of selective bypassing and mapping
The Selective Bypassing and Mapping (SBM) focuses on the management of two performance-critical resources on GPGPU, namely the computation resources and memory resources. Since a heterogeneous application contains various types of tasks, SBM will first identify whether the tasks in the heterogeneous application would benefit by possessing extra resources. The key concept of SBM is to apply fine-grained management of both Compute Unit allocation and cache accesses for a heterogeneous application. The two approaches adopted by SBM to achieve the goal are Selective Data Bypassing and Adaptive Workgroup Mapping.

Selective Data Bypassing: SBM utilizes the cache bypassing to enhance the cache utilization and enables efficient data accesses for heterogeneous applications. Unlike the previous approaches that either bypasses the last level cache or all the caches, we extend the bypassing to a fine-grained manner which can dynamically coordinate the data accesses to bypass different levels of caches. Fig. 5(a) illustrates an example of different bypassing schemes used in this paper. Workgroups from task F1 adopt normal accesses to both L1 and L2 caches. The data accesses can choose to bypass only the L2 cache (e.g. workgroups from F2), or skip both L1 and L2 cache (e.g. workgroups from F3) to access the main memory directly. In general, a Cache Friendly task will go through the normal cache hierarchy. If a task is Cache Unfriendly, which means having more cache space does not provide performance gain, the data accesses will be treated with proper bypassing schemes. Bypassing the caches for Cache Unfriendly workgroups can avoid potential cache contention and free up more cache space for Cache Friendly workgroups. When both types (Cache Friendly and Cache Unfriendly) of tasks are executed together on the same GPGPU, SBM further allows the Cache Friendly task to utilize the cache space that are not used by the Cache Unfriendly task in order to enhance the cache utilization and performance.

Adaptive Workgroup mapping: The workgroups from different tasks can share the same Compute Unit. Sharing the Compute Unit enables the flexibility to utilize the computation resources in a Compute Unit more efficiently. The mapping scheme can choose to equally assign the same number of workgroups from each task to Compute Units or assign different numbers of workgroups from tasks to the Compute Units based on their properties. For example, the Computation Intensive tasks can be assigned a larger share of the Compute Units than the less Computation Intensive tasks. In Fig. 5(b), workgroups from task F1 and F2 are equally assigned to Compute Unit CU1, while CU2 has been assigned more workgroups from F3 than F4.

SBM chooses the number of workgroups mapped to a Compute Unit according to their characteristics. For example, when there are two workgroups executing on the same Compute Unit where one workgroup is Computation Intensive and the other one is Computation Unfriendly, SBM would map more workgroups from the Computation Intensive task than the Computation Unfriendly one. The adaptive mapping approach provides more computation resources to the Computation Intensive workgroups. However, executing two tasks in a Compute Unit might cause contention at L1 cache and could degrade the overall performance. To alleviate the cache contention, we would let the less Cache Unfriendly task bypass the caches.


Download : Download high-res image (149KB)
Download : Download full-size image
3.2. Profiling flow of SBM
Since a heterogeneous application contains various types of tasks, before applying the proposed selective approaches discussed in the previous section, SBM introduces a profiling flow to identify whether the tasks would benefit by possessing extra resources. The profiling flow of SBM is divided into two steps to perform run time profiling at the early phase of the application execution. These profiles return the properties of the tasks in a heterogeneous application and identify the most suited architecture configurations for different tasks.


Download : Download high-res image (222KB)
Download : Download full-size image
Step 1: Measuring cache friendliness. SBM determines the bypassing and mapping policies based on the properties of a heterogeneous application. The property is profiled at run time to determine if a task is either Cache Friendly and Computation Intensive or Unfriendly respectively. The profiling flow is performed at the beginning of the application execution. The first step is to profile the cache friendliness property of a task, as shown in Algorithm 1. It is divided into phases using different architecture configurations to test the property of a task. The schemes and their target properties are listed in Table 3. The IPC of each configuration is used as the performance indicator. The higher the IPC, the more suited the architecture configuration for the application. The length of a phase is defined by users, and the guideline is to find the properties of tasks within a short period of profiling phases. According to our experiments, a phase of 40 thousand cycles can effectively capture the inherited properties of an application.

Table 3. Architecture configurations.

Architecture Config.	Properties
Base	- No cache bypassing.
- Workgroups from each task are mapped to separate Compute Units.
B1_L12	- Bypassing 1st task through L1 and L2.
- Workgroups’ mapping similar to Base.
B2_L12	- Bypassing 2nd task through L1 and L2.
- Workgroups’ mapping similar to Base.
B1_L2	- Bypassing 1st task through L2.
- Workgroups’ mapping similar to Base.
B2_L2	- Bypassing 2nd task through L2.
- Workgroups’ mapping similar to Base.
MWG	- Multiple Workgroup in a Compute Unit.
- No cache bypassing
MWGB1 MWGB2	- MWG which Bypasses all caches for 1st or 2nd task
- Selective data bypassing
The profiling flow of Algorithm 1 will explore the profiling schemes listed in Table 3. Each of them represents a possible resource management scheme that could be applied to enhance the performance. The essential flow of Algorithm 1 is to determine if a task is Cache Friendly by having more caching resource. Algorithm 1 will also determine which of the two tasks (B1 or B2) has higher performance improvements when it has more caching. Algorithm 1 starts by profiling five configuration schemes, Base, B1_L12, B2_L12, B1_L2 and B2_L2 (Line 1). If B1_L12 (or B2_L12) is better than B1/2_L2 (B1_L2 and B2_L2), cache bypassing is benefiting the application. Therefore, B1_L12 (or B2_L12) is used as the final scheme respectively. If neither B1_L12 nor B2_L12 is better than B1/2_L2, Algorithm 1 will choose either B1_L2 or B2_L2 or Base depending on which one has higher IPC (Line 2 to Line 4). Bypassing L1 cache only (i.e. B1/2_L1) is not considered in the Algorithm while bypassing L2 cache only is considered due to the following reasons. First, according to profiling, the less Cache Friendly task in B1/2_L2 poses very limited data reuse therefore the data access can be already well supported by the assigned L1 cache. Second, bypassing L1 cache only should not provide additional performance benefits for the other concurrently executing tasks, since they have their own L1 caches. Third, L2 cache in a GPGPU usually support a write-back policy [21] with write buffers. The adverse impact on latency from bypassing L2 (write directly to memory) can be alleviated by L2 write buffer. Fourth, bypassing L2 provides additional cache ways for the other executing task(s) which could benefit from more L2 cache space.

Step 2: Finding proper utilization of Compute Units. The second step is to determine the proper number of computation resources assigned to a task. After Algorithm 1, SBM selectively bypasses the cache accesses of the tasks. SBM then uses Algorithm 2 to determine which task is more Computation Intensive by having more executed instructions per cycle. Algorithm 2 is composed of a series of tuning phases, where each phase increases the number of workgroups mapped to the Compute Units from the Computation Intensive task. The tuning phases terminate when the IPC starts to degrade or the limit of Compute Unit workgroup occupancy is reached. Based on results concluded from extensive experiments, four tuning phases are enough to find the best approach. Although we do believe the comprehensive experiments have covered a wide range of various heterogeneous applications, the tuning process may need to be slightly adjusted if the heterogeneous applications are posing significantly different behavior from the ones used in this paper. With Algorithm 2, proper quantities of workgroups will be assigned from each task to Compute Units. Note that the tasks may not benefit from dispatching more workgroups to the Compute Units. In this case, equal number of workgroups is mapped from each of the tasks to the Compute Units.

After Algorithm 2 is completed, SBM has determined which task should bypass the caches (less cache friendly task) and which task needs to have more workgroups per each Compute Unit. SBM will then choose the most suited architecture configuration for workgroups from the heterogeneous application. The architecture configuration includes the proper cache bypassing scheme and workgroup mapping to Compute Units. The heterogeneous application will then be executed under the chosen architecture configuration until the application is completed.

4. Supporting SBM in GPGPU architecture
The baseline AMD GCN architecture supports concurrent execution of tasks by having each of the tasks in a different command queue [4]. The scheduler maps workgroups from different command queues asynchronously to the Compute Units. The main purpose of the current scheduler is to maximize the computation parallelism in a greedy manner. The current scheduler of GCN is unaware of the execution behavior and resource demand that could be considerably different between dispatched tasks. To support workgroups mapping in SBM, a low complexity SMC (Selective Mapping Controller) is introduced to assist the selective workgroup mapping policy. As illustrated in Fig. 6(a), the workgroups will be stored in the command queues. The ACE (Asynchronous Compute Engines) in a GPGPU architecture will parse incoming workgroups and dispatch them to the Compute Units (CUs) based on the mapping policy. The added SMC is a simple but effective module to facilitate the selective mapping policy determined by SBM. As in Fig. 6, SMC implements six bits of SBM system registers (SBM_Reg) to record the tasks characteristics determined by the profiling steps. Connected through a simple AND gate, SMC can provide information to set proper architecture configurations. The information of IPC can be retrieved from the existing GPGPU performance counters [5] that are capable of counting both the instructions and the cycles. After the profiling steps, SBM_Reg should be programmed with the most suited architecture configuration for the target task.


Download : Download high-res image (266KB)
Download : Download full-size image
Fig. 6. (a) Selective Mapping Controller (SMC) implemented in Asynchronous Compute Engine (ACE) to selectively map workgroups from each task according to occ_th in SBM_REG (b) Control scheme for cache bypassing that selectively bypasses the cache accesses according to $F in SBM_REG.

As listed in Table 4, each field of the SBM_REG specifies the needed microarchitecture configuration. The arch_cfg uses two bits to specify the main scheme that will be used, including B_L2, B_L12, MWG, and MWGB. When a cache bypassing scheme (B_L2 or B_L12 or MWGB) is selected, $F is used to select the task that should bypass the cache. Both CI and occ_th are used when MWGB configuration is selected. The Computation Intensive task is determined from the CI register. The occ_th sets a threshold to the number of workgroups that will be sharing the computation engines with the Computation Intensive task. For example, for Compute Unit occupancy of eight workgroups, the default setting (when occ_th=0) is to assign four workgroups to each of the two tasks. When occ_th=1, one more workgroup will be assigned to the Computation Intensive task. Five workgroups will be assigned to the Computation Intensive task and only three workgroups to the Computation Unfriendly task. Based on the characteristic recorded in SBM_REG, the system selects the proper architecture configuration. ACE will then assign the workgroups from command queues to the Compute Units with the selected configuration. In general, SBM would let one task bypass the cache hierarchy to enable more efficient data accesses as well as better utilization of cache space. The system interconnection between CUs and memory hierarchy should be configurable on the bypassing mechanism, which is already supported in modern GPGPUs [19], [26]. Fig. 6(b) illustrates a simple concept of the implementation. SBM_REG will signal the cache bypassing with the register $F. When $F is false, the data access can be forwarded around the cache hierarchy and access the main memory directly. The extra hardware enhancement needed by SBM is also very minor when compared to a GPGPU system. As illustrated in Fig. 6(a), the added SMC controller only supports task information to the Work Dispatcher in the original GPGPU architecture, and is not on the critical path of task dispatching. SMC also poses very low implementation cost since it only requires six registers and simple logics to record the features of an application. The performance indicators (i.e. IPC) can be obtained from the existing performance counters in modern GPGPU. The decision making and controlling logics for SMC are of low complexity and almost negligible from the GPGPU system-wise point of view.


Table 4. Specification of SBM System register (SBM_REG)

Field Names	Descriptions
arch_cfg (2 bits)	Architecture configuration, which specifies one of the four architecture configurations, including B_L2, B_L1, MWG, and MWGB.
$F (1 bit)	Specifies the Cache Friendly task
CI (1 bit)	Specifies the Computation Intensive task
occ_th (2 bits)	Compute Unit occupancy threshold ranging from 0 to 3. occ_th is the absolute difference between the workgroups mapped from each task to a compute-unit.

Table 5. Tasks used in the experiments.

Task	Description	Source	WG size
hist	Histogram	AMD SDK	128 × 1
conv	Convolution	AMD SDK	8 × 8, 128 × 1, 256 × 1
sf	Sobel-Filter	AMD SDK	8 × 8, 128 × 1, 256 × 1
mm	Matrix-Multiplication	AMD SDK	8 × 8, 128 × 1
fw	Fast-Walsh	AMD SDK	8 × 8, 128 × 1, 256 × 1
dct	Discrete-Cosine-Transform	AMD SDK	8 × 8, 128 × 1
pf	Path Finder	Rodinia	128 × 1
hs3d	Hot Spot 3d	Rodinia	128 × 1
lava	LavaMD	Rodinia	128 × 1
nn	Nearest Neighbor	Rodinia	128 × 1
5. Modeling heterogeneous execution
To model the execution of a heterogeneous application, we implement both the GPGPU architecture enhancement proposed in the previous section, as well as heterogeneous applications composing various workgroups. We combine workgroups from two tasks with different behavior. The workgroups from the same task behave similarly, but pose different behaviors from workgroups of the other task. The workgroups of two tasks are constructed in the same OpenCL code, and initiated for execution concurrently from the same heterogeneous application. This heterogeneous application therefore contains workgroups with two different behaviors. The list of the tasks we use in this paper is shown in Table 5. The first column lists the abbreviation of the tasks. The second column lists the functionality of the tasks. The third column lists the task sources. The fourth column lists the 2D workgroup sizes. The constructed heterogeneous applications tasks are then identified in the OpenCL API. For the sake of simplicity, we conduct our study for two types of tasks. Each task is composed of a number of workgroups, and workgroups would follow specific execution behavior. The proposed SBM applies fine-grained resource and execution management for the tasks with different characteristics. The same concept could be extended to GPGPU systems with more tasks and queues.

This paper uses Multi2Sim [23] to model the GPGPU architecture. The SMC introduced in the previous section is also modeled and integrated into Multi2Sim. The base GPGPU architecture in this paper applies the AMD Southern Islands 7970. There are total 32 Compute Units (CUs) in the GPGPU. Each CU is implemented with a 16 KB of L1 cache and 64 KB of local memory. Every CU contains four SIMD engines to support concurrent execution of tasks. Two command queues are modeled in Multi2Sim to support the execution of two types of tasks. The OpenCL API is extended to facilitate the identification of the tasks from the application. The selective mapping controller (SMC) is aware of the workgroups from different queues, and will dispatch the workgroups for execution based on the chosen mapping policy.

Although the GCN architecture used in Multi2Sim is for previous GPGPU generation, the latest GPGPU architecture, such as AMD Vega [1] announced in late 2017, is still based on GCN architecture. The major enhancements applied to the current generation GCN are on deploying faster Compute Units, using higher memory bandwidth of HBM2, enabling new graphic processing method, and supporting packed math of various arithmetic precision. The hardware scheduler in modern GCN architectures still allocates resources in a coarse-grained scheme without considering the fine-grained characteristics of tasks and disparate demands for computation and memory resources. There are no effective solutions to handle disparate behavior between concurrently executed tasks. Therefore, the issues raised in the paper still exist in modern GPGPUs and could happen in the next generation of general purpose high throughput processing.

6. Experimental results
6.1. Heterogeneous applications
The benchmarks used in this paper are adopted from two different benchmark suites namely AMD SDK v2.9.1 [3] and Rodinia [8], [9]. The computation tasks from these benchmark suites pose significantly different functions and characteristics. In this paper, the ten different tasks (Table 5) from benchmarks are combined into total 17 heterogeneous applications (Table 6). The combinations of these tasks can represent a wide range of heterogeneous applications with disparate execution behavior and data access patterns.

We construct heterogeneous applications by combining various widely used tasks listed in Table 5 with different execution behaviors into the same application. Table 6 lists all the heterogeneous applications created by mixing the tasks from Table 5. The third column in Table 6 is divided into two sub-columns; each sub-column lists the task characteristic profiled by SBM. In the third column, the labels CNS, CS, CUF denote Computation Intensive Non-Saturating, Computation Intensive Saturating and Computation Unfriendly respectively. The label $F denotes the task is Cache Friendly, which also implies that the other task is Cache Unfriendly. Among these tasks, the performance of some tasks is sensitive to data access latency, while others are determined mainly by the computation throughput. An interesting observation is that the characteristic of a task could change when pairing with different tasks. The characteristic of a task in an application could be impacted by the other task and behaves differently from the case when running the task alone. For example, in dct_fw, the task dct is identified as Computation Intensive Non-Saturating and Cache Friendly; while in dct_sf, the dct is characterized as Computation Intensive Non-Saturating without cache friendliness. This phenomenon was discussed in Section 2.3. The fourth column specifies the architecture configurations chosen by SBM. The labels for different architecture configurations are listed in Table 3.


Table 6. Heterogeneous applications used in the experiments.

Application	WG	Characteristics	Architecture
Task1	Task2	size	Task1	Task2	configuration
dct	conv	8 × 8	CNS.$F	CS	MWGB2
dct	fw	8 × 8	CS.$F	CS	B2_L2
dct	hist	128 × 1	CUF.$F	CS	MWGB2
dct	mm	8 × 8	CS.$F	CS	MWGB2
dct	sf	8 × 8	CS	CS.$F	B1_L2
fw	conv	256 × 1	CNS	CNS.$F	B1_L2
fw	hist	128 × 1	CUF.$F	CNS	MWGB2
fw	sf	128 × 1	CS.$F	CS.$F	MWG
fw	mm	128 × 1	CS.$F	CS	MWGB2
hist	mm	128 × 1	CS.$F	CS	MWGB2
hist	conv	128 × 1	CS	CS.$F	MWGB1
hist	sf	128 × 1	CUF	CUF.$F	MWGB1
mm	conv	128 × 1	CS	CS.$F	MWGB1
sf	mm	8 × 8	CUF.$F	CS	MWGB2
sf	conv	256 × 1	CNS.$F	CNS	B2_L12
pf	hs3d	128 × 1	CNS.$F	CNS	B2_L12
lava	nn	128 × 1	CNS	CNS.$F	B1_L2
Note: CNS: Computation Intensive Non-Saturating. CS: Computation Intensive Saturating. CUF: Computation Unfriendly. $F: Cache Friendly

6.2. Performance evaluation
We use the Multi2Sim simulator and the SBM model discussed in Section 5 to comprehensively evaluate the performance of different approaches. We compare the performance of SBM to other existing approaches. The first approach, BASE, assigns the heterogeneous workgroups onto two evenly separated groups of Compute Units (CUs). This is similar to the approaches used in [24] and [13]. BASE distributes each of the two types of workgroups to a different workgroup queue and each workgroup queue is associated to half of the CUs. In this paper, since there are total 32 CUs in the target GPGPU, the first type of workgroups is assigned to the first half of CUs (CU0 to CU15) while the second type of workgroups is assigned to the second half of CUs (CU16 to CU31). The second approach, MWG, maps equal quantities of workgroups from both tasks to the same CU without cache bypassing. This is the same approach used in [15]. Each CU from the 32 CUs is assigned an equal share from the workgroups of both tasks. Both BASE [13], [24] and MWG [15] do not apply any cache bypassing scheme. The third approach, EWM [16], statically profiles each task individually and then manages the workgroup mapping based on the profiled behavior of each task. Both BASE and MWG treat different tasks equally and are considered coarse-grained policies. EWM can manage the execution of tasks according to their properties, and is considered a fine-grained policy. The restrictions and more detail discussion of BASE, MWG and EWM will be discussed in Section 6.3. The proposed SBM differs from previous approaches in the following ways. First, SBM applies a light-weight run time profiling to properly track the behavior impact when running different tasks concurrently. Second, the resources, including both computation and caches, are managed proportional to the requirement of each task in a fine-grained manner.

Fig. 7 illustrates the IPCs of different approaches. Overall, SBM outperforms all the other approaches and achieves in average 57%, 138%, and 58% better IPCs than BASE, MWG, and EWM respectively. For applications that cannot be properly handled by other approaches, SBM can attain up to 181.5%, 885.1%, and 378.3% higher IPCs than BASE, MWG, and EWM respectively. When compared with BASE, SBM achieves the maximum performance enhancement for the heterogeneous application fw_mm. The architecture configuration chosen by SBM is MWGB2, which bypasses the second task (mm), while sharing each CU between mm and fw. Since mm is less Cache Friendly than fw, SBM lets mm bypass the caches. While mm is more Computation Intensive than fw, the selected architecture gives mm more computation resources. Note that the performance shown in Fig. 7 is the IPC running under the chosen architecture configuration of each approach. The IPC does not include the overhead needed to profile the task behavior. The overhead of profiling will be discussed in Section 6.5.

Fig. 8 compares the average normalized turnaround time (ANTT) [10] of different approaches. ANTT is a metric for the multi-program workloads to evaluate the performance for the tasks within the heterogeneous application. The ANTT of a particular task is the ratio of IPC_alone to IPC_happ. IPC_alone is the IPC when running a task alone and IPC_happ represents the IPC when running the tasks in heterogeneous application together. The overall ANTT is the summation of the individual ANTT of each task. Fig. 8 shows the ANTT for different architecture configurations normalized to the ANTT of the BASE policy (BN_ANTT). On average, the proposed SBM improves the ANTT by 27%, 36% and 21% over policies of BASE, MWG and EWM respectively. BASE and MWG allocate equal resources and privileges for all of the tasks in a heterogeneous application. This scheme usually hurts the performance of all the tasks in the heterogeneous application significantly when comparing to the scheme when running each task alone. EWM does allocate resources based on the profiling of each task. However, EWM profiles each task in a standalone manner and ignores the impact from the concurrently executed tasks, and indicates improper behavior of each task.


Download : Download high-res image (205KB)
Download : Download full-size image
Fig. 8. Base normalized ANTT (BN_ANTT) for different heterogeneous applications. Profiling overhead is not included to reflect the effectiveness of SBM on practical GPGPU applications.

6.3. Limitations of different management policies
SBM can properly capture the characteristics of various combinations of heterogeneous tasks using the light-weight profiling approach discussed in Section 3.2. SBM resource requirement aware approach enables it to accurately capture the actual behavior of a heterogeneous application and attain superior performance with the proper architecture configuration. The BASE approach tried to distribute workgroups onto two evenly separated groups of CUs. Although the approach is simple, it cannot distinguish the characteristics of different tasks within a heterogeneous application. This issue is aggravated when the tasks can share a Compute Unit within the GPGPU. Another situation that cannot be identified by BASE is when one of the tasks can bypass the caches without hurting its performance while at the same time enhancing the performance of the other task by giving more cache space. The above case can be observed in the fw_mm heterogeneous application. The task fw is Cache Friendly while mm is less Cache Friendly. Bypassing the cache accesses for mm provides more cache space for the Cache Friendly fw without hurting the performance of mm. Since both fw and mm are Computation Intensive, sharing the Compute Units between them provides better resource utilization. For fw_mm, the better resource utilization provided by SBM achieves 181% higher IPC and 57% shorter ANTT than BASE.

MWG lets all tasks share each Compute Unit and access all the caches in the hierarchy. MWG clarifies the need to alleviate the cache contention or the structural stall between the tasks in a heterogeneous application. When one of the tasks is Cache Unfriendly, it causes a significant cache contention. For example, the overall performance of dct_mm is considerably worse than other schemes mainly due to the contention caused by the cache accesses from mm. For dct_mm, the scheme chosen by SBM (i.e. MWGB2) alleviates the contention and therefore attains better performance, and attains 895% higher IPC than MWG. For dct_mm, SBM still preserves the turnaround time by having 94% shorter ANTT.

EWM [16] profiles each task separately at compile time. This approach can obtain most of the characteristics of a heterogeneous application when compared with BASE and MWG. However, EWM has shown some restrictions since it performs worse than MWG or BASE in some of the heterogeneous applications, such as fw_sf. This is because the separate profiling cannot capture the change of task behavior when concurrently executed with another task. EWM could mistakenly categorize two tasks that are equally Computation Intensive as one for Computation Intensive and the other for Computation Unfriendly. In this case, EWM gives more computation engines to one of the tasks than the other. The Computation Intensive task that does not have enough computation engines would result in poor performance. BASE and MWG, on the other hand, gives equal computation engines to both of the Computation Intensive tasks and have shown better performance than EWM. The similar issue could happen when EWM mistakenly categorize Computation Intensive and Computation Unfriendly tasks as both equally Computation Intensive. In this case, EWM will try to assign as many computation engines to the tasks as possible. However, the performance of the Computation Unfriendly task would be degraded when assigning more-than-needed computation-engines to it. SBM can avoid the above issues by capturing the correct run time characteristics, and determining an appropriate occupancy threshold (register occ_th in SBM_REG) for each of the tasks. SBM has attained 17.2% better performance and 9% shorter ANTT than EWM in fw_hist, when fw is Computation Unfriendly and hist is Computation Intensive.

6.4. Adaptability
To consider the adaptability and scalability of our approach under different microarchitectures, we analyze the impact of changing the number of Compute Units and cache sizes on the performance in Fig. 9, Fig. 10 respectively. The IPC is normalized to MWG IPC as in this microarchitecture the tasks always share L1 caches and the impact of cache contention is clear. We found a dramatic performance improvement when increasing the number of Compute Units as L2 cache contention increases and L1 cache contention happens in more private caches. Alternatively, decreasing L1 cache size increases the cache contention as both tasks share the same cache. Tasks that suffer from thrashing evict the data of the other Cache Friendly task. This issue is addressed by the proposed SBM scheme. SBM addresses the issue by either bypassing the cache accesses of the less Cache Friendly task as in MWGB policy or using different caches when both of the tasks are Cache Friendly as in BASE or B_L2 policies. Considerable performance enhancement is achieved when using SBM for systems with small caches and larger number of Compute Units because of the dynamic adaptation of SBM which tolerates the decrease in cache size.

6.5. Analysis of profiling overhead
To comprehend the properties of tasks in a heterogeneous application, existing approaches usually profile the heterogeneous applications before determining the proper execution and resource management schemes. BASE uses similar resource management scheme as in [24] and [13]. The authors in [13] used a fair rotation, such as round-robin, to serve memory requests from different tasks and require no profiling before executing a heterogeneous application. The work in [24] needed to profile the complete execution of a task. The profiling overhead mainly depends on the complete execution time of a task and could be as long as 10 billion GPGPU cycles. MWG, on the other hand, only profiled a task for the complete execution of a workgroup. This is similar to the scheme adopted in [15].


Download : Download high-res image (142KB)
Download : Download full-size image
Fig. 11. Performance results of different profiling periods. IPC is normalized to the optimal profiling IPC.

According to the above discussion, the existing approaches either profile the complete execution of a task or the execution of a workgroup. Profiling a complete task can surely capture the execution behavior of heterogeneous applications but would cause considerable profiling overhead. The overhead of profiling a workgroup is much less than profiling a complete task, but the length of a workgroup would also determine whether the execution behavior of a task can be properly characterized. If a workgroup is too short, the collected data during profiling are not representative to reflect the actual behavior of a task. On the other hand, a long workgroup could induce high profiling overhead. SBM takes the early phases of execution to profile the behavior of a heterogeneous application. When performing the computation, SBM collects and analyzes the behavior of a heterogeneous application when running under different architectural configurations. SBM then chooses the most suited configuration to meet the requirement of the application. The profiling results and architectural configuration can be used when the same tasks are running on a GPGPU. SBM should re-profile the performance only when the dispatched tasks are changed. In Fig. 11, we compare different profiling periods and show the average performance drop for short and long profiling periods. The loss in performance is 12% when using a short profiling period as 15 K cycles while it is only 1% for 40 K cycles periods. The profiling period of 60 K cycles does not cause any performance drop and gives the optimal profiling result. However, previous research has shown that a warm architecture state could improve the sampling efficiency [6]. In our work, we found that the result of subtracting the warming up profiling result at 15 k cycles from the 40 k cycles profiling result (15 k–40 K cycles) is similar to using a profiling period of 60 k cycles. In our case, both 60 k cycles and 15 k–40-k cycles profiling result in the optimal profiling result. As a result, SBM chooses 15 k–40 k cycles as a profiling period to properly capture the functional behavior of a heterogeneous application. The profiling period of 15 K to 40 K cycles is a user defined parameter which assumes the execution behavior of heterogeneous applications is relatively stable during the overall execution period. However, this setting (15 K-40 K) is able to capture the characteristics of different tasks obtained from different benchmark suites, and has been proved in the extensive experiments using tasks from both AMD SDK [3] and Rodinia [8], [9]. Since the average execution period for a workgroup is 18.5 K cycles, profiling only a workgroup is not sufficient to characterize a task and would likely return an inappropriate decision. Note that the system is still performing effectual computation on the application when SBM is profiling the task behavior. The longer the execution time, the minor the effect of the profiling period, while the more significantly SBM can contribute to the overall performance. SBM requires a maximum of total nine profiling periods to characterize a heterogeneous application. Each profiling period accounts for a minor (less than 4% in average) part of the benchmark execution time. Although the overhead of SBM profiling phases accounts for an average of 35.3% of the benchmark execution time, in practical GPGPU applications, the profiling overhead is about 1%. These benchmarks are used for performance evaluation and are much shorter than the runtime of practical GPGPU applications. For example, the execution time of a practical GPGPU application in [11] is about 34 s which accounts for 34 million GPGPU cycles based on the GPGPU specification. In this case, all of our nine profiling periods take only 1% from the overall execution time. This is a worthwhile investment since a properly chosen architecture configuration could boost the overall performance significantly. The architecture configurations tuned for the heterogeneous application can be beneficial for the rest of the main execution.

7. Conclusion
Modern GPGPU supports heterogeneous applications that contain tasks with different run time characteristics and resource utilization. The heterogeneous behavior requires proper resource management policy to attain efficient execution. This paper proposes heterogeneity aware Selective Bypassing and Mapping (SBM) to manage both computing and cache resources for heterogeneous applications in a fine-grain manner. The run time profiling scheme of SBM can properly characterize the disparate behavior of the concurrently executed tasks in a heterogeneous application, and selectively apply suited cache management and workgroup mapping policies to each task. When compared with the coarse-grained policies of BASE and MWG, the proposed SBM can achieve an average of 57% and 138% performance enhancement respectively. When compared with the fine-grained policy EWM, the proposed SBM can achieve an average of 58% and up to 378% performance enhancement. Although GPGPU simulators have been widely used to study the performance impact of microarchitectural changes, our future work will continue to validate the detail impact on the PPA (Power, Performance, and Area) of our approach with an RTL implementation.

