Abstract
Load balancing, in Cloud Computing (CC) environment, is defined as the method of splitting workloads and computing properties. It enables the enterprises to manage workload demands or application demands by distributing the resources among computers, networks or servers. In this research article, a new load balancing algorithm is proposed as a hybrid of firefly and Improved Multi-Objective Particle Swarm Optimization (IMPSO) technique, abbreviated as FIMPSO. This technique deploys Firefly (FF) algorithm to minimize the search space where as the IMPSO technique is implemented to identify the enhanced response. The IMPSO algorithm works by selecting the global best (gbest) particle with a small distance of point to a line. With the application of minimum distance from a point to a line, the gbest particle candidates could be elected. The proposed FIMPSO algorithm achieved effective average load for making and enhanced the essential measures like proper resource usage and response time of the tasks. The simulation outcome showed that the proposed FIMPSO model exhibited an effective performance when compared with other methods. From the simulation outcome, it is understood that the FIMPSO algorithm yielded an effective result with the least average response time of 13.58ms, maximum CPU utilization of 98%, memory utilization of 93%, reliability of 67% and throughput of 72% along with a make span of 148, which was superior to all the other compared methods.

Previous
Next 
Keywords
Cloud computing

Firefly

Load balancing

Task scheduling

IMPSO

1. Introduction
Cloud Computing (CC) is a rapidly developing concept in the domain of distributed computing which could be applied in diverse fields such as data storage, data analysis and IoT applications [10]. CC is an advanced technology that can change the way how traditional businesses work. It offers various facilities to registered clients in the form of online services which like to avoid user investment in computing architecture. Some of the services by CC are Infrastructure-as-a-Service (IaaS), Software-as-a-Service (SaaS) and Platform-as-a-Service (PaaS) [17]. For every service, a user should request the Cloud Service Provider (CSP) via internet. The CSP has to manage the resources in order to satisfy the requests received from clients. SP leverages the scheduling technique to schedule the input requests as well as control the processing resource in an effective manner. Task scheduling and resource management provide high economic rate and resource application until the specified limits. The main barriers in the utilization of CC operations are allocating and scheduling the resources. In order to overcome these complexities, many researchers showed their interest in task scheduling process in CC. The main work of task scheduling is to organize the input request in a definite way so that every resource is utilized in an efficient manner. Each service is provided to numerous clients and several tasks might be executed at the same time. If the system does not apply scheduling method, then it results in longer waiting time for the process to be performed. Unfortunately, few requests get terminated due to long waiting time that exceeded the maximum limit. While scheduling is carried out, the concern scheduler is required to monitor some of the limitations such as behavior of the task, size of the request, execution time of the task, available resource and the load induced on the resources.

Task scheduling is the main problem involved in CC. The merit involved in CC is its efficient usage of all the resources which can occur only when the task scheduling is carried out properly [2]. Hence, both task scheduling and resource allocation have been considered as mandatory operations for all processes to be performed. At present, people using internet could gather information from any location at any time without any idea about the host structure. These kinds of hosting infrastructures include different systems and abilities which are controlled by CSP. CC improves the potentials of host infrastructure that can make use of internet services. CSP gains efficiency through facilitating the clients by making use of Cloud Services (CS). Clients apply the CS to employ the whole processing services from software to hardware. The services offered in CC follow the pay-as-you-go model. The utilization of resources may increase or decrease depending on the CSP users and their demand for applications. It is considered as a merit of CC though it comes with additional cost. The CS user can choose different services based on the application required. However, this freedom of optional service may lead to challenging issues that needs to be detected properly. CC research has two main parts namely task scheduling and resource allocation. The efficient usage of the resource is based on scheduling and load balancing techniques to avoid the arbitrary allocation of sources. CC aims at resolving difficult operations using scheduling techniques.

Load balancing and task scheduling are important players in CC environment; thus, the current research work proposes out a novel technique in order to obtain better results with the help of hybridizing firefly and Improved Multi-Objective Particle Swarm Optimization (IMPSO) technique, abbreviated as, FIMPSO. It applies Firefly (FF) algorithm to minimize the search space whereas the IMPSO technique is implemented to identify the enhanced response. IMPSO works by selecting the global best (gbest) particle with a small distance of point to the line. With the application of minimum distance from a point to line, the gbest particle candidates could be elected. The simulation outcome from the proposed model was found to be too optimized when compared with alternate models [1], [3], [12], [16], [24].

The upcoming sections are planned as follows. Relevant studies are discussed in Section 2. The proposed FIMPSO algorithm is described in Section 3 and validated in Section 4. At last, the conclusions are drawn in Section 5.

2. Related works
Cloud Computing (CC) is described as a virtual distributed computing that shares the maximum resources among its customers across a large area with the help of internet. The resources could be requested and applied by many cloudlets simultaneously. Clients can access the centralized resource at anytime, any where over the internet. In the literature [5], a CloudSim simulator was proposed to simulate the cloudlets on virtual system. The load balancers allocate the cloudlets’ functions on data centers, while the whole workload and few classes present in scheduling are explained. Buyya et al. [26] followed a principle to schedule the works from one particle to another as mentioned. This scheduling process depends upon the energy of a node in managing the workload. If the load is passed on to the Virtual Machines (VM), it results in underbalanced nodes. Khiyaita et al. [15] described various models, their importance and the algorithms of load balancing to resolve the issues because of the imbalanced node.

Load scheduling is the process of allocating and operating cloudlets on VM in an optimal way to decrease the computational cost. It is applied to decrease the transfer time, time taken for waiting, response time, execution time in addition to the operational cost. Chaudhary and Kumar [6] opined that scheduling might be dynamic or static while allocating the load as depicted in the table analysis. Dynamic techniques apply bio-inspired algorithms for scheduling the workloads. The swarm-based algorithms are employed to allocate load in CC in order to fix the consecutive objects based on the velocity and location of particles. There are merits and demerits associated with different load balancing techniques. The scheduling model is enhanced with round robin algorithm. This task scheduling process works on the basis of meta-heuristic models. The roulette selection model, based on random scheduling, is determined after several iterations and fitness values. The static algorithm does not come under the category of meta-heuristic technique to manage the load in cloud. This technique incurs low operational cost when compared to primitive models such as First Come First Serve (FCFS) and Round Robin scheduling. Kennedy and Eberhart [13] presented a meta-heuristic approach called Particle Swarm Optimization (PSO). This work depends on a set of particle methods that follow a flock of birds to transform from one source to another. The optimal position in search space has relied on the velocity of the previous particle. In 11 scheduling techniques, the static and alternate models of GA, SA, Min–Min, Max–Min, Tabu Search and so on are described. Pacini et al. [18] discussed the operation of swarm optimization that resulted in improved solutions.

In the literature [27], a constraint-based PSO scheme was used to allocate the tasks in sequential nodes. Under the application of CC platform, the PSO algorithm was described in the literature [19]. The appropriate particle is evaluated through the fitness function. Then, the velocity of a particle is obtained by the particle’s best position (pbest) in process and global best position (gbest) in a swarm. When compared to existing works and relevant algorithms, it reduces the total cost of computation. It concentrates on the concurrence of Ant Colony Optimization (ACO) technique. As the name itself describes, it discusses about the role of an ant while it finds its food. It offers the application of a PSO algorithm to distribute the resources on VM in the cloud.

Garg and Buyya [8] signified the Network Simulator to schedule each load from the system. The study deployed PSO for scheduling cloudlets based on VMs in CloudSim, which depends upon a group of particles in search space. A new searching mechanism was introduced based on Newton’s law of gravity named ‘Gravitational Search Algorithm’ (GSA). It did not consider the operation of storing optimal positions for future applications. Here, the fitness value can be used to compute the size of the particle. The location of the secondary particle is nothing but a sum of velocity and position of the existing particle. GSA is employed in filter modeling functions. The force could be measured using gravitational constant which is a significant one. The application of fuzzy segmentation, based on gravitational searching model, was done in the literature [11] from a collection of satellite images in order to identify the information.

In the previous study [20], noise filtering ultrasound images were used to assess the GSA model. The novel element through Binary Gravitational Search Algorithm (BGSA) was presented to optimize the scheduling operation which is produced from various platforms. A hybrid GSA was introduced in the literature [14] using orthogonal crossover as well as pattern searching to schedule the load in CC environment. Two efficient GSA optimization schemes were developed in the previous study [23] to improve the diversity of particles and utilize the memory models in mathematical calculations. It established security measures on the basis of behavioral graphs and implied the concentration on load balance as well as service allotment in the CC platform. The GSA depends upon repulsive and attractive forces to resolve the optimization issue.

There are recent works conducted in this research arena [4], [21], [22], [25] in which the study [4] proposed PROUD, a new approach to secure the outsource data designcryption procedure for edge servers to lessen the computational overhead on the client side. Besides, a new CLoud scientific wOrkflowSchedUlingalgoRithm based on attack–defensE game model (CLOSURE) [25] was also proposed in the literature. Table 1 summarizes the reviewed works in terms of its objective and underlying algorithms. Though there are several works available in the literature, some of the methods do not consider the characteristics of CC environment yet it needs to improve the performance in several aspects.


Table 1. Summary of related works.

Reference	Objective	Algorithm used
[6]	Deployed a scheduling algorithm for load allocation in a dynamic or static way	Swarm intelligence techniques, Roulette selection
[13]	Developed a scheduling technique in CC environment	PSO algorithm
[18]	Analysis of different algorithms	Swarm intelligence techniques
[27]	To allocate the tasks in sequential nodes	Constraint-based PSO scheme
[8]	For scheduling cloudlets based on VMs in CloudSim	PSO algorithm
[14]	Developed a scheduling technique in CC environment	Hybrid GSA using orthogonal crossover
3. Proposed system
The proposed FIMPSO algorithm involves the hybridization of FF and IMPSO algorithms. The presented FIMPSO algorithm reaches the effective average load for making and enhances the essential measures such as proper resource usage and response time of the tasks. In the following subsections, the processes involved in the proposed algorithm are discussed in detail.

3.1. FF algorithm
The proposed model is established as a Hybrid of FF and IMPSO which is explained briefly. Generally, the FF technique is classified into three premises [7]:

1.
Every FF belongs to the same gender. Only the opposite sex attracts each other.

2.
In fireflies, the attraction is corresponding to brightness i.e., a lower brightened FF would be consumed by a high intensified FF. Hence the attraction and brightness of a FF are reduced when the distance gets increased between the fireflies. If a FF is not brighter when compared to a specific fly, then it moves randomly in such a way that no flies are attracted.

3.
An objective function is used to compute the brightness of fireflies.

The process involved in FF algorithm is illustrated in Fig. 1. The fundamental concepts involved in FF algorithm are light intensity and attraction.

Fireflies’ attraction is measured by defining its intensity, whereas the brightness is acquired from an objective function while optimizing an issue. Light intensity and attraction are estimated through the equations where 
 and 
 denote brightness and attraction respectively. (1)
(2)

The distance among the fireflies  in the place 
 and  in position 
 are denoted by Euclidean distance formula, where 
 indicates the FF position  in spatial dimension : (3)
The moving FF  against the brighter FF  is implied by the following equation: (4)
FF could move using three levels such as:


Download : Download high-res image (282KB)
Download : Download full-size image
Fig. 1. Flowchart of FF algorithm.

•
The present state of FF,

•
Fixed absorption value with the application of high intensified FF,

•
The random movement is defined by  and identical distribution is in the interval .

3.2. IMPSO algorithm
The flowchart of the proposed technique for IMPSO algorithm is depicted in Fig. 2. The steps involved in IMPSO algorithm are given below.

(1)
Initialization of PSO;

(a)
Fix a swarm set number  as well as particle dimension ;

(b)
Set a computation range for variable  values i.e., 
 and 
;

(c)
Particle speed control, variable 

(d)
Position of swarm and initialization of speed, random particle formation, , , , 2, .

(2)
Parameter evolution;

(a)
 Iterations 
;

(b)
Higher and lower inertia weight, value as 
;

(c)
Training factors 
 and 
.

(3)
Evaluation of objective function

For  to  ( is the population size)

For  to  ( is objective function numbers)

(a)
Compute 

(b)
Particle generalization

(5)
 
(4)
Particle position initialization:  and 
 Best particle found in 

(5)
Archive initialization Save the non-dominated solutions which are identified in  into archive

(6)
If iterations does not accomplish 

(b)
Explore 
 (l) from archive Line 
 solver for non-dominated particle  from archive. Distance 
 from  to line 
. If 
, particle  in archive would be declared as the GBG, 
.

(c)
Updating the position as well as particle speed 
(6)
 where  denotes the number of iterations whereas 
 and 
 represent the random values in the range of . If  expands over the boundaries, then it is combined by fixing the decision parameter which is similar to the value of corresponding lower or upper boundary while the velocity is increased by . Hence it finds the opposite direction.

(d)
Perform mutation on .

(e)
Calculate 

(f)
Update the archive If the particles are not dominated by recorded solutions, then it introduces new non-dominated solutions in  within the archive. Every solution present in the archive is dominated by a novel solution. Once the archive is completed, a replaced solution must be computed on the basis of CD value. The personal best solution for all the particles present in  should be upgraded. If the present 
 dominates the location in memory, then the position of the particle is updated with the function of 
.

(g)
Improved iteration value 

(7)
Cycle value gets improved until the iteration requirements are attained.

3.3. Global best guide (GBG)
As declared earlier, some essential MOPSO techniques exist. In every technique, there is a suggestion to discover the GBG. In this division, several techniques, benefits, and drawbacks are considered after which a novel technique is begun to discover the GBG.

3.3.1. Multi-objective PSO
Here, the objective spaces are separated into hyper-cubes prior to the selection of GBG to every particle. Then, a fitness value is allotted for every hypercube based upon the number of elite particles inside it. If the elite particles are hypercube, then the fitness value is lesser comparatively. Next, the selected roulette-wheel is executed for the hypercube while one is chosen. At last, GBG is an arbitrary particle selected from the chosen hypercube. Consequently, GBG is chosen by making use of the roulette-wheel chosen technique arbitrarily. Probably, a particle does not choose a proper guide as its global guide.

3.3.2. Multi-objective Optimization using dynamic neighborhood PSO
This method applies a dynamic neighborhood strategy. This research article reveals the concept of two-objective optimizations; the GBG of a particle is identified in objective space. Initially, the distance from particle , as well as other alternate particles, are evaluated with respective values, which is termed as fixed objective, where the ‘k’ local neighbors are found on the basis of distance of calculation. Then the local optima between other neighbors are measured using second objective value and is named as GBG ??
 for particle . Therefore, it consists of fixed objective selection which is conducted utilizing the prior knowledge of objective functions. While the 1-D optimization technique is helpful in handling multi-objective functions. Hence, the selection of GBG is based on single objective function.

3.3.3. Crowding Distance in Multi-objective PSO (CD-MOPSO)
Crowding Distance value offers an estimation about the solution’s density which is surrounded by it. Initially, an enveloped archive records the independent solutions that are identified in prior iterations. The non-dominated solutions are employed as GBG of the particles in swarm. Then, the solutions present in archives are filtered by reducing the CD value. Consequently, the GBG of a particle is chosen among non-dominated solutions which is composed of maximum CD values. Different guides are selected for all the particles from particular portion of a dataset. Thus, the GBG could be selected with the assistance of CD value; however, it is a random selection process [9]. In order to resolve the demerits involved in MDPL-MPOSO, FIMPSO is proposed to identify the global best guidance for every particle. The fundamental model named Minimum Distance of Point to Line (MDPL) is established. Afterwards, it computes the global best guide for all particles in population. In 2D system, a straight line  can be identified with the help of origin point  and anypoint . Line  is described below: (7)
 
 
Point 
 is external line , distance  between the points  and line  is explained as follows (8)
 
Likewise, the 3-D coordinate system has a straight line which is generated by origin point  and any point  as described by (9)
 
 
 
Point 
 is an exterior line , distance  between the points  and line  is mentioned by: (10)
 
 
 
 
 
 By utilizing the basic model of a distance of point to line with regard to the objective space, the identification of GBG 
 between the collection members to particle  of 2 objective optimized populations is as follows.

First, a line 
 is drawn of the point  with coordinates 
 in 2-objective spaces.  is the connected non-dominated particle in store. 
 is described as pursues: (11)
 
 
In the 2nd, the distance 
 is computed from point  by co-ordinates 
 in the objective space store to the population particle  to line 
. 
 is described as pursues: (12)
 
At last, the store particle  is regarded as 
 with respect to optimized output as distance 
 from  to stores line 
 is the smallest. 
 is described as follows (13)
In every particle through the smallest distance to the line of the records, the member should choose the part which records the member as the GBG. Consequently, MDPL-MOPSO could establish a generally-suitable guide as its global guide to every particle in the population.

3.4. Hybridization of FIMPSO algorithm
The proposed model is a combination of FF and IMPSO techniques. Both response speed and accuracy can be improved in this new approach and is deemed to be very effective. The feature of the FF technique is composed of maximum convergence; however it gets decreased in the exploration process. Later, the IMPSO technique is incorporated with improved sensitivity in the beginning phase. Therefore, the random behavior of particles in IMPSO results in the sensitivity of primary population. If the initial population is not selected properly, then the algorithm could not attain an optimal solution. To obtain good result in this case, the presented model should initialize the optimized population which is achieved by the application of FF algorithm.The evaluation function is described as follows: (14)
 
 
Where:

•
: Minimum run time,

•
: Maximum run time,

•
: Minimum input time,

•
: Maximum input time.

At this point, every implemented procedure is estimated to choose an optimal execution order. In order to schedule various operations, FIMPSO technique has obtained two different attributes as input i.e., Arrival time and run time. The estimated measure is explained by: (15)
 
 

4. Performance validation
The proposed FIMPSO algorithm was simulated using MATLAB tool. Every task had its login time and run time, which under went arbitrary initialization and was offered using Job shop software. For comparison purposes, a set of algorithms namely Round Robin (RR), First Come First Service (FCFS), Short jobs First (SJF) and Genetic Algorithm (GA) were employed. Moreover, the set of measures used to investigate the results were execution time, resource utilization, reliability, make span and throughput.

4.1. Implementation setup
Table 2 shows the size of various synthetic datasets along with the number of tasks linked to it. The table shows that the extra large task consists of 800–1000 tasks and the size of the tasks is in the range of 100000–200000MI. Similarly, the large task consists of 600–700 tasks whereas the size of the tasks is in the range of 70000–100000MI. Likewise, the medium-sized task consists of 400–500 tasks with task sizes ranging between 50000–70000MI. In the same way, the small-sized task consists of 100–200 tasks with 30000-50000MI sized tasks. [7]. In addition, the task sizes were created in a random fashion during run time and the size is defined by Millions of Instructions (MI). Besides, the research utilized 80 servers with a variety of resource capacities and loads. Every server hosted different kinds of edVM instances with various CPU and memory capacities as shown in Table 3.


Table 2. Size of synthetic datasets.

Type of tasks	Number of tasks	Size of tasks (MI)
Extra-large	800–1000	100000–200000
Large	600–700	70000–100000
Medium	400–500	50000–70000
Small	100–200	30000–50000

Table 3. Type of VM instances [7].

Name of VM instances	CPU capacity (MIPS)	Memory capacity (GB)
Extra-large	35000	20
Large	25000	15
Medium	20000	10
Small	10000	5
4.2. Analysis of the results in terms of execution time
Table 4 provides a detailed comparison of different scheduling methods with FIMPSO algorithm in terms of average load, average turnaround time and average response time. The table values indicate that the FIMPSO attained better results over other scheduling algorithms compared, in a considerable way. When measuring the results in terms of average turn around time, it can be noted that the maximum average turn around time required by IPSO and FF techniques were of 57.74 ms and 55.54 ms respectively. A slightly low average turnaround time was required by RR, FCFS and SJF methods with average turnaround time being 41.98 ms, 41.87 ms and 41.56 ms respectively. After that, GA attained manageable results with a moderate average turnaround time of 26.57 ms. At the same time, the IPSO-FF algorithm exhibited a competitive average turnaround time of 22.13 ms. However, the presented FIMPSO algorithm yielded an effective outcome with the least average turnaround time of 21.09 ms. Similarly, when measuring the results in terms of average response time, it can be noted that the maximum average response time required by IPSO and FF techniques were of 49.23 ms and 48.87 ms respectively. A slightly lower average response time was required by RR, FCFS and SJF methods with the average response time being 30.50 ms, 30.84 ms and 30.24 ms respectively. Next to this, GA attained manageable results with a moderate average response time of 20.30 ms. Simultaneously, the IPSO-FF algorithm exhibited a competitive average response time of 15.21 ms. However, the proposed FIMPSO algorithm yielded an effective outcome with the least average response time of 13.58 ms.


Table 4. Comparison between the proposed method and other scheduling methods.

Methods	Average load (ms)	Average turnaround time (ms)	Average response time (ms)
RR	0.430	41.98	30.50
FCFS	0.460	41.87	30.84
SJF	0.495	41.56	30.24
GA	0.310	26.57	20.30
IPSO	0.457	57.74	49.23
Firefly	0.470	55.54	48.87
FF-IPSO	0.259	22.13	15.21
FIMPSO	0.247	21.09	13.58
4.3. Analysis of results in terms of CPU utilization
Fig. 3 shows the results attained by the presented model in terms of CPU utilization. The figure indicates that the FIMPSO achieved the maximum CPU utilization under all types of tasks in comparison with other methods. Under small types of tasks, both RD and WRR models showed poor CPU utilization by achieving minimum utilization of 45% and 49%, respectively. At the same time, both DLB and LB-BC methods tried to manage well by attaining a slight increase in CPU utilization i.e., 52% and 57%. Next to that, both LB-RC and FF-IPSO algorithms offered closer results to FIMPSO by attaining 67% and 69% CPU utilization respectively. But, the presented model exhibited superior performance by attaining the maximum CPU utilization of 71%.

On the other hand, under extra large types of tasks, both RD and WRR models showed only poor CPU utilization with minimum utilization of 75% and 79% respectively. At the same time, both DLB and LB-BC methods tried to manage well by attaining a mild increase in the CPU utilization i.e., 84% and 89% respectively. Next to that, both LB-RC and FF-IPSO algorithms offered closer results to FIMPSO by attaining 94% and 96% CPU utilization respectively. But, the presented model yielded superior performance by attaining the maximum CPU utilization of 98% under extra large tasks. Therefore, it can be inferred that the presented FIMPSO algorithm is highly effective in terms of CPU utilization under all types of tasks, irrespective of the task sizes. The figure also states that the increase in number of tasks results in increase in CPU utilization.


Download : Download high-res image (227KB)
Download : Download full-size image
Fig. 3. Comparative results analysis in terms of CPU utilization.

4.4. Analysis of results in terms of memory utilization
A detailed analysis conducted between FIMPSO and existing methods in terms of memory utilization is shown in Fig. 4. From the figure, it is understood that the FIMPSO achieved the maximum memory utilization under all types of tasks in comparison with other methods. Under small types of tasks, both RD and WRR models showed poor memory utilization by achieving only the minimum memory utilization of 40% and 44% respectively. At the same time, both DLB and LB-BC methods tried to manage well by attaining a slight increase in the memory utilization up to 49% and 53% respectively. Next to that, both LB-RC and FF-IPSO algorithms achieved closer results to FIMPSO by attaining 57% and 58% memory utilization respectively. Nevertheless, the presented model exhibited superior performance by attaining the maximum memory utilization of 60%. On the other hand, under extra large types of tasks, both RD and WRR models showed poor memory utilization by achieving only a minimum memory utilization of 72% and 76% respectively.

At the same time, both DLB and LB-BC methods tried to manage well by attaining a slight increase in the memory utilization up to 81% and 84%. Next to that, both LB-RC and FF-IPSO algorithms achieved closer results to FIMPSO by attaining 89% and 91% memory utilization respectively. But, the presented model yielded superior performance by attaining the maximum memory utilization of 93% under extra large tasks. Therefore, it can be inferred that the presented FIMPSO algorithm is highly effective in terms of memory utilization under all the types of tasks, irrespective of its size. The figure also states that the increase in number of tasks results in the increase of memory utilization.


Download : Download high-res image (224KB)
Download : Download full-size image
Fig. 4. Comparative results analysis in terms of memory utilization.

4.5. Analysis of results in terms of reliability
The reliability analysis of various methods was performed between FIMPSO and existing methods as shown in Fig. 5. Under small types of tasks, both RD and WRR models showed poor reliability by achieving only the minimum reliability of 76% and 81% respectively. At the same time, both DLB and LB-BC methods tried to manage well by attaining a slight increase in the reliability up to 87% and 92%. Next to that, the LB-RC algorithm yielded closer results to FF-IPSO and FIMPSO by attaining the reliability of 92%.

However, the presented and LB-RC models yielded superior performance by attaining the maximum reliability of 100%. On the other hand, under extra large types of tasks, both RD and WRR models showed poor reliability by achieving 35% and 39% minimum reliability only. At the same time, both DLB and LB-BC methods tried to manage well by attaining a slight increase in the reliability up to 45% and 54%. Next to that, both LB-RC and FF-IPSO algorithms achieved closer results to FIMPSO by attaining each 63% identical reliability. However, the presented model exhibited the superior performance by attaining the maximum reliability of 67% under extra large tasks. Therefore, it can be inferred that the presented FIMPSO algorithm is highly effective in reliability under all the types of tasks irrespective of the task sizes. It is also inferred that the reliability gets decreased when the number of tasks increases.


Download : Download high-res image (221KB)
Download : Download full-size image
Fig. 5. Comparative results analysis in terms of reliability.

4.6. Analysis of results in terms of make span
Fig. 6 provides a detailed comparison of different scheduling methods with FIMPSO algorithm in terms of make span. The figure indicates that the FIMPSO yielded better results over the compared scheduling algorithms in a considerable way.

When measuring the results in terms of extra-large tasks and make span, it is noted that the maximum make span of 280 and 280 were required by RD and WRR techniques respectively. Next, slightly lower make span was required by DLB, LB-BC and LB-RC methods with the make span of 273, 261 and 153 respectively. At the same time, the IPSO-FF algorithm required a competitive make span of 150. However, the presented FIMPSO algorithm was effective with the least make span requirement of 148. It is to be noted that the make span gets gradually increased with the increasing number of tasks.


Download : Download high-res image (189KB)
Download : Download full-size image
Fig. 6. Comparative results analysis in terms of make span.

4.7. Analysis of results in terms of average throughput
Finally, an extensive average throughput analysis of various methods was performed between FIMPSO and existing methods as shown in the Fig. 7. Under small types of tasks, both RD and WRR models showed poor average throughput by achieving only a minimum average throughput of 65% and 72% respectively. At the same time, both DLB and LB-BC methods tried to manage well by attaining a slight increase in the average throughput of 81% and 90%.

Next to that, both LB-RC and FF-IPSO algorithms yielded closer results to FIMPSO by attaining the identical average throughput of 96% and 98% respectively. But, the presented model exhibited superior performance by attaining the maximum average throughput of 100% under extra large tasks. On the other hand, under extra large types of tasks, both RD and WRR models showed poor average throughput by achieving only a minimum average throughput of 30% and 36% respectively. At the same time, both DLB and LB-BC methods tried to manage well by attaining a slight increase in the average throughputs of 44% and 53%. Next to that, both LB-RC and FF-IPSO algorithms achieved closer results to FIMPSO by attaining the identical average throughput of 65% and 68% respectively. But, the presented model excelled through its superior performance by attaining the maximum average throughput of 72% under extra large tasks.


Download : Download high-res image (219KB)
Download : Download full-size image
Fig. 7. Comparative results analysis in terms of average throughput.

4.8. Discussion
The results achieved by the proposed model under several aspects are listed below.

•
FIMPSO algorithm yielded an effective outcome with the least average response time of 13.58 ms.

•
The proposed model attained the maximum CPU utilization of 98% under extra large tasks.

•
The presented model reached the maximum memory utilization of 93% under extra large tasks.

•
Besides, the maximum reliability of 67% under extra large tasks was offered by the proposed method along with a make span of 148. It also attained the maximum average throughput of 72% under extra large tasks.

Therefore, it can be inferred that the presented FIMPSO algorithm has an effective average throughput under all types of tasks, irrespective of the task sizes. It can also be inferred that the average throughput gets decreased when the number of tasks increase.

5. Conclusion
This paper has presented an energy efficient load balancing algorithm in cloud environment using FIMPSO algorithm. The presented algorithm incorporates the benefits of both FF and IMPSO algorithms. The presented FIMPSO algorithm achieved an effective average load for making and enhanced the essential measures like proper resource usage and response time of the tasks. For experimentation, the set of measures used to investigate the results were execution time, resource utilization, reliability, make span and throughput. The simulation outcome showed that the proposed FIMPSO model excelled in its performance over the compared methods. From the simulation outcome, it is understood that the FIMPSO algorithm achieved effective results with the least average response time of 13.58 ms, maximum CPU utilization of 98%, memory utilization of 93%, reliability of 67% and throughput of 72% along with a make span of 148, which was superior to all other compared methods. The future scope is inclusive of improvements in the presented FIMPSO algorithm to use the data deduplication algorithms.