Abstract
Network congestion is one of the critical reasons for degradation of data throughput performance in Networks-on-Chip (NoCs), with delays caused by data-buffer queuing in routers. Local buffer or router congestion impacts on network performance as it gradually spreads to neighbouring routers and beyond. In this paper, we propose a novel approach to NoC traffic prediction using Spiking Neural Networks (SNNs) and focus on predicting local router congestion so as to minimize its impact on the overall NoCs throughput. The key novelty is utilizing SNNs to recognize temporal patterns from NoC router buffers and predicting traffic hotspots. We investigate two neural models, Leaky Integrate and Fire (LIF) and Spike Response Model (SRM) to check performance in terms of prediction coverage. Results on prediction accuracy and precision are reported using a synthetic and real-time multimedia applications with simulation results of the LIF based predictor providing an average accuracy of 88.28%–96.25% and precision of 82.09%–96.73% as compared to 85.25%–95.69% accuracy and 73% and 98.48% precision performance of SRM based model when looking at congestion formations 30 clock cycles in advance of the actual hotspot occurrence.

Previous
Next 
Keywords
Networks-on-Chip

Congestion prediction

Network traffic

Spiking Neural Networks

1. Introduction
The downsizing of the transistor has enabled many processing cores to be integrated on a single chip to form a scalable and parallel multi-processor System-on-Chip [4]. These devices are highly dependent on resource utilization and system reliability as they exploit parallel computation for performance gains and require scalable, high interconnect bandwidth between processor cores [33]. In particular, for multi-processor systems the interconnect challenges includes communication topology, routing schemes, arbitration, switching, and flow control [51]. Networks-on-Chip (NoC) has been proposed [8] to address these communication challenges where information is communicated as small packets of data. The NoC provides high bandwidth and concurrent communication channels for transferring of data between multiple cores [42]. NoCs face congestion problems due to uneven workload distribution across many routers [51] leading to hotspots of congestion. The Quality of Service (QoS) of a NoC is a measurement of run time performance. One promising solution to maximize the QoS is adaptive routing algorithms which aim to balance the traffic congestion across many routers in the NoC [37].

To date neural networks have shown success in the domains of pattern recognition and prediction [31] and in particular, Spiking Neural Networks (SNNs) can now be implemented in hardware [34], [35] with low area overheads. Therefore, neural networks can be explored as a mechanism for prediction of NoC traffic congestions, i.e., identify hotspots before they occur. In this paper, the temporal nature of SNNs is harnessed for hotspot prediction in NoCs. The SNN training algorithm of SpikeProp with Leaky Integrate and Fire (LIF) and also Spike Response Model (SRM) neuron models are explored in the prediction of router congestion. The ultimate goal of the proposed work is to use a hardware scalable SNNs to predict network congestion and support a congestion handling scheme to minimize router traffic unbalance.

Major contributions of this paper include:

•
Novel use of SNNs in the prediction of congestion in NoC architectures.

•
Simulation and evaluation of two SNN models to evaluate prediction coverage on NoC architecture.

•
Validation of results and performance of SNN models under different traffic applications generated by Noxim simulator.

Section 2 provides background on SNN networks and existing approaches. Section 3 reports on the SNN-based NoC hotspot prediction methodology, and the experimental setup is outlined in Section 4. Section 5 presents simulation results and Section 6 provides a conclusion and outlines future work.

2. Preliminaries
This section gives a brief introduction to the cause and effects of congestion on NoC architectures and also presents an overview of existing research on congestion prediction.

2.1. Congestion and effects on NoC performance
In NoC architectures, routing switches and network channels are susceptible to congestion due to the unbalanced load distribution caused by executing applications [37]. Different routing algorithms i.e., XY, odd–even etc. are employed to overcome traffic unbalancing problem in on-chip interconnects. Goal of routing algorithms is to distribute load across all available routing paths. Routing algorithms can be classified as Deterministic and Adaptive where Deterministic algorithms follow a fixed path from source to destination node. Adaptive algorithms can change routing path depending on current network conditions. These algorithms mostly use buffer information to identify on-path congestion where routers have limited numbers of available buffers to compensate unbalanced traffic flows. One promising solution is to add additional buffers on input and output ports of routers. These buffers can avoid the backpressure effect by queuing more input packets and stopping the spread of congestion towards neighbouring nodes however queuing will cost further delays in packet transmission that will also affect overall system performance (latency and throughput). The congestion problem will still persist if these buffers are filled [27], [43]”.

Consider that a 4-input buffer NoC router (router-X) encounters with burst traffic. The neighbouring/on-path routers are equipped with an adaptive routing algorithm which can react to congested nodes and re-route data through alternative path. Hence the data packet is routed towards the destination node thus keeping the flow away from the congested node. The data packets queued at input buffers add to the network delay as these packets are soaked out of network thus affecting network throughput.

Now, consider a router-X with 6-input buffers to provide more space to incoming data packets. The additional buffers will compensate the formation and spread of congestion towards neighbouring node. As the adaptive routing algorithm will have available spaces in router-X therefore data packets will be forwarded towards router-X. As the router is encountered with burst traffic it may soon run-out of buffers. The data packets queued in the additional buffers have to wait more than usual before getting forwarded by the router-X. These additional packet delays will add further to the network delay. As more packets are soaked out of the network this will further affect network throughout.

Path congestion occurs in three phases [16] including (1) Switch contention inside router. If a number of packets from different input channels compete for the same output channel, switch contention will occur (shown in Fig. 1(a)); (2) Router congestion. Several inputs from multiple channels are competing to connect with same output channel, only one channel will get access to output channel and rest of packets are queued at input buffers of their respective channels; and (3) Channel congestion. As input buffers have limited data storage capacity and may run out of space on arriving of new data. Queue of packets causes back pressure to neighbour routers and data starts queuing in buffers of neighbour nodes. This back pressure untimely leading to cluster or network congestion as shown in Fig. 1(b).


Download : Download high-res image (274KB)
Download : Download full-size image
Fig. 1. (a). Illustration of switch contention (dashed lines show multiple switch request to the North output). (b). Effect of congestion and Backpressure.

Network topology, arbitration, switching and data flow mechanisms must be considered to avoid NoC congestion [51]. Buffer, links and status register are on-chip resources of NoC architecture and flow control mechanisms use these resources to minimize transmission delay [11]. Nodes with higher routing loads are more susceptible to congestion [27], [51]. Mostly, NoC architecture is​ employed with Warm-hole Flow Control (WFC) mechanism to packetize data into small chunks (called flits: header, body and tail) and transmit them in a pipelined form. Routing functions establish set of routing channels as soon as it receives header flit. Body flits followed by tail flit routed on same channels. As tail flit leaves the router, routing function cancel the reservation and reallocate it to next on queue header flit. Unfortunately, blockage of header flit along established path causes body and tail flits to wait on host router [6]. Queuing of packetized data leads router into congestion that will ultimately cause back pressure to neighbour routers. Virtual Channels (VC) can be used with buffers to minimize effects of flow control mechanisms.

Recent studies [16], [24], [51] showed that local congestion has a significant impact on network performance. Data transmission between source to destination nodes can suffer with high latency if congestion is encountered. The proposed work focuses on the router congestion to avoid local congestion and restrict the spreading of congestion to neighbouring routers.

2.2. Related work
Adaptive routing algorithms with local or global information require congestion information across all nodes to make decisions. Depending on path diversity, adaptive algorithms re-route data packets around congestion nodes or flatten the distribution of traffic among the links. The key inhibitor in an adaptive routing algorithm is its need to have local and global network information, which is difficult to obtain without increased area and delays in decision making.

Regular mesh NoCs use a static two-dimensional routing algorithm called XY routing [29]. In XY, data travels in the X-dimension before moving in the Y-dimension while transversing from source to destination nodes. A congestion aware adaptive XY routing (DyXY) [32] approach was proposed to improve XY latency problems. Depending on the neighbour congestion status, DyXY transmits data in either X or Y directions. The Odd–Even (OE) routing algorithm [49] is proposed to avoid dead-lock caused in XY routing. DyAD [7] is a hybrid routing algorithm which gives both static and adaptive routing conditions. Depending on path congestion, DyAD dynamically switches between routing algorithms. If path is clear then the OE (static) routing algorithm will be deployed to transmit data, if the path is congested then the odd–even turn (adaptive) algorithm will be used to transmit data towards destination node. A routing controller in adaptive routing algorithms determines the minimal path to optimize NoC performance. Therefore overall performance of routing algorithms is dependent on the routing controller [24]. Most routing algorithms monitor local traffic to identify on path congestion and re-route data through less congested node. This approach uses congestion information of the neighbouring routers to identify and forward data packets through least congested on-path nodes. These routing algorithms have misjudgement problems  [52] as they forward data packets towards congested nodes. These queued data packets add into the spread of congestion across non-congested neighbouring nodes, thus affecting network performance by increasing the packet latency and decreasing network throughput [51], [52].

Congestion-Aware Adaptive Routing (CAAR) methods [24] are key research areas in tackling NoC congestion. CAAR use selection functions to react to congestion. Output buffer length (OBL) selection functions [6] use buffer occupancy information of downstream routers to make decisions. Selection strategies based on Neighbours-on-Path concepts use free slot information of all neighbours to select next hop. Some selection functions use switch contention information as an additional metric for decision purpose. A Path-Congestion Aware Adaptive Routing (PCAR) algorithm uses free buffer slots and crossbar demand as metrics to a selection function. An odd–even turn model for mesh platform with build-in guaranteeing QoS arbitration considers buffer information and switch values to identify congestion-free minimal path [24].

All these works are reactive to congestion and use real-time network values i.e. available buffer slots, crossbar demand etc. to calculate on path congestion. These congestion-aware techniques are helpful in reducing the effect of an already occurred congestion, via bypassing the flow of data through other available, less congested channels. Hence, minimizing the workload of overburdened routers and therefore enhancing overall NoC efficiency. These CAAR algorithms have no prior information about congestion until they reach close to the congested node. NoC efficiency can be increased further if congestion in a network or node is predicted before arrival of data packets close to congested nodes. It provides enough time for a routing node to react on potential congestion and re-route data through alternative paths. This advanced prediction and data re-routing technique reduces the probability of predicted nodes entering into a congestion state, thus improving overall network throughput [11].

Work presented in [23] used a low power application-driven traffic pattern table for predicting end to end traffic to dynamically adjust the working frequencies/voltages of the routers in NoC. It achieves 86% dynamic power savings in NoC links with 21% packet latency overheads. Other solutions include the execution of task mapping at run time, to meet real-time traffic constraints [14]. However due to time variation in application behaviour, task mapping has limited effort on improving the latency performance [17].

2.2.1. Neural network and prediction
Neural Networks (NNs) comprise of neurons (computational nodes) and synapses (communication links) to classify nonlinear and dynamic behaviours of systems. Depending on type of neural architecture and mathematical modelling, NNs can be classified into different categories. Neurons in Artificial Neural Network (ANN) are organized in different topologies, usually in the form of layers, where information processed in each neuron layer is transferred to subsequent layers and collated an output layer. Neurons are modelled by an activation function that is applied when a change in neuron input occurs [3]. Comparably to synapses in biological neurons, ANN neurons are connected through synaptic weights and ANNs use learning algorithms i.e., backpropagation to update synaptic weights to establish a relationship between connected (input and output) neurons [28]. To date neural networks have shown improvement in the domains of pattern recognition and prediction [1] [48] [25], [30]. Therefore, neural networks will be explored as a mechanism for detection and prediction of network traffic congestion patterns in real-time applications.

An Evolving Fuzzy Neural Network (EFuNN) based low latency path prediction algorithm is proposed in [46] to minimize latency issues caused by congestion-aware routing algorithms. The proposed work showed slight improvement in routing latency by incorporating existing congestion information however, the model failed to predict on path congestion.

An ANN based hotspot prediction mechanism was previously presented in [27] that monitored online statistical data to predict hotspot interconnect with an average efficiency of between 62%–92%. However, it is not scalable with high latency and hardware overheads. In the approach of [12], a two layered neural network technique is demonstrated which is able to detect and re-route data under congestion. It uses the hamming network to compute the link buffer utilization and then uses the competitive network to find the worst congestion node. Throughput was increased by 15%, however it does not explore the prediction of congestion.

These approaches used ANNs to predict congestion and to the best of the authors’ knowledge none have considered SNNs in classifying temporal NoC traffic patterns for prediction of congestion.

SNNs are third generation NNs and are claimed as more suitable learning tools for processing spatiotemporal information [53]. SNNs take rate encoded inputs as spikes and process encoded information between the input and output layers of the neural network in a temporal format. In ANNs, neural layers are updated with inputs in a single time step, whereas in SNNs inputs are applied in temporal form over a time series of steps. The key advantage is that SNNs exhibit low hardware area and power overheads compared to ANNs as the core neural computations are different  [20].

NoC architectures process digital data and the traffic patterns that flow between NoC are temporal/discrete in nature. Therefore, this work proposed SNN based prediction model that process temporal NoC traffic patterns to predict local congestion.

2.2.2. SNN model
SNNs closely mimic the operations of the biological nervous systems by incorporating different neural information dimensions such as time, frequency, and phase [39], [50]. Depending on neural complexity, different SNN models are proposed to present more biologically realistic neurons [34]. This work explores Leaky Integrate and Fire (LIF) neurons & Spike Response Model (SRM) neurons to model neural networks.

LIF neural model: The LIF neuron model is effective in modelling biological neurons for simulating spiking networks [41] because it provides good trade-off between computational complexity and hardware area cost. LIF neurons are driven by exponentially decaying synaptic currents generated by its synaptic afferents [54]. It integrates all synaptic input currents to produce a spike after it reaches a certain threshold [19]. The neuron membrane potential  of th neuron at time of ‘’ leaks out to a resting potential in the absence of an input current. It is described by (1)where  describes the membrane time constant of the neuron,  is the membrane resistance and  is synaptic current of th neuron. On arrival of each spike, membrane potential changes. When it crosses certain threshold, neuron fires a spike and goes to a refractory period.

SRM neural model: Spike response model is generalized variation of LIF neuron model. Neuron membrane potential excitation and fire are explicitly relay on time of the last spike [26]. The membrane potential ‘’ of cell ‘’ at time ‘’,  is defined as: (2)where  (also called linear filter of membrane) is linear response to input current . ‘’ is the time of last spike of neuron . The time dependency in membrane potential enables the refractoriness in neuron. Kernel ‘’ is response of neuron to its own spike.

Spikeprop-learning algorithm: In past few decades different learning algorithms for SNNs have been proposed to impose a precise input–output mapping of spike trains to SNNs. Bothe’s SpikeProp  [10] is based on arithmetic calculations and the concept is very similar to the back propagation algorithm for ANNs. SpikeProp learns and updates the cost function in terms of the difference between actual firing times  at output neuron j and the desired firing time  at output neuron  by (3)

To minimize the squared difference E, the weight  of each separate connection  of the synaptic terminal between pre-synaptic input of neuron  to neuron  should be calculated by (4)where  is learning rate of the network.

This work focuses on identifying patterns that leads to congestion and evaluates prediction capability of the proposed SNN for NoC architecture on synthetic and multimedia applications.

3. SNN-based NOC traffic hotspot prediction
This paper proposes the novel use of an SNN in predicting NoC traffic congestion. This work proposed two congestion prediction models based on LIF and SRM neurons. Both models are integrated with SpikeProp learning algorithm to update synaptic weights between spiking neurons. This section discusses proposed prediction methodology and congestion criteria to identify and predict local congestion in NoC architecture.

3.1. Proposed prediction methodology
To design an SNN based prediction methodology for NoC architectures, we considered all factors that leads towards the formation of congestion. Local congestion does not happen immediately but sources from an overburden node and accumulates over time. As more data is pushed towards an overloaded node, it starts to queue data in the input buffers and starts to effect network throughput. When this router enters congestion there are often many routers which are not congested. Due to the process of backpressure, these uncongested nodes begin to enter into a congestion state. NoC traffic is highly dependent on following three factors: (i) location of the router, (ii) application running on the system and (iii) Packet Injection Rate (PIR). The approach of [27] investigated the effeteness of buffer utilization data in determining NoC behaviour especially for congestion prediction.

This work considered router input buffer utilization as a congestion prediction parameter pattern. Every router has a different buffer utilization pattern depending on the mapped application therefore, every router will be treated independently to identify congestion. We propose a layer of SNN over the routing nodes where each router is connected with its own SNN. This model will help to identity the location of a router and its congestion status. SNNs takes buffer utilization as an input and gives predictive congestion status as an output. The proposed prediction model is scalable with network sizes, traffic scenarios, topologies and NoC simulator as each node requires its own SNN that is responsible for prediction of local congestion.


Download : Download high-res image (269KB)
Download : Download full-size image
Fig. 2. 4 × 4 NoC integrated with proposed SNN based congestion prediction model at each NoC node.

Routing algorithms are responsible for data transmission from source to destination nodes. Adaptive routing algorithms and CAAR algorithms are only able to see congestion in their neighbouring nodes (discussed in Section 2) and often suffered with misjudgement issues [52]. These routing algorithms gets an opportunity to use prediction information to re-route the data through alternative nodes and hence preventing the node to enter into congestion state without effecting network throughput. Using the hotspot status of all routers and traffic patterns we may predict the congestion of whole network however this is not the scope of this initial work.

Routers can be classified based on spatial location such as (1) Inner router and (2) Corner router. Due to uneven traffic distribution, Inner routers are more prone to network congestion as compared to corner routers  [51]. Fig. 2 depicts an example 4 × 4 NoC architecture, where Router ID# [11, 12, 21, 22 etc.] are inner routers and Router ID# [00, 10, 20, 30 etc.] are corner routers. Routers are interconnected with bi-directional channels through buffers. Buffer utilization data will be extracted from these buffers of each router for training and validation of SNN to predict NoC congestion.

3.2. Congestion criteria
Network congestion is one of the critical reasons for degradation of data throughput performance in NoCs. Congestion occurs when a network fails to deliver packets from source to destination within a predefined time (time required for data from the source node towards destination node i.e., latency). Data queues in input buffers are the foremost cause of delays that effects network latency and throughput performance. Congestion adds delays in packet re-transmission which impacts predefined transmission time/network latency. Buffer Utilization is an important NoC performance parameter [40],  [27] and have been demonstrated as a most effective parameter in identifying congestion. The proposed congestion prediction criteria is based on overall input buffer utilization. For example, the router shown in Fig. 3 has five inputs (North, East, South, West and Local Interface). Each input buffer can store up to 4 data packets, i.e. for five inputs there are a total of 20 data slots.

For any given router, if the buffer slots of all input channels are 50% or more occupied by queued data, with at least one fully occupied channel (full channel=4), then the router will be considered as congested. Therefore, for any given traffic scenario and selection strategy, utilization patterns that contains one fully occupied input channel with overall occupancy level of 50% or more, are defined as congestion patterns, and those patterns that produce occupancy level less than 50% are considered as non-congestion patterns. Compared to previous congestion criteria in [2], [27], the proposed congestion criteria is able to label each congestion pattern irrespective of traffic distribution, injection rate, topology and network size.

The hardware implementation of the SNN prediction model manages NoC congestion and that requires time to process utilization information in order to make a prediction on local congestion and to re-route data through alternative paths to avoid the potential congestion hotspot. The SNN requires 5–7 clock cycles to predict a router status and also requires additional clock cycles to forward information towards the congestion handling mechanisms. The congestion handling mechanism consumes 3–4 clock cycles to process congestion information and make an alternative routing path decision.

Therefore, from early experimentation we considered an optimized 30 clocks look-ahead benchmark to predict local congestion and forward prediction information towards the congestion handling mechanism for suppression of any negative impact. In this work it is assumed that the proposed SNN predictor will predict congestion 30 clock cycles in advance to provide enough time for the congestion handling algorithm to react on it.

3.3. Prediction model
NoC router throughput is measured in terms of the number of flits per clock cycle. Local congestion drastically affects network throughput by forcing routers to queue data at input buffers [51]. Using both synthetic and multi-media application traces, we have collected buffer utilization levels of all network routers. Data extracted from each router is used for training and validation of its connected SNN predictor (as shown in Fig. 4). To evaluate our approach, synthetic traffic patterns and a multimedia application were run independently on the NoC simulator [2] to generate utilization data for each router. Every node has its own SNN to predict local congestion (discussed in Section 3.1). Therefore, for each traffic trace, buffer utilization patterns of each node will be fed into its own SNN for training and validation. The PIR was increased to generate congestion in NoC architecture.

As explained in Section 2, a single congested router may affect performance of the whole network by forcing its neighbouring routers to enter into a congestion state. By monitoring the router input buffer occupancy levels, the spiking neural network will be able to identify and predict potential hotpots caused by either back pressure of neighbour routers or by high traffic flow/loads.

The scope of this work is limited to evaluating the prediction coverage of the proposed (LIF and SRM based) SNN models under various traffic conditions, and to identify an efficient solution based on prediction performance. The SNN model with the higher congestion prediction coverage will be integrated inside each NoC router to minimize the impact of potential congestion and thus improves network throughput and latency.

3.4. SNN encoding
SNN encoding depends on the temporal pattern at which neurons generate spikes. Spiking neural models work on spike arrival time and encode information in the spatial domain. SNN neurons read spikes over specific periods of time before reacting on it. This work used SpikeProp as a learning algorithm that works on spike propagation from the input layer to the output layer. So, neurons in the SNN layer will read the time of the input spike, process it through a hidden layer and then produce a spike to the output layer. An example timing diagram of the proposed prediction method is shown in Fig. 5. Neural Time Frame (NTF) is defined as time required by SNN to process information from input layer neuron till generation of spike at output layer neurons. NTF is used for monitoring spike patterns and at the end of the fixed time, the NTF window provides time to spiking neurons to undergo through refractory period. We assumed that SNN runs at higher frequency (SNN_clk) to generate output spike with in one NoC clock cycle (NoC_clk). As spikes are based on current buffer levels, each neuron will fire a spike based on its current occupancy level. In this approach, it is assumed that each input buffer has four available slots to pipeline incoming data and spike time associated to them are , , ,  and  respectively, where  depicts all slots are empty and  reflects all slots are occupied. Also  is the maximum time at which a neuron fires a spike; after that time, it is reset to zero. The SNN processes the data in time  and gives an output in time  ( < < ), where  is the maximum output spike time. The output of the SNN indicates a router status as either Congested () or un-congested (). To minimize latency and sufficiently predict ahead of the time of congestion occurring,  must be small in duration. During simulations, SNN models with different parameters were explored to minimize overall processing time; e.g. the number of hidden layers and number of neurons in each layer that not only decreases training time but also improves the prediction capability of proposed model.

4. Experimental setup
This section presents the experimental setup established to generate utilization data from NoC architectures and the use of data in SNN prediction of congestion hotspots. Synthetic and multimedia application traffic patterns are used as target applications. These applications are mapped to the network simulator to generate buffer utilization data for each router. PIR is increased to generate router congestion. Datasets with predefined congestion values are used for training of proposed prediction models. NoC architectures use a random selection strategy with an XY routing algorithm which enables each router to send data randomly hence creating the probability of congestion at different NoC nodes.


Table 1. Noxim configuration parameters.

Simulator	Noxim
Topology	Mesh
Network size	4 × 4
Arbitration	Round robin
Selection strategy	Random
Packet size	Two flits
Traffic distribution	Synthetic, multimedia
Routing algorithm	XY routing
Buffer depth	8 flits/4 Packet slots
Flit size	32-bit
4.1. Simulation setup
Experiments were carried out using synthetic and multimedia application data traces for checking the effectiveness of the proposed SNN predictor. These traces contained mapping information for each NoC node (e.g., source node, destination node and PIR) to generate network traffic. Inner nodes are more prone to congestion therefore this work utilized a 4 × 4 2D mesh topology with random selection strategy that provides enough inner and corner hotspot nodes (shown in Fig. 4) to analyse prediction coverage of the proposed prediction model under different traffic patterns. Depending on the location of the router, each router is connected between 2–4 neighbouring routers through network channels. We used the Noxim simulator  [15] which is an open source cycle accurate simulator to extract buffer utilization data for the SNN training and validation. All routers have 2 VCs per input port, and each input has 4 buffering slots. Parameters used for Noxim are defined in Table 1. Simulations were run for 1,000 clocks to generate buffer utilization data. Input buffer patterns that lead to congestion after 30 clocks cycles were defined as congestion occurring patterns, and patterns which did not transform into congestion after 30 cycles are noted as un-congested patterns. Collected data was divided (60:40) for training and validation purposes, respectively.

It is assumed that buffer information for each input will arrive as an individual packet and will be updated after a fixed time. As the SNN predictor receives inputs as temporal patterns, buffer levels are encoded as a spike time of input to a neuron. The time required for the SNN to process input patterns and generate an output is termed as a processing time, as noted in Fig. 5.

4.1.1. Traffic scenarios
To check the effectiveness of the proposed SNN predictors we considered two traffic scenarios: (a) Synthetic traffic scenario and (b) real-time application scenario.

4.1.1.1. Synthetic traffic scenarios.
The Noxim simulator provides built-in synthesis traces to evaluate network performance under varied traffic loads. These traces include transpose1, transpose 2, butterfly and shuffle. A 4 × 4 2D mesh topology (shown in Fig. 2) is used as the structure for performance evaluation of the synthetic traffic.

Transpose traffic: In transpose1, router  sends packets to , where N and M represents size of mesh along horizontal and vertical axis respectively.

Butterfly traffic: Packets are sent using  configuration, where n is used to direct output stages and k is the radix of each switch.

Shuffle traffic: This traffic send packets from  to .

4.1.1.2. Real-time multi-media traffic scenarios.
We considered a Multimedia System (MMS) and Moving Picture Experts Group-4 (MPEG-4) application traces to analyse performance on real-time multi-media traffic scenarios. MMS and MPEG-4 application traces (source node, destination node and PIR) are mapped into the Noxim simulator. More details on extraction of real-time multi-media traffic traces to map on 2D NoC are discussed in  [22]. Once the applications are mapped, data packets are traversed through 2D mesh using routing algorithms and a selection strategy (Table 1) to generate utilization datasets.

MultiMedia System (MMS): A generic MMS benchmark consisting of the H.263 video encoder–decoder and mp3 audio encoder–decoder was designed in [27] to evaluate performance of heterogeneous architectures. The MMS application includes 40 tasks that are scheduled across NoC nodes using mapping information (traces) generated using  [18], [38]. The MMS traces are mapped on 4 × 4 NoC using Noxim.

Moving Picture Experts Group-4 (MPEG-4): MPEG-4 is a widely used standard video decoder application to benchmark system performance  [5], [9], [45]. MPEG4 application traces mimic the communication of interactive audio-visual scenes between nodes and they are generated using Mesh based On-Chip interconnection Architectures (MOCA) in  [22]. Using the Noxim simulator, these traces are mapped across NoC nodes to generate buffer utilization patterns.

4.1.2. Hotspot generation
Routers are more likely to get congested under high traffic loads. A small increment in PIR increases the amount of data in the NoC channels hence increases network latency. Fig. 6 shows the effect of PIR on network latency using synthetic and multi-media applications. The figure shows that a high PIR leads the network into congestion. After a certain increment level in PIR, the network reached saturation. With further increases in PIR the network throughput plateaus out as the routers become congested. To generate congestion conditions in the NoC architecture, PIR must be high enough to maintain the network in saturation condition. Depending on the routing algorithm used, saturation points fall at different PIR values for distinct traffic scenario. As illustrated from Fig. 6, every traffic pattern has a different saturation point, but all traffic patterns enter into congestion after PIR=0.5. In this work, PIR is increased up to 0.5 to force routers to enter into congestion. This enables the SNN predictor to be evaluated. Fig. 7 illustrates the overall traffic distribution to determine the percentage of traffic labelled as congested and non-congested. It is depicted that transpose-1 and transpose-2 traffic scenarios have generated the highest density (>30%) of congestion patterns whereas the shuffle traffic scenario generated (11%) the least congestion causing patterns.

Fig. 8 shows the number of flits routed by each router during 1,000 clocks cycles: routers 0,0 through to 3,3 as per Fig. 2. For the MMS application, it is depicted from generated patterns that inner routers have more traffic loads compared to corner routers. Because of application mapping and routing algorithm, these routers have to process more data packets and are more prone to congestion. It is also clear from utilization datasets that some router does not even enter into congestion because of traffic distribution. As the MMS application has a static traffic distribution, patterns are likely to repeat randomly. According to the proposed congestion criteria in Section 3.2, it is evident from Fig. 8, that only three routers ID#[11, 12 and 13] entered into congestion state. If congestion for these routers is predicted in advance, then the network can prevent the impact from back pressure via re-routing packets on alternate paths. The same selection criteria will be used for MPEG-4, transpose1, transpose2, Butterfly and Shuffle traffic.


Download : Download high-res image (150KB)
Download : Download full-size image
Fig. 8. Maximum buffer utilization on each router for MMS.

4.1.3. SNN architecture
The SNN predictor is modelled and simulated using MATLAB and uses a fully connected 3-layer model (shown in Fig. 4). In the proposed prediction model, the number of connected input channels defines the number of input layer neurons, and the level of connected buffers reflects the time of first spike to input neuron. The number of input channels depends on the location of the node in the NoC. For the 4 × 4 NoC, inner routers have more input channels as compared to corner router (as shown in Fig. 2). The output layer of the SNN has one neuron and its spike time will determine the predicted router status (i.e., congested, non-congested) after 30 clocks. The number of hidden layers and number of neurons in each hidden layer depends on the complexity of the dataset (utilization patterns). In order to minimize network size without compromising network performance, we analysed different numbers of hidden layers with different number of neurons in each layer, i.e., to find the optimal hidden layer size. The SNN is optimized to a size where further reduction in hidden layer neurons will start reducing the prediction performance. During the SNN training, the neuron membrane threshold ‘’ and learning rate ‘’ help to regulate synaptic weights to generate spikes within the predefined time (NTF). These parameters are tuned to improve the learning rate of synapse weights and firing times of the SNN with in one NTF. Simulations were run on training datasets to achieve less than 5% Mean Square Error (MSE). The SNN predictor was developed in MATLAB for training and validation of NoC hotspot data. The parameters used for the configuration of the proposed SNN model are defined in Table 2.

The proposed model is highly scalable and can be implemented with any NoC size, mapping information, selection strategy and architecture. For any NoC configuration, the SNN is required to train on initial traffic patterns generated using a defined configuration to predict local congestion. The experimental setup has considered synthetic and multimedia applications to analyse prediction coverage under varying traffic scenarios. Each application has different traces for mapping on 4 × 4 NoC (discussed in Section 4.1.1 Traffic scenarios). As a change in the mapping information changes source/destination nodes thus altering traffic flow which ultimately changes the location of congestion. Section 4.1.2 discussed experimentation conditions that leads to hotspot formation and in Fig. 8 an example of hotspot formation in an 4 × 4 NoC using MMS application traces is depicted.


Table 2. SNN configuration parameters.

Parameter	Value
Neurons in input layer	3–4 for corner nodes and 5 for inner nodes
Neurons in hidden layer	15
Neurons in output layer	1 neuron (congested/non-congested)
Threshold ()	40
Learning rate ()	3
Mean Square Error (MSE)	 5%
Neural Computation begins with the start of a neural time frame (Fig. 5) and buffer utilization data arrives at the input neurons of the connected SNN predictor. A control signal (NTF) is part of the SNN that will allow input neurons to read utilization values from the Noxim simulator. Neurons in the SNNs are trained to produce spikes at each layer within predefined time (,  and  (Section 3.4)). After a predefined time, a control signal disables the SNN from reading inputs from the buffers. The SNN will take time to process the information and a compute result. Data is processed through the network layers in the form of spikes and the final result is aggregated in the output network layer. The spike time of the output neuron determines the status of the router as congested or non-congested after 30 clock cycles. For a given input, if the SNN predictor spikes within a defined time ‘’ (i.e. time is used to classify patterns as congested) then the network is more prone to getting congested; if the SNN predictor spikes after that time ‘’, then the NoC router will not get congested for the next 30 clock cycles. In MATLAB, SNNs are trained to generate output spikes within the output spike time . Once an output neuron spike and information is forwarded to the congestion handling mechanism, the control signal (NTF) resets and enables the input layer of neurons to read buffer utilization values from Noxim and to then propagate spikes through the hidden layer towards output layer to predict potential hotspots.

4.1.4. Performance analysis parameters
The proposed prediction models are analysed using two confusion matrix-based performance parameters: prediction accuracy  and prediction precision . (5)
 
(6)
 
 where positive  associates with congestion patterns and negative  is associated with non-congestion patterns.  and  define correct predictions of the congestion patterns  and non-congestion patterns , respectively. Simulation results include average prediction accuracy and prediction precision of the whole mesh network as well as inner and corner nodes to further analyse prediction performance at each node.

5. Results and discussion
This section provides simulation results and outlines hardware utilization of proposed SNN based NoC traffic predictor.

5.1. Simulation analysis
To evaluate prediction performance of the proposed congestion method, the dataset is divided into two parts for training and validation. Experimental datasets extracted from noxim simulator are divided 60% for training and 40% for validation. Neural algorithms (SNN models) are trained for each traffic scenario to compare prediction capabilities of SRM-based SNN predictor with SpikeProp used as the learning algorithm (SRM-Spikeprop), and the LIF neuron based SNN predictor with SpikeProp to update synaptic weights during training (LIF-Spikeprop). For simulation analysis, we use congestion prediction accuracy and prediction precision as parameter to evaluate performance. To evaluate performance of both neuron models, results are analysed as an average prediction accuracy of (i) the whole NoC and (ii) Inner and corner routers only.

5.1.1. Performance evaluation under synthetic traffic application
Built-noxim synthetic traces were used to generate router buffer utilization patterns in the NoC. To determine the prediction accuracy and prediction coverage of synthetic traffic, buffer utilization patterns from each router were fed to the SNN for the particular router to predict its congestion 30 clocks in advance.

The average prediction accuracy of the proposed SNN network models for the four synthetic traffic patterns is provided in Table 3. The LIF model shows an accuracy between 88.28% to 94.84% as compared to SRM model that predicts congestion between 85.28% to 92.91% accuracy.

Table 4 provides the performance of LIF-SpikeProp and SRM-Spikeprop SNN models on synthetic application traces for Corner and Inner router. For inner routers, the SRM based model gives an overall prediction accuracy of between 76.25% and 100% as compared to LIF-based SNN predictor with between 83.19% and 100%. For the corner router, the LIF model predictor exhibits between 86.98% and 97.19%, with the SRM model between 86.19% and 98.46% accuracy.


Table 3. Average prediction accuracy under synthetic traffic traces.

LIF-Spikeprop (%)	SRM-Spikeprop (%)
Transpose1	88.28	85.28
Transpose2	90.63	92.72
Butterfly	90.23	88.28
Shuffle	94.84	92.91

Table 4. Average prediction accuracy of inner and corner nodes SNN under synthetic traffic traces.

LIF-Spikeprop (%)	SRM-Spikeprop (%)
Corner	Inner	Corner	Inner
Transpose1	89.98	83.19	86.19	82.56
Transpose2	88.88	95.88	91.67	95.88
Butterfly	86.98	100.00	84.38	100.00
Shuffle	97.19	87.81	98.46	76.25
Table 5 shows prediction precision performance of the proposed LIF-SpikeProp and SRM-Spikeprop based SNN models on synthetic application traces. The LIF-based prediction model has shown 82.09%91.79% prediction precision as compared to 73.00%–91.97% prediction precision for the SRM-based SNN model.


Table 5. Average prediction precision under synthetic traffic traces.

LIF-Spikeprop (%)	SRM-Spikeprop (%)
Transpose1	91.97	91.97
Transpose2	82.09	73.00
Butterfly	88.66	88.66
Shuffle	85.42	85.23
5.1.2. Performance evaluation under real-time multi-media application
To determine the prediction accuracy and prediction coverage on real-time applications, the MPEG-4 [22] and MMS [33] applications traces were mapped on the Noxim simulator to generate buffer utilization patterns.

Table 6 depicts the average congestion prediction performance of LIF and SRM models on real-time multimedia applications. SRM model shows 95.45% to 95.69% accuracy as compared with LIF model 95.73% to 96.25% average prediction accuracy.


Table 6. Average prediction accuracy under real-time multimedia traffic traces.

LIF-Spikeprop (%)	SRM-Spikeprop (%)
MPEG-4	95.73	95.45
MMS	96.25	95.69
Table 7 shows accuracy results for LIF-Spikeprop and SRM-SpikeProp. For corner routers, the LIF-SpikeProp is able to predict congestion between 96.48% to 98.21% accurately compared with SRM-SpikeProp where 95.73% to 98.21% accuracy is achieved. However, in the inner routers the LIF model shows an accuracy of 88.31% to 99.56% as compared with SRM based model with accuracy of 87.19% to 99.56%.


Table 7. Average prediction accuracy of inner and corner nodes SNN under real-time multimedia traffic traces.

LIF-Spikeprop (%)	SRM-Spikeprop (%)
Corner	Inner	Corner	Inner
MPEG-4	98.21	88.31	98.21	87.19
MMS	96.48	99.56	95.73	99.56
The prediction precision performance of proposed LIF-based and SRM-based SNN models on multi-media application traces are provided in Table 8. SRM based prediction model predicted congestion with more precision (97.47%–98.48%) as compared to LIF based SNN model (93.92%–96.73%).

5.2. Discussion
In the synthetic traffic application, patterns are more likely to repeat after certain time intervals. In a regular traffic flow, these patterns are easy to predict. As depicted in Table 3, Butterfly traffic shows 100% prediction accuracy in corner routers. That happens because of the nature of regular application, where an increase in PIR increases network delay but does not affect traffic patterns in the network. Problems arise when nodes start entering into the congestion state. Congestion in a node causes delays in transmission of the data flowing through the congested node. Congestion changes the behaviour of data flow patterns and hence makes difficulties for prediction algorithm. Neural networks are exceptionally good at learning under changing behaviour, specifically for temporal patterns in digital NoC, SNNs are an ideal candidate to predict congestion. Congestion prediction is difficult for those routers which show irregularities in utilization patterns under high PIR.

Simulation results shows that LIF-SpikeProp performed well in predicting congestion under different traffic conditions as compared to the SRM-SpikeProp model. In the synthetic traffic application, SRM-Spikeprop model predicts congestion with 92.91% maximum average accuracy as compared to LIF-SpikeProp model 94.84%. In the real-time multimedia application, LIF-SpikeProp model predicts congestion up to 95.69% accurately as compared to SRM based model that shows 96.25% peak average prediction accuracy. Moreover, LIF based prediction model gives an overall prediction precision between 82.09% and 96.73% as compared to SRM-based SNN predictor between 73% and 98.48%.

As explained in Section 2.1, Inner routers are more susceptible to congestion. Table 4, Table 7 show the prediction accuracy of both models in inner and corner nodes. Results for inner routers shows that both neural models gives an overall maximum prediction accuracy of 100% (butterfly traffic traces). Whereas in corner nodes, LIF-SpikeProp and SRM-SpikeProp are capable to predict congestion with 98%–21% and 98.46% peak average accuracy respectively. Overall results show that LIF model predicts congestion with more accurately and precisely as compared to SRM mode.


Table 8. Average prediction precision under multimedia traffic traces.

LIF-Spikeprop (%)	SRM-Spikeprop (%)
MPEG-4	96.73	97.47
MMS	93.92	98.48
The scope of this work is limited to check congestion prediction coverage of spiking neurons in NoC architecture. The overall efficiency of network is dependent on congestion prediction and congestion handling mechanism. In this work, the SNN congestion predictor showed good prediction accuracy and in terms of prediction coverage, the LIF-Spikeprop predictor was more accurate as compared to SRM-Spikeprop. The output of the SNN predictor is ideally forwarded to a routing algorithm or congestion handling mechanism to enable a new routing decision to be made in advance of the congestion being created. This aspect of the work is not addressed in this paper and focuses only on the prediction capability.

5.3. Hardware area overhead
The work from [36] provide area estimates in evaluating the compactness of the proposed SNN predictor. By using the same approximation for neural model (shown in Fig. 4) we can calculate hardware overhead with respect to EMBRACE static and adaptive router [13], [21]. In NoC architecture, one static router, one adaptive router, one neurons, and one synapse will utilize ,  and  area respectively using 90 nm CMOS technology. In this work we proposed a layer of SNN predictors over the NoC where each router is connected to its own SNN. Therefore, we will evaluate hardware overhead in terms of SNN to router overhead percentage. Hardware cost for one fully connected 3-layer SNN model [5:10:1] is . Therefore, hardware costs for SNN are 0.14% of static router whereas in adaptive router SNN increases the hardware by 0.51%. SRM neuron consumes 2.5 times more hardware area as compared to LIF model [44], [47]. Comparing with EMBRACE static and adaptive router, SRM costs 0.35% and 1.26% area overhead. Analysis shows that overall hardware cost for SNN predictors is very low which makes them suitable for implementation on NoC for congestion prediction.

5.4. Comparison against existing neural prediction techniques
In the NoC, ANNs are used in congestion aware routing algorithms. The only work of congestion prediction in NoCs is presented in [27], where an ANN-based predictor forecasts congestion based on synthetic and real-time multimedia applications. It uses a multi-layer ANN as an independent processing element (PE) in the NoC architecture that is capable of reading utilization data of neighbouring routers and predicting congestion with an accuracy ranging from 65% to 92%.

Due to the temporal nature of buffer utilization patterns, the SNN based prediction model showed an improved accuracy of 88.28%–95.69% compared to the ANN model [27]. Our proposed SNN predictor is computationally fast and requires less clock cycles to process data for the prediction task. The ANN model used one neural network for a 4 × 4 cluster which increases workload in determining exact congestion location to routing algorithm. However, our work proposed a layer of SNNs where every router has its own SNN predictor. In terms of hardware overhead, the proposed models utilized 0.14%–0.51% and 0.35%–1.26% additional hardware as compared to 5.6% for the ANN based predictor. Thus, the two proposed configuration costs only exhibit minimal hardware overhead and provide improved prediction accuracy.

6. Conclusion
In this work we proposed a novel SNN based congestion predictor that uses buffer utilization to predict congestion 30 clocks in advance of it occurring. Every router is associated with one SNN to identify hotspot locations. The proposed SNN prediction strategy is based on the integration of the backpropagation SpikeProp training algorithm with two spiking LIF and SRM neurons to encode synaptic weights. The two LIF and SRM neuron models were explored to assess which is most effective for prediction. Both SNN Predictors were trained and validated on synthetic and multimedia application traffic traces to evaluate prediction performances. This work explored and analysed the prediction performance of proposed SNN predictors on (i) overall NoC network array and (ii) on Inner and corner routers only. Simulation results showed that LIF-SpikeProp provided better prediction performance with 88.28%–96.25% accuracy and 82.09%–96.73% precision as compared to SRM-SpikeProp with 85.25%–95.69% accuracy and 73%-98.48% prediction precision. Both models performed exceptionally well in predicting NoC congestion. Results suggests that the LIF-Spikeprop model has a slight performance advantage on SRM-Spikeprop model.

Future work includes exploration of other parameters i.e. channel latency, travelling time and link utilization to enhance prediction performance. In next step we will introduce and assess congestion handling/routing mechanisms that will receive the prediction outcome from the SNN and be able to avoid the creation of the congested path by re-routing the data in advance.

