Abstract
Parallel computations with irregular memory access patterns are often limited by the memory subsystems of multi-core CPUs, though it can be difficult to pinpoint and quantify performance bottlenecks precisely. We present a method for estimating volumes of data traffic caused by irregular, parallel computations on multi-core CPUs with memory hierarchies containing both private and shared caches. Further, we describe a performance model based on these estimates that applies to bandwidth-limited computations. As a case study, we consider two standard algorithms for sparse matrix–vector multiplication, a widely used, irregular kernel. Using three different multi-core CPU systems and a set of matrices that induce a range of irregular memory access patterns, we demonstrate that our cache simulation combined with the proposed performance model accurately quantifies performance bottlenecks that would not be detected using standard best- or worst-case estimates of the data traffic volume.

Previous
Next 
Keywords
Cache simulation

Performance model

Sparse matrix–vector multiplication

Intel Xeon

AMD Epyc

1. Introduction
Performance is a high priority in scientific computations, and so meticulous work is devoted to optimising the underlying code. During such optimisation efforts, performance models are valuable tools for directing attention towards pressure points, and indicating when optimisations are good enough and expending further effort would be unproductive. For instance, the popular Roofline model [41] bounds performance in terms of a CPU’s peak computational capacity and memory bandwidth together with an algorithm’s computational intensity. Because CPUs have hierarchical memories, the bandwidth and computational intensity can vary depending on the memory hierarchy level that is considered. Moreover, the computational intensity depends not only on parameters such as cache size, but also on the memory access pattern of the computation. Recently, more elaborate performance models have been developed for stencil codes [8], [32], [46], where they have been used to evaluate the effectiveness of spatial and temporal blocking optimisations. In these cases, the amounts of data transferred between levels of the memory hierarchy are known in advance, because memory accesses are predictable and depend only on the problem size and the order of the stencil. Unfortunately, this is not the case for irregular computations, where memory access patterns depend on data that may only be known at runtime.

When faced with irregular access patterns, the typical approach is to derive estimates of the memory traffic for the worst- or best-case scenarios. These are “paper and pencil” estimates that have the advantage of being cheap to produce, not requiring any implementation or actual machine to run. On the other hand, such estimates are crude and can in reality be far from the true data traffic volumes, thereby rendering little help in understanding the actual performance that is achieved. For example, Fig. 1 shows worst- and best-case estimates for sparse matrix–vector multiplication (SpMV), a widely used computational kernel that suffers from both irregularity and low computational intensity. Due to the considerable difference between the best- and worst-case data traffic, these estimates cannot provide much confidence if they are used to evaluate whether the performance of a given kernel implementation is good enough. In this case, more accurate estimation of data traffic volumes is needed for performance validation. In general, numerous computational kernels face the same issues due to irregular memory accesses that arise through the use of sparse data structures, such as graphs or unstructured meshes.

In this paper, we present a method for quantifying the amounts of data transferred between levels of a multi-core CPU’s memory hierarchy during irregular computations. The estimated data traffic volumes are produced by a trace-driven cache simulation that relies on a few basic assumptions and a simplified model of the memory hierarchy. Moreover, the method applies to memory hierarchies with shared caches, a common feature of contemporary multi-core CPUs, and a case that is not always addressed by existing analytical cache models [1], [12]. Because the proposed method is based on tracing a sequence of memory references, it requires some amount of computation that is likely to be at least as much as the cost of executing the kernel itself. However, the method remains applicable in cases where the actual machine in question is not available, or the data traffic cannot be quantified directly through hardware monitoring facilities, for example, because these facilities are unavailable, unreliable or the results are not easily interpreted.


Download : Download high-res image (266KB)
Download : Download full-size image
Fig. 1. Worst- and best-case data traffic volumes for sparse matrix–vector multiplication. The data traffic estimates (Eqs. (5), (6)) are based on a standard algorithm (see Section 4) and are shown for matrices from the SuiteSparse Matrix Collection [9]. For comparison, actual data traffic volumes are shown based on the measured number of cache misses for the 32 KiB L1 data cache with 64 byte cache lines on an Intel Sandy Bridge Xeon E5-2650 CPU. (See Section 5.3 for details on how these measurements were obtained.) Notice that there is a wide gap between the best- and worst-case data traffic volumes. For a given sparse matrix, the actual volume of data traffic depends on the size of the CPU cache and the sparsity pattern of the matrix.

Because of its importance and familiarity as an irregular computational kernel, we use SpMV to demonstrate that our cache simulation accurately quantifies the volumes of data transfers in the memory hierarchies of two Intel-based multi-core CPU systems. In turn, these data transfer volumes are used to give accurate performance predictions that are unavailable through the use of simple worst- or best-case estimates. We also give performance predictions for an AMD Epyc CPU, and explore some limitations of the proposed method using a variant of SpMV that not only includes irregular reads, but also irregular writes. Ultimately, these predictions result in a quantitative understanding of SpMV performance, which, for example, can be used to check that the observed performance of a given implementation matches our expectations, and that the implementation is free from hidden performance issues.

The remainder of this paper is organised as follows. In the next section, we describe our cache simulation approach for estimating the data traffic volumes of computations with irregular memory accesses. In Section 3, we present a performance model for bandwidth-limited computations, where the relevant data traffic volumes are used together with realistic memory and CPU cache bandwidths. Next, in Section 4, we recall standard SpMV algorithms for matrices in the compressed sparse row (CSR) and coordinate (COO) storage formats. We also review known bounds on the volume of data traffic generated by the CSR SpMV algorithm, which is later used to compare with the results of our cache simulation method. Then, in Section 5, we describe experiments that are used to validate the estimated data traffic volumes and the performance model for the studied SpMV algorithms. Finally, we briefly discuss related work in Section 6 and draw our conclusions in Section 7.

2. Quantifying data traffic for irregular, parallel computations
To estimate the data traffic volume for a given computation on a multi-core CPU system, we consider the sequence of load and store operations that would be performed by the participating CPUs. Then we simulate a cache’s behaviour using a simplified model based on the established ideal-cache model [12], which is ordinarily used in the design of cache-oblivious algorithms. We depart from the ideal-cache model in two ways. First, for practical reasons, we assume a least recently used replacement policy, instead of the ideal-cache model’s optimal policy, also known as Bélády’s algorithm. Second, we extend our cache model to incorporate multi-level memory hierarchies and caches that are shared by multiple processors.

2.1. A simplified cache model
Consider the case of a sequential computation in a memory hierarchy consisting of main memory and a single CPU cache. More specifically, consider a cache of size  that is partitioned into cache lines of size , such that there are  cache lines. Data is moved between main memory and the cache in contiguous blocks of size , called cache blocks. These transfers originate from load or store operations issued by the CPU.

Furthermore, we make the following three assumptions regarding the behaviour of the cache:

•
First, we assume that the cache operates with demand caching, which means that a cache block is placed into the cache only when the CPU issues a load or store referencing a memory address corresponding to that cache block. This assumption precludes the use of hardware prefetching, a common feature in multi-core CPUs that is used to speculatively fetch data before it is requested. However, to allow hardware prefetching would complicate our cache simulation in ways that are most likely specific to particular hardware.

•
Second, the cache is assumed to be fully associative, meaning that any cache block can be placed into any cache line. In this way, we disregard conflict misses, that is, cache misses due to cache blocks that would be mapped to the same set in a set-associative or direct-mapped cache. The mapping of cache blocks to sets is hardware-specific, as well as documented poorly or not at all. Moreover, the overall effect of conflict misses is likely to be small and to only have an impact on a few, very specific memory access patterns.

•
Third, we assume that cache lines are evicted according to a least recently used policy whenever the cache is full. This policy can be easily simulated, and it also seems to approximate real hardware accurately enough for our purposes.

The volume of data traffic between the cache and main memory is deduced by counting transfers of cache blocks that result from a given sequence of loads and stores. The procedure is described in the following and summarised in Algorithm 1. Throughout the cache simulation, we maintain a least recently used list with  entries, one per cache line, to keep track of the cache blocks that occupy the cache. The list is also used to decide which cache line to evict, if the cache is full and more room is needed for incoming cache blocks. Initially, the list is empty, thereby representing a cold cache that does not contain any relevant data prior to the computation. Next, each load or store references a memory address, , and a corresponding block of main memory, indexed by . If the referenced cache block is found in the least recently used list, then it is moved to the front of the list. Otherwise, a cache miss occurs. In this case, the number of cache misses is increased by one, and, if the least recently used list is full, the last entry of the list is removed. Finally, the referenced cache block is added to the front of the list. Thus, the total number of cache misses is equal to the number of new entries that were inserted at the front of the least recently used list.


Download : Download high-res image (133KB)
Download : Download full-size image
2.2. Multi-level hierarchies, private and shared caches
In the case of a memory hierarchy with multiple cache levels, we consider the CPU’s working memory to be the highest level of the memory hierarchy, whereas main memory is the lowest level. Caches that lie near the CPU are referred to as higher-level caches, and, conversely, caches farther from the CPU are lower-level caches. Furthermore, we assume that each cache is inclusive of any higher-level caches, which means that cache blocks contained in a higher-level cache are also contained in lower-level caches. For example, data that is brought to a first-level (L1) cache must also be placed into a last-level cache. With this assumption, each cache can be treated individually. Letting 
 denote the size of the th cache and  the size of blocks that are transferred from the next, lower level of the memory hierarchy, we obtain the corresponding number of cache misses 
 by using a least recently used list with 
 entries and following Algorithm 1.

Next, consider a parallel computation with multiple CPUs, where each CPU has a private cache. For the private cache belonging to the th processor, let 
 and 
 denote the cache size and cache line size, respectively. This case is again handled by applying Algorithm 1 to each cache individually. However, only the loads and stores that are issued by the th processor are used to count the number of cache misses 
 for the corresponding private cache.

Now, consider a parallel computation with multiple CPUs sharing a cache. Generally speaking, each CPU issues its load and store instructions independently of the others. Therefore, the order in which a shared cache receives data requests is affected by factors such as the scheduling of threads onto CPUs. Moreover, if CPUs retrieve data from different levels of the memory hierarchy, they might proceed at different speeds because of varying memory access latencies—that is, the time it takes to serve each request depends on where in the memory hierarchy the data is located. Ultimately, the order of data requests to a shared cache, and thus the number of cache misses, may differ between runs of the same, identical program due to competition between the CPUs.

However, we propose to approximate the order of data requests to a shared cache by assuming that requests are submitted by processors in a round-robin fashion. In turn, each CPU issues its next load or store operation, resulting in an interleaving of the data requests from each CPU to the shared cache. The number of cache misses for each CPU is obtained from the interleaved sequence of loads and stores by applying Algorithm 1 with the following modification: Rather than estimating only a total number of cache misses, we maintain a counter 
 for each CPU, which is incremented whenever a cache miss is caused by a load or store that was issued by the th processor. The total number of cache misses is 
. Attributing data traffic to individual CPUs will be important when these data traffic estimates are tied to the overall performance in Section 3.

Finally, for systems with a non-uniform memory access (NUMA) architecture, it may be necessary to distinguish between multiple memory controllers or channels that may serve data in response to last-level cache misses. In this case, memory addresses are partitioned into NUMA domains according to some memory policy, and separate counters are used for the cache misses for each NUMA domain.

2.3. Limitations
We have already mentioned some limitations of the proposed cache simulation method. For instance, we ignore conflict misses and hardware prefetching, and, in the case of a shared cache, we assume a simple round-robin scheduling of requests from participating CPUs. In addition, the proposed method treats stores in the same way as loads and ignores any additional data transfers that may be required to maintain cache coherence for multi-core CPUs with private caches. In practice, a store will typically result in two data transfers. First, the relevant cache block is transferred from a lower-level cache or main memory before the store operation is carried out in the working memory of the CPU. Second, the modified cache line must be written back to lower-level caches and, eventually, to main memory. However, our proposed cache simulation method does not account for traffic resulting from writing back modified cache lines, or additional traffic arising from write-related cache coherency effects, such as false sharing. For kernels whose performance is significantly impacted by any of the above-mentioned effects, the current approach would have to be extended or improved.

3. A performance model based on data traffic and bandwidth
In this section, we describe a performance model for computations that are limited by cache or memory bandwidth by tying the execution time to relevant data traffic volumes between levels inside a memory hierarchy. In the following model, we assume that computation and memory accesses overlap, and, moreover, that the dominant cost is due to memory accesses, so that computations can be neglected. In addition, data must be transferred between adjacent memory hierarchy levels, and we assume that these transfers occur in parallel.

For the th cache in a multi-level hierarchy, recall that  is the cache line size and let 
 be the number of cache misses for the th processor that is attached to the th cache. Then 
 is the data traffic volume that is transferred to the th cache from lower levels of the memory hierarchy on account of the th processor. Furthermore, let 
 denote the sustainable, single-core bandwidth for transferring the data, and let 
 denote the aggregate bandwidth that is available to all the CPU cores that are sharing the th cache. Although hardware vendors occasionally specify theoretical bandwidth numbers, these are rarely achievable in practice. Instead, realistic, sustainable bandwidths are typically determined by a benchmark programme, such as STREAM [25].

First, performance may be limited due to data traffic induced by individual CPU cores. In this case, a lower bound on the execution time  is given by the per-core data transfer that takes the longest time, that is, (1) 
 
 
 
 
where  is the number of floating-point operations performed, and  is the performance in floating-point operations per unit time. In other words, a lower bound on the execution time  translates to an equivalent upper bound on performance.

Second, whenever resources in a memory hierarchy are shared by multiple CPU cores, performance may be limited due to the total data traffic volume and a limited, aggregate bandwidth, (2) 
 
 
 
 
In particular, for data transferred between main memory and last-level caches, the aggregate bandwidth does not scale linearly with the number of CPU cores, but instead saturates before all the cores are in use. Moreover, many multi-core CPUs have a NUMA architecture, where main memory is partitioned into NUMA domains, and the bandwidth of each NUMA domain is limited. In some cases, it is also necessary to take into account the fact that groups of CPU cores have an affinity to their local NUMA domain. This is especially the case for problems where a significant amount of data is transferred between CPU cores and their remote NUMA domains, since the bandwidth of such transfers tends to be severely limited.

4. Sparse matrix–vector multiplication
The multiplication of a sparse matrix with a dense vector, or SpMV, is a prime example of an irregular, parallel computation. It is also a fundamental computational kernel that appears in numerous scientific applications. For example, SpMV is performed repeatedly in iterative methods for solving sparse linear systems, such as Krylov subspace methods [30]. The efficiency of these methods often hinges on the SpMV computations that are required during each iteration, but it is well known that SpMV has a low computational intensity, and, therefore, its performance lies well below peak computational capacity [34], [37]. More precisely, every non-zero value in a sparse matrix gives rise to one multiplication and one addition, but typically requires loading at least two floating-point numerical values and one integer index. In the general case, the locations of the matrix non-zeros are represented explicitly, which means that half of the floating-point values are read indirectly, indexed by the column indices of the matrix non-zeros. These indirect reads can induce a highly irregular memory access pattern, which prevents data reuse in the caches. In any case, performance is typically limited by the speed at which data can be retrieved from CPU caches or main memory, rather than the peak floating-point capacity. But the relationship between the locations of matrix non-zeros, also called the sparsity pattern, and the amounts of data that must be read from each level of the memory hierarchy are difficult to characterise. In other words, for a given matrix, it is not obvious which level of the memory hierarchy constitutes a performance bottleneck.

There is a vast literature devoted to optimising SpMV performance, including low-level code optimisations [40], and optimisations for reducing the volume of data traffic, such as register blocking [29], [38], cache blocking [28], matrix re-orderings [15], [43], and compression [39]. In addition, advanced SpMV algorithms have been proposed, either based on the standard compressed sparse row format and variants thereof [6], [23], [26], [44] or other sparse matrix formats [14], [19], [22]. Furthermore, sparse matrix partitioning schemes [3], [7], [18], [36], [45] are used to improve the parallel efficiency of SpMV through better load balancing and reduced communication. Although there has recently been a considerable effort to optimise SpMV for GPUs [11], we focus our attention on multi-core CPUs, which still represent a relevant and important case.

Note, however, that we do not propose any new optimisations or storage formats for sparse matrices. Instead, we aim to understand the performance of irregular, bandwidth-limited computations, by using a pair of simple, well known SpMV algorithms as examples.

4.1. Compressed sparse row
Compressed sparse row (CSR) is a standard storage format for general sparse matrices, where the non-zero matrix entries and their locations are stored explicitly. Consider a sparse matrix 
 with  rows,  columns, and  non-zeros. In other words, there are  distinct pairs of row and column indices, 
, such that 
. If the non-zeros are arranged in ascending order according to their row indices, then the matrix  can be represented in the CSR format with  non-zero values 
 and column indices 
, and  row pointers 
. The row pointers 
 and 
 are the indices of the first and last non-zeros of the th row, respectively. That is, a non-zero 
 belongs to row 
 if 
.

For a source vector 
 and destination vector 
, the SpMV  is defined as (3)

A common strategy for computing SpMV in parallel is to partition the rows of the matrix and distribute the parts among multiple processors or threads. In this way, each processor computes its destination vector values 
 according to the row partitioning. The non-zero values 
, column indices 
, and destination vector elements 
 are partitioned along with the rows. In contrast, the source vector elements 
 may be shared by several processors, though the extent of this sharing depends on the column indices 
.

The code in Algorithm 2 shows a straightforward C implementation of shared-memory parallel SpMV for a matrix in the CSR format. Here, OpenMP is used to parallelise the outer loop by distributing the rows among a given number of threads. The schedule clause of the omp for compiler directive specifies that the loop is to be divided into chunks, consisting of chunk_size consecutive loop iterations, and that the chunks are assigned to threads in a round-robin fashion. Unless otherwise noted, we omit the chunk_size, which implies that the threads are assigned one large chunk each.


Download : Download high-res image (101KB)
Download : Download full-size image
4.2. Coordinate storage format
The coordinate (COO) format is another scheme for storing sparse matrices, though it is not commonly used for SpMV due to its poor performance. The main reason is that COO does not compress the row and column indices, which leads to a larger memory footprint and more memory traffic during an SpMV calculation compared to CSR. However, a COO-based SpMV algorithm is interesting because it features irregular writes, which are not required for a CSR-based SpMV. Therefore, we include COO-based SpMV as an additional irregular kernel that we use to investigate the accuracy of the proposed cache simulation method.

A sparse matrix  with  non-zeros is stored in the COO format by means of three arrays: the row indices 
, column indices 
, and non-zero values 
. The non-zeros are not required to be sorted by their row or column indices, and may thus appear in any order. Given a source vector 
 and destination vector 
, the SpMV  is computed as (4)

The code in Algorithm 3 shows a naive C implementation of shared-memory parallel SpMV for a matrix in the COO format. Note that care must be taken to avoid race conditions due to the irregular writes. For this reason, an omp atomic directive is used within the loop. Although there is a substantial synchronisation overhead imposed by atomic writes, and there are more efficient ways of implementing COO SpMV that avoid this overhead, we deliberately choose the current version to study the effect of irregular writes to a shared array.


Download : Download high-res image (93KB)
Download : Download full-size image
4.3. Best- and worst-case bounds on data traffic for CSR SpMV
In this section, we recall well known bounds on the number of cache misses for the sequential version of the CSR SpMV algorithm. Let  be a sparse matrix with  rows,  columns, and  non-zeros in the CSR format. Furthermore, assume that row pointers and column indices are stored as four-byte integers, and non-zero values are stored as eight-byte double precision floats. We define the size of the matrix as the memory footprint of the CSR format, that is,  bytes. Moreover, the working set is the combined size of the matrix , and the vectors  and , that is, .

To determine the number of cache misses incurred by Algorithm 2, assume, for the sake of simplicity, that each of the arrays r, j, a, x, and y is aligned to a cache line boundary. Otherwise, there is at most one additional cache miss for each of the arrays. Further, suppose that the cache does not contain any relevant data to begin with, which holds for the first application of SpMV in an iterative solver, but also during subsequent iterations provided that the working set exceeds the size of the cache under consideration. In the best case, there are only compulsory misses because data must be brought into the cache whenever it is first accessed. Consequently, a lower bound on the number of cache misses  coincides with the number of cache lines occupied by the working set, (5)
 
 
 
 
 
where  is the cache line size. The lower bound is attained only if (a) there are no conflict misses, that is, data is never evicted due to using a set-associative or direct-mapped cache when it would otherwise have been reused, and (b) there are no capacity misses because the sparsity pattern of the matrix leads to perfect reuse of the source vector elements 
, and, in addition, the row pointers 
 and destination vector 
 remain in the cache between iterations of the outer loop over the rows.

To obtain a reasonable upper bound, we disregard conflict misses and assume that only the source vector suffers from capacity misses. This is equivalent to the cache being fully associative and the matrix rows being short enough that row pointers and destination vector elements do not suffer capacity misses. In the worst case, every access to a source vector element 
 incurs a cache miss, and the number of cache misses  is bounded from above by (6)
 
 
 
 
 
As shown in Fig. 1, the worst- and best-case estimates provide only a rough idea of the actual data traffic volume . More accurate estimates of  need to take the sparsity pattern of the matrix into account, as considered in Section 4.4.

Bounds similar to those given above have been presented previously in the literature in various forms, for example, by Heras et al. [15] for the case where the source vector  fits in cache, but irregular accesses may cause conflict misses due to using a direct-mapped or set-associative cache. Vuduc et al. [38] consider the case where a register blocking optimisation has been applied. Bender et al. [5] prove tight lower bounds on the number of cache misses of a sequential SpMV algorithm for large classes of general sparse matrices, and they also describe an asymptotically optimal algorithm based on cache-aware or cache-oblivious sorting that attains these lower bounds. See also Ballard et al. [4, Section 6] for a survey regarding both sequential and parallel SpMV, where the authors consider movement of data between levels of a memory hierarchy, as well as between parallel processors connected across a network. A different approach to estimating the data traffic volume of SpMV is described by Temam and Jalby [33] based on a probabilistic model. However, their model is primarily concerned with cache misses caused by conflicts in a direct-mapped cache, and assumes that the matrix non-zeros are uniformly distributed, which is rarely the case.

4.4. Data traffic estimates based on cache simulation
By applying the cache simulation method presented in Section 2, we obtain an alternative estimate for the number of cache misses in the CSR SpMV algorithm. To do so, we apply Algorithm 1 to the sequence of loads and stores that would be issued by Algorithm 2. There are in total  loads and  stores, where  and  are the number of matrix rows and non-zeros, respectively. To break it down, the inner loop performs  loads to fetch the non-zero values, column indices and source vector elements, while the outer loop performs  loads for the row pointers, and  loads and  stores for the destination vector. Multi-level memory hierarchies and shared caches are handled as described in Section 2.2. In this case, if  processors are used, we assume that the th processor is assigned consecutive rows from  up to .

Crucially, this method of quantifying data traffic volumes for SpMV produces estimates that inherently depend on the sparsity pattern of the matrix. In this way, our estimates more accurately capture the true volume of data traffic generated by a given SpMV computation compared to the best- and worst-case estimates of Eqs. (5), (6). Moreover, our method also incorporates the cache size, and it can therefore produce different estimates for each level of a memory hierarchy. These estimates can subsequently be used to diagnose performance bottlenecks that could not be identified by previous estimates.

Finally, we observe that the above procedure may be similarly applied to the COO SpMV kernel in Algorithm 3, other SpMV algorithms, or other irregular kernels. To do so, one must deduce the relevant sequence of load and store operations, and corresponding memory addresses, for the computation of interest.

5. Numerical experiments
In this section, we describe experiments that test the accuracy of data traffic estimates obtained with the cache simulation method described in Section 2, focusing on the CSR-based SpMV kernel in Algorithm 2. Next, we use the data traffic estimates for CSR SpMV to evaluate the performance model from Section 3. Finally, we also evaluate the data traffic estimates for the COO SpMV kernel in Algorithm 3.

5.1. Sparse matrices
The data used in this study consists of real-valued matrices, most of which are from the SuiteSparse Matrix Collection [9], a large set of sparse matrices from a variety of real-world applications. The selected matrices were mainly chosen because they exhibit a variety of sparsity patterns and are large enough that the SpMV working set exceeds the last-level caches of our test systems. In addition, we have included two matrices from a cardiac electrophysiology simulation that have previously been used by Langguth et al. [20]. Both matrices are derived from a finite volume method on the same tetrahedral mesh, but each matrix represents a different numbering of the mesh cells. Thus, the matrices have the same number of rows, columns and non-zeros, but very different sparsity patterns, see Fig. 2. For a list of the matrices used in our experiments, see Table 1, which also displays some high-level statistics about the sparsity pattern of each matrix. It is also worth noting that the matrices spal_004, GL7d19, sx-stackoverflow, and Lynx68, have particularly irregular sparsity patterns.


Table 1. Matrices from the SuiteSparse Matrix Collection [9] used in our experiments. The SuiteSparse matrices are sorted according to the number of columns.

Matrix	Rows	Columns	Non-zeros	Non-zeros per row
Mean	Median	Std.	Max
TSOPF_RS_b2383	0.04M	0.04M	16M	424	6	484	983
spal_004	0.01M	0.3M	46M	4 525	5 063	1 492	6 029
RM07R	0.4M	0.4M	37M	98	125	69	295
relat9	12.3M	0.5M	39M	4	4	0	4
HV15R	2.0M	2.0M	283M	140	156	54	484
GL7d19	1.9M	2.0M	37M	20	20	3	121
sx-stackoverflow	2.6M	2.6M	36M	14	2	138	38 148
FullChip	3.0M	3.0M	27M	9	6	1 807	2 312 481
Freescale1	3.4M	3.4M	19M	6	5	2	27
circuit5M	5.6M	5.6M	60M	11	5	1 357	1 290 501
Hardesty3	8.2M	7.6M	40M	5	5	0	5
Lynx68a	6.8M	6.8M	112M	16	17	2	17
Lynx68_reordereda	6.8M	6.8M	112M	16	17	2	17
a
These matrices are not found in the SuiteSparse Matrix Collection.


Download : Download high-res image (489KB)
Download : Download full-size image
Fig. 2. The sparsity patterns of the Lynx68 and Lynx68_reordered matrices. The shaded areas represent the locations of non-zero matrix entries. Both matrices have the same number of rows, columns and non-zeros, but they have very different sparsity patterns.

5.2. Experimental setup
For our experiments, we used three multi-core CPU systems: a dual-socket Intel Sandy Bridge Xeon E5-2650, a dual-socket Intel Skylake Xeon Platinum 8168, and a dual-socket AMD Epyc 7601. On all three systems, each processor core has an L1 instruction cache and L1 data cache, as well as a unified L2 cache for both instructions and data. In addition, there are L3 caches on each system, which may also contain both instructions and data. For the Intel processors, there is a shared L3 cache for each socket, whereas the AMD Epyc has an 8 MiB L3 cache for each group of four processor cores, also referred to as a compute complex. Similarly, there is a NUMA domain for each socket on the Intel systems, whereas a NUMA domain on the AMD Epyc is associated with a group of two compute complexes, or eight processor cores. Every cache uses a common cache line size of 64 bytes. The main characteristics of the hardware are summarised in Table 2. Recall that in a memory hierarchy, a lower-level cache is said to be inclusive of a higher-level cache if every cache block contained in the higher-level cache is also present in the lower-level cache. We note that the L3 cache is inclusive of the L2 cache on Sandy Bridge, but not on Skylake [17, Ch. 2].

In the following experiments, turbo boost is disabled, and the intel˙pstate or acpi-cpufreq performance scaling driver is used with the “performance” scaling governor. As a consequence, the CPU clock frequency is capped at 2.0 GHz on Sandy Bridge, 2.7 GHz on Skylake, and 2.2 GHz on Epyc, respectively. Furthermore, each thread is pinned to its own CPU core, and memory that is accessed by a given thread is initialised so that a first touch policy will place the corresponding memory pages in the NUMA domain closest to the thread.


Table 2. Multi-core CPU systems used in our experiments, including the size and set associativity of each cache, memory configuration, and theoretical memory bandwidth.

Sandy Bridge	Skylake	Epyc
CPUs	2 × Intel Xeon E5-2650	2 × Intel Xeon Platinum 8168	2 × AMD Epyc 7601
CPU cores	2 × 8	2 × 24	2 × 32
L1 cache per core	32 KiB, 8-way	32 KiB, 8-way	32 KiB, 8-way
L2 cache per core	256 KiB, 8-way	1024 KiB, 16-way	512 KiB, 16-way
L3 cache per core	2560 KiB, 20-way	1408 KiB, 11-way	2048 KiB, 16-way
Memory channels per socket	4 × 1 600 MHz	6 × 2 666 MHz	8 × 2 666 MHz
Memory bandwidth	2 × 51.2 GB/s	2 × 128.0 GB/s	2 × 170.6 GB/s
Moreover, each system is running Ubuntu 18.04.2 LTS with Linux kernel version 4.15, and code is compiled using GCC 7.4.0 with the compiler flags -O3 -fopenmp -march=native.

As an initial verification of the SpMV kernel in Algorithm 2, Table 3 shows that the serial performance on Sandy Bridge closely matches a reference implementation provided by the Intel Math Kernel Library (MKL) 2017. In addition, we compare the performance of our implementation on both Sandy Bridge and Epyc using different compilers. For a few matrices, the Intel C compiler attains a slight performance increase, while the performance slightly degrades for other matrices. In particular, for matrices that are irregular, the difference in performance is negligible. Both because it is widely used and freely available, we will stick to using GCC for the remaining experiments.


Table 3. Serial performance (in Gflop/s) of a CSR-based SpMV kernel. The first column is the performance of a reference implementation on Sandy Bridge provided by the function mkl_cspblas_dcsrgemv from the Intel Math Kernel Library (MKL) 2017. The remaining columns compare the performance of our implementation compiled with GCC and ICC on both Sandy Bridge and Epyc.

Matrix	Sandy Bridge	Epyc
Intel MKL 2017	GCC 7.4.0	ICC 17.0.0	GCC 7.4.0	ICC 19.0.5
TSOPF_RS_b2383	1.60	1.54	1.93	1.67	2.61
spal_004	1.11	1.15	1.19	1.55	2.15
RM07R	1.36	1.51	1.76	1.76	2.45
relat9	0.64	0.67	0.59	1.04	0.92
HV15R	1.38	1.34	1.66	1.77	2.49
GL7d19	0.26	0.26	0.26	0.39	0.36
sx-stackoverflow	0.23	0.25	0.23	0.35	0.32
FullChip	0.90	0.97	0.92	1.26	1.19
Freescale1	0.87	0.97	0.86	1.37	1.27
circuit5M	1.14	1.23	1.17	1.74	1.88
Hardesty3	1.02	1.14	0.98	1.74	1.60
Lynx68	0.21	0.21	0.21	0.34	0.30
Lynx68_reordered	0.66	0.65	0.69	1.33	1.24
5.3. Estimating and measuring data traffic for CSR-based SpMV
For each matrix in Table 1, we use the method described in Section 2 to estimate the number of cache misses 
 and 
 incurred by the th processor for the L1 and L2 caches, respectively. Because the L3 cache is shared, the corresponding estimates are produced by interleaving load and store instructions issued by each processor, as described in Section 2.2. Moreover, we obtain separate estimates for the number of L3 cache misses 
 for each NUMA domain . We use a common cache line size of 64 bytes, and the cache sizes listed in Table 2. Based on an estimated number of cache misses per core, 
, we deduce the corresponding volume of data traffic, 
. In the results that follow, we focus on the total data traffic volumes for each cache level, 
 for  and 
.

To validate the estimated data traffic volumes, we now compare them to data traffic measurements on our Intel-based multi-core CPUs. Unfortunately, currently available tools for hardware performance monitoring on Linux, in particular, libpfm4 [10], do not support the hardware performance events that are required to directly measure data traffic throughout the memory hierarchy of AMD Epyc processors. Therefore, we focus on the Intel-based systems for now, although we consider the AMD Epyc system once again when evaluating the performance of the CSR-based SpMV kernel in Section 5.5. Measurements of the actual data traffic volumes are obtained from Performance Monitoring Units (PMUs) in the hardware that can be configured to report various hardware performance events [16, Ch. 18]. In our case, these events are accessed by providing appropriate event encodings to the perf_event_open system call, and using the returned file descriptor to read values from hardware performance counters. The library libpfm4 [10] is used to translate event names to event encodings. Moreover, we follow guidelines given by Molka et al. [27] to select appropriate hardware performance events for data traffic measurements by using microbenchmarks to correlate hardware events with data transfers between levels of the memory hierarchy. The events that have been chosen as a result of this procedure are shown in Table 4.


Table 4. Hardware performance events recorded for each processor core. Each event includes cache lines brought to the specified cache due to loads, stores, and hardware prefetches. Event names follow the conventions of the library libpfm4 [10].

Cache level	Sandy Bridge	Skylake
L1   L2	l1-dcache-load-misses	l1-dcache-load-misses
L2   L3	l2_lines_in:any	l2_lines_in:any
L3   DRAM	offcore_response_0:
any_data:any_rfo:
l3_miss:snp_any	offcore_response_0:
any_data:any_rfo:
l3_miss_local:
l3_miss_miss_remote_hop1_dram:
snp_any

Table 5. Estimated and measured total data traffic (in MiB) on Intel Sandy Bridge Xeon E5-2650 for SpMV with Algorithm 2.

Matrix	L1   L2	L2  L3	L3  DRAM
Meas.
MiB	Est.
MiB	Err.
%	Meas.
MiB	Est.
MiB	Err.
%	Meas.
MiB	Est.
MiB	Err.
%
Single core
 TSOPF_RS_b2383	188	186	−1.1	187	186	−0.5	186	186	0.0
 spal_004	1335	1054	−21.0	956	839	−12.2	529	528	−0.2
 RM07R	503	488	−3.0	472	456	−3.4	441	439	−0.5
 relat9	1505	1477	−1.9	1114	1086	−2.5	594	595	0.2
 HV15R	3597	3522	−2.1	3480	3397	−2.4	3308	3291	−0.5
 GL7d19	3526	2714	−23.0	2679	2631	−1.8	573	532	−7.2
 sx-stackoverflow	3429	2565	−25.2	2600	2446	−5.9	749	717	−4.3
 FullChip	500	490	−2.0	474	467	−1.5	454	447	−1.5
 Freescale1	387	381	−1.6	385	375	−2.6	347	344	−0.9
 circuit5M	1180	1185	0.4	923	958	3.8	937	944	0.7
 Hardesty3	742	734	−1.1	650	618	−4.9	616	618	0.3
 Lynx68	9103	5560	−38.9	6873	5532	−19.5	4367	4270	−2.2
 Lynx68_reordered	5297	5131	−3.1	2965	2891	−2.5	1428	1428	0.0
Single socket
 TSOPF_RS_b2383	188	186	−1.1	188	186	−1.1	185	186	0.5
 spal_004	1344	1054	−21.6	962	840	−12.7	515	529	2.7
 RM07R	504	488	−3.2	474	456	−3.8	420	438	4.3
 relat9	1506	1477	−1.9	1112	1086	−2.3	584	591	1.2
 HV15R	3600	3522	−2.2	3493	3397	−2.7	3220	3357	4.3
 GL7d19	3527	2714	−23.1	2679	2631	−1.8	751	673	−10.4
 sx-stackoverflow	3433	2565	−25.3	2592	2615	−6.5	791	749	−5.5
 FullChip	501	490	−2.2	481	467	−2.9	427	439	2.8
 Freescale1	388	381	−1.8	389	375	−3.6	327	338	3.4
 circuit5M	1181	1185	0.3	928	958	3.2	905	943	4.2
 Hardesty3	742	734	−1.1	658	619	−5.9	576	618	7.3
 Lynx68	9105	5560	−38.9	6923	5532	−20.1	4418	4316	−2.3
 Lynx68_reordered	5302	5131	−3.2	2996	2891	−3.5	1467	1465	−0.1
Dual socket
 TSOPF_RS_b2383	188	186	−1.1	187	185	−1.1	185	185	0.0
 spal_004	1346	1054	−21.7	959	840	−12.4	542	531	−2.0
 RM07R	504	488	−3.2	475	456	−4.0	451	439	−2.7
 relat9	1510	1477	−2.2	1113	1087	−2.3	591	592	0.2
 HV15R	3602	3522	−2.2	3499	3397	−2.9	3413	3357	−1.6
 GL7d19	3528	2714	−23.1	2688	2631	−2.1	617	563	−8.8
 sx-stackoverflow	3434	2565	−25.3	2613	2446	−6.4	795	732	−7.9
 FullChip	502	490	−2.4	483	467	−3.3	449	438	−2.4
 Freescale1	388	381	−1.8	390	375	−3.8	354	348	−1.7
 circuit5M	1181	1185	0.3	928	958	3.2	895	917	2.5
 Hardesty3	742	734	−1.1	668	619	−7.3	620	618	−0.3
 Lynx68	9099	5560	−38.9	6937	5532	−20.3	4402	4290	−2.5
 Lynx68_reordered	5302	5131	−3.2	3054	2892	−5.3	1481	1463	−1.2
For each matrix in Table 1, we compute SpMV in parallel using Algorithm 2, and, for each thread, we record the hardware performance events listed in Table 4. These events provide the number of cache misses per core for each cache level, from which the total data traffic volumes are computed.

The estimated and measured total data traffic volumes for CSR SpMV using a single core, single socket, and both sockets are shown in Table 5, Table 6 for Sandy Bridge and Skylake, respectively. Also, estimated data traffic volumes for the single core and single socket cases on AMD Epyc are shown in Table 7. In the following, we refer to the data traffic volume from the L2 cache to the L1 cache as  traffic, and so on for the remaining levels of the hierarchy.

Overall, the correspondence between measured and estimated data traffic is close, which shows that the main contributions to data traffic are captured by our simulation scheme, even with a simplified model of the memory hierarchy. Most estimates are within a few percent of the measured data traffic volumes, but there are some notable exceptions.


Table 6. Estimated and measured total data traffic (in MiB) on Intel Skylake Xeon Platinum 8168 for SpMV with Algorithm 2.

Matrix	L1   L2	L2  L3	L3  DRAM
Meas.
MiB	Est.
MiB	Err.
%	Meas.
MiB	Est.
MiB	Err.
%	Meas.
MiB	Est.
MiB	Err.
%
Single core
 TSOPF_RS_b2383	188	186	−1.1	187	186	−0.5	187	186	−0.5
 spal_004	1332	1054	−20.9	858	838	−2.3	554	528	−4.7
 RM07R	503	488	−3.0	471	454	−3.6	456	439	−3.7
 relat9	1497	1477	−1.3	878	842	−4.1	595	591	−0.7
 HV15R	3598	3522	−2.1	3446	3368	−2.3	3383	3287	−2.8
 GL7d19	3209	2714	−15.4	2401	2397	−0.2	504	479	−5.0
 sx-stackoverflow	3240	2565	−20.8	2190	2173	−0.8	519	552	6.4
 FullChip	500	490	−2.0	482	461	−4.4	431	436	1.2
 Freescale1	387	381	−1.6	388	369	−4.9	339	340	0.3
 circuit5M	1180	1185	0.4	963	957	−0.6	933	915	−1.9
 Hardesty3	741	734	−0.9	623	618	−0.8	622	618	−0.6
 Lynx68	8729	5560	−36.3	5551	5480	−1.3	3260	3539	8.6
 Lynx68_reordered	5247	5131	−2.2	1584	1529	−3.5	1497	1424	−4.9
Single socket
 TSOPF_RS_b2383	191	186	−2.6	192	185	−3.6	186	186	0.0
 spal_004	1318	1054	−20.0	860	838	−2.6	532	528	−0.8
 RM07R	518	488	−5.8	474	454	−4.2	442	448	1.4
 relat9	1500	1477	−1.5	871	843	−3.2	591	588	−0.5
 HV15R	3625	3522	−2.8	3456	3368	−2.5	3382	3360	−0.7
 GL7d19	3210	2714	−15.5	2420	2397	−1.0	484	456	−5.8
 sx-stackoverflow	3243	2565	−20.9	2263	2173	−4.0	517	555	7.4
 FullChip	504	490	−2.8	513	461	−10.1	430	428	−0.5
 Freescale1	388	381	−1.8	395	369	−6.6	333	332	−0.3
 circuit5M	1189	1185	−0.3	1058	957	−9.5	935	930	−0.5
 Hardesty3	742	734	−1.1	634	619	−2.4	625	618	−1.1
 Lynx68	8732	5560	−36.3	5594	5480	−2.0	2589	3599	39.0
 Lynx68_reordered	5249	5131	−2.2	1635	1535	−6.1	1509	1494	−1.0
Dual socket
 TSOPF_RS_b2383	194	187	−3.6	199	185	−7.0	187	185	−1.1
 spal_004	1320	1054	−20.2	866	840	−3.0	535	528	−1.3
 RM07R	525	488	−7.0	481	454	−5.6	441	447	1.4
 relat9	1504	1477	−1.8	882	844	−4.3	593	589	−0.7
 HV15R	3683	3522	−4.4	3467	3368	−2.9	3346	3360	0.4
 GL7d19	3213	2714	−15.5	2425	2397	−1.2	483	473	−2.1
 sx-stackoverflow	3254	2565	−21.2	2342	2173	−7.2	504	552	9.5
 FullChip	511	490	−4.1	544	462	−15.1	410	428	4.4
 Freescale1	389	381	−2.1	398	369	−7.3	316	332	5.1
 circuit5M	1196	1185	−0.9	1073	957	−10.8	908	927	2.1
 Hardesty3	744	734	−1.3	653	620	−5.1	632	618	−2.2
 Lynx68	8734	5560	−36.3	5623	5480	−2.5	2586	3564	37.8
 Lynx68_reordered	5254	5131	−2.3	1658	1540	−7.1	1488	1482	−0.4

Table 7. Estimated total data traffic (in MiB) on AMD Epyc 7601 for SpMV with Algorithm 2. Measurements of the data traffic on Epyc are missing due to a lack of reliable hardware performance events for measuring data traffic throughout the memory hierarchy.

Matrix	L1   L2
MiB	L2  L3
MiB	L3  DRAM
MiB
Single core
 TSOPF_RS_b2383	187	186	186
 spal_004	1054	839	530
 RM07R	488	455	439
 relat9	1477	967	618
 HV15R	3522	3369	3301
 GL7d19	2714	2548	1111
 sx-stackoverflow	2565	2340	1207
 FullChip	490	463	457
 Freescale1	381	371	351
 circuit5M	1185	958	945
 Hardesty3	734	618	618
 Lynx68	5560	5514	5017
 Lynx68_reordered	5131	1786	1438
Single socket
 TSOPF_RS_b2383	186	185	185
 spal_004	1054	840	607
 RM07R	488	456	446
 relat9	1477	968	614
 HV15R	3522	3369	3359
 GL7d19	2714	2548	1136
 sx-stackoverflow	2565	2340	1219
 FullChip	490	463	453
 Freescale1	381	371	359
 circuit5M	1185	958	937
 Hardesty3	734	619	618
 Lynx68	5560	5514	5020
 Lynx68_reordered	5131	1792	1475
First, for some of the most irregular matrices, spal_004, GL7d19, Lynx68, and sx-stackoverflow, our method underestimates the  traffic on both Sandy Bridge and Skylake by about 15–40%. Also, the  traffic on Sandy Bridge is underestimated by about 10–20% for spal_004 and Lynx68. These discrepancies are likely caused by conflict misses due to the caches being set-associative, rather than fully associative, as we assume in our simplified model of the memory hierarchy. Observe how the estimates of the  traffic on Skylake are more accurate, probably due to the increased cache size and associativity, which should result in fewer conflict misses. Now, there is also another, plausible reason for underestimating the data traffic volume: There may be additional, needless data traffic in cases where hardware prefetching mechanisms place data into a CPU cache and the data is later evicted without being used. This is possible because hardware prefetchers may transfer data from lower levels of a memory hierarchy, even though there are no outstanding requests referencing the data. The indirect and irregular memory access patterns that occur in SpMV can easily render standard hardware prefetching mechanisms inefficient, though this will naturally depend a great deal on the sparsity pattern of the matrix. While it would be possible to assess the impact of hardware prefetchers by explicitly disabling them, this is beyond the scope of our current work.

Second, the cache simulation overestimates the  traffic for the matrices sx-stackoverflow and Lynx68 on Skylake by about ten and forty percent, respectively. The fact that this does not occur on Sandy Bridge suggests that the overestimation is related to the L3 cache not being inclusive of the L2 cache on Skylake. It is likely that a more accurate estimate could be produced if non-inclusivity was taken into account, for example, by combining the size of the L2 and L3 caches.

5.4. Measuring CPU cache and memory bandwidth
Before we can relate the relevant data traffic volumes in a memory hierarchy to an algorithm’s performance, we must first benchmark the CPU cache and main memory bandwidths of our test systems. We do so by using a modified version of the STREAM bandwidth benchmark [25].

The original STREAM benchmark only includes computational kernels with regular memory access patterns, and it is not clear that the measured bandwidths are relevant for an irregular computation, such as SpMV. For example, the standard kernels benefit greatly from the compiler’s automatic vectorisation, but the CSR SpMV kernel in Algorithm 2 usually does not. In addition, the STREAM kernels do not include indirect memory accesses, and, moreover, they perform a larger proportion of writes than is typical for SpMV. Therefore, we supplement the STREAM benchmark with an additional computational kernel, which more closely resembles SpMV, and consists of computing a dot product with indirect memory accesses. This is similar to the approach used by Vuduc et al. [38].

The measured bandwidths for the different kernels are shown in Table 8. The sizes of the arrays used by each kernel are selected so that the data fits in the appropriate level of the memory hierarchy, and times are measured using clock_gettime with CLOCK_MONOTONIC. The Triad kernel is included mostly as a reference, since the numbers it produces are often used in performance evaluations, for example, in the Roofline model [41].

The Triad kernel has a higher throughput than the indirect dot product whenever data is read from one of the CPU caches. This is because the compiler generates vectorised code for Triad, which alleviates bottlenecks that are limiting the performance of the indirect dot product kernel. Meanwhile, for the per-socket memory bandwidth, the Triad kernel produces a result that is about 75% of the indirect dot product. This difference is caused by the following two facts. First, the indirect dot product performs no stores, whereas the Triad kernel performs one store for every two loads. Second, the bandwidths reported by STREAM assume that the traffic generated by a store is the same as for a load. However, in practice, stores usually generate twice as much data traffic, that is, sixteen bytes per store for Triad, because a cache line is transferred from main memory before a store is performed, and, in addition, the modified cache line is written back to main memory afterwards. Although it is possible to use non-temporal stores to avoid bringing cache lines from main memory before a store, this requires either using a different compiler, such as the Intel C Compiler, or hand-coded optimisation, because GCC will not generate such code.


Table 8. Memory and CPU cache bandwidth (in GB/s) measured by the STREAM benchmark [25]. In addition to the standard Triad kernel, we have added a kernel for computing a dot product with indirect memory accesses. The latter kernel is representative of the SpMV computation in Algorithm 2, and it corresponds to SpMV with a matrix in the CSR format that has a single, dense row. The bandwidths per NUMA domain and per socket are measured by using numactl to place memory in a single NUMA domain or socket, and running the benchmarks with all cores belonging to the given NUMA domain or socket, respectively.

Triad
Indirect dot product
Sandy
Bridge	Skylake	Epyc	Sandy
Bridge	Skylake	Epyc
, per core	81.4	212.0	63.2	13.1	13.2	8.7
, per core	29.2	88.8	64.9	13.3	13.4	8.8
, per core	17.9	24.2	51.3	12.7	12.5	8.8
per core	9.1	11.4	18.3	9.8	10.1	8.7
per NUMA domain	28.1	74.8	21.0	37.3	99.8	29.6
per socket	28.1	74.8	85.4	37.3	99.8	118.5
all sockets	56.8	139.2	168.4	70.6	184.5	225.7
5.5. Evaluating performance for CSR-based SpMV
In this section, we apply the performance model from Section 3 to evaluate the performance of the standard CSR SpMV algorithm. The performance model uses the data traffic estimates from Section 5.3 and CPU cache and memory bandwidths from Section 5.4.

First, we record the actual execution time and performance, so that we can later compare to our performance predictions. For each matrix in Table 1, we compute SpMV in parallel using Algorithm 2 and measure the execution time. To account for variations in the measurements, we obtain a random sample of one hundred trials, and let  denote the sample mean execution time. The performance is , where  is the number of matrix non-zeros.

For each cache level, we compute upper bounds on performance based on Eq. (1) using per-core data traffic and bandwidth. That is, the maximum of the data traffic volumes among all cores is used along with the single-core bandwidth of data transfers to the corresponding level of the memory hierarchy. Finally, there is an upper bound on performance based on Eq. (2) and the total traffic and bandwidth of each NUMA domain. For the bandwidths, we have used the numbers given in Table 8 for the “indirect dot product” kernel.

Upper bounds on performance for CSR-based SpMV on Sandy Bridge, Skylake, and Epyc are shown in Table 9, Table 10, Table 11. Again, for the Intel systems, we present results using a single core, single socket and both sockets. For AMD Epyc, we present single core and single socket results.


Table 9. Upper bounds on performance (in Gflop/s) on Intel Sandy Bridge Xeon E5-2650 for SpMV with Algorithm 2. The column “” is based on the number of loads from the L1 cache to registers. The next four columns are based on the estimated data traffic volumes in Table 5. The “per core” bounds use the maximum data traffic volume among all cores together with the single-core CPU cache or memory bandwidth. The “per socket” bound is based on the largest of the per-socket traffic volumes and the aggregate memory bandwidth of a single socket. The relevant bandwidths are from the “indirect dot product” kernel in Table 8. The smallest of the upper bounds is displayed in bold and represents the bottleneck predicted by our model.

Matrix	
per core	
per core	
per core	
per core	per socket
Single core
 TSOPF_RS_b2383	1.31	2.21	2.11	1.63	–
 spal_004	1.31	1.11	1.33	1.63	–
 RM07R	1.31	1.95	1.99	1.60	–
 relat9	1.31	0.67	0.87	1.22	–
 HV15R	1.31	2.04	2.02	1.61	–
 GL7d19	1.31	0.35	0.34	1.31	–
 sx-stackoverflow	1.31	0.36	0.36	0.94	–
 FullChip	1.31	1.38	1.38	1.11	–
 Freescale1	1.31	1.26	1.22	1.03	–
 circuit5M	1.31	1.27	1.51	1.18	–
 Hardesty3	1.31	1.40	1.59	1.22	–
 Lynx68	1.31	0.51	0.49	0.49	–
 Lynx68_reordered	1.31	0.55	0.93	1.46	–
Single socket
 TSOPF_RS_b2383	4.52	7.60	7.25	5.60	6.19
 spal_004	10.38	8.25	10.55	12.88	6.21
 RM07R	9.92	14.62	15.13	12.07	6.09
 relat9	8.26	3.92	5.10	8.18	4.69
 HV15R	10.11	15.61	15.51	12.11	6.00
 GL7d19	9.91	2.65	2.67	6.91	3.95
 sx-stackoverflow	3.30	0.90	0.92	2.61	3.44
 FullChip	4.21	3.46	3.49	3.00	4.31
 Freescale1	5.99	5.11	4.98	4.53	3.98
 circuit5M	2.21	1.86	2.47	1.95	4.49
 Hardesty3	10.32	10.92	12.56	9.69	4.66
 Lynx68	10.30	3.40	3.25	3.27	1.84
 Lynx68_reordered	10.40	4.35	7.34	11.27	5.42
Dual socket
 TSOPF_RS_b2383	9.04	15.19	14.51	11.20	6.22
 spal_004	24.42	16.50	21.10	25.38	12.35
 RM07R	19.82	29.70	30.25	24.15	11.79
 relat9	16.52	7.66	9.93	16.18	7.74
 HV15R	20.21	31.22	31.03	24.27	11.98
 GL7d19	18.78	5.06	5.20	19.38	9.22
 sx-stackoverflow	4.80	1.31	1.36	4.67	5.36
 FullChip	5.21	4.19	4.19	4.48	7.43
 Freescale1	10.83	8.73	8.49	9.31	6.70
 circuit5M	3.36	3.66	3.62	3.55	6.65
 Hardesty3	20.64	21.83	25.12	19.39	9.22
 Lynx68	20.60	6.79	6.51	9.35	3.48
 Lynx68_reordered	20.73	8.65	14.53	22.42	10.75

Table 10. Upper bounds on performance (in Gflop/s) on Intel Skylake Xeon Platinum 8168 for SpMV with Algorithm 2. The column “” is based on the number of loads from the L1 cache to registers. The next four columns are based on the estimated data traffic volumes in Table 6. The “per core” bounds use the maximum data traffic volume among all cores together with the single-core CPU cache or memory bandwidth. The “per socket” bound is based on the largest of the per-socket traffic volumes and the aggregate memory bandwidth of a single socket. The relevant bandwidths are from the “indirect dot product” kernel in Table 8. The smallest of the upper bounds is displayed in bold and represents the bottleneck predicted by our model.

Matrix	Reg.L1 per core	L1L2 per core	L2L3 per core	L3DRAM	
per core	per socket
Single core
 TSOPF_RS_b2383	1.32	2.22	2.07	1.67	–
 spal_004	1.32	1.12	1.31	1.68	–
 RM07R	1.32	1.96	1.97	1.64	–
 relat9	1.32	0.67	1.10	1.27	–
 HV15R	1.32	2.05	2.00	1.66	–
 GL7d19	1.32	0.35	0.37	1.50	–
 sx-stackoverflow	1.32	0.36	0.40	1.26	–
 FullChip	1.32	1.39	1.38	1.18	–
 Freescale1	1.32	1.27	1.22	1.07	–
 circuit5M	1.32	1.28	1.48	1.25	–
 Hardesty3	1.32	1.41	1.56	1.26	–
 Lynx68	1.32	0.51	0.49	0.61	–
 Lynx68_reordered	1.32	0.56	1.74	1.51	–
Single socket
 TSOPF_RS_b2383	13.67	22.96	21.42	17.31	16.55
 spal_004	30.70	24.58	30.58	38.67	16.64
 RM07R	29.46	43.52	44.66	36.09	15.92
 relat9	24.96	11.71	19.76	25.88	12.61
 HV15R	28.38	43.58	42.72	34.73	16.04
 GL7d19	26.92	7.28	8.39	31.26	15.58
 sx-stackoverflow	5.80	1.59	1.96	6.02	12.43
 FullChip	5.81	4.69	4.53	4.35	11.84
 Freescale1	15.99	12.40	12.53	13.50	10.85
 circuit5M	3.02	2.27	3.22	2.76	12.18
 Hardesty3	31.19	33.35	37.09	29.97	12.46
 Lynx68	31.10	10.26	9.71	12.35	5.90
 Lynx68_reordered	31.29	12.96	40.30	33.58	14.21
Dual socket
 TSOPF_RS_b2383	27.31	45.92	42.84	34.61	16.64
 spal_004	59.30	47.20	57.93	74.12	33.29
 RM07R	53.36	79.80	81.20	65.61	31.01
 relat9	49.93	22.13	38.70	50.03	20.89
 HV15R	56.73	87.17	85.43	69.91	31.96
 GL7d19	46.53	12.72	16.79	55.31	29.85
 sx-stackoverflow	8.16	2.24	2.85	9.56	17.73
 FullChip	6.42	5.23	4.96	5.83	20.35
 Freescale1	30.86	21.03	21.48	28.04	18.86
 circuit5M	5.17	3.85	5.20	5.26	17.59
 Hardesty3	62.39	64.62	74.19	59.94	24.68
 Lynx68	62.19	20.51	19.41	35.81	11.48
 Lynx68_reordered	62.27	25.68	80.59	67.15	28.43

Table 11. Upper bounds on performance (in Gflop/s) on AMD Epyc 7601 for SpMV with Algorithm 2. The column “” is based on the number of loads from the L1 cache to registers. The next four columns are based on the estimated data traffic volumes in Table 7. The “per core” bounds use the maximum data traffic volume among all cores together with the single-core CPU cache or memory bandwidth. The “per NUMA domain” bound is based on the maximum traffic volume among NUMA domains and the aggregate bandwidth of a single NUMA domain. The relevant bandwidths are from the “indirect dot product” kernel in Table 8. The smallest of the upper bounds is displayed in bold and represents the bottleneck predicted by our model.

Matrix	
per core	
per core	
per core	
per core	per NUMA domain
Single core
 TSOPF_RS_b2383	0.88	1.46	1.46	1.44	–
 spal_004	0.88	0.74	0.92	1.45	–
 RM07R	0.88	1.29	1.38	1.42	–
 relat9	0.88	0.44	0.68	1.05	–
 HV15R	0.88	1.35	1.41	1.42	–
 GL7d19	0.88	0.23	0.25	0.56	–
 sx-stackoverflow	0.88	0.24	0.26	0.50	–
 FullChip	0.88	0.91	0.97	0.97	–
 Freescale1	0.88	0.83	0.86	0.89	–
 circuit5M	0.88	0.84	1.04	1.05	–
 Hardesty3	0.88	0.93	1.10	1.09	–
 Lynx68	0.88	0.34	0.34	0.37	–
 Lynx68_reordered	0.88	0.36	1.05	1.29	–
Single socket
 TSOPF_RS_b2383	12.14	20.88	20.88	10.32	8.54
 spal_004	27.83	21.53	29.80	20.16	17.16
 RM07R	25.20	36.99	39.30	20.72	18.25
 relat9	22.19	10.06	15.57	14.05	11.52
 HV15R	25.12	38.01	39.93	20.88	18.36
 GL7d19	22.74	6.08	7.37	12.39	6.24
 sx-stackoverflow	4.43	1.20	1.41	3.78	4.31
 FullChip	4.04	3.19	3.29	4.70	9.34
 Freescale1	14.18	10.24	10.95	10.13	8.49
 circuit5M	3.04	2.50	3.28	4.20	6.67
 Hardesty3	27.73	28.29	33.95	16.78	14.56
 Lynx68	27.64	8.96	9.04	10.06	4.47
 Lynx68_reordered	27.75	11.28	32.28	19.90	17.94
Generally speaking, no single level of the memory hierarchy can be regarded as a bottleneck for all of the tested matrices. For a single core, bottlenecks sometimes appear between the registers and L1 cache, between L1 and L2, and also between L3 and main memory. Whenever additional cores are used, performance is frequently limited by the aggregate memory bandwidth. However, for some matrices, including GL7d19, sx-stackoverflow, and Lynx68_reordered,  traffic remains the limiting factor. In these cases, the aggregate memory bandwidth per socket or NUMA domain is plentiful, but the irregular traffic between the L1 and L2 caches is so substantial that it limits performance. In addition, the sparsity patterns of circuit5m and FullChip result in severely unbalanced workloads among multiple processors, which is not surprising due to both matrices having at least one very long row. Note that the performance bounds increase only marginally when moving from a single core to multiple cores, far from the ideal speedup one might hope for with a perfectly balanced workload. Moreover, in both cases, the per-core bounds are lower than the per-socket or per NUMA domain bounds, indicating that the aggregate memory bandwidth is not saturated.

Comparing the two Intel systems, our predictions show very little difference between Sandy Bridge and Skylake in the single-core case. However, when looking at an entire socket or full machine, the Skylake system is predicted to perform much better, mostly due to its higher aggregate memory bandwidth. In addition, the increased size of Skylake’s L2 cache means that some matrices, such as Lynx68_reordered, generate less  data traffic compared to Sandy Bridge. However, this does not seem to significantly impact performance because data transfers between the L2 and L3 caches never constituted a bottleneck to begin with.

For Epyc, the single core performance is constrained by traffic between registers and the L1 cache or the L1 and L2 caches. This stems from the fact that the measured per core bandwidths of the indirect dot product kernel are essentially the same regardless of which level of the memory hierarchy that data is read from, and there is naturally more traffic to the smaller, higher-level caches. For a single socket, the performance predictions are quite similar to a single socket on the Skylake system. The bottlenecks associated with each matrix are at the same level of the memory hierarchy, with the only exception being TSOPF_RS_b2383, which is foremost limited by the aggregate  traffic on Epyc and  traffic on Sandy Bridge and Skylake.

Finally, our performance predictions are compared to the measured performance in Table 12 for Sandy Bridge and Skylake, and in Table 13 for Epyc. For comparison, we also compute a “best case” upper bound using the best case estimate of the data traffic from Eq. (5) and the main memory bandwidth for the number of cores being used.

For matrices such as RM07R, HV15R, and Hardesty3, whose sparsity patterns are regular and lead to good load balancing, the best case estimates are very close to the measured performance, but our cache simulation estimates are at least of the same quality. On the other hand, the two methods produce significantly different results for matrices with irregular sparsity patterns that cause a large amount of additional, irregular data traffic. Examples of such irregular matrices are circuit5m, GL7d19, sx-stackoverflow, FullChip, Freescale1, and Lynx68. The performance predicted by the cache simulation is always within a factor of three of the measured performance. Compare this to the prediction based on the best case data traffic, which is off by a factor of three or more in 23 out of 72 cases on Intel and 8 out of 36 cases on Epyc. Moreover, among the most irregular matrices, for example, sx-stackoverflow, the best case prediction misses by more than a factor of ten.


Table 12. Comparison of measured and estimated performance (in Gflop/s) for CSR-based SpMV with Algorithm 2 on Sandy Bridge and Skylake. The “best case” predictions are based on the best case data traffic estimate Eq. (5) and main memory bandwidth. The “cache sim.” predictions produced by our method are the smallest of the upper bounds from Table 9, Table 10. In many cases, our method quantifies performance bottlenecks more accurately and attributes them to data transfers to and from various levels in the memory hierarchy.

Matrix	Sandy Bridge	Skylake
Meas.
Gflop/s	Best case	Cache sim.	Meas.
Gflop/s	Best case	Cache sim.
Est.
Gflop/s	Err.
%	Est.
Gflop/s	Err.
%		Est.
Gflop/s	Err.
%	Est.
Gflop/s	Err.
%
Single core
 TSOPF_RS_b2383	1.14	1.63	43	1.31	15	1.19	1.68	41	1.32	11
 spal_004	0.85	1.63	92	1.11	31	0.88	1.68	91	1.12	28
 RM07R	1.10	1.61	46	1.31	19	1.20	1.66	38	1.32	10
 relat9	0.51	1.23	142	0.67	32	0.66	1.27	91	0.67	1
 HV15R	1.12	1.61	44	1.31	17	1.20	1.66	39	1.32	10
 GL7d19	0.19	1.50	691	0.34	81	0.36	1.55	334	0.35	−2
 sx-stackoverflow	0.17	1.46	737	0.36	106	0.35	1.50	331	0.36	4
 FullChip	0.76	1.38	81	1.11	46	1.05	1.42	35	1.18	12
 Freescale1	0.74	1.25	69	1.03	39	1.09	1.29	19	1.07	−2
 circuit5M	0.93	1.61	73	1.18	27	1.12	1.66	48	1.25	12
 Hardesty3	0.86	1.23	44	1.22	43	1.18	1.27	7	1.26	7
 Lynx68	0.16	1.48	821	0.49	203	0.23	1.53	570	0.49	103
 Lynx68_reordered	0.59	1.48	149	0.55	−7	0.80	1.53	91	0.56	−31
Single socket
 TSOPF_RS_b2383	3.77	6.19	64	4.52	20	11.40	16.57	45	13.67	20
 spal_004	4.87	6.19	27	6.21	28	14.21	16.55	17	16.64	17
 RM07R	5.83	6.11	5	6.09	4	16.71	16.36	−2	15.92	−5
 relat9	2.60	4.69	80	3.92	51	8.97	12.54	40	11.71	31
 HV15R	5.09	6.14	21	6.00	18	15.07	16.44	9	16.04	6
 GL7d19	1.29	5.72	343	2.65	105	7.52	15.31	104	7.28	−3
 sx-stackoverflow	0.51	5.55	998	0.90	78	1.85	14.86	702	1.59	−14
 FullChip	1.88	5.24	178	3.00	59	3.51	14.01	299	4.35	24
 Freescale1	2.49	4.77	91	3.98	60	0.26	12.77	55	10.85	31
 circuit5M	1.47	6.12	317	1.86	27	2.20	16.38	646	2.27	3
 Hardesty3	3.58	4.68	31	4.66	30	9.09	12.52	38	12.46	37
 Lynx68	1.03	5.64	449	1.84	79	4.96	15.10	204	5.90	19
 Lynx68_reordered	3.38	5.64	67	4.35	29	11.65	15.10	30	12.96	11
Dual socket
 TSOPF_RS_b2383	5.57	11.72	110	6.22	12	13.36	30.63	129	16.64	25
 spal_004	9.00	11.71	30	12.35	37	26.58	30.60	15	33.29	25
 RM07R	10.61	11.57	9	11.79	11	31.81	30.24	−5	31.01	−3
 relat9	4.74	8.87	87	7.66	62	15.44	23.18	50	20.89	35
 HV15R	11.09	11.63	5	11.98	8	29.11	30.39	4	31.96	10
 GL7d19	2.52	10.83	329	5.06	101	13.46	28.31	110	12.72	−6
 sx-stackoverflow	0.76	10.51	1287	1.31	73	2.70	27.46	916	2.24	−17
 FullChip	2.43	9.91	309	4.19	73	4.71	25.91	450	4.96	5
 Freescale1	4.21	9.04	114	6.70	59	15.81	23.62	49	18.86	19
 circuit5M	2.32	11.59	400	3.36	45	3.87	30.28	683	3.85	−0
 Hardesty3	5.98	8.86	48	9.22	54	16.26	23.15	42	24.68	52
 Lynx68	1.98	10.68	438	3.48	75	9.31	27.91	200	11.48	23
 Lynx68_reordered	6.92	10.68	54	8.65	25	23.33	27.91	20	25.68	10

Table 13. Comparison of measured and estimated performance (in Gflop/s) for CSR-based SpMV with Algorithm 2 on Epyc. The “best case” predictions are based on the best case data traffic estimate Eq. (5) and main memory bandwidth. The “cache sim.” predictions produced by our method are the smallest of the upper bounds from Table 11.

Matrix	Epyc, single core	Epyc, single socket
Meas.
Gflop/s	Best case	Cache sim.	Meas.
Gflop/s	Best case	Cache sim.
Est.
Gflop/s	Err.
%	Est.
Gflop/s	Err.
%		Est.
Gflop/s	Err.
%	Est.
Gflop/s	Err.
%
 TSOPF_RS_b2383	0.89	1.44	62	0.88	−2	7.67	19.67	156	8.54	11
 spal_004	0.84	1.44	72	0.74	−12	14.23	19.65	38	17.16	21
 RM07R	1.00	1.43	43	0.88	−12	17.41	19.42	12	18.25	5
 relat9	0.88	1.09	24	0.44	−50	8.75	14.89	70	10.06	15
 HV15R	1.00	1.43	43	0.88	−12	15.95	19.52	22	18.36	15
 GL7d19	0.36	1.34	276	0.23	−35	5.15	18.18	253	6.08	18
 sx-stackoverflow	0.33	1.30	292	0.24	−28	1.37	17.64	1184	1.20	−13
 FullChip	0.84	1.22	46	0.88	5	2.69	16.64	519	3.19	19
 Freescale1	1.12	1.11	−1	0.83	−26	5.78	15.17	162	8.49	47
 circuit5M	1.12	1.43	27	0.84	−25	2.67	19.45	629	2.50	−6
 Hardesty3	1.38	1.09	−21	0.88	−36	8.58	14.87	73	14.56	70
 Lynx68	0.28	1.32	366	0.34	19	3.71	17.93	384	4.47	21
 Lynx68_reordered	1.03	1.32	27	0.36	−65	14.52	17.93	23	11.28	−22
In cases such as GL7d19, sx-stackoverflow, and Lynx68, where there are still discrepancies between the measured and performance predicted by the cache simulation on Sandy Bridge and Skylake, it is likely that the irregular sparsity patterns of these matrices result in the SpMV computation being limited more by memory latency than bandwidth. Regarding the single core estimates on Epyc, they appear to consistently underestimate the performance, suggesting that the per core bandwidths obtained from the indirect dot product kernel is actually lower than the real throughput that is achieved during CSR SpMV.

5.6. Estimating data traffic and performance for COO-based SpMV
The measured and estimated total data traffic volumes for COO SpMV with Algorithm 3 are shown in Table 14. To keep the discussion short, we show results for the case of a single core and a single socket on Sandy Bridge. For a single core, the results are quite similar to those presented in Table 5 for CSR SpMV in terms of accuracy. The  and  traffic is underestimated for a few matrices, likely due to conflict misses. In the single socket case, the data traffic for the private L1 and L2 caches is often severely underestimated. Notice, in particular, the matrix spal_004, which has very few rows, and is therefore likely to suffer from false sharing. That is, to maintain cache coherency, two different CPU cores writing to the same cache block may cause the cache block to be transferred from a shared level of the memory hierarchy for every write. Since the L3 cache is shared by all the cores on a single socket, there is no false sharing for the  traffic, and the estimates are quite accurate. We suspect that in the dual socket case, requirements for cache coherence between the L3 caches of the two sockets will lead to false sharing and similar issues also for the  traffic.

Finally, the performance of the COO SpMV kernel on Sandy Bridge is shown in Table 15 together with performance predictions based on best-case estimates of the memory traffic as well as predictions based on the cache simulation. Five of the matrices benefit somewhat from the cache simulation, whereas there is almost no difference between the two predictions for the rest of the matrices. For a single core, the performance predictions from the cache simulation are within fifty percent of the measured performance for all except four of the matrices. However, the performance impact of going from a single core to a full socket is far from ideal due to the cost of using atomic writes. As a result, the performance predictions become less accurate in general. Moreover, for spal_004 and FullChip there is a significant slowdown as performance is crippled by false sharing. Since our performance model does not account for false sharing, the performance estimates are not even close in these cases.


Table 14. Estimated and measured total data traffic (in MiB) on Intel Sandy Bridge Xeon E5-2650 for COO-based SpMV with Algorithm 3.

Matrix	L1   L2	L2  L3	L3  DRAM
Meas.
MiB	Est.
MiB	Err.
%	Meas.
MiB	Est.
MiB	Err.%	Meas.
MiB	Est.
MiB	Err.
%
Single core
 TSOPF_RS_b2383	296	289	−2.4	281	289	2.8	247	248	0.4
 spal_004	776	736	−5.2	766	735	−4.0	706	707	0.1
 RM07R	660	626	−5.2	621	595	−4.2	582	579	−0.5
 relat9	2902	2143	−26.2	2518	2045	−18.8	1357	1300	−4.2
 HV15R	4768	4585	−3.8	4604	4469	−2.9	4380	4361	−0.4
 GL7d19	3669	2849	−22.3	2843	2773	−2.5	687	686	−0.1
 sx-stackoverflow	3646	2657	−27.1	2560	2371	−7.4	772	791	2.5
 FullChip	610	583	−4.4	590	558	−5.4	546	540	−1.1
 Freescale1	453	440	−2.9	459	434	−5.4	405	404	−0.2
 circuit5M	1421	1390	−2.2	1174	1164	−0.9	1140	1150	0.9
 Hardesty3	880	861	−2.2	806	851	5.6	734	738	0.5
 Lynx68	9453	5960	−37.0	7291	5933	−18.6	4823	4757	−1.4
 Lynx68_reordered	5670	5556	−2.0	3590	3464	−3.5	1831	1830	−0.1
Single socket
 TSOPF_RS_b2383	357	289	−19.0	377	289	−23.3	247	247	0.0
 spal_004	6640	736	−88.9	15514	735	−95.3	705	707	0.3
 RM07R	666	626	−6.0	632	595	−5.9	591	588	−0.5
 relat9	2949	2143	−27.3	2567	2045	−20.3	1702	1603	−5.8
 HV15R	4789	4585	−4.3	4626	4469	−3.4	4488	4426	−1.4
 GL7d19	3834	2849	−25.7	3006	2773	−7.8	934	885	−5.2
 sx-stackoverflow	4279	2657	−37.9	3325	2371	−28.7	819	823	0.5
 FullChip	1672	583	−65.1	2237	558	−75.1	545	534	−2.0
 Freescale1	457	440	−3.7	466	434	−6.9	407	403	1.0
 circuit5M	1509	1390	−7.9	1333	1164	−12.7	1140	1157	1.5
 Hardesty3	881	861	−2.3	809	851	5.2	734	738	0.5
 Lynx68	9474	5960	−37.1	7343	5933	−19.2	4863	4803	−1.2
 Lynx68_reordered	5686	5556	−2.3	3602	3464	−3.8	1891	1876	−0.8

Table 15. Comparison of measured and estimated performance (in Gflop/s) for COO-based SpMV with Algorithm 3 on Sandy Bridge. The “best case” predictions are based on a best case estimate of the data traffic, whereas the “cache sim.” predictions are based on estimates of memory traffic produced by our cache simulation for each level of the memory hierarchy.

Matrix	Single core	Single socket
Meas.
Gflop/s	Best case	Cache sim.	Meas.
Gflop/s	Best case	Cache sim.
Est.
Gflop/s	Err.
%	Est.
Gflop/s	Err.
%		Est.
Gflop/s	Err.
%	Est.
Gflop/s	Err.
%
TSOPF_RS_b2383	0.86	1.22	42	1.22	42	0.99	4.65	370	4.65	370
spal_004	0.83	1.22	47	1.22	47	0.33	4.65	1309	4.65	1309
RM07R	0.84	1.21	44	1.21	44	1.30	4.62	255	4.53	248
relat9	0.19	1.05	453	0.46	142	0.65	4.00	515	1.73	166
HV15R	0.85	1.22	44	1.21	42	1.31	4.63	253	4.55	247
GL7d19	0.17	1.16	582	0.33	94	0.69	4.43	542	2.57	272
sx-stackoverflow	0.16	1.14	612	0.35	119	0.55	4.35	691	2.73	396
FullChip	0.64	1.10	72	0.92	44	0.10	4.19	4090	3.54	3440
Freescale1	0.62	1.04	68	0.87	40	1.00	3.95	295	3.33	233
circuit5M	0.74	1.12	51	0.97	31	0.99	4.26	330	3.66	270
Hardesty3	0.76	1.02	34	1.02	34	1.31	3.90	198	3.90	198
Lynx68	0.15	1.15	667	0.44	193	0.61	4.39	620	1.65	170
Lynx68_reordered	0.39	1.15	195	0.51	31	0.79	4.39	456	4.05	413
6. Related work
The cache simulation method we have presented builds on analytical cache models [1] and trace-driven memory simulation [35], both of which are well known methods for studying cache performance. In their survey, Uhlig and Mudge [35] compare a number of advanced tools for trace-driven memory simulation that cope with various cache configurations, such as associativity and replacement policies. Our approach is to develop a model that is as simple as possible, but accurate enough to diagnose potential performance issues. Our cache simulation method is somewhat related to the model described by Aho, Denning, and Ullman [2] for page replacements in a virtual memory computer, which is similarly based on counting page replacements generated by a sequence of memory references. In addition, we draw heavily on the ideal-cache model [12], though we explicitly incorporate multi-level hierarchies and shared caches. Shared caches have previously been studied based on statistical models that use high-level information such as a thread’s average fetch rate [31].

Regarding performance models, Langguth et al. [21] developed a performance model for irregular applications that was used to study a special case of SpMV, where matrices were derived from a finite volume method on an unstructured, tetrahedral mesh. Our current work may be seen as a continuation that provides a general method for estimating data traffic volumes in a memory hierarchy.

In the context of SpMV, Heras et al. [15] have used a cache simulation technique for conflict misses in direct-mapped and set-associative caches. Also, Yzelman and Bisseling [42] have used similar methods to evaluate sparse matrix re-ordering strategies for SpMV, though their simulator is limited to a single cache. Goumas et al. [13] and Williams et al. [40] have performed experimental evaluations of SpMV performance on various multi-core CPU architectures, identifying several potential performance bottlenecks that depend on matrix structure as well as characteristics of the computing hardware. While Goumas et al. produce a set of guidelines for optimising SpMV kernels, Williams et al. rely on automatic tuning to select appropriate optimisations. In both cases, memory bandwidth is identified as a significant performance bottleneck, but neither of these studies attempt to quantify the impact of data traffic from main memory or any other level of the memory hierarchy. In other work, Vuduc et al. [38] have modelled SpMV performance based on cache miss estimates, sustainable memory bandwidth and memory access latencies. However, the data traffic volume is based on a best case scenario that is often too optimistic, especially for matrices with highly irregular sparsity patterns. Malossi et al. [24] proposed to use machine learning techniques to characterise SpMV performance, though the disadvantages are a potentially expensive re-training step needed to calibrate the model for each new hardware architecture, and also that the method does not identify performance bottlenecks explicitly, even though it correlates sparse matrix features with SpMV performance.

7. Conclusion
The performance of irregular, bandwidth-limited computations, such as SpMV, is dictated by data transfers between levels of a CPU’s memory hierarchy. Even though it is fairly easy to acquire worst- and best-case estimates of the data traffic, these estimates are not always sufficient for locating and quantifying bottlenecks because a precise characterisation of irregular data traffic is missing. We have presented a cache simulation method that accurately quantifies data traffic in a multi-core CPU’s memory hierarchy in the presence of irregular memory access patterns. Further, we have shown that this method extracts the most important contributions to data traffic while only requiring basic hardware characteristics to be specified, such as the size of each cache and its cache lines. Consequently, our method should be applicable to other hardware architectures and cache hierarchies than the multi-core systems we have considered here.

Regarding SpMV, future efforts could use the quantitative performance model presented here to evaluate specific optimisations, such as matrix re-orderings. The data traffic estimates produced by the cache simulation method could potentially be used to tune algorithms and optimisations, though this may place some requirements on how fast such simulations can be carried out. Although we have deliberately focused on a pair of simple and well known SpMV kernels, the presented methods are also applicable to more advanced SpMV algorithms and other irregular computations. For example, one might consider problems related to unstructured meshes, such as the assembly of sparse matrices in finite element methods. These algorithms also suffer from irregular memory accesses, and, moreover, they are often produced through automated code generation and therefore require extensive optimisation. Many relevant optimisations represent trade-offs between different amounts of data traffic and computation, so that more accurate data traffic estimates may help to choose suitable optimisations