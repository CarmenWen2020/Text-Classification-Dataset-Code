ABSTRACT
We design a novel, communication-efficient, failure-robust protocol for secure aggregation of high-dimensional data. Our protocol
allows a server to compute the sum of large, user-held data vectors from mobile devices in a secure manner (i.e. without learning
each user’s individual contribution), and can be used, for example,
in a federated learning setting, to aggregate user-provided model
updates for a deep neural network. We prove the security of our
protocol in the honest-but-curious and active adversary settings,
and show that security is maintained even if an arbitrarily chosen
subset of users drop out at any time. We evaluate the efficiency
of our protocol and show, by complexity analysis and a concrete
implementation, that its runtime and communication overhead remain low even on large data sets and client pools. For 16-bit input
values, our protocol offers 1.73× communication expansion for 2
10
users and 2
20-dimensional vectors, and 1.98× expansion for 2
14
users and 2
24-dimensional vectors over sending data in the clear

KEYWORDS
privacy-preserving protocols, secure aggregation, machine learning,
federated learning
1 INTRODUCTION
Machine learning models trained on sensitive real-world data promise
improvements to everything from medical screening [46] to disease outbreak discovery [37]. And the widespread use of mobile
devices means even richer—and more sensitive—data is becoming
available [35].
However, large-scale collection of sensitive data entails risks. A
particularly high-profile example of the consequences of mishandling sensitive data occurred in 1988, when the video rental history
of a nominee for the US Supreme Court was published without his
consent [4]. The law passed in response to that incident remains
relevant today, limiting how online video streaming services can
use their user data [42].
This work outlines an approach to advancing privacy-preserving
machine learning by leveraging secure multiparty computation
(MPC) to compute sums of model parameter updates from individual users’ devices in a secure manner. The problem of computing
a multiparty sum where no party reveals its update in the clear—
even to the aggregator—is referred to as Secure Aggregation. As
described in Section 2, the secure aggregation primitive can be used
to privately combine the outputs of local machine learning on user
devices, in order to update a global model. Training models in this
way offers tangible benefits—a user’s device can share an update
knowing that the service provider will only see that update after it
has been averaged with those of other users.
The secure aggregation problem has been a rich area of research: different approaches include works based on generic secure
† Research performed during an internship at Google.
Session E5: Privacy-Preserving Analytics CCS’17, October 30-November 3, 2017, Dallas, TX, USA 1175
multi-party computation protocols, works based on DC-nets, works
based on partially- or fully-homomorphic threshold encryption, and
works based on pairwise masking. We discuss these existing works
in more detail in Section 9, and compare them to our approach.
We are particularly focused on the setting of mobile devices,
where communication is extremely expensive, and dropouts are
common. Given these constraints, we would like our protocol to
incur no more than twice as much communication as sending the
data vector to be aggregated in the clear, and would also like the
protocol to be fully robust to users dropping at any point. We believe
that previous works do not address this mixture of constraints,
which is what motivates our work.
1.1 Our Results
We present a protocol for securely computing sums of vectors,
which has a constant number of rounds, low communication overhead, robustness to failures, and which requires only one server
with limited trust. In our design the server has two roles: it routes
messages between the other parties, and it computes the final result. We present two variants of the protocol: one is more efficient
and can be proven secure against honest but curious adversaries,
in the plain model. The other guarantees privacy against active
adversaries (including an actively adversarial server), but requires
an extra round, and is proven secure in the random oracle model. In
both cases, we can show formally that the server only learns users’
inputs in aggregate, using a simulation-based proof as is standard
for MPC protocols. Both variants we present are practical and we
present benchmark results from our prototype implementation.
1.2 Organization
In Section 2 we describe the machine learning application that
motivates this work. In Section 3 we review the cryptographic
primitives we use in our protocol. We then proceed to give a highlevel overview of our protocol design in Section 4, followed by a
formal protocol description in Section 5. In Section 6 we prove security against honest-but-curious (passive) adversaries and include
a high-level discussion of privacy against active adversaries.In Section 7, we give performance numbers based both on theoretical
analysis as well as on a prototype implementation. Finally, we discuss some issues surrounding practical deployments and future
work in Section 8 and conclude with a discussion of related work
in Section 9.
2 SECURE AGGREGATION FOR FEDERATED
LEARNING
Consider training a deep neural network to predict the next word
that a user will type as she composes a text message. Such models
are commonly used to to improve typing efficacy for a phone’s onscreen keyboard [30]. A modeler may wish to train such a model
on all text messages across a large population of users. However,
text messages frequently contain sensitive information; users may
be reluctant to upload a copy of them to the modeler’s servers.
Instead, we consider training such a model in a Federated Learning
setting, wherein each user maintains a private database of her text
messages securely on her own mobile device, and a shared global
model is trained under the coordination of a central server based
upon highly processed, minimally scoped, ephemeral updates from
users [43, 50].
These updates are high-dimensional vectors based on information from the user’s private database. Training a neural net is typically done by repeatedly iterating over these updates using a variant
of a mini-batch stochastic gradient descent rule [15, 29]. (See Appendix B for details.)
Although each update is ephemeral and contains no more (and
typically significantly less) information than the user’s private
database, a user might still be concerned about what information
remains. In some circumstances, it is possible to learn invididual
words that a user has typed by inspecting that user’s most recent
update. However, in the Federated Learning setting, the server
does not need to access any individual user’s update in order to
perform stochastic gradient descent; it requires only the elementwise weighted averages of the update vectors, taken over a random
subset of users. Using a Secure Aggregation protocol to compute
these weighted averages1 would ensure that the server may learn
only that one or more users in this randomly selected subset wrote
a given word, but not which users.
Federated Learning systems face several practical challenges.
Mobile devices have only sporadic access to power and network
connectivity, so the set of users participating in each update step
is unpredictable and the system must be robust to users dropping
out. Because the neural network may be parameterized by millions of values, updates may be large, representing a direct cost
to users on metered network plans. Mobile devices also generally
cannot establish direct communications channels with other mobile
devices (relying on a server or service provider to mediate such
communication) nor can they natively authenticate other mobile
devices.
Thus, Federated Learning motivates a need for a Secure Aggregation protocol that:
(1) operates on high-dimensional vectors
(2) is highly communication efficient, even with a novel set of
users on each instantiation
(3) is robust to users dropping out
(4) provides the strongest possible security under the constraints of a server-mediated, unauthenticated network
model
3 CRYPTOGRAPHIC PRIMITIVES
In this section, we discuss the cryptographic primitives and assumptions needed for our construction.
3.1 Secret Sharing
We rely on Shamir’s t-out-of-n Secret Sharing [48], which allows a
user to split a secret s into n shares, such that any t shares can be
used to reconstruct s, but any set of at most t − 1 shares gives no
information about s.
The scheme is parameterized over a finite field F of size at least
l > 2
k
(where k is the security parameter of the scheme), e.g.
F = Zp for some large public prime p. We note that such a large
field size is needed because our scheme requires clients to secret
1Computing a secure weighted average given a secure sum operation is straightfoward;
for detail, see Appendix B.
Session E5: Privacy-Preserving Analytics CCS’17, October 30-November 3, 2017, Dallas, TX, USA 1176
Cloud-Hosted Mobile Intelligence Federated Learning Federated Learning with Secure Aggregation
Figure 1: Left: In the cloud-centric approach to machine intelligence, user devices interact with cloud-hosted models, generating logs that can be used as training examples. The logs from many users are combined and used to improve the model, which
is then used to serve future user requests. Middle: In Federated Learning, machine intelligence models are shipped to users’
devices where they are both evaluated and trained locally. Summaries of improved models are shared with the server, where
they are aggregated into a new model and deployed to user devices. Right: When Secure Aggregation is added to Federated
Learning, the aggregation of model updates is logically performed by the virtual, incorruptible third party induced by the
secure multiparty communication, so that the cloud provider learns only the aggregated model update.
share their secret keys (whose length must be proportional to the
security parameter for the security proof to go through). We also
assume that integers 1,. . . ,n (which will be used to denote users
in the protocol) can be identified with distinct field elements in F.
Given these parameters, the scheme consists of two algorithms. The
sharing algorithm SS.share(s,t,U) → {(u,su )}u ∈U takes as input
a secret s, a set U of n field elements representing user IDs, and a
threshold t ≤ |U |; it produces a set of shares su , each of which is
associated with a different u ∈ U. The reconstruction algorithm
SS.recon({(u,su )}u ∈V ,t) → s takes as input the threshold t and
the shares corresponding to a subset V ⊆ U such that |V |≥ t, and
outputs a field element s.
Correctness requires that ∀s ∈ F,∀t,n with 1 ≤ t ≤ n, ∀U ⊆ F
where |U |= n, if {(u,su )}u ∈U ← SS.share(s,t,U), V ⊆ U and
|V |≥ t, then SS.recon({(u,su )}u ∈V ,t) = s. Security requires that
∀s,s
′ ∈ F and any V ⊆ U such that |V |< t:
{{(u,su )}u ∈U ← SS.share(s,t,U) : {(u,su )}u ∈V } ≡
{{(u,su )}u ∈U ← SS.share(s
′
,t,U) : {(u,su )}u ∈V }
where “≡” denotes that the two distributions are identical.
3.2 Key Agreement
Key Agreement consists of a tuple of algorithms
(KA.param,KA.gen,KA.agree). The algorithm KA.param(k) →
pp produces some public parameters (over which our scheme will
be parameterized). KA.gen(pp) → (s
SK
u
,s
PK
u
) allows any party u
to generate a private-public key pair. KA.agree(s
SK
u
,s
PK
v
) → su,v
allows any user u to combine their private key s
SK
u with the public
key s
PK
v
for any v (generated using the same pp), to obtain a private
shared key su,v between u and v.
The specific Key Agreement scheme we will use is Diffie-Hellman
key agreement [19], composed with a hash function. More specifically, KA.param(k) → (G
′
,д,q,H) samples group G
′ of prime order q, along with a generator д, and a hash functionH
2
; KA.gen(G
′
,
д,q,H) → (x,д
x
) samples a random x ← Zq as the secret key s
SK
u
,
and д
x
as the public key s
PK
u
; and KA.agree(xu ,д
xv ) → su,v outputs su,v = H((д
xv )
xu ).
2
In practice, one can use SHA-256.
Correctness requires that, for any key pairs generated by users u
and v (using KA.gen and the same parameters pp), KA.agree(s
SK
u
,
s
PK
v
) = KA.agree(s
SK
v
,s
PK
u
). For security, in the honest but curious
model, we want that for any adversary who is given two honestly
generated public keys s
PK
u
and s
PK
v
(but neither of the corresponding secret keys s
SK
u or s
SK
v
), the shared secret su,v computed from
those keys is indistinguishable from a uniformly random string.
This exactly mirrors the Decisional Diffie-Hellman (DDH) assumption, which we recall below:
Definition 3.1 (Decisional Diffie-Hellman assumption). Let G(k) →
(G
′
,д,q,H) be an efficient algorithm which samples a group G
′ of
order q with generator д, as well as a function H : {0,1}
∗ → {0,1}
k
.
Consider the following probabilistic experiment, parameterized by
a PPT adversary M, a bit b and a security parameter k.
DDH-Expb
G,M
(k):
(1) (G
′
,д,q,H) ← G(k)
(2) a ← Zq;A ← д
a
(3) b ← Zq; B ← д
b
(4) if b = 1, s ← H(д
ab ), else s
$← {0,1}
k
(5) M(G
′
,д,q,H,A,B,s) → b
′
(6) Output 1 if b = b
′
, 0 o/w.
The advantage of the adversary is defined as
Adv
DDH
G,M
(k) := |Pr[ DDH-Exp1
G,M
(k) = 1]−
Pr[DDH-Exp0
G,M
(k) = 1]|
We say that the Decisional Diffie-Hellman assumption holds for
G if for all PPT adversaries M, there exists a negligible function ϵ
such that Adv
DDH
G,M
(k) ≤ ϵ(k).
Note that, traditionally, the Diffie-Hellman assumption does not
directly involve a hash function H (i.e. line step 4 is substituted with
“if b = 1,s ← д
ab , else s
$← G
′
”), and therefore to get from a random
element of the group G
′
to a uniformly random string (which is
necessary to be used as the seed for a PRG, or to sample secret
keys for other primitives), one has to compose д
ab with a secure
randomness extractor (which composes well with this specific key
agreement operation). For simplicity, we choose to incorporate such
an extractor function H in the assumption.
Session E5: Privacy-Preserving Analytics CCS’17, October 30-November 3, 2017, Dallas, TX, USA 1177
In order to prove security against active adversaries (Theorem
6.5), we need a somewhat stronger security guarantee for Key
Agreement, namely that an adversary who is given two honestly
generated public keys s
PK
u
and s
PK
v
, and also the ability to learn
KA.agree(s
SK
u
,s
PK ) and KA.agree(s
SK
v
,s
PK ) for any s
PK s of its
choice (but different from s
PK
u
and s
PK
v
), still cannot distinguish
su,v from a random string. In order to get this stronger property,
we need to rely on a slight variant of the Oracle Diffie-Hellman
assumption (ODH) [2], which we call Two Oracle Diffie-Hellman
assumption (2ODH):
Definition 3.2 (Two Oracle Diffie-Hellman assumption (2ODH)).
Let G(k) → (G
′
,д,q,H) be an efficient algorithm which samples
a group G
′ of order q with generator д, as well as a function
H : {0,1}
∗ → {0,1}
k
. Consider the following probabilistic experiment, parameterized by a PPT adversary M, a bit b and a security
parameter k.
2ODH-Expb
G,M
(k):
(1) (G
′
,д,q,H) ← G(k)
(2) a ← Zq;A ← д
a
(3) b ← Zq; B ← д
b
(4) if b = 1, s ← H(д
ab ), else s
$← {0,1}
k
(5) M Oa (·),Ob (·)
(G
′
,д,q,H,A,B,s) → b
′
(6) Output 1 if b = b
′
, 0 o/w.
where Oa(X) returns H(X
a
) on any X ̸= B (and an error on input B)
and similarly Ob
(X) returns H(X
b
) on any X ̸= A. The advantage
of the adversary is defined as
Adv
2O DH
G,M
(k) := |Pr[ 2ODH-Exp1
G,M
(k) = 1]−
Pr[2ODH-Exp0
G,M
(k) = 1]|
We say that the Two Oracle Diffie-Hellman assumption holds for
G if for all PPT adversaries M, there exists a negligible function ϵ
such that Adv
2O DH
G,M
(k) ≤ ϵ(k).
This assumption can be directly used to prove the security property we need for Key Agreement: the two oracles Oa(·),Ob
(·) formalize the ability of the adversary M to learn KA.agree(s
SK
u
,s
PK )
and KA.agree(s
SK
v
,s
PK ) for different s
PK , and the negligible advantage of M in the above game corresponds to an inability to
distinguish between s = su,v ← H(д
ab ), and s
$← {0,1}
k
.
3.3 Authenticated Encryption
(Symmetric) Authenticated Encryption combines confidentiality
and integrity guarantees for messages exchanged between two
parties. It consists of a key generation algorithm that outputs a
private key3
, an encryption algorithm AE.enc that takes as input
a key and a message and outputs a ciphertext, and a decryption
algorithm AE.dec that takes as input a ciphertext and a key and
outputs either the original plaintext, or a special error symbol ⊥.
For correctness, we require that for all keys c ∈ {0,1}
k
and all
messages x, AE.dec(c,AE.enc(c,x)) = x. For security, we require
indistinguishability under a chosen plaintext attack (IND-CPA) and
ciphertext integrity (IND-CTXT) as defined in [7]. Informally, the
3Without loss of generality, we make the simplifying assumption that the key generation algorithm samples keys as uniformly random strings.
guarantee is that for any adversary M that is given encryptions of
messages of its choice under a randomly sampled key c (where c is
unknown to M), M cannot distinguish between fresh encryptions
under c of two different messages, nor can M create new valid
ciphertexts (different from the ones it received) with respect to c
with better than negligible advantage.
3.4 Pseudorandom Generator
We require a secure Pseudorandom Generator [9, 54] PRG that
takes in a uniformly random seed of some fixed length, and whose
output space is[0,R)
m (i.e. the input space for the protocol). Security
for a Pseudorandom Generator guarantees that its output on a
uniformly random seed is computationally indistinguishable from
a uniformly sampled element of the output space, as long as the
seed is hidden from the distinguisher.
3.5 Signature Scheme
The protocol relies on a standard UF-CMA secure signature scheme
(SIG.gen,SIG.sign,SIG.ver). The key generation algorithm
SIG.gen(k) → (d
PK ,d
SK ) takes as input the security parameter
and outputs a secret key d
SK and a public key d
PK ; the signing
algorithm SIG.sign(d
SK ,m) → σ takes as input the secret key and
a message and outputs a signature σ; the verification algorithm
SIG.ver(d
PK ,m,σ) → {0,1} takes as input a public key, a message
and a signature, and returns a bit indicating whether the signature
should be considered valid. For correctness, we require that ∀m,
Pr[(d
PK
,d
SK ) ←SIG.gen(k),σ ← SIG.sign(d
SK
,m) :
SIG.ver(d
PK
,m,σ) = 1] = 1
Security demands that no PPT adversary, given a fresh honestly
generated public key and access to an oracle producing signatures
on arbitrary messages, should be able to produce a valid signature
on a message on which the oracle was queried on with more than
negligible probability.
3.6 Public Key Infrastructure
To prevent the server from simulating an arbitrary number of clients
(in the active-adversary model), we require the support of a public
key infrastructure that allows clients to register identities, and sign
messages using their identity, such that other clients can verify this
signature, but cannot impersonate them. In this model, each party
u will register 
u,d
PK
u

to a public bulletin board during the setup
phase. The bulletin board will only allow parties to register keys
for themselves, so it will not be possible for the attacking parties to
impersonate honest parties.
4 TECHNICAL INTUITION
We note that our protocol is quite similar to the work of Ács and
Castelluccia [3], and we give a detailed comparison between our
approaches in Section 9. As in their protocol, we divide the parties
into two classes: a single server S that aggregates inputs from n
client parties U. Each user4 u ∈ U holds a private vector xu of
dimension m; for simplicity we assume that the elements of xu
and P
u ∈U xu are in ZR for some R. The goal of the protocol is to
4We use the terms user and client interchangeably.
Session E5: Privacy-Preserving Analytics CCS’17, October 30-November 3, 2017, Dallas, TX, USA 1178
compute P
u ∈U xu in a secure fashion: at a high level, we guarantee
that the server only learns a sum of the clients’ inputs containing
contributions from at least a large fraction of the users and that the
users learn nothing.
4.0.1 Masking with One-Time Pads. The first observation is that
P
u ∈U xu can be computed with perfect secrecy if xu is masked in
a particular way. Assume a total order on users, and suppose each
pair of users (u,v), u < v agree on some random vector su,v . If u
adds this to xu and v subtracts it from xv , then the mask will be
canceled when their vectors are added, but their actual inputs will
not be revealed. In other words, each user u computes:
yu = xu +
X
v ∈U:u<v
su,v −
X
v ∈U:u>v
sv,u (mod R)
and sends yu to the server, and the server computes:
z =
X
u ∈U
yu
=
X
u ∈U
*
.
,
xu +
X
v ∈U:u<v
su,v −
X
v ∈U:u>v
sv,u
+
/
-
=
X
u ∈U
xu (mod R)
There are two shortcomings to this approach. The first is that
the users must exchange the random vectors su,v , which, if done
naively, would require quadratic communication overhead (|U |×|x |).
The second is that there is no tolerance for a party failing to complete the protocol: if a user u drops out after exchanging vectors
with other users, but before submitting yu to the server, the vector
masks associated with u would not be canceled in the sum z.
4.0.2 Efficient Communication and Handling Dropped Users. We
notice that we can reduce the communication by having the parties
agree on common seeds for a pseudorandom generator (PRG) rather
than on the entire mask su,v . These shared seeds will be computed
by having the parties broadcast Diffie-Hellman public keys and
engaging in a key agreement.
One approach to handling dropped-out users would be to notify
the surviving users of the drop-out, and to have them each reply
with the common seed they computed with the dropped user. This
approach still has a problem: additional users may drop out in the
recovery phase before replying with the seeds, which would thus
require an additional recovery phase for the newly dropped users’
seeds to be reported, and so on, leading the number of rounds up
to at most the number of users.
We resolve this problem by using a threshold secret sharing
scheme and having each user send shares of their Diffie-Hellman
secret to all other users. This allows pairwise seeds to be recovered
even if additional parties drop out during the recovery, as long as
some minimum number of parties (equal to the threshold) remain
alive and respond with the shares of the dropped users’ keys.
This approach solves the problem of unbounded recovery rounds,
but still has an issue: there is a possibility that a user’s data might
accidentally be leaked to the server. Consider a scenario where a
user u is too slow in sending her yu to the server. The server assumes that the user has dropped, and asks all other users to reveal
Generate DH keypairs <cu ,cu > and <su ,su >
User Server
Round 0:
Advertise Keys
Round 1:
Share Keys
Round 2:
Masked Input
Collection
Round 3:
Consistency
Checks
Send signed public keys <cu, su , σu> PK PK
Broadcast list of received public keys to all users in u1
Validate signatures, generate bu and compute su,v
Compute t-out-of-n secret shares for bu and su
Send encrypted shares of bu and su
Forward received encrypted shares
Compute masked input yu
Send yu
Wait for enough users u3⊆ u2
Send a list of at least t survived users: u3 ⊆ u2
Collect signatures
SK
SK PK
SK
Sign u3 and reply with a signature σu
SK PK
Round 4:
Unmasking
Send a list {v, σv}of survived users from u4 ⊆ u3
Compute x (the final aggregated value)
Reconstruct secrets
Abort if |u4| < t, validate signatures PK Send shares of bu for alive users and su for dropped
Abort if |u3| < t
'
'
Wait for enough users u1⊆ u
Wait for enough users u2⊆ u1
Figure 2: High-level view of our protocol. Red,
underlined parts are required to guarantee security in
the active-adversary model (and not necessary in the
honest-but-curious one).
their shares of u’s secret key, in order to remove u’s uncancelled
masks from z. However, just after receiving these shares and computing each of the su,v values, the server may receive the delayed
yu from u. The server is now able to remove all the masks from
yu , and learn xu in the clear, breaking security for u. Moreover, an
adversarial server in the active model can similarly learn xu simply
by lying about whether user u has dropped out.
4.0.3 Double-Masking to Protect Security. To resolve this new
security problem, we introduce a double-masking structure that
protects xu even when the server can reconstruct u’s masks.
First, each user u samples an additional random seed bu during
the same round as the generation of the su,v values. During the
secret sharing round, the user also generates and distributes shares
of bu to each of the other users. When generating yu , users also
add this secondary mask:
yu = xu + PRG (bu )
+
X
v ∈U:u<v
PRG
su,v

−
X
v ∈U:u>v
PRG
sv,u

(mod R)
During the recovery round, the server must make an explicit
choice with respect to each user u: from each surviving member
v, the server can request either a share of the common secret su,v
associated with u or a share of the bu for u; an honest user v will
never reveal both kinds of shares for the same user. After gathering
at least t shares of su,v for all dropped users and t shares of bu for
all surviving users, the server can subtract off the remaining masks
to reveal the sum.
4.0.4 Putting it all Together. We summarize our protocol in Figure 2 and its asymptotic costs in Figure 3. The computational cost
is quadratic for the users, and cubic for the server. As the size of the
data vector gets large, the communication and storage overhead for
Session E5: Privacy-Preserving Analytics CCS’17, October 30-November 3, 2017, Dallas, TX, USA 11  
User Server5
computation O(n
2 + mn) O(mn2
)
communication O(n + m) O(n
2 + mn)
storage O(n + m) O(n
2 + m)
Figure 3: Cost summary for the protocol.
each of the clients and the server using our protocol approaches a
multiplicative constant over sending the data in the clear.
5 A PRACTICAL SECURE AGGREGATION
PROTOCOL
The protocol is run (in a synchronous network) between a server
and a set of n users, and consists of four rounds. Each user u holds
as input a vector xu (of equal lengthm) consisting of elements from
ZR for some R. The server has no input, but can communicate with
the users through secure (private and authenticated) channels. At
any point, users can drop out of the protocol (in which case they
stop sending messages completely), and the server will be able to
produce a correct output as long as t of them survive until the
last round. To simplify the notation we assume that each user u
is assigned a unique “logical identity” (also denoted with u) in the
form of an integer between 1 and n, so that no two honest users
share the same index6
.
A complete description is provided in Figure 4. We stress that, in
the figure, when we say that the server “collects messages from at
least t users”, we mean that the server receives the messages from
all users that have not dropped out/aborted in that round (recall
that we prove our results in the synchronous setting), and aborts
if the number of messages received is less than t. In a practical
implementation, the server would wait until a specified timeout
(considering all users who did not respond in time to have dropped
out), and abort itself if not enough messages are received before
such timeout.
To prove security in the active adversary model, we also assume
the existence of a Public Key Infrastructure, which for simplicity
we abstract away by assuming all clients receive as input (from a
trusted third party) public signing keys for all other clients.
Overall, the protocol is parameterized over a security parameter
k, which can be adjusted to bound the success probability of any
attacker. In all theorems, we implicitly assume that the number
of clients n is polynomially bounded in the security parameter.
Moreover, some of the primitives also require additional global
parameters.
We note that Figure 4 presents both variants of the protocol:
in the honest but curious case, since all parties are following the
protocol honestly, we can avoid the use of signatures and the need
for a PKI (which, most notably, allows us to avoid the ConsistencyCheck round entirely).
6 SECURITY ANALYSIS
In our security arguments, we will make use of the following technical lemma. It says that if users’ values have uniformly random
5The server can reconstruct n secrets from aligned (t, n)-Shamir shares in O(t
2 + nt)
by caching Lagrange coefficients; see section 7.2 for details.
6These identities will be bound to the users’ keys by a PKI. We rely on this in the
active-adversary setting.
pairwise masks added to them, then the resulting values look uniformly random, conditioned on their sum being equal to the sum
of the users’ values. In other words, the pairwise masks hide all
information about users’ individual inputs, except for their sum.
Lemma 6.1. Fix n, m, R, U with |U |= n, and {xu }u ∈U where
∀u ∈ U,xu ∈ Z
m
R
. Then,
{{pu,v
$← Z
m
R
}u<v , pu,v := −pv,u∀u > v
: {xu +
X
v ∈U\ {u }
pu,v (mod R)}u ∈U }
≡
{{wu
$← Z
m
R
}u ∈U s.t. X
u ∈U
wu =
X
u ∈U
xu (mod R)
: {wu }u ∈U }
where “≡” denotes that the distributions are identical.
We omit the proof, noting that it can be proved easily by induction on n.
6.1 Honest but Curious Security
Here, we argue that our protocol is a secure multiparty computation
in the honest but curious setting, regardless of how and when parties abort. In particular, we prove that when executing the protocol
with threshold t, the joint view of the server and any set of less
than t (honest) users does not leak any information about the other
users’ inputs, besides what can be inferred from the output of the
computation. Before formally stating our result, we introduce some
notation.
We will consider executions of our secure aggregation protocol
where the underlying cryptographic primitives are instantiated
with security parameter k, a server S interacts with a set U of n
users (denoted with logical identities 1,. . . ,n) and the threshold is
set to t. In such executions, users might abort at any point during
the execution, and we denote with Ui the subset of the users that
correctly sent their message to the server at round i − 1, such that
U ⊇ U1 ⊇ U2 ⊇ U3 ⊇ U4 ⊇ U5. For example, users in U2 \ U3
are exactly those that abort before sending the message to the server
in Round 2, but after sending the message of Round 1. If Round
ConsistencyCheck has been omitted, define U4 := U3.
Denote the input of each useru with xu , and with xU′ = {xu }u ∈U′
the inputs of any subset of users U′ ⊆ U.
In such a protocol execution, the view of a party consists of its
internal state (including its input and randomness) and all messages
this party received from other parties (the messages sent by this
party do not need to be part of the view because they can be determined using the other elements of its view). Moreover, if the party
aborts, it stops receiving messages and the view is not extended
past the last message received.
Given any subset C ⊆ U ∪ {S} of the parties, let
REALU,t,k
C
(xU ,U1,U2,U3,U4,U5) be a random variable representing the combined views of all parties in C in the above protocol
execution, where the randomness is over the internal randomness
of all parties, and the randomness in the setup phase.
Our first theorem shows that the joint view of any subset of
honest users (excluding the server) can be simulated given only the
Session E5: Privacy-Preserving Analytics CCS’17, October 30-November 3, 2017, Dallas, TX, USA 1180
Secure Aggregation Protocol
• Setup:
– All parties are given the security parameter k, the number of users n and a threshold value t, honestly generated pp ← KA.gen(k), parameters
m and R such that Zm
R
is the space from which inputs are sampled, and a field F to be used for secret sharing. All users also have a private
authenticated channel with the server.
– All users u receive their signing key d
S K
u from the trusted third party, together with verification keys d
P K
v bound to each user identity v.
• Round 0 (AdvertiseKeys):
User u:
– Generate key pairs (c
P K
u , c
S K
u ) ← KA.gen(pp), (s
P K
u , s
S K
u ) ← KA.gen(pp), and generate σu ← SIG.sign(d
S K
u , c
P K
u | |s
P K
u ).
– Send (c
P K
u | |s
P K
u | |σu ) to the server (through the private authenticated channel) and move to next round.
Server:
– Collect at least t messages from individual users in the previous round (denote with U1 this set of users). Otherwise, abort.
– Broadcast to all users in U1 the list {(v, c
P K
v , s
P K
v , σv )}v∈U1
and move to next round.
• Round 1 (ShareKeys):
User u:
– Receive the list {(v, c
P K
v , s
P K
v , σv )}v∈U1
broadcasted by the server. Assert that |U1 | ≥ t, that all the public key pairs are different, and that
∀v ∈ U1, SIG.ver(d
P K
v , c
P K
v | |s
P K
v , σu ) = 1.
– Sample a random element bu ← F (to be used as a seed for a PRG).
– Generate t-out-of-|U1 | shares of s
S K
u : {(v, s
S K
u,v )}v∈U1 ← SS.share(s
S K
u , t, U1)
– Generate t-out-of-|U1 | shares of bu : {(v, bu,v )}v∈U1 ← SS.share(bu, t, U1)
– For each other user v ∈ U1 \ {u }, compute eu,v ← AE.enc(KA.agree(c
S K
u , c
P K
v ), u | |v | |s
S K
u,v | |bu,v )
– If any of the above operations (assertion, signature verification, key agreement, encryption) fails, abort.
– Send all the ciphertexts eu,v to the server (each implicitly containing addressing information u, v as metadata).
– Store all messages received and values generated in this round, and move to the next round.
Server:
– Collect lists of ciphertexts from at least t users (denote with U2 ⊆ U1 this set of users).
– Sends to each user u ∈ U2 all ciphertexts encrypted for it: {eu,v }v∈U2
and move to the next round.
• Round 2 (MaskedInputCollection):
User u:
– Receive (and store) from the server the list of ciphertexts {eu,v }v∈U2
(and infer the set U2). If the list is of size < t, abort.
– For each other user v ∈ U2 \ {u }, compute su,v ← KA.agree(s
S K
u , s
P K
v ) and expand this value using a PRG into a random vector
pu,v = ∆u,v · PRG(su,v ), where ∆u,v = 1 when u > v, and ∆u,v = −1 when u < v (note that pu,v + pv,u = 0 ∀u ̸= v). Additionally, define
pu,u = 0.
– Compute the user’s own private mask vector pu = PRG(bu ). Then, Compute the masked input vector yu ← xu + pu +
P
v∈U2
pu,v (mod R)
– If any of the above operations (key agreement, PRG) fails, abort. Otherwise, Send yu to the server and move to the next round.
Server:
– Collect yu from at least t users (denote with U3 ⊆ U2 this set of users). Send to each user in U3 the list U3.
• Round 3 (ConsistencyCheck):
User u:
– Receive from the server a list U3 ⊆ U2 consisting of at least t users (including itself). If U3 is smaller than t, abort.
– Send to the server σ
′
u ← SIG.sign(d
S K
u , U3).
Server:
– Collect σ
′
u from at least t users (denote with U4 ⊆ U3 this set of users). Send to each user in U4 the set {v, σ
′
v }v∈U4
.
• Round 4 (Unmasking):
User u:
– Receive from the server a list {v, σ
′
v }v∈U4
. Verify that U4 ⊆ U3, that |U4 | ≥ t and that SIG.ver(d
P K, U3, σ
′
v ) = 1 for all v ∈ U4 (otherwise
abort).
– For each other user v in U2 \ {u }, decrypt the ciphertext v
′
| |u
′
| |s
S K
v,u | |bv,u ← AE.dec(KA.agree(c
S K
u , c
P K
v ), ev,u ) received in the MaskedInputCollection round and assert that u = u
′ ∧ v = v
′
.
– If any of the decryption operations fail (in particular, the ciphertext does not correctly authenticate), abort.
– Send a list of shares to the server, which consists of s
S K
v,u for users v ∈ U2 \ U3 and bv,u for users in v ∈ U3.
Server (generating the output):
– Collect responses from at least t users (denote with U5 this set of users).
– For each user in u ∈ U2 \ U3, reconstruct s
S K
u ← SS.recon({s
S K
u,v }v∈U5
, t) and use it (together with the public keys received in the
AdvertiseKeys round) to recompute pv,u for all v ∈ U3 using the PRG.
– For each user u ∈ U3, reconstruct bu ← SS.recon({bu,v }v∈U5
, t) and then recompute pu using the PRG.
– Compute and output z =
P
u∈U3
xu as P
u∈U3
xu =
P
u∈U3
yu −
P
u∈U3
pu +
P
u∈U3,v∈U2\U3
pv,u
Figure 4: Detailed description of the Secure Aggregation protocol. Red, underlined parts are required to guarantee security in
the active-adversary model (and not necessary in the honest-but-curious one).
Session E5: Privacy-Preserving Analytics CCS’17, October 30-November 3, 2017, Dallas, TX, USA 1181
knowledge of the inputs of those users. Intuitively, this means that
those users learn “nothing more” than their own inputs.
Theorem 6.2 (Honest But Curious Security, against clients
only). There exists a PPT simulator SIM such that for all k,t,U
with t ≤ |U |,xU ,U1,U2,U3,U4,U5 and C such that C ⊆ U,
U ⊇ U1 ⊇ U2 ⊇ U3 ⊇ U4 ⊇ U5, the output of SIM is perfectly
indistinguishable from the output of REALU,t,k
C
:
REALU,t,k
C
(xU ,U1,U2,U3,U4,U5)
≡
SIMU,t,k
C
(xC,U1,U2,U3,U4,U5)
Proof. Note that, since the view of the server is omitted, the
joint view of the parties in C does not depend (in an information theoretic sense) on the inputs of the parties not in C. The simulator can
therefore produce a perfect simulation by running the honest but
curious users on their true inputs, and all other users on a dummy
input (for example, a vector of 0s), and outputting the simulated
view of the users in C. In more detail, the only value sent by the
honest parties which depend on their input is yu (sent to the server
in round MaskedInputCollection). One can easily note that the
response sent by the server to the users in round MaskedInputCollection just contains a list of user identities which depends
on which users responded on the previous round, but not on the
specific yu values of the responses. This means that the simulator can use dummy values for the inputs of all honest parties not
in C, and the joint view of users in C will be identical to that in
REALU,t,k
. □
In our next theorem, we consider security against an honestbut-curious server, who can additionally combine knowledge with
some honest-but-curious clients. We show that any such group
of honest-but-curious parties can be simulated given the inputs
of the clients in that group, and only the sum of the values of the
remaining clients. Intuitively, this means that those clients and
the server learn “nothing more” than their own inputs, and the
sum of the inputs of the other clients. Additionally, if too many
clients abort before Round Unmasking, then we show that we
can simulate the view of the honest-but-curious parties given no
information about the remaining clients’ values. Thus, in this case,
the honest-but-curious parties learn nothing about the remaining
clients’ values.
Importantly, the view to be simulated must contain fewer than t
honest-but-curious clients, or else we cannot guarantee security.
Theorem 6.3 (Honest But Curious Security, with curious server). There exists a PPT simulator SIM such that for all
t,U,xU ,U1,U2,U3,U4, and C such that C ⊆ U ∪ {S}, |C \ {S}|< t,
U ⊇ U1 ⊇ U2 ⊇ U3 ⊇ U4 ⊇ U5, the output of SIM is computationally indistinguishable from the output of REALU,t,k
C
:
REALU,t,k
C
(xU ,U1,U2,U3,U4,U5)
≈c SIMU,t,k
C
(xC,z,U1,U2,U3,U4,U5)
where
z =




P
u ∈U3\C xu if |U3 |≥ t
⊥ otherwise.
Proof. We prove the theorem by a standard hybrid argument.
We will define a simulator SIM through a series of (polynomially
many) subsequent modifications to the random variable REAL, so
that any two subsequent random variables are computationally
indistinguishable.
Hyb0 This random variable is distributed exactly as REAL,
the joint view of the parties C in a real execution of the
protocol.
Hyb1
In this hybrid, we change the behavior of simulated
honest parties in the set U2 \ C, so that instead of using
KA.agree(c
SK
u
,c
PK
v
) to encrypt and decrypt messages to
other users v in the same set, they use a uniformly random
encryption key cu,v chosen by the simulator. The Decisional Diffie-Hellman assumption (as recalled in Definition
3.1) guarantees that this hybrid is indistinguishable from
the previous one.
Hyb2
In this hybrid, we substitute all ciphertexts encrypted
by honest parties in the set U2 \ C and sent to other honest
parties with encryptions of 0 (padded to the appropriate
length) instead of shares of s
SK
u
and bu . However, the honest clients in that set continue to respond with the correct
shares of s
SK
u
and bu in Round Unmasking. Since only
the contents of the ciphertexts have changed, IND-CPA
security of the encryption scheme guarantees that this
hybrid is indistinguishable from the previous one.
Hyb3 Define:
U∗
=




U2 \ C if z = ⊥
U2 \ U3 \ C otherwise.
This hybrid is distributed exactly as the previous one, but
here we substitute all shares of bu generated by parties
u ∈ U∗
and given to the corrupted parties in Round
ShareKeys with shares of 0 (using a different sharing of
0 for every u ∈ U∗
). Note that, in this hybrid and the previous one, the adversary does not receive any additional
shares of bu for users u in the set U∗
in Round Unmasking, either because the honest clients do not reveal shares
of bu for such u, or because all honest clients abort (when
|U3 |< t, which happens exactly when z = ⊥). Thus, MC’s
joint view contains only |C|< t shares of each bu . The
properties of Shamir’s secret sharing thus guarantee that
the distribution of any |C| shares of 0 is identical to the
distribution of an equivalent number of shares of any given
secret bu , making this hybrid identically distributed to the
previous one.
Hyb4
In this hybrid, for all parties u ∈ U∗
, instead of computing pu ← PRG(bu ), we set it to be a uniformly random
vector (of the appropriate size).
Note that, in the previous hybrid, since bu is chosen
uniformly at random and its shares given to the adversary
are substituted with shares of 0, the output of the random
variable does not depend on the seed of the PRG except
through the PRG’s output. Therefore, the only change in
this hybrid boils down to substituting the output of a PRG
(on a randomly generated seed otherwise independent from
the joint view of parties in C) with a uniformly random
Session E5: Privacy-Preserving Analytics CCS’17, October 30-November 3, 2017, Dallas, TX, USA 1182
value. Therefore, leveraging the security of the PRG, we
can argue that this hybrid is indistinguishable from the
previous one.
Hyb5
For all parties u ∈ U∗
, in Round MaskedInputCollection, instead of sending:
yu ← xu + pu +
X
v ∈U2
pu,v
we send:
yu ← pu +
X
v ∈U2
pu,v
Since pu was changed in the previous hybrid to be uniformly random and independent of any other values,
xu + pu is also uniformly random, and so this hybrid and
the previous hybrid are identically distributed. Further, this
hybrid and all subsequent hybrids do not depend on the
values xu for u ∈ U∗
.
Note: If z = ⊥, then we can ignore the further hybrids,
and let SIM be as described in Hyb5
, since SIM can already
simulate REAL without knowing xu for any u ∈ C/ . Therefore in the following hybrids we assume z ̸= ⊥.
Hyb6 This random variable is distributed exactly as the previous one, but here we substitute all shares of s
SK
u generated
by parties u ∈ U3 \ C and given to the corrupted parties
in Round ShareKeys with shares of 0 (using a different
sharing of 0 for every u ∈ U3 \ C). Following an analogous
argument to that for Hyb3
, the properties of Shamir’s secret
sharing guarantee that this hybrid is identically distributed
to the previous one.
Hyb7 We fix a specific user u
′ ∈ U3 \ C. For this user,
and each other user u ∈ U3 \ C, in order to compute
the value yu sent to the server, we substitute the joint
noise key (which would be computed by u
′
and u as
su
′
,u = su,u
′ ← KA.agree(s
SK
u
′
,s
PK
u
)) with a uniformly
random value (which will used by both parties as a PRG
seed).
In more detail, for each user u ∈ U3 \ C \ {u
′
}, a value
s
′
u
′
,u
is sampled uniformly at random and, instead of sending
yu ← xu + pu +
X
v ∈U2
pu,v
SIM sends
y
′
u ← xu + pu +
X
v ∈U2\ {u
′
}
pu,v + ∆u,u
′ · PRG(s
′
u
′
,u
)
and accordingly
y
′
u
′ ← xu
′ + pu
′ +
X
v ∈U2
∆u
′
,v · PRG(s
′
u
′
,v
)
where ∆u,v = 1 when u > v and ∆u,v = −1 when u < v.
It is easy to see that the Decisional Diffie-Hellman Assumption (Definition 3.1) guarantees that this hybrid is
indistinguishable from the previous one7
.
7
It is important to note here that, in the previous hybrids, we removed all shares of
s
S K
u
for u ∈ U3 \ C from the joint view of parties in C. Without doing so, we could
not reduce to the security of DH Key Agreement.
Hyb8
In this hybrid, for the same party u
′
chosen in the previous hybrid and all other parties v ∈ U3 \ C, instead
of computing pu
′
,v ← ∆u
′
,v · PRG(s
′
u
′
,v
), we compute it
using fresh randomness ru
′
,v (of the appropriate size) as
pu
′
,v ← ∆u
′
,v · ru
′
,v .
Note that, in the previous hybrid, since s
′
u
′
,v
is chosen
uniformly at random (and independently from the DiffieHellman keys), the output of the random variable does not
depend on the seed of the PRG except through the PRG’s
output. Therefore, the only change in this hybrid boils
down to substituting the output of a PRG (on an randomly
generated seed otherwise independent from the joint view
of parties in C) with a uniformly random value. Therefore,
leveraging the security of the PRG, we can argue that this
hybrid is indistinguishable from the previous one.
Hyb9
In this hybrid, for all users u ∈ U3 \ C, in round
MaskedInputCollection instead of sending:
yu ← xu + pu +
X
v ∈U2
pu,v
= xu + pu +
X
v ∈U3\C
pu,v +
X
v ∈U2\U3\C
pu,v
we send:
yu ← wu + pu +
X
v ∈U2\U3\C
pu,v
Where {wu }u ∈U3\C are uniformly random, subject to
P
U3\C wu =
P
U3\C xu = z. Invoking Lemma 6.1 with
n = |U3 \ C|, we have that this hybrid is identically distributed to the previous one. Moreover, note that to sample
from the random variable described by this hybrid, knowledge of the individual xu for u ∈ U3 \ C is not needed, and
their sum z is sufficient.
We can thus define a PPT simulator SIM that samples from the distribution described in the last hybrid. The argument above proves that
the output of the simulator is computationally indistinguishable
from the output of REAL, completing the proof. □
6.2 Privacy against Active Adversaries
In this section we discuss our argument for security against active
adversaries (detailed proofs can be found the in full version of this
paper). By active adversaries, we mean parties (clients or the server)
that deviate from the protocol, sending incorrect and/or arbitrarily
chosen messages to honest users, aborting, omitting messages, and
sharing their entire view of the protocol with each other, and also
with the server (if the server is also an active adversary).
We note that we only show input privacy for honest users: it is
much harder to additionally guarantee correctness and availability
for the protocol when some users are actively adversarial. Such
users can distort the output of the protocol by setting their input values xu to be out of range8
, by sending inconsistent Shamir shares to
other users in Round ShareKeys, or by reporting incorrect shares
8Typically, each element of xu is expected to be from a range [0, RU ) ⊂ [0, R),
such that the sum of all xu is in [0, R). However, an actively adversarial user could
choose xu outside the expected range, i.e. on [RU , R), allowing the adversarial user
disproportionate impact on protocol’s result, thus undermining correctness.
Session E5: Privacy-Preserving Analytics CCS’17, October 30-November 3, 2017, Dallas, TX, USA 1183
to the server in Round Unmasking. Making such deviations efficient to detect and possibly recover from is left to future work.
We note some key differences between the argument for honestbut-curious security, and the argument for privacy against active
adversaries.
The first key difference is that, for the proof against active adversaries, we assume that there exists a public-key infrastructure (PKI),
which guarantees to users that messages they receive came from
other users (and not the server). Without this assumption, the server
can perform a Sybil attack on the users in Round ShareKeys, by
simulating for a specific user u all other users v in the protocol and
thus receiving all u’s key shares and recovering that users’ input.
Alternatively, we can require the server to act honestly in its first
message (in Round ShareKeys). Specifically, the server must honestly forward the Diffie-Hellman public keys it receives to all other
users, allowing them to set up pairwise private and authenticated
channels amongst themselves.
However, if we assume a PKI, then we observe that the server’s
power in the remainder of the protocol is reduced to lying to users
about which other users have dropped out: since all user-to-user
messages (sent in round ShareKeys) are authenticated through an
authenticated encryption scheme, the server cannot add, modify
or substitute messages, but rather, can only fail to deliver them.
Note, importantly, that the server can try to give a different view
to each user of which other users have dropped out of the protocol.
In the worst case, this could allow the server to learn a different
set of shares from each user in Round Unmasking, allowing it to
potentially reconstruct more secrets than it should be allowed to.
The ConsistencyCheck round is included in the protocol to deal
with this issue. The inclusion of the ConsistencyCheck round is
the second key difference with the honest-but-curious proof.
The final key difference is that we need the proof to be in the
random oracle (RO) model. To see why, notice that honestly acting
users are essentially “commited” to their secrets and input by the
end of the MaskedInputCollection round. However, the server
can adaptively choose which users drop after the MaskedInputCollection round. This causes problems for a simulation proof,
because the simulator doesn’t know honest users’ real inputs, and
must use dummy information in the earlier rounds, thus “committing” itself to wrong values that are potentially easily detectable.
The random oracle adds a trapdoor for the simulator to equivocate, so that even if it commits to dummy values in early rounds,
it can reprogram the random oracle to make the dummy values
indistinguishable from honest users’ values. A full proof of security
appears in the full version of this paper.
6.3 Interpretation of Results
We summarize our system for the different security models we
consider in Figure 5.
6.3.1 Security against only clients. In each of Theorems 6.2 and
6.4, we see that the joint view of any subset of clients, honest or
adversarial, can be simulated given no information about the values
of the remaining clients. This means, no matter how we set our t
parameter, clients on their own learn nothing about other clients.
Theorem 6.4 (Privacy against actively adversarial users,
with honest server). There exists a PPT simulator SIM such that
Threat model Minimum
threshold
Minimum
inputs in sum
Client-only adversary 1 t
Server-only adversary ⌊
n
2
⌋ + 1 t
Clients-Server collusion ⌊
2n
3
⌋ + 1 t − nC
Figure 5: Parameterization for different threat models. “Minimum threshold” denotes the minimum value of t required
for security in the given threat model. “Minimum inputs in
the sum” denotes a lower bound on the number of users’ values that are included in the sum learned by the server. n denotes the total number of users, while nC is the number of
corrupt users.
for all PPT adversaries MC, all k,t,U,xU\C,C ⊆ U, the output of
SIM is perfectly indistinguishable from the output of REALU,t,k
C
:
REALU,t,k
C
(MC,xU\C) ≡ SIMU,t,k
C
(MC)
Proof. See full version of this paper. □
6.3.2 Security against only the server. From Theorems 6.3 and
6.5, we see that if we set nC = 0, that is, there are no clients who
cheat or collaborate with the server, then setting t ≥ ⌊ n
2
⌋ + 1
guarantees that the sum learned by the server contains the values
of at least t >
n
2
clients, and the protocol can deal with up to ⌈
n
2
⌉ −1
dropouts.
Theorem 6.5 (Privacy against active adversaries, including the server). There exists a PPT simulator SIM such that for all
k,t,U,C ⊆ U ∪ {S} and xU\C, letting n = |U | and nC = |C ∩ U |,
if 2t > n + nC, then the output of SIM is computationally indistinguishable from the output of REALU,t,k
:
REALU,t,k
C
(MC,xU\C) ≈c SIM
U,t,k,Idealδ
{xu }u∈U\C
C
(MC)
where δ = t − nC.
Proof. See full version of this paper. □
6.3.3 Security against a server colluding with clients. From Theorems 6.3 and 6.5, we see that we can allow a server (honest or
adversarial) to collaborate with up to nC = ⌈
n
3
⌉ −1 users (honest or
adversarial), if we set t ≥ ⌊ 2n
3
⌋ + 1, at the same time guaranteeing
that the sum learned by the server contains the values of at least n
3
clients. Additionally, the protocol is robust to up to ⌈
n
3
⌉ − 1 users
dropping out.
For all the results above, we reiterate that if we want security
against servers that are allowed to actively deviate from the protocol
(whether or not they collaborate with clients), we must use include
the protocol features highlighted in Figure 4.
7 EVALUATION
We summarize the protocol’s performance in Table 3. All calculations below assume a single server and n users, where each user
holds a data vector of size m. We evaluate the honest-but-curious
version of the protocol, and ignore the cost of the PKI, all signatures,
and Round ConsistencyCheck. We note that including their cost
Session E5: Privacy-Preserving Analytics CCS’17, October 30-November 3, 2017, Dallas, TX, USA 1184
does not change any of the asymptotics, and only slightly increases
the computation and communication costs.
7.1 Performance Analysis of Client
Computation cost: O(n
2 + mn). Each user u’s computation cost
can be broken up as (1) Performing the 2n key agreements, which
take O(n) time, (2) Creating t-out-of-n Shamir secret shares of s
SK
u
and bu , which is O(n
2
) and (3) Generating values pu and pu,v for
every other user v for each entry in the input vector by stretching
one PRG seed each, which takes O(mn) time in total. Overall, each
user’s computation is O(n
2 + mn).
Communication cost: O(n + m). The communication costs of
each user can be broken up into 4 parts: (1) Exchanging keys with
each other user by sending 2 and receiving 2(n − 1) public keys, (2)
Sending 2(n − 1) and receiving 2(n − 1) encrypted secret shares, (3)
Sending a masked data vector of size m

log2
R

to the server, and
(4) Sending the server n secret shares, for an overall communication
cost of 2naK + (5n − 4)aS + m

log2
R

, where aK and aS are the
number of bits in a key exchange public key and the number of bits
in a secret share, respectively. Overall, the user’s communication
complexity is O(n + m). Assuming inputs for each user are on the
same range [0,RU − 1], we require R = n(RU − 1) + 1 to avoid
overflow. A user could transmit its raw data using m

log2
RU

bits.
Taking aK = aS = 256 bits implies a communication expansion
factor of 256(7n−4)+m⌈log2 R⌉
m⌈log2 RU ⌉
. For RU = 216 (i.e. 16-bit input values),
m = 220 elements, and n = 210 users, the expansion factor is 1.73×;
for n = 214 users, it is 3.62×. For m = 224 elements and n = 214
users, the expansion factor is 1.98×.
Storage cost: O(n +m). The user must store the keys and secretshares sent by each other user, which are O(n) in total, and the data
vector (which it can mask in-place), which has size O(m).
7.2 Performance Analysis of Server
Computation cost: O(mn2
). The server’s computation cost can be
broken down as (1) Reconstructing n t-out-of-n Shamir secrets (one
for each user), which takes total time O(n
2
), and (2) generating and
removing the appropriate pu,v and pu values from the sum of the
yu values received, which takes time O(mn2
) in the worst case.
We note that reconstructing n secrets in the Shamir scheme
takes O(n
3
) time in the general case: each secret reconstruction
SS.recon({(u,su )}u ∈U′,t) → s amounts to interpolating a polynomial L over the points encoded by the shares and then evaluating
at 0, which can be accomplished via Lagrange polynomials:
s = L(0) = X
u ∈U′
su
Y
v ∈U′\ {u }
v
v − u
(mod p)
Each reconstruction requires O(n
2
) computation and we must
perform n reconstructions, implying O(n
3
) total time. However,
in our setting, we can perform all of the reconstructions in O(n
2
)
time by observing that all of our secrets will be reconstructed from
identically-indexed sets of secret shares – that is, U′
is fixed across
all secrets, because in round Unmasking, each user that is still
alive sends a share of every secret that needs to be reconstructed.
Therefore, we can precompute the Lagrange basis polynomials
ℓu =
Y
v ∈U′\ {u }
v
v − u
(mod p)
in O(n
2
) time and O(n) space, then reconstruct each of n secrets
in O(n) time as L(0) = P
u ∈U′ su ℓu (mod p) resulting in a total
computational cost of O(n
2
) to reconstruct all the secrets.
We also note that the O(mn2
) term can be broken into O(m(n −
d)+md(n−d)), where d is the number of users that dropped from the
protocol. In practice, d may be significantly smaller than n, which
would also reduce the server’s computation cost.
Communication cost:O(n
2+mn). The server’s communication
cost is dominated by its mediation of all pairwise communications
between users, which is O(n
2
), and also for receiving masked data
vectors from each user, which is O(mn) in total.
Storage cost: O(n
2 +m). The server must store t shares for each
user, which is O(n
2
) in total, along with an m-element buffer in
which to maintain a running sum of yu as they arrive.
7.3 Prototype Performance
In order to measure performance, we implemented a prototype in
Java, with the following cryptographic primitives:
• For Key Agreement, we used Elliptic-Curve Diffie-Hellman
over the NIST P-256 curve, composed with a SHA-256 hash.
• For Secret Sharing, we used standard t-out-of-n Shamir
Sharing.
• For Authenticated Encryption, we used AES-GCM with
128-bit keys.
• For the Pseudorandom Number Generator, we used AES
in counter mode.
We assume an honest-but-curious setting, and thus omitted the
portions of Figure 4 special to active clients from our simulations.
We note that these omissions would not change the overall shape
of our results in practice, since, as we discuss below, the bulk of the
costs involve masking, storing and sending the large data vector.
Additionally, we assume that when clients drop out of the protocol, that they drop after sending their shares to all other clients,
but before sending their masked input to the server. This is essentially the “worst case” dropout, since all other clients have already
incorporated the dropped clients’ masks, and the server must perform an expensive recovery computation to remove them. We also
assumed that client’s data vectors had entries such that at most
3 bytes are required to store the sum of up to all clients’ values
without overflow.
We ran single-threaded simulations on a Linux workstation with
an Intel Xeon CPU E5-1650 v3 (3.50 GHz), with 32 GB of RAM. Wallclock running times and communication costs for clients are plotted
in Figure 6. Wall clock running times for the server are plotted in
Figure 7, with different lines representing different percentages of
clients dropping out. Figure 8 shows wall-clock times per round
for both the client and the server. We omit data transfer plots for
the server, as they are essentially identical to those for the client,
except higher by a factor of n. This is because the incoming data
of the server is exactly the total outgoing data of all clients, and
vice versa. We also do not plot bandwidth numbers for different
Session E5: Privacy-Preserving Analytics CCS’17, October 30-November 3, 2017, Dallas, TX, USA 1185
(a) Wall-clock running time per client, as the number of clients increases. The data vector size is fixed to 100K entries.
(b) Wall-clock running time per client, as the size of the data vector
increases. The number of clients is fixed to 500.
(c) Total data transfer per client, as the number of clients increases.
Different lines show different data vector sizes. Assumes no dropouts.
(d) Total data expansion factor per client, as compared to sending the
raw data vector to the server. Different lines represent different values
of n. Assumes no dropouts.
Figure 6: Client Running Time and Data Transfer Costs. All wall-clock running times are for a single-threaded client implemented in Java, and ignore communication latency. Plotted points represent averages over 10 end-to-end iterations, and error
bars represent 95% confidence intervals. (Error bars are omitted where measured standard deviation was less than 1%).
numbers of dropouts, as the number of dropouts does not have a
significant impact on this metric.
In our simulations, for both the client and the server, almost all
of the computation cost comes from expanding the various PRG
seeds to mask the data vector. Compared to this, the computational
costs of key agreement, secret sharing and reconstruction, and
encrypting and decrypting messages between clients, are essentially
negligible, especially for large choices of n and data vector size. This
suggests that using an optimized PRG implementation would yield
a significant running-time improvement over our prototype.
As seen in Figures 6a and 6b, the running time of each client
increases linearly with both the total number of clients and the
number of data vector entries, but does not change significantly
when more clients drop out. In Figure 6c, the communication expansion factor for each client increases as the total number of clients
increases, but this increase is relatively small compared to the impact of increasing the size of the data vector. This is also reflected
in Figure 6d, where the communication expansion factor for each
client increases as the total number of clients increases, but falls
quickly as the size of the data vector increases. This shows that the
cost of messages between clients amortizes well as the size of the
data vector increases.
In the case of the server, Figures 7a and 7b show that the running time of the server increases significantly with the fraction of
dropouts. This is because, for each dropped client u, the server must
remove that client’s pairwise masks pu,v from each other surviving
client v, which requires (n − d) PRG expansions, where d is the
number of dropped users. In contrast, each undropped user entails
only a single PRG expansion, to remove its self-mask. The high
cost of dealing with dropped users is also reflected in the server
running times in Figure 8.
In Figure 9, we show the results of running the protocol over a
Wide Area Network (WAN). The server and clients were run on
geographically seperated datacenters, with contention for CPU and
network. We give the standard deviations of the running times,
which reflects this contention, and occasional machine failures
(<1% of clients per execution). We observe that the clients have
a somewhat shorter runtime than the server: this is because the
Session E5: Privacy-Preserving Analytics CCS’17, October 30-November 3, 2017, Dallas, TX, USA 1186
(a) Wall-clock running time for the server, as the number of clients
increases. The data vector size is fixed to 100K entries.
(b) Wall-clock running time for the server, as the size of the data vector
increases. The number of clients is fixed to 500.
Figure 7: Server Running Time and Data Transfer Costs. All wall-clock running times are for a single-threaded server implemented in Java, and ignore communication latency. Plotted points represent averages over 10 end-to-end iterations. Error bars
are omitted where measured standard deviations are less than 1%.
Num. Clients Dropouts AdvertiseKeys ShareKeys MaskedInputColl. Unmasking Total
Client 500 0% 1 ms 154 ms 694 ms 1 ms 849 ms
Server 500 0% 1 ms 26 ms 723 ms 1268 ms 2018 ms
Server 500 10% 1 ms 29 ms 623 ms 61586 ms 62239 ms
Server 500 30% 1 ms 28 ms 514 ms 142847 ms 143389 ms
Client 1000 0% 1 ms 336 ms 1357 ms 5 ms 1699 ms
Server 1000 0% 6 ms 148 ms 1481 ms 3253 ms 4887 ms
Server 1000 10% 6 ms 143 ms 1406 ms 179320 ms 180875 ms
Server 1000 30% 8 ms 143 ms 1169 ms 412446 ms 413767 ms
Figure 8: CPU wall clock times per round. All wall-clock running times are for a single-threaded servers and clients implemented in Java, and ignore communication latency. Each entry represents the average over 10 iterations. The data vector size
is fixed to 100K entries with 24 bit entries.
Num. Clients Total Runtime Per-Client StdDev Server Total Runtime StdDev Total Communication Per Client
500 13159 ms 6443 ms 14670 ms 6574 ms 0.95 MB
1000 23497 ms 6271 ms 27855 ms 6874 ms 1.15 MB
Figure 9: End-to-End running time for the protocol, executed over a wide-area-network. All running times are for a singlethreaded servers and clients running in geographically separated datacenters, and include computation time, network latency,
and time spent waiting for other participants. Each entry represents the average over 15 iterations, with iterations more than
3 standard deviations from the mean discarded. The data vector size is fixed to 100K entries with 62 bits per entry, and there
are no induced dropouts (beyond <1% that occurred naturally).
server has to run the additional (expensive) unmasking step after
all clients have completed.
8 DISCUSSION AND FUTURE WORK
Identifying and Recovering from Abuse The security proof in
Theorem 6.5 guarantees that when users’ inputs are learned by the
server, they are always in aggregate with the values of other users.
However, we do not protect against actively adversarial clients that
try to prevent the server from learning any sum at all. For example,
an attacker-controlled client could send malformed messages to
other clients, causing enough of them to abort that the protocol
fails before the server can compute its output. Ideally, we would
like such abuse by corrupt clients to be efficiently identifiable, and
the protocol to gracefully recover from it. However, the problem of
assigning blame for abuse is subtle, and often adds several rounds
to protocols. We leave this problem to future work.
Enforcing Well-formed Inputs Our protocol also does not
verify that users’ inputs are well-formed or within any particular
bounds, so actively adversarial users could send arbitrary values of
their choice, that can cause the output learned by the server to also
be ill-formed. For our specific machine learning application, we
will be able to detect obviously malformed outputs and can simply
Session E5: Privacy-Preserving Analytics CCS’17, October 30-November 3, 2017, Dallas, TX, USA 1187
run the protocol again with a different set of clients. However, an
adversarial client may be able to supply “slightly” malformed input
values that are hard to detect, such as double its real values.
A possible solution is to use zero-knowledge proofs that the
client inputs are in the correct range. Unfortunately, even using the
best-known garbled circuit techniques [34], even one such proof
would be more costly than the entire protocol. We leave the problem
of guaranteeing well-formed inputs from the clients to future work.
Reducing Communication Further In the protocol we describe, all clients exchange pairwise masks with all other clients.
However, it may be sufficient to have the clients exchange masks
with only a subset of other clients, as long as these subsets of clients
do not form disjoint clusters. In fact, previous works (notably, Ács
et al. [3]) use this approach already. However, in our setting, this
requires extra care because the server facilitates the communication among clients, and an actively adversarial server can choose
dropouts based on its knowledge of which pairs of clients exchanged
masks with each other. We leave this improvement to future work.
9 RELATED WORK
As noted in Section 2, we emphasize that our focus is on mobile
devices, where bandwidth is expensive, and dropouts are common,
and in our setting there is a single service provider. Consequently,
our main goal is to minimize communication while guaranteeing
robustness to dropouts. Computational cost is an important, but
secondary, concern. These constraints will motivate our discussion
of, and comparison with, existing works.
Works based on Multiple non-Colluding Servers: To overcome the constraints of client devices, some previous work has
suggested that clients distribute their trust across multiple noncolluding servers, and this has been deployed in real-world applications [10]. The recently presented Prio system of Gibbs and
Boneh [16] is, from the perspective of the client devices, noninteractive, and the computation among the servers is very lightweight. Prio also allows client inputs to be validated, something
our current system cannot do, by relying on multiple servers.
Araki et al. recently presented a generic three-party computation
protocol that achieves very high throughput [5]. This protocol could
also be used in a setting where non-colluding servers are available,
with the clients sending shares to each server that will be combined
online.
Works based on Generic Secure Multiparty Computation:
As noted in Section 1, there is a long line of work showing how
multiple parties can securely compute any function using generic
secure MPC [8, 18, 27, 40, 41]. These works generally fall into two
categories: those based on Yao’s garbled circuits, and those based
on homomorphic encryption or secret sharing. The protocols based
on Yao’s garbled circuits are better suited to 2- or 3-party secure
computation and do not directly extend to hundreds of users.
MPC protocols based on secret sharing, however, can extend
to hundreds of users. In addition, these protocols have become
relatively computationally efficient, and can be made robust against
dropouts. Boyle et al. studied generic MPC at such scale, relying on
a particular ORAM construction to help localize the computation
and avoid broadcasts [11]. Some works, notably [12], optimize these
generic techniques for the specific task of secure summation, and
have publicly available implementations.
However, the weakness of generic MPC protocols based on secretsharing is communication cost. In all such protocols, each user
sends a secret-share of its entire data vector to some subset of the
other users. To guarantee robustness, this subset of users must be
relatively large: robustness is essentially proportional to the size
of the subset. Additionally, each secret share is as long as the size
of the entire data vector. In our setting the constraints on total
communication make these approaches unworkable.
Works based on Dining Cryptographers Networks: Dining
cryptographers networks, or DC-nets, are a type of communication
network which provide anonymity by using pairwise blinding of
inputs [14, 28], similarly to our secure aggregation protocol. The
basic version of DC-nets, in which a single participant at a time
sends an anonymous message, can be viewed as the restricted case
of secure aggregation in which all users except for one have an
input of 0.
Recent research has examined increasing the efficiency of DCnets protocols and allowing them to operate in the presence of
active adversaries [17]. But previous DC-nets constructions share
the flaw that, if even one user aborts the protocol before sending
its message, the protocol must be restarted from scratch, which can
be very expensive [36].
Works based on Pairwise Additive Masking: Pairwise blinding using additive stream ciphers has been explored in previous
work [3, 24, 31, 33], presenting different approaches to dealing with
client failures.
The work of Ács and Castelluccia [3], and the modification suggested by [31], are the most closely related to our scheme, and have
an explicit recovery round to deal with failures. Their protocols
operate very similarly to ours: pairs of clients use Diffie-Hellman
key exchange to agree on pairwise masks, and send the server their
data vectors, summed with each of their pairwise masks and also
a “self-mask”. In the recovery step, the server tells the remaining
clients which other clients dropped out, and each remaining client
responds with the sum of their (uncancelled) pairwise masks with
the dropped users, added to their “self-mask”. The server subtracts
these "recovery" values from the masked vectors received earlier,
and correctly learns the sum of the undropped users’ data.
However, their recovery phase is brittle: if additional users drop
out during the recovery phase, the protocol cannot continue. Simply repeating the recovery round is not sufficient, since this has
the potential to leak the “self-masks” of the surviving users, which
in turn can leak their data vectors. Moreover, since the entire sum
of the masks is sent, this round requires almost as much communication as the rest of the protocol, making further client failures
during this step likely.
Schemes based on (Threshold) Homomorphic Encryption
Schemes based on threshold additively-homomorphic cryptosystems (e.g. the Paillier cryptosystem [38, 47]) can handle client
dropouts, but are either computationally expensive or require additional trust assumptions. For example, Paillier-based schemes
require an expensive-to-generate set of threshold decryption keys,
that must either be generated and distributed by a trusted third
party or generated online with an expensive protocol. Similarly the
Session E5: Privacy-Preserving Analytics CCS’17, October 30-November 3, 2017, Dallas, TX, USA 1188
pairing-based scheme of Leontiadis et al. [39] calls for a trusted
dealer to set up the keys.
The schemes of Shi et al. [49] and Chan et al. [13] use an approach
similar to ours, but in the exponent in some group (the latter scheme
extends the former to provide robustness against client dropouts).
They also consider the need for differential privacy and give a
rigorous analysis of distributed noise generation. Unfortunately,
the size of the group elements is too large for our setting, and their
schemes also call for a trusted dealer.
Halevi, Lindell and Pinkas [32] present a protocol that uses homomorphic encryption to securely compute the sum in just one
round of interaction between the server and each of the clients
(assuming a PKI is already in place). Their protocol has the advantage that all parties do not need to be online simultaneously for
the protocol to execute. However, the protocol also requires the
communication to be carried out sequentially between the clients
and the server. More importantly for our setting, their protocol
does not deal with clients dropping out: all clients included in the
protocol must respond before the server can learn the decrypted
sum.
10 CONCLUSION
We have presented a practical protocol for securely aggregating
data while ensuring that clients’ inputs are only learned by the
server in aggregate. The overhead of our protocol is very low, and
it can tolerate large numbers of failing devices, making it ideal for
mobile applications. We require only one service provider, which
simplifies deployment. Our protocol has immediate applications
to real-world federated learning, and we expect to deploy a full
application in the near future.