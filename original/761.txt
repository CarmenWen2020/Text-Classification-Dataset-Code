Anomaly detection is a critical step towards building a secure and
trustworthy system. e primary purpose of a system log is to
record system states and signicant events at various critical points
to help debug system failures and perform root cause analysis. Such
log data is universally available in nearly all computer systems.
Log data is an important and valuable resource for understanding
system status and performance issues; therefore, the various system logs are naturally excellent source of information for online
monitoring and anomaly detection. We propose DeepLog, a deep
neural network model utilizing Long Short-Term Memory (LSTM),
to model a system log as a natural language sequence. is allows
DeepLog to automatically learn log paerns from normal execution,
and detect anomalies when log paerns deviate from the model
trained from log data under normal execution. In addition, we
demonstrate how to incrementally update the DeepLog model in
an online fashion so that it can adapt to new log paerns over time.
Furthermore, DeepLog constructs workows from the underlying
system log so that once an anomaly is detected, users can diagnose
the detected anomaly and perform root cause analysis eectively.
Extensive experimental evaluations over large log data have shown
that DeepLog has outperformed other existing log-based anomaly
detection methods based on traditional data mining methodologies.
CCS CONCEPTS
•Information systems → Online analytical processing; •Security
and privacy → Intrusion/anomaly detection and malware mitigation;
KEYWORDS
Anomaly detection; deep learning; log data analysis.
1 INTRODUCTION
Anomaly detection is an essential task towards building a secure
and trustworthy computer system. As systems and applications
get increasingly more complex than ever before, they are subject
to more bugs and vulnerabilities that an adversary may exploit to
launch aacks. Such aacks are also geing increasingly more
sophisticated. As a result, anomaly detection has become more
challenging and many traditional anomaly detection methods based
on standard mining methodologies are no longer eective.
System logs record system states and signicant events at various
critical points to help debug performance issues and failures, and
perform root cause analysis. Such log data is universally available
in nearly all computer systems and is a valuable resource for understanding system status. Furthermore, since system logs record
noteworthy events as they occur from actively running processes,
they are an excellent source of information for online monitoring
and anomaly detection.
Existing approaches that leverage system log data for anomaly
detection can be broadly classied into three groups: PCA based
approaches over log message counters [39], invariant mining based
methods to capture co-occurrence paerns between dierent log
keys [21], and workow based methods to identify execution anomalies in program logic ows [42]. Even though they are successful in
certain scenarios, none of them is eective as a universal anomaly
detection method that is able to guard against dierent aacks in
an online fashion.
is work proposes DeepLog, a data-driven approach for anomaly detection that leverages the large volumes of system logs. e
key intuition behind the design of DeepLog is from natural language processing: we view log entries as elements of a sequence
that follows certain paerns and grammar rules. Indeed, a system log is produced by a program that follows a rigorous set of
logic and control ows, and is very much like a natural language
(though more structured and restricted in vocabulary). To that end,
DeepLog is a deep neural network that models this sequence of log
entries using a Long Short-Term Memory (LSTM) [18]. is allows
DeepLog to automatically learn a model of log paerns from normal execution and ag deviations from normal system execution
as anomalies. Furthermore, since it is a learning-driven approach,
it is possible to incrementally update the DeepLog model so that it
can adapt to new log paerns that emerge over time.
Challenges. Log data are unstructured, and their format and semantics can vary signicantly from system to system. It is already
challenging to diagnose a problem using unstructured logs even
aer knowing an error has occurred [43]; online anomaly detection
from massive log data is even more challenging. Some existing
methods use rule-based approaches to address this issue, which
requires specic domain knowledge [41], e.g., using features like
“IP address” to parse a log. However, this does not work for general
purpose anomaly detection where it is almost impossible to know
a priori what are interesting features in dierent types of logs (and
to guard against dierent types of aacks).
Anomaly detection has to be timely in order to be useful so that
users can intervene in an ongoing aack or a system performance
issue [10]. Decisions are to be made in streaming fashion. As
Session F2: Insights from Log(in)s CCS’17, October 30-November 3, 2017, Dallas, TX, USA 1285
a result, oine methods that need to make several passes over
the entire log data are not applicable in our seing [22, 39]. We
would also like to be able to detect unknown types of anomalies,
rather than gearing towards specic types of anomalies. erefore,
previous work [44] that use both normal and abnormal (for specic
types of anomalies) log data entries to train a binary classier for
anomaly detection is not useful in this context.
Another challenge comes from concurrency. Clearly, the order of log messages in a log provides important information for
diagnosis and analysis (e.g., identify the execution path of a program). However, in many system logs, log messages are produced
by several dierent threads or concurrently running tasks. Such
concurrency makes it hard to apply workow based anomaly detection methods [42] which use a workow model for a single task
as a generative model to match against a sequence of log messages.
Lastly, each log message contains rich information such as a log
key and one or more metric values, as well as its timestamp. A
holistic approach that integrates and utilizes these dierent pieces
of information will be more eective. Most existing methods [22,
32, 39, 41, 42, 44] analyze only one specic part of a log message
(e.g., the log key) which limits the types of anomalies they can
detect.
Our contribution. A Recurrent Neural Network (RNN) is an arti-
cial neural network that uses a loop to forward the output of last
state to current input, thus keeping track of history for making predictions. Long Short-Term Memory (LSTM) networks [13, 18, 27]
are an instance of RNNs that have the ability to remember long-term
dependencies over sequences. LSTMs have demonstrated success
in various tasks such as machine translation [35], sentiment analysis [8], and medical self-diagnosis [20].
Inspired by the observation that entries in a system log are a
sequence of events produced by the execution of structured source
code (and hence can be viewed as a structured language), we design
the DeepLog framework using a LSTM neural network for online
anomaly detection over system logs. DeepLog uses not only log
keys but also metric values in a log entry for anomaly detection,
hence, it is able to capture dierent types of anomalies. DeepLog
only depends on a small training data set that consists of a sequence
of “normal log entries”. Aer the training phase, DeepLog can
recognize normal log sequences and can be used for online anomaly
detection over incoming log entries in a streaming fashion.
Intuitively, DeepLog implicitly captures the potentially nonlinear and high dimensional dependencies among log entries from
the training data that correspond to normal system execution paths.
To help users diagnose a problem once an anomaly is identied,
DeepLog also builds workow models from log entries during its
training phase. DeepLog separates log entries produced by concurrent tasks or threads into dierent sequences so that a workow
model can be constructed for each separate task.
Our evaluation shows that on a large HDFS log dataset explored
by previous work [22, 39], trained on only a very small fraction
(less than 1%) of log entries corresponding to normal system execution, DeepLog can achieve almost 100% detection accuracy on
the remaining 99% of log entries. Results from a large OpenStack
log convey a similar trend. Furthermore, DeepLog also provides
the ability to incrementally update its weights during the detection phase by incorporating live user feedback. More specically,
DeepLog provides a mechanism for user feedback if a normal log
entry is incorrectly classied as an anomaly. DeepLog can then use
such feedback to adjust its weights dynamically online over time
to adapt itself to new system execution (hence, new log) paerns.
2 PRELIMINARIES
2.1 Log parser
We rst parse unstructured, free-text log entries into a structured
representation, so that we can learn a sequential model over this
structured data. As shown by several prior work [9, 22, 39, 42, 45],
an eective methodology is to extract a “log key” (also known as
“message type”) from each log entry. e log key of a log entry e
refers to the string constant k from the print statement in the source
code which printed e during the execution of that code. For example,
the log key k for log entry e =“Took 10 seconds to build instance.” is
k =Took * seconds to build instance., which is the string constant from
the print statement printf(”Took %f seconds to build instance.”, t). Note
that the parameter(s) are abstracted as asterisk(s) in a log key. ese
metric values reect the underlying system state and performance
status. Values of certain parameters may serve as identiers for
a particular execution sequence, such as block_id in a HDFS log
and instance_id in an OpenStack log. ese identiers can group
log entries together or untangle log entries produced by concurrent
processes to separate, single-thread sequential sequences [22, 39,
42, 45]. e state-of-the-art log parsing method is represented by
Spell [9], an unsupervised streaming parser that parses incoming
log entries in an online fashion based on the idea of LCS (longest
common subsequence).
Past work on log analysis [22, 39, 42, 44] have discarded timestamp and/or parameter values in a log entry, and only used log keys
to detect anomalies. DeepLog stores parameter values for each log
entry e, as well as the time elapsed between e and its predecessor,
into a vector −→v e . is vector is used by DeepLog in addition to
the log key. An example is given in Table 1, which shows the parsing results for a sequence of log entries from multiple rounds of
execution of virtual machine (VM) deletion task in OpenStack.
2.2 DeepLog architecture and overview
e architecture of DeepLog is shown in Figure 1 with three main
components: the log key anomaly detection model, the parameter value anomaly detection model, and the workow model to
diagnose detected anomalies.
Training stage. Training data for DeepLog are log entries from
normal system execution path. Each log entry is parsed to a log key
and a parameter value vector. e log key sequence parsed from a
training log le is used by DeepLog to train a log key anomaly detection model, and to construct system execution workow models
for diagnosis purposes. For each distinct key k, DeepLog also trains
and maintains a model for detecting system performance anomalies
as reected by these metric values, trained by the parameter value
vector sequence of k.
Detection stage. A newly arrived log entry is parsed into a log
key and a parameter value vector. DeepLog rst uses the log key
Session F2: Insights from Log(in)s CCS’17, October 30-November 3, 2017, Dallas, TX, USA 1286
log message (log key underlined) log key parameter value vector
t1 Deletion of le1 complete k1 [t1 − t0, le1Id]
t2 Took 0.61 seconds to deallocate network … k2 [t2 − t1, 0.61]
t3 VM Stopped (Lifecycle Event) k3 [t3 − t2]
… … …
Table 1: Log entries from OpenStack VM deletion task.
Training Stage
D etection Stage
Log Key
Anomaly Detection
model
......
Parameter Value
Anomaly Detection
model
for each log key
Workflows
normal execution
log file
Log
Parser ......
......
......
......
each log entry = log key + parameter value vector A new log entry
Log
Parser
parameter
value vector
+
Train model
Construct workflow Anomaly?
Yes No,
check
vector
Anomaly?
Yes
Diagnosis
Update
model if
false positive
log key
Train models
No
Figure 1: DeepLog architecture.
anomaly detection model to check whether the incoming log key is
normal. If yes, DeepLog further checks the parameter value vector
using the parameter value anomaly detection model for that log key.
e new entry will be labeled as an anomaly if either its log key or
its parameter value vector is predicted being abnormal. Lastly, if
it is labeled being abnormal, DeepLog’s workow model provides
semantic information for users to diagnose the anomaly. Execution
paerns may change over time or were not included in the original
training data. DeepLog also provides the option for collecting user
feedback. If the user reports a detected anomaly as false positive,
DeepLog could use it as a labeled record to incrementally update
its models to incorporate and adapt to the new paern.
2.3 reat model
DeepLog learns the comprehensive and intricate correlations and
paerns embedded in a sequence of log entries produced by normal
system execution paths. Henceforth, we assume that system logs
themselves are secure and protected, and an adversary cannot aack
the integrity of a log itself. We also assume that an adversary cannot
modify the system source code to change its logging behavior and
paerns. at said, broadly speaking, there are two types of aacks
that we consider.
(1) Aacks that lead to system execution misbehavior and hence
anomalous paerns in system logs. For instance, Denial of Service
(DoS) aacks which may cause slow execution and hence performance anomalies reected in the log timestamp dierences from
the parameter value vector sequence; aacks causing repeated
server restarts such as Blind Return Oriented Programming (BROP)
aack [5] shown as too many server restart log keys; and any attack that may cause task abortion such that the corresponding log
sequence ends early and/or exception log entries appear.
(2) Aacks that could leave a trace in system logs due to the
logging activities of system monitoring services. An example is
suspicious activities logged by an Intrusion Detection System (IDS).
3 ANOMALY DETECTION
3.1 Execution path anomaly
We rst describe how to detect execution path anomalies using the
log key sequence. Since the total number of distinct print statements
(that print log entries) in a source code is constant, so is the total
number of distinct log keys. Let K = {k1, k2, . . . , kn } be the set of
distinct log keys from a log-producing system source code.
Once log entries are parsed into log keys, the log key sequence
reects an execution path that leads to that particular execution
order of the log print statements. Let mi denote the value of the
key at position i in a log key sequence. Clearly, mi may take one of
the n possible keys from K, and is strongly dependent on the most
recent keys that appeared prior to mi
.
We can model anomaly detection in a log key sequence as a multiclass classication problem, where each distinct log key denes
a class. We train DeepLog as a multi-class classier over recent
context. e input is a history of recent log keys, and the output is a
probability distribution over the n log keys from K, representing the
probability that the next log key in the sequence is a key ki ∈ K.
Figure 2 summarizes the classication setup. Suppose t is the
sequence id of the next log key to appear. e input for classi-
cation is a window w of the h most recent log keys. at is,
w = {mt−h, . . . ,mt−2,mt−1}, where each mi
is in K and is the log
key from the log entry ei
. Note that the same log key value may
appear several times in w. e output of the training phase is a
model of the conditional probability distribution Pr[mt = ki
|w] for
each ki ∈ K(i = 1, . . . ,n). e detection phase uses this model to
make a prediction and compare the predicted output against the
observed log key value that actually appears.
Training stage. e training stage relies on a small fraction of log
entries produced by normal execution of the underlying system.
For each log sequence of length h in the training data, DeepLog
Session F2: Insights from Log(in)s CCS’17, October 30-November 3, 2017, Dallas, TX, USA 1287
DeepLog
Input: h recent
log keys up to Output: conditional probability
of next log key given the input
recent sequence
Figure 2: An overview of log key anomaly detection model.
updates its model for the probability distribution of having ki ∈ K
as the next log key value. For example, suppose a small log le
resulted from normal execution is parsed into a sequence of log
keys: {k22, k5, k11, k9, k11, k26}. Given a window size h = 3, the
input sequence and the output label pairs to train DeepLog will be:
{k22, k5, k11 → k9}, {k5, k11, k9 → k11}, {k11, k9, k11 → k26}.
Detection stage. DeepLog performs anomaly detection in an online, streaming seing. To test if an incoming log key mt
(parsed
from an incoming log entry et
) is to be considered normal or abnormal, we send w = {mt−h, ...,mt−1} to DeepLog as its input. e
output is a probability distribution Pr[mt
|w] = {k1 : p1, k2 : p2, ...,
kn : pn } describing the probability for each log key from K to
appear as the next log key value given the history.
In practice, multiple log key values may appear as mt
. For instance, if the system is aempting to connect to a host, then mt
could either be ‘Waiting for * to respond’ or ‘Connected to *’; both
are normal system behavior. DeepLog must be able to learn such
paerns during training. Our strategy is to sort the possible log
keys K based on their probabilities Pr[mt
|w], and treat a key value
as normal if it’s among the top д candidates. A log key is agged
as being from an abnormal execution otherwise.
3.1.1 Traditional N-gram language model. e problem of ascribing probabilities to sequences of words drawn from a xed
vocabulary is the classic problem of language modeling, widely
studied by the natural language processing (NLP) community [24].
In our case, each log key can be viewed as a word taken from
the vocabulary K. e typical language modeling approach for
assigning probabilities to arbitrarily long sequences is the N-gram
model. e intuition is that a particular word in a sequence is
only inuenced by its recent predecessors rather than the entire
history. In our seing, this approximation is equivalent to seing
Pr(mt = ki
|m1, . . . ,mt−1) = Pr(mt = ki
|mt−N , . . . ,mt−1) where
N denotes the length of the recent history to be considered.
For training, we can calculate this probability using relative frequency counts from a large corpus to give us maximum likelihood
estimates. Given a long sequence of keys {m1,m2, . . . ,mt }, we
can estimate the probability of observing the i
th key ki using the
relative frequency counts of {mt−N , . . . ,mt−1,mt = ki } with respect to the sequence {mt−N , . . . ,mt−1}. In other words, Pr(mt =
ki
|m1, . . . ,mt−1) = count(mt−N , . . ., mt−1, mt = ki)/count(mt−N ,
. . .,mt−1). Note that we will count these frequencies using a sliding
window of size N over the entire key sequence.
To apply the N-gram model in our seing, we simply use N as
the history window size, i.e., we set h = N in our experiments when
the N-gram model is used where h is the history sliding window
size as depicted in Figure 2. We use this as a baseline method.
3.1.2 The LSTM approach. In recent years, neural language models that use recurrent neural networks have been shown to be highly
eective across various NLP tasks [3, 25]. Compared to a N-gram
LSTM
block Output of last state
is forwarded as
current input state
LSTM
block
LSTM
block
DeepLog Output
Input
LSTM
block
LSTM
block
LSTM
block
 Roll out
Stack up
LSTM
block
LSTM
block
LSTM
block
LSTM
block
Figure 3: A detailed view of log key anomaly detection
model using stacked LSTM.
language model, a LSTM-based one can encode more intricate patterns and maintain long-range state over a sequence [34]. Complex
paerns and interleaving log entries from concurrent tasks in a system log can render a traditional language model less eective. us,
DeepLog uses a LSTM neural network [18] for anomaly detection
from a log key sequence.
Given a sequence of log keys, a LSTM network is trained to
maximize the probability of having ki ∈ K as the next log key value
as reected by the training data sequence. In other words, it learns
a probability distribution Pr(mt = ki
|mt−h, . . ., mt−2, mt−1) that
maximizes the probability of the training log key sequence.
Figure 3 illustrates our design. e top of the gure shows a
single LSTM block that reects the recurrent nature of LSTM. Each
LSTM block remembers a state for its input as a vector of a xed
dimension. e state of an LSTM block from the previous time
step is also fed into its next input, together with its (external) data
input (mt−i
in this particular example), to compute a new state
and output. is is how historical information is passed to and
maintained in a single LSTM block.
A series of LSTM blocks form an unrolled version of the recurrent
model in one layer as shown in the center of Figure 3. Each cell
maintains a hidden vector Ht−i and a cell state vector Ct−i
. Both
are passed to the next block to initialize its state. In our case, we
use one LSTM block for each log key from an input sequence w (a
window of h log keys). Hence, a single layer consists of h unrolled
LSTM blocks.
Within a single LSTM block, the input (e.g. mt−i
) and the previous output (Ht−i−1) are used to decide (1) how much of the previous
cell state Ct−i−1 to retain in state Ct−i
, (2) how to use the current
input and the previous output to inuence the state, and (3) how
to construct the output Ht−i
. is is accomplished using a set of
gating functions to determine state dynamics by controlling the
amount of information to keep from input and previous output, and
the information ow going to the next step. Each gating function
is parameterized by a set of weights to be learned. e expressive
capacity of an LSTM block is determined by the number of memory
Session F2: Insights from Log(in)s CCS’17, October 30-November 3, 2017, Dallas, TX, USA 1288
units (i.e. the dimensionality of the hidden state vector H). Due to
space constraints, we refer the reader to NLP primers (e.g., [12])
for a formal characterization of LSTMs.
e training step entails nding proper assignments to the weights
so that the nal output of the sequence of LSTMs produces the desired label (output) that comes with inputs in the training data set.
During the training process, each input/output pair incrementally
updates these weights, through loss minimization via gradient descent. In DeepLog, an input consists of a window w of h log keys,
and an output is the log key value that comes right aer w. We use
the categorical cross-entropy loss for training.
Aer training is done, we can predict the output for an input
(w = {mt−h, . . . ,mt−1}) using a layer of h LSTM blocks. Each log
key in w feeds into a corresponding LSTM block in this layer.
If we stack up multiple layers and use the hidden state of the
previous layer as the input of each corresponding LSTM block in
the next layer, it becomes a deep LSTM neural network, as shown at
the boom of Figure 3. For simplicity, it omits an input layer and an
output layer constructed by standard encoding-decoding schemes.
e input layer encodes the n possible log keys from K as one-hot
vectors. at is, a sparse n-dimensional vector −→u i
is constructed
for the log key ki ∈ K, such that −→u i[i] = 1 and −→u i[j] = 0 for all
other j , i. e output layer translates the nal hidden state into
a probability distribution function using a standard multinomial
logistic function to represent Pr[mt = ki
|w] for each ki ∈ K.
e example at the boom of Figure 3 shows only two hidden
layers, but more layers can be used.
3.2 Parameter value and performance anomaly
e log key sequence is useful for detecting execution path anomalies. However, some anomalies are not shown as a deviation from
a normal execution path, but as an irregular parameter value. ese
parameter value vectors (for the same log key) form a parameter value vector sequence, and these sequences from dierent log
keys form a multi-dimensional feature space that is important for
performance monitoring and anomaly detection.
Baseline approach. A simple approach is to store all parameter
value vector sequences into a matrix, where each column is a parameter value sequence from a log key k (note that it is possible to
have multiple columns for k depending on the size of its parameter
value vector). Row i in this matrix represents a time instance ti
.
Consider the log entries in Table 1 as an example. ere are
3 distinct log key values in this example, and the sizes of their
parameter value vectors are 2, 2, and 1 respectively. Hence, row
1 in this matrix represents time instance t1 with values [t1 − t0,
le1Id, null, null, null]. Similarly, row 2 and row 3 are [null, null,
t2 − t1, 0.61, null] and [null, null, null, null, t3 − t2] respectively.
We may also ask each row to represent a range of time instances
so that each row corresponds to multiple log messages within that
time range and becomes less sparse. But the matrix will still be very
sparse when there are many log key values and/or exists some large
parameter value vectors. Furthermore, this approach introduces
a delay to the anomaly detection process, and it is also dicult to
gure out a good value for the length of each range.
Given this matrix, many well-known data-driven anomaly detection methods can be applied, such as principal component analysis
(PCA) and self-organizing maps (SOM). ey are useful towards
capturing correlation among dierent feature dimensions. However, a major limitation of this method in the context of log data is
that oen times the appearance of multiple log keys at a particular
time instance is equally likely. For instance, the order of k1 and
k2 in Table 1 is arbitrary due to concurrently running tasks. is
phenomena, and the fact that the matrix is sparse, render these techniques ineective in our seing. Lastly, they are not able to model
auto-correlation that exists in a parameter value vector sequence
(regular paerns over time in a single vector sequence).
Our approach. DeepLog trains a parameter value anomaly detection model by viewing each parameter value vector sequence (for a
log key) as a separate time series.
Consider the example in Table 1. e time series for the parameter value vector sequence of k2 is: {[t2−t1, 0.61], [t
0
2
−t
0
1
, 1]}. Hence,
our problem is reduced to anomaly detection from a multi-variate
time series data. It is possible to apply an LSTM-based approach
again. We use a similar LSTM network as shown in Figure 3 to
model a multi-variate time series data, with the following adjustments. Note that a separate LSTM network is built for the parameter
value vector sequence of each distinct log key value.
Input. e input at each time step is simply the parameter value
vector from that timestamp. We normalize the values in each vector
by the average and the standard deviation of all values from the
same parameter position from the training data.
Output. e output is a real value vector as a prediction for the
next parameter value vector, based on a sequence of parameter
value vectors from recent history.
Objective function for training. For the multi-variate time series
data, the training process tries to adjust the weights of its LSTM
model in order to minimize the error between a prediction and an
observed parameter value vector. us, mean square loss is used to
minimize the error during the training process.
Anomaly detection. e dierence between a prediction and an
observed parameter value vector is measured by the mean square
error (MSE). Instead of seing a magic error threshold for anomaly
detection purpose in an ad-hoc fashion, we partition the training data to two subsets: the model training set and the validation
set. For each vector −→v in the validation set, we apply the model
produced by the training set to calculate the MSE between the prediction (using the vector sequence from before −→v in the validation
set) and −→v . At every time step, the errors between the predicted
vectors and the actual ones in the validation group are modeled as
a Gaussian distribution.
At deployment, if the error between a prediction and an observed value vector is within a high-level of condence interval
of the above Gaussian distribution, the parameter value vector of
the incoming log entry is considered normal, and is considered
abnormal otherwise.
Since parameter values in a log message oen record important
system state metrics, this method is able to detect various types of
performance anomalies. For example, a performance anomaly may
reect as a “slow down”. Recall that DeepLog stores in each parameter value vector the time elapsed between consecutive log entries.
e above LSTM model, by modeling parameter value vector as a
multi-variate time series, is able to detect unusual paerns in one
Session F2: Insights from Log(in)s CCS’17, October 30-November 3, 2017, Dallas, TX, USA 1289
or more dimensions in this time series; the elapsed time value is
just one such dimension.
3.3 Online update of anomaly detection models
Clearly, the training data may not cover all possible normal execution paerns. System behavior may change over time, additionally
depending on workload and data characteristics. erefore, it is
necessary for DeepLog to incrementally update weights in its LSTM
models to incorporate and adapt to new log paerns. To do this,
DeepLog provides a mechanism for the user to provide feedback.
is allows DeepLog to use a false positive to adjust its weights. For
example, suppose h = 3 and the recent history sequence is {k1, k2,
k3}, and DeepLog has predicted the next log key to be k1 with probability 1, while the next log key value is k2, which will be labeled
as an anomaly. If user reports that this is a false positive, DeepLog
is able to use the following input-output pair {k1, k2, k3 → k2}
to update the weights of its model to learn this new paern. So
that next time given history sequence {k1, k2, k3}, DeepLog can
output both k1 and k2 with updated probabilities. e same update
procedure works for the parameter value anomaly detection model.
Note that DeepLog does not need to be re-trained from scratch.
Aer the initial training process, models in DeepLog exist as several multi-dimensional weight vectors. e update process feeds in
new training data, and adjusts the weights to minimize the error
between model output and actual observed values from the false
positive cases.
4 WORKFLOW CONSTRUCTION FROM
MULTI-TASKS EXECUTION
Each log key is the execution of a log printing statement in the
source code, while a task like VM creation will produce a sequence
of log entries. Intuitively, the order of log entries produced by a task
represents an execution order of each function for accomplishing
this task. As a result, we can build a workow model as a nite state
automaton (FSA) to capture the execution path of any task. is
workow model can also be used to detect execution path anomalies, but it is less eective compared to DeepLog’s LSTM model
due to its inability to capture inter-task dependencies and nondeterministic loop iterations. However, the workow model is very
useful towards enabling users to diagnose what had gone wrong in
the execution of a task when an anomaly has been detected.
Given a log sequence generated by the repeated executions of a
task, there have been several works exploring the problem of work-
ow inference [4, 21, 42]. CloudSeer [42] represents the state of the
art in anomaly detection using a workow model. CloudSeer has
several limitations. Firstly, the anomalies it can detect are limited
to log entries having ”ERROR” logging level and log entries not
appearing. Furthermore, its workow model construction requires
a log le with repeated executions of only one single task. Other
previous works [4, 21] on workow construction from a log le
also suer from this limitation. In practice, a log le oen contains
interleaving log entries produced by multiple tasks and potentially
concurrently running threads within a task.
4.1 Log entry separation from multiple tasks
An easy case is when multiple programs concurrently write to the
same log (e.g., Ubuntu’s system log). Oen each log entry contains
the name of the program that created it. Another easy case is when
the process or task id is included in a log entry. Here, we focus on
the case where a user program is executed repeatedly to perform
dierent, but logically related, tasks within that program. An important observation is that tasks do not overlap in time. However, the
same log key may appear in more than one task, and concurrency
is possible within each task (e.g., multiple threads in one task).
Consider OpenStack administrative logs as an example. For
each VM instance, its life cycle contains VM creation, VM stop,
VM deletion and others. ese tasks do not overlap, i.e., VM stop
can only start aer VM creation has completed. However, the same
log key may appear in dierent tasks. For example, a log message
“VM Resumed (Lifecycle Event)” may appear in VM creation, VM start,
VM resume and VM unpause. ere could be concurrently running
threads inside each task, leading to uncertainty in the ordering
of log messages corresponding to one task. For instance, during
VM creation, the order of two log messages “Took * seconds to build
instance” and “VM Resumed (Lifecycle Event)” is uncertain.
Our goal is to separate log entries for dierent tasks in a log le,
and then build a workow model for each task based on its log
key sequence. at said, the input of our problem is the entire log
key sequence parsed from a raw log le, and the output is a set of
workow models, one for each task identied.
4.2 Using DeepLog’s anomaly detection model
4.2.1 Log key separation. Recall that in DeepLog’s model for
anomaly detection from log keys, the input is a sequence of log
keys of length h from recent history, and the output is a probability
distribution of all possible log key values. An interesting observation is that its output actually encodes the underlying workow
execution path.
Intuitively, given a log key sequence, our model predicts what
will happen next based on the execution paerns it has observed
during the training stage. If a sequence w is never followed by a
particular key value k in the training stage, then Pr[mt = k|w] = 0.
Correspondingly, if a sequence w is always followed by k, then
Pr[mt = k|w] = 1. For example, suppose on a sequence “25→54”,
the output prediction is “{57:1.00}”, we know that “25→54→57” is
from one task. A more complicated case is when a sequence w is
to be followed by a log key value from a group of dierent keys;
the probabilities of these keys to appear aer w sum to 1.
To handle this case, we use an idea that is inspired by small
invariants mining [21].
Consider a log sequence “54→57”, and suppose the predicted
probability distribution is “{18: 0.8, 56: 0.2}”, which means that
the next step could be either “18” or “56”. is ambiguity could
be caused by using an insucient history sequence length. For
example, if two tasks share the same workow segment “54→57”,
the rst task has a paern “18→54→57→18” that is executed 80%
of the time, and the second task has a paern “31→54→57→56”
that is executed 20% of the time. is will lead to a model that
predicts “{18: 0.8, 56: 0.2}” given the sequence “54→57”.
We can address this issue by training models with dierent
history sequence lengths, e.g., using h = 3 instead of h = 2 in this
case. During workow construction, we use a log sequence length
that leads to a more certain prediction, e.g. in the above example the
Session F2: Insights from Log(in)s CCS’17, October 30-November 3, 2017, Dallas, TX, USA 1290
25 18 54 57
18 56
56 18
1. [25, 18, 54] -> {57: 1.00}
2. [18, 54, 57] -> {18: 0.8, 56: 0.2}
3. [54, 57, 18] -> {56: 1.00}
 [54, 57, 56] -> {18: 1.00}
25 18 54 57
18
56
31
31
(a) An example of concurrency detection
25 18 54 57
24 60
26 37
1. [25, 18, 54] -> {57: 1.00}
2. [18, 54, 57] -> {24: 0.8, 26: 0.2}
3. [54, 57, 24] -> {60: 1.00}
 [54, 57, 26] -> {37: 1.00}
25 18 54 57
24
26 37
60
(b) An example of new task detection
26 37 39 40 39 40
1. H: [26, 37, 39] -> {40: 1.00}
2. H: [37, 39, 40] ->{39: 1.00}
3. H: [39, 40, 39] -> {40: 1.00}
26 37 39 40
(c) An example of loop identication
Figure 4: Examples of using LSTM for task separation and workow construction.
sequence “18→54→57” will lead to the prediction {18: 1.00} and
the sequence “31→54→57” will lead to the prediction {56: 1.00}.
If we have ruled out that a small sequence is a shared segment
from dierent tasks (i.e., increasing the sequence length for training
and prediction doesn’t lead to more certain prediction), the challenge now is to nd out whether the multi-key prediction output
is caused by either concurrency in the same task or the start of a
dierent task. We call this a divergence point.
We observe that, as shown in Figure 4a, if the divergence point is
caused by concurrency in the same task, a common paern is that
keys with the highest probabilities in the prediction output will
appear one aer another, and the certainty (measured by higher
probabilities for less number of keys) for the following predictions
will increase, as keys for some of the concurrent threads have
already appeared. e prediction will eventually become certain
aer all keys from concurrent threads are included in the history
sequence.
On the other hand, if the divergence point is caused by the start of
a new task, as shown in Figure 4b, the predicted log key candidates
(“24” and “26” in the example) will not appear one aer another. If
we incorporate each such log key into the history sequence, the
next prediction is a deterministic prediction of a new log key (e.g.,
“24→60”, “26→37”). If this is the case, we stop growing the workow
model of the current task (stop at key “57” in this example), and
start constructing workow models for new tasks. Note that the
two “new tasks” in Figure 4b could also be an “if-else” branch, e.g.,
“57→if (24→60→…) else (26→37→…)”. To handle such situations,
we apply a simple heuristic: if the “new task” has very few log keys
(e.g., 3) and always appears aer a particular task Tp , we treat it as
part of an “if-else” branch of Tp , otherwise as a new task.
4.2.2 Build a workflow model. Once we can distinguish divergence points caused by concurrency (multiple threads) in the same
task and new tasks, we can easily construct workow models as
illustrated in Figure 4a and Figure 4b. Additional care is needed to
identify loops. e detection of a loop is actually quite straightforward. A loop is always shown in the initial workow model as an
unrolled chain; see Figure 4c for an example. While this workow
chain is initially “26→37→39→40→39→40”, we could identify the
repeated fragments as a loop execution (39→40 in this example).
4.3 Using density-based clustering approach
4.3.1 Log key separation. Another approach is to use a densitybased clustering technique. e intuition is that log keys in the
Table 2: Co-occurrence matrix within distance d
k1 … kj … kn
k1 pd (1, 1) pd (1, j)
…
ki pd (i, 1) pd (i, j) =
fd
(ki
,kj
)
d·f (ki
)
)
…
kn pd (n, 1) pd (n, j)
same task always appear together, but log keys from dierent tasks
may not always appear together as the ordering of tasks is not
xed during multiple executions of dierent tasks. is allows us
to cluster log keys based on co-occurrence paerns, and separate
keys into dierent tasks when co-occurrence rate is low.
In a log key sequence, the distance d between any two log keys is
dened as the number of log keys between them plus 1. For example,
given the sequence {k1, k2, k2}, d(k1, k2) = [1, 2],d(k2, k2) = 1
(note that there are two distance values between the pair (k1, k2)).
We build a co-occurrence matrix as shown in Table 2, where each
element pd
(i, j) represents the probability of two log keys ki and kj
appearing within distance d in the input sequence. Specically, let
f (ki) be the frequency of ki
in the input sequence, and fd
(ki
, kj)
be the frequency of pair (ki
, kj) appearing together within distance
d in the input sequence. We dene pd
(i, j) =
fd
(ki
,kj
)
d·f (ki
)
, which shows
the importance of kj to ki
.
For example, when d = 1, p1(i, j) =
f1
(ki
,kj
)
f (ki
)
= 1 means that for
every occurrence of ki
, there must be a kj next to it. Note that in this
denition, f (ki) in the denominator is scaled by d because while
counting co-occurrence frequencies within d, a key ki
is counted by
d times. Scaling f (ki) by a factor of d ensures that Ín
j=1
fd
(i, j) = 1
for any i. Note that we can build multiple co-occurrence matrices
for dierent distance values of d.
With a co-occurrence matrix for each distance value d that we
have built, our goal is to output a set of tasks TASK = (T1,T2, ...).
e clustering procedure works as follows. First, for d = 1, we
check if any p1(i, j) is greater than a threshold τ (say τ = 0.9), when
it does, we connect ki
, kj together to form T1 = [ki
, kj]. Next, we
recursively check if T1 could be extended from either its head or
tail. For example, if there exists kx ∈ K such that p1(ki
, kx ) > τ ,
we further check if p2(kj
, kx ) > τ , i.e., if kj and kx have a large
co-occurrence probability within distance 2. If yes, T1 = [kx , ki
, kj],
otherwise we will add T2 = [ki
, kx ] to TASK.
is procedure continues until no task T in TASK could be further extended. In the general case when a task T to be extended
Session F2: Insights from Log(in)s CCS’17, October 30-November 3, 2017, Dallas, TX, USA 1291
has more than 2 log keys, when checking if kx could be included
as the new head or tail, we need to check if kx has a co-occurrence
probability greater than τ with each log key in T up to distance d
0
,
where d
0
is the smaller of: i) length of T , and ii) the maximum value
of d that we have built a co-occurrence matrix for. For example,
to check if T = [k1, k2, k3] should connect k4 at its tail, we need to
check if min(p1(k3, k4),p2(k2, k4),p3(k1, k4)) > τ .
e above process connects sequential log keys for each task.
When a task T1 = [ki
, kj] cannot be extended to include any single key, we check if T1 could be extended by two log keys, i.e., if
there exists kx , ky ∈ K, such that p1(ki
, kx ) + p1(ki
, ky ) > τ , or
p1(kj
, kx ) + p1(kj
, ky ) > τ . Suppose the laer case is true, the next
thing to check is whether kx and ky are log keys produced by concurrent threads in task T1. If they are, pd
(kj
, kx ) always increases
with larger d values, i.e., p2(kj
, kx ) > p1(kj
, kx ), which is intuitive
because the appearance ordering of keys from concurrent threads
is not certain. Otherwise kx and ky do not belong to T1, thus we
add T2 = [kj
, kx ] and T3 = [kj
, ky ] into TASK instead.
Finally, for each task T in TASK, we eliminate T if its sequence
is included as a sub-sequence in another task.
4.3.2 Build workflow model. Once a log key sequence is separated out and identied for each task, the workow model construction for a task follows the same discussion from Section 4.2.2.
4.4 Using the workow model
4.4.1 Set parameters for DeepLog model. In Section 3.1, we’ve
shown that DeepLog requires several input parameters, in particular, it needs the length of history sequence window h (for training
and detection), and the number of top д log keys in the predicted
output probability distribution function to be considered normal.
Seing a proper value for h and д is problem dependent. Generally speaking, larger h values will increase the prediction accuracy because more history information is utilized in LSTM, until
it reaches a point where keys that are far back in history do not
contribute to the prediction of keys to appear. At this point continuing to increase h does not hurt the prediction accuracy of LSTM,
because LSTM is able to learn that only the recent history in a long
sequence maers thus ignore the long tail. However, a large h value
does have a performance impact. More computations (and layers)
are required for both training and prediction, which slows down
the performance of DeepLog. e value of д, on the other hand,
regulates the trade-o between true positives (anomaly detection
rate) and false positives (false alarm rate).
e workow model provides a guidance to set a proper value
for both h and д. Intuitively, h needs to be just large enough to
incorporate necessary dependencies for making a good prediction,
so we can set h as the length of the shortest workow. e number
of possible execution paths represents a good value for д, hence, we
set д as the maximum number of branches at all divergence points
from the workows of all tasks.
4.4.2 Using workflow to diagnose detected anomalies. Whenever
an anomaly is detected by DeepLog, the workow model can be
used to help diagnose this anomaly and understand how and why
it has happened. Figure 5 shows an example. Using a history
sequence [26, 37, 38], the top prediction from DeepLog is log key
39 (suppose д = 1), however the actual log key appeared is 67,
which is an anomaly. With the help of a workow model for this
26 37 38 39 40 41
67 Actual Execution
Prediction (Correct Path)
37: instance: * Terminating instance
38: instance: * Instance destroyed successfully
39: instance: * Deleting instance files *
40: instance: * Deletion of * complete
41: instance: * Took * seconds to destroy the instance on the hypervisor
67: instance: * Error from libvirt during unfilter. Code=* Error=*
Figure 5: Anomaly diagnosis using workow.
task, users could easily identify the current execution point in
the corresponding workow, and further discover that this error
happened right aer “Instance destroyed successfully” and before
“Deleting instance les *”, which means that this error occurred
during cleanup aer destroying a VM.
4.5 Discussion
Previous works [4, 21, 42] focused on constructing workows from
multiple executions of just one task. e basic idea in their approach
follows 3 steps: 1) mine temporal dependencies of each pair of log
keys; 2) construct a basic workow from the pairwise invariants
identied in step 1; 3) rene workow model using the input log key
sequence. A major limitation is that they are not able to work with
a log sequence that contains multiple tasks or concurrent threads
in one task, which is addressed by our study. Our task separation
methodology also provides useful insights towards the workow
construction for each task.
5 EVALUATION
DeepLog is implemented using Keras [6] with TensorFlow [2] as the
backend. In this section, we show evaluations of each component
and the overall performance of DeepLog, to show its eectiveness
in nding anomalies from large system log data.
5.1 Execution path anomaly detection
is section focuses on evaluating the log key anomaly detection
model in DeepLog. We rst compare its eectiveness on large
system logs with previous methods, and then investigate the impact
of dierent parameters in DeepLog.
5.1.1 Previous methods. Previous work on general-purpose log
anomaly detection follows a similar procedure: they rst extract a
log key from each log message, and then perform anomaly detection
on the log key sequence.
e Principal Component Analysis (PCA) method [39] assumes
that there are dierent “sessions” in a log le that can be easily
identied by a session id aached to each log entry. It rst groups
log keys by session and then counts the number of appearances
of each log key value inside each session. A session vector is of
size n, representing the number of appearances for each log key
in K in that session. A matrix is formed where each column is
a log key, and each row is one session vector. PCA detects an
abnormal vector (a session) by measuring the projection length
on the residual subspace of transformed coordinate system. is
approach is shown to be more eective than its online counterpart
Session F2: Insights from Log(in)s CCS’17, October 30-November 3, 2017, Dallas, TX, USA 1292
online PCA [38] especially in reducing false positives, but this is
clearly an oine method and cannot be used for online anomaly
detection. e implementation is open-sourced by [17].
Invariant Mining (IM) [22] constructs the same matrix as the PCA
approach does. IM rst mines small invariants that could be satised
by the majority of vectors, and then treats those vectors that do
not satisfy these invariants as abnormal execution sessions. is
approach is shown to be more eective than an earlier work [11]
which utilizes workow automata. e implementation is opensourced by [17].
TFIDF is developed in [44]. Although its objective is for IT system
failure prediction, which is dierent from anomaly detection as
shown in [39]. Nevertheless, we still included this method in our
evaluation as it also uses a LSTM-based approach. ere are several
key dierences. TFIDF groups log keys by time windows (each time
window is dened by a user parameter), and then models each time
window (called “epoch”) using a TF-IDF (term-frequency, inverse
document frequency) vector. e Laplace smoothing procedure it
uses requires the knowledge of the total number of epochs (hence
the entire log le). TFIDF constructs a LSTM model as a binary
classier, which needs both labeled normal and abnormal data for
training. Not only are anomaly log entries hard to obtain, but also,
new types of anomalies that are not included in training data may
not be detected. In contrast, DeepLog trains its LSTM model to be
a multi-class classier, and only requires normal data to train.
CloudSeer is a method designed specically for multi-user OpenStack log [42]. It builds a workow model for each OpenStack VMrelated task and uses the workow for anomaly detection. ough it
achieves acceptable performance on OpenStack logs (a precision of
83.08% and a recall of 90.00% as reported in the paper), this method
does not work for other types of logs (e.g., HDFS log) where the
paerns of log keys are much more irregular. For example, CloudSeer only models log keys that “appear the same number of times”
in every session. In HDFS logs, only 3 out of 29 log keys satisfy
this criterion. Furthermore, this method cannot separate log entries
for dierent tasks in one log into separate sequences. It relies on
multiple identiers to achieve this, which is not always possible for
general-purpose logs. us it is not compared against here.
5.1.2 Log data sets and set up.
HDFS log data set. It is generated through running Hadoop-based
map-reduce jobs on more than 200 Amazon’s EC2 nodes, and labeled by Hadoop domain experts. Among 11, 197, 954 log entries
being collected, about 2.9% are abnormal, including events such
as “write exception”. is was the main data set rstly used by an
oine PCA-based [39] method, and subsequently used by several
other work including online PCA [38] and IM-based [22] methods.
Details of this dataset could be found in [38, 39].
OpenStack log data set. We deployed an OpenStack experiment
(version Mitaka) on CloudLab [30] with one control node, one network node and eight compute nodes. Among 1, 335, 318 log entries
collected, about 7% are abnormal. A script was running to constantly execute VM-related tasks, including VM creation/deletion,
stop/start, pause/unpause and suspend/resume. VM tasks were
scheduled with the paern of a regular expression (Create (Stop Start)
{0,3} (Pause Unpause) {0,3} (Suspend Resume) {0,3} Delete)+. A VM life
cycle starts with “VM create” and ends with “VM delete”, while task
pairs such as “Stop-Start”, “Pause-Unpause” and “Suspend-Resume”
may randomly appear from 0 to 3 times within a life cycle. INFO
level logs from nova-api, nova-scheduler and nova-compute were
collected and forwarded for analysis using Elastic Stack [33]. ree
types of anomalies were injected at dierent execution points: 1)
neutron timeout during VM creation; 2) libvirt error while destroying a VM; 3) libvirt error during cleanup aer destroying a VM.
Set up. To execute PCA-based and IM-based methods, we group
log entries into dierent sessions by an identier eld, which for
HDFS log is block_id and for OpenStack log is instance_id. Each
session group is a life cycle of one block or a VM instance respectively. We then parse each log entry into a log key. DeepLog can be
applied directly on log keys to train its weights and subsequently
be used to detect anomalies, while other methods require one more
step. ey need to count the number of appearances for each distinct log key within each session, and build a matrix where each
column is a distinct log key (so there will be n columns) and each
row represents a session vector, and the value of a cell Vij in the
matrix represents the count of log key kj
in the i-th session.
DeepLog needs a small fraction of normal log entries to train its
model. In the case of HDFS log, only less than 1% of normal sessions
(4,855 sessions parsed from the rst 100,000 log entries compared
to a total of 11,197,954) are used for training. Note that DeepLog
can pinpoint which log entry (with its corresponding log key) is
abnormal, but in order to use the same measures to compare with
competing methods, we use “session” as the granularity of anomaly
detection, i.e., a session C is considered an abnormal session as long
as there exists at least one log key from C being detected abnormal.
Table 3 summarizes the two data sets. Note that PCA and IM
are unsupervised oine methods that do not require training data,
whereas DeepLog only needs a training data produced by normal
system execution, and TFIDF requires both normal and abnormal
data to train.
Log Number of sessions n: Number
data set Training data (if needed) Test data of log keys
HDFS 4,855 normal; 553,366 normal; 29
1,638 abnormal 15,200 abnormal
OpenStack 831 normal; 5,990 normal; 40
50 abnormal 453 abnormal
Table 3: Set up of log data sets (unit: session).
In addition to the number of false positives (FP) and false negatives (FN), we also use standard metrics such as Precision, Recall
and F-measure. Precision= TP
TP+FP (TP stands for true positive) shows
the percentage of true anomalies among all anomalies detected;
Recall= TP
TP+FN measures the percentage of anomalies in the data
set (assume that we know the ground-truth) being detected; and
F-measure=2·Precision·Recall
Precision+Recall is the harmonic mean of the two.
By default, we use the following parameter values for DeepLog:
д = 9, h = 10, L = 2, and α = 64 and investigate their impacts
in our experiments. Recall д decides the cuto in the prediction
output to be considered normal (i.e., the д log key values with top-д
probabilities to appear next are considered normal), and h is the
window size used for training and detection. L and α denote the
number of layers in DeepLog and the number of memory units in
one LSTM block respectively. For all other methods, we explored
Session F2: Insights from Log(in)s CCS’17, October 30-November 3, 2017, Dallas, TX, USA 1293
PCA IM TFIDF N-gram DeepLog
false positive (FP) 277 2122 95833 1360 833
false negative (FN) 5400 1217 1256 739 619
Table 4: Number of FPs and FNs on HDFS log.
their parameter space and report their best results. When the Ngram method is used, we set N = 1 unless otherwise specied since
this shows the best performance for the N-gram method.
5.1.3 Comparison. Table 4 shows the number of false positives
and false negatives for each method on HDFS data. PCA achieves
the fewest false positives, but at the price of more false negatives.
Figure 6a shows a more in-depth comparison using recall, precision
and F-measure. Note that TFIDF is omied from this gure because
of limited space and its very poor relative performance.
Clearly, DeepLog has achieved the best overall performance, with
an F-measure of 96%. Our baseline solution N-gram also achieves
good performance when history length is 1. But, its performance
drops dramatically as history window becomes longer. In contrast,
LSTM-based approach is more stable as shown in Section 5.1.4.
Figure 6b investigates the top-д approach used by DeepLog’s
prediction algorithm. Let Dt be the set of top-д log key values
predicted by DeepLog at t, and mt be the actual log key value
appeared in the data at t. To see the impact of this strategy, we
study the CDF of Pr[mt ∈ Dt ] for dierent д values. Among over
11,000,000 log keys (that are labeled as normal) to predict, 88.9%
of DeepLog’s top prediction matches mt exactly; and 96.1% mt
’s
are within DeepLog’s top 2 predictions. When д = 5, 99.8% of
normal mt
’s are within Dt
, meanwhile the anomaly detection rate
is 99.994% (only one anomalous session was undetected).
Figure 7a shows the performance over OpenStack data set. e
PCA approach shows reasonable performance on this data set but
with low precision (only 77%), whereas even though IM has achieved
a perfect recall in this case, it has very poor precision of only 2%
(almost all VM instances are detected as abnormal executions).
is is because that OpenStack logs were generated randomly as
described in Section 5.1.2. Note that how many times that log keys
like (Stop Start) may appear in a life cycle of a VM (dened by a
pair of Create and Delete) is uncertain. is makes it really hard for
IM to nd the “stable small invariants” for anomaly detection.
To test this hypothesis, we generated a second data set with
a deterministic paern like (Create Delete)+, resulting in a total of
5,544 normal VM executions and 170 anomalous ones. We denote
this data set as OpenStack II and the result is shown in Figure 7b.
IM performs very well on this data set with more regular paerns.
However the recall for the PCA method drops to only 2% in this
case because the normal paern in the data is too regular, rendering
PCA method which detects anomalies by variance not working.
On the other hand, DeepLog demonstrates excellent performance
on both OpenStack logs with a F-measure of 98% and 97% respectively. Lastly, it is also important to note that PCA and IM are oine
methods, and they cannot be used to perform anomaly detection
per log entry. ey are only able to detect anomaly at session level,
but the notion of session may not even exist in many system logs.
5.1.4 Analysis of DeepLog. We investigate the performance impact of various parameters in DeepLog including: д, h, L, and α. e
results are shown in Figure 8. In each experiment, we varied the
values of one parameter while using the default values for others.
Precision Recall F-measure
(a) Accuracy on HDFS.
0.5
0.6
0.7
0.8
0.9
1.0
1.1
0.98
0.67
0.79
0.88
0.95
0.91 0.92
0.96 0.94 0.95 0.96 0.96
PCA
IM
N-gram
DeepLog
g=1 g=3 g=5 g=7 g=9 g=11
(b) Cumulative probability of top g predictions.
0.86
0.88
0.90
0.92
0.94
0.96
0.98
1.00
0.889
0.961
0.981
0.993
0.998 0.9994 0.9998
Figure 6: Evaluation on HDFS log.
Precision Recall F-measure
(a) Accuracy on OpenStack I.
0.0
0.2
0.4
0.6
0.8
1.0
0.77
0.99
0.87
0.02
1.0
0.05
0.91
1.0
0.96 0.95
1.0 0.98
Precision Recall F-measure
(b) Accuracy on OpenStack II.
0.0
0.2
0.4
0.6
0.8
1.0 1.0
0.02
0.03
0.83
1.0
0.91
0.83
1.0
0.91
0.94
0.99
0.97
PCA IM N-gram DeepLog
Figure 7: Evaluation on OpenStack log.
In general, the performance of DeepLog is fairly stable with respect
to dierent values, i.e., it is not very sensitive to the adjustment of
any one or combinations of these parameter values. is makes
DeepLog easy to deploy and use in practice. e results are fairly
intuitive to understand as well. For example, Figure 8c shows that
a larger д value leads to higher precision but lower recall. us,
д could be adjusted to achieve higher true positive rate or lower
false positive rate. Lastly, DeepLog’s prediction cost per log entry
is only around 1 millisecond on our standard workstation, which
could be further improved by beer hardware such as using a GPU.
1 2 3 4 5
(a) Number of layers: L.
0.5
0.6
0.7
0.8
0.9
1.0
32 64 128 192 256
(b) Number of memory units: ®.
0.5
0.6
0.7
0.8
0.9
1.0
g=7 g=8 g=9 g=10 g=11 g=12
(c) Top g predictions as normal.
0.5
0.6
0.7
0.8
0.9
1.0
5 6 7 8 9 10
(d) Window size: h.
0.5
0.6
0.7
0.8
0.9
1.0
Precision Recall F-measure
Figure 8: DeepLog performance with dierent parameters.
5.2 Parameter value and performance anomaly
To evaluate the eectiveness of DeepLog at detecting parameter
value and performance (including elapsed time between log entries)
Session F2: Insights from Log(in)s CCS’17, October 30-November 3, 2017, Dallas, TX, USA 1294
anomalies, we used system logs from the OpenStack VM creation
task. is data set includes both types of anomalies: performance
anomaly (late arrival of a log entry) and parameter value anomaly
(a log entry with a much longer VM creation time than others).
Experiment setup. As before, we deployed an OpenStack experiment on CloudLab, and wrote a script to simulate that multiple
users are constantly requesting VM creations and deletions. During
OpenStack VM creation, an important procedure is to copy the
required image from controller node to a compute node (where the
VM will be created). To simulate a performance anomaly which
could be possibly caused by a DoS aack, we throle the network
speed from the controller to compute nodes at two dierent points,
to see if these anomalies could be detected by DeepLog.
Anomaly detection. As described in Section 3.2, we separate log
entries into two sets, one set is for model training and the other
set (called the validation set) is to apply the model to generate the
Gaussian distribution of MSEs (mean square error). In subsequent
online detection phase, for every incoming parameter value vector
−→v , DeepLog checks if the MSE between −→v and the predication
output (a vector as well) from its model is within an acceptable
condence interval of the Gaussian distribution of MSEs from the
validation set.
Figure 9 shows the detection results for the parameter value
vectors of dierent log keys, where x-axis represents the id of the
VM being created (i.e., dierent VM creation instances), and y-axis
represents the MSE between the parameter value vector and the
prediction output vector from DeepLog. e horizontal lines in each
gure are the condence interval thresholds for the corresponding
MSE Gaussian distributions. Figure 9a and 9b represent two log keys
where their parameter value vectors are normal during the entire
time. Figure 9c and 9d illustrate that the parameter value vectors
for keys 53 and 56 are successfully detected as being abnormal at
exactly the two time instances where we throled the network
speed (i.e., injected anomalies).
For each abnormal parameter value vector detected, we identi-
ed the value that diers the most with the prediction, to identify
the abnormal column (feature). We found out that the two abnormal parameter value vectors for key 53 are due to unusually large
elapsed time values. On the other hand, key 56 is “Took * seconds to
build instance.”, and not surprisingly, its two abnormal parameter
value vectors were caused by unusually large values (for seconds).
5.3 Online update and training of DeepLog
Section 5.1 has demonstrated that DeepLog requires a very small
training set (less than 1% of the entire log) and does not require
user feedback during its training phase. But it is possible that a
new system execution path may show up during detection stage,
which is also normal, but is detected as anomalous since it was not
reected by the training data. To address this issue, this section
evaluates the eectiveness of DeepLog’s online update and training
module as described in Section 3.3. We demonstrate this using
the dierence in detection results with and without incremental
updates, in terms of both eectiveness and eciency.
5.3.1 Log data set. e log data set used in this section is Blue
Gene/L supercomputer system logs 1
, which contains 4,747,963 log
1CFDR Data, hps://www.usenix.org/cfdr-data
0 50 100 150
VM id
(a) Log key 25
0.00
0.05
0.10
0.15
0.20
0.25
0.30
MSE
CI=99%
CI=99.9%
CI=98%
0 50 100 150
VM id
(b) Log key 45
0.00
0.01
0.02
0.03
0.04
0.05
MSE
CI=99%
CI=99.9%
CI=98%
0 50 100 150
VM id
(c) Log key 53
0.0
0.2
0.4
0.6
0.8
MSE
CI=99%
CI=99.9%
CI=98%
0 50 100 150
VM id
(d) Log key 56
0.00
0.02
0.04
0.06
0.08
0.10
MSE
CI=99%
CI=99.9%
CI=98%
Figure 9: Anomaly detection for parameter value vectors with different condence intervals (CIs).
entries, of which 348,460 entries are labeled as anomalies. We chose
this data set because of an important characteristic: many log keys
only appeared during a specic time period. is means that the
training data set may not contain all possible normal log keys, let
alone all possible normal execution paerns.
5.3.2 Evaluation results. We conducted two experiments, one
uses the rst 1% normal log entries as training data and the other
uses the rst 10% log entries for training. In both seings, the
remaining 99% or 90% entries are used for anomaly detection. We
set L = 1, α = 256, д = 6, h = 3.
Precision Recall F-measure
(a) First 1% dataset for training.
0.0
0.2
0.4
0.6
0.8
1.0
0.16
1.00
0.27
0.82
1.00
0.90
Precision Recall F-measure
(b) First 10% dataset for training.
0.0
0.2
0.4
0.6
0.8
1.0
0.16
1.00
0.28
0.88
1.00
0.93
without online training with online training
Figure 10: Evaluation on Blue Gene/L log.
Figure 10 shows the results for without and with online training
for both experiments. In the case of “without online training”, we
run DeepLog to test incoming log entries without any incremental
update. While for the case of “with online training”, we assume
there is an end user who reports if a detected anomaly is a false
positive. If so, DeepLog uses that sample (now a labeled record)
to update its model to learn this new paern. Figure 10 shows
that without online training, with only 1% oine training data,
this results in many false positives (hence very low Precision rate).
ough increasing its training data to 10% slightly increases the
Precision, its performance is still unsatisfactory. On the other hand,
DeepLog with online training signicantly improves its Precision,
and hence F-measure scores. With a true positive rate of 100%
(perfect recall) in both seings, online training reduces false positive
Session F2: Insights from Log(in)s CCS’17, October 30-November 3, 2017, Dallas, TX, USA 1295
rate from 40.1% to 1.7% for 1% training data, and from 38.2% to 1.1%
for 10% training data, respectively.
Table 5 shows the amortized cost to check each log entry. For
the online training case, we reported time taken for both detection
and online update (if an update is triggered). e results show that
online update and training does increase the amortized cost per log
entry, but only slightly. is is because many log entries will not
trigger an update. Note that online update and online detection can
be executed in parallel; an update is carried out while the model is
using the current weights to continue performing detection.
Table 5: Amortized cost to check each log entry
training data percentage 1% 10%
without online training (milliseconds) 1.06 1.11
with online training (milliseconds) 3.48 2.46
5.4 Security log case studies
Anomalies having log keys that never showed up in normal logs
used for training (e.g., “ERROR” or “exception” log messages) are
easy to detect. DeepLog can eectively detect much more subtle
cases. For example, in HDFS log, “Namenode not updated aer
deleting block” anomaly is shown as a missing log key in a session;
and “Redundant addStoredBlock” anomaly is shown as an extra log
key. is means that for any aack that may cause any change of
system behavior (as reected through logs), it can be detected. In
what follows, we investigate system logs containing real aacks to
demonstrate the eectiveness of DeepLog.
5.4.1 Network security log. Network security is of vital importance. Both rewall and intrusion detection system (IDS) produce
logs that can be used for online anomaly detection.
To test the performance of DeepLog on network security logs, we
used the VAST Challenge 2011 data set, specically, Mini Challenge
2 — Computer Networking Operations [1]. is challenge is to
manually look for suspicious activities by visualization techniques.
It comes with ground truth for anomalous activities. For all anomalies in the ground truth, Table 6 shows the results of DeepLog. e
only suspicious activity not being detected is the rst appearance
of an undocumented computer IP address.
Table 6: VAST Challenge 2011 network security log detection
suspicious activity detected?
Day 1: Denial of Service aack Yes, log key anomaly in IDS log
Day 1: port scan Yes, log key anomaly in IDS log
Day 2: port scan 1 Yes, log key anomaly in IDS log
Day 2: port scan 2 Yes, log key anomaly in IDS log
Day 2: socially engineered aack Yes, log key anomaly in rewall log
Day 3: undocumented IP address No
e only false positive case happened when DeepLog reported a
log message that repeatedly appeared many times in a short period
as an anomaly. is is due to an event that suddenly became bursty
and printed the same log message many times in a short time range.
is is not identied by the VAST Challenge as a suspicious activity.
5.4.2 BROP aack detection. Blind Return Oriented Programming (BROP) aack [5] leverages a fact that many server applications restart aer a crash to ensure service reliability. is kind
of aack is powerful and practical because the aacker neither
relies on access to source code nor binary. A stack buer overow
vulnerability, which leads server to crash, is sucient to carry out
this aack. In a BROP exploit, the aacker uses server crash as a
signal to help complete a ROP aack which achieves executing a
shellcode. However, the repeated server restarting activities leave
many atypical log messages in kernel log as shown below, which is
easily detected by DeepLog.
nginx[*]: segfault at * ip * sp * error * in nginx[*]
nginx[*]: segfault at * ip * sp * error * in nginx[*]
nginx[*]: segfault at * ip * sp * error * in nginx[*]
......
5.5 Task separation and workow construction
We implemented the proposed methods in Section 4 and evaluated
on a log with various OpenStack VM-related tasks. Both LSTM
approach and density-based clustering approach could successfully
separate all tasks. e rst method requires LSTM; it is a supervised
method which requires training data to be provided. e second
method uses clustering on co-occurrences of log keys within a certain distance threshold, which is an unsupervised method. Hence,
it doesn’t require training, but it does require parameter τ as the
distance threshold.
Specically, for density-based clustering approach, with a suf-
ciently large threshold value τ ∈ [0.85, 0.95], there is a clear
separation of all tasks. Note that the value of τ cannot be too large
(e.g., seing τ = 1), as a background process may produce log entries at random locations that will break log entries from the same
task apart.
Next we use a part of the VM creation workow, to show how it
provides useful diagnosis of the performance anomaly in Section 5.2.
Recall in Section 5.2, parameter value vector anomaly is identied
on the time elapsed value of log key 53, and on the parameter
position of log key 56 (which represents how many seconds to build
instance). As shown in Figure 11, once an anomaly is detected by
DeepLog, we know the time taken to build that instance is abnormal,
but we don’t know why. en, since the elapsed time between log
key 53 and its previous log key is too big, by investigating the
workow model constructed by DeepLog, its previous key is 52:
“Creating image”, so we know that VM creation took longer time
than usual because the time to create image was too long. Further
investigation following this procedure may reveal that it was caused
by slow network speed from control node to compute node.
6 RELATED WORK
Primarily designed for recording notable events to ease debugging,
system event logs are abundantly informative and exist practically
on every computer system, making them a valuable resource to
track and investigate system status. However, since system logs are
largely composed of diverse, freeform text, analytics is challenging.
Numerous log mining tools have been designed for dierent systems. Many use rule-based approaches [7, 15, 28, 29, 31, 32, 40, 41],
which, though accurate, are limited to specic application scenarios and also require domain expertise. For example, Beehive [41]
Session F2: Insights from Log(in)s CCS’17, October 30-November 3, 2017, Dallas, TX, USA 1296
44 23 52 53
32
25
51
18
57 54
56
18
44: instance: * Attempting claim: memory * disk * vcpus * CPU
51: instance: * Claim successful
23: instance: * GET * HTTP\/1.1" status: * len: * time: *
52: instance: * Creating image
53: instance: * VM Started (Lifecycle Event)
32: instance: * VM Paused (Lifecycle Event)
18: instance: * VM Resumed (Lifecycle Event)
.......
56: instance: * Took * seconds to build instance
Figure 11: OpenStack VM Creation workow.
identies potential security threats from logs by unsupervised clustering of data-specic features, and then manually labeling outliers.
Oprea [28] uses belief propagation to detect early-stage enterprise
infection from DNS logs. PerfAugur [32] is designed specically to
nd performance problems by mining service logs using specialized features such as predicate combinations. DeepLog is a general
approach that does not rely on any domain-specic knowledge.
Other generic methods that use system logs for anomaly detection typically apply a two-step procedure. First, a log parser [9, 14,
16, 23, 36, 37] is used to parse log entries to structured forms, which
typically only contain “log keys” (or “message types”). Parameter
values and timestamps are discarded except for identiers which
are used to separate and group log entries. en, anomaly detection
is performed on log key sequences. A typical way is to generate
a numeric vector for each session or time window, by counting
unique log keys or using more sophisticated approaches like TF-IDF.
e matrix comprising of these vectors is then amenable to matrixbased unsupervised anomaly detection methods such as Principal
Component Analysis (PCA) [38, 39] and invariant mining (IM) [22].
Constructing such a matrix is oen an oine process, and these
methods are not able to provide log-entry level anomaly detection
(rather, they can only operate at session level). We refer the reader
to [17] for an overview and comparison on these methods.
Supervised methods [17, 44] use normal and abnormal vectors to
train a binary classier that detects future anomalies. A downside of
such methods is that unknown anomalies not in training data may
not be detected. Furthermore, anomalous data are hard to obtain
for training. We have shown in our evaluation that using only a
small portion of normal data to train, DeepLog can achieve online
anomaly detection with beer performance. Moreover, DeepLog
also uses timestamps and parameter values for anomaly detection
which are missing in previous work.
Workow construction has been studied largely using log keys
extracted from oine log les [4, 11, 21, 42] . It has been shown
that workow oers limited advantage for anomaly detection [11,
42]. Instead, a major utility of workows is to aid system diagnosis [4, 21]. However, all past work assumes a log le to model only
contains repeated executions of one single task. In this paper, we
propose methods to automatically separate dierent tasks from log
les in order to build workow models for dierent tasks.
Besides workows, other systems that perform anomaly diagnosis using system logs include DISTALYZER [26] that diagnoses
system performance issues by comparing a problematic log against
a normal one, LogCluster [19] which clusters and organizes historical logs to help future problem identication, and Stitch [45] that
extracts dierent levels of identiers from system logs and builds
a web interface for users to visually monitor the progress of each
session and locate performance problems. Note that they are for
diagnosis purposes once an anomaly has been detected, and cannot
be used for anomaly detection itself.
7 CONCLUSION
is paper presents DeepLog, a general-purpose framework for
online log anomaly detection and diagnosis using a deep neural
network based approach. DeepLog learns and encodes entire log
message including timestamp, log key, and parameter values. It
performs anomaly detection at per log entry level, rather than at
per session level as many previous methods are limited to. DeepLog
can separate out dierent tasks from a log le and construct a work-
ow model for each task using both deep learning (LSTM) and
classic mining (density clustering) approaches. is enables eective anomaly diagnosis. By incorporating user feedback, DeepLog
supports online update/training to its LSTM models, hence is able
to incorporate and adapt to new execution paerns. Extensive evaluation on large system logs have clearly demonstrated the superior
eectiveness of DeepLog compared with previous methods.
Future work include but are not limited to incorporating other
types of RNNs (recurrent neural networks) into DeepLog to test
their eciency, and integrating log data from dierent applications
and systems to perform more comprehensive system diagnosis (e.g.,
failure of a MySQL database may be caused by a disk failure as
reected in a separate system log).