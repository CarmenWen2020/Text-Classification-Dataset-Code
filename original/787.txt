Machine learning has become increasingly prominent and is widely
used in various applications in practice. Despite its great success,
the integrity of machine learning predictions and accuracy is a
rising concern. The reproducibility of machine learning models
that are claimed to achieve high accuracy remains challenging, and
the correctness and consistency of machine learning predictions in
real products lack any security guarantees.
In this paper, we initiate the study of zero knowledge machine
learning and propose protocols for zero knowledge decision tree
predictions and accuracy tests. The protocols allow the owner of a
decision tree model to convince others that the model computes
a prediction on a data sample, or achieves a certain accuracy on a
public dataset, without leaking any information about the model
itself. We develop approaches to efficiently turn decision tree predictions and accuracy into statements of zero knowledge proofs.
We implement our protocols and demonstrate their efficiency in
practice. For a decision tree model with 23 levels and 1,029 nodes, it
only takes 250 seconds to generate a zero knowledge proof proving
that the model achieves high accuracy on a dataset of 5,000 samples
and 54 attributes, and the proof size is around 287 kilobytes.
CCS CONCEPTS
• Security and privacy → Privacy-preserving protocols.
KEYWORDS
Zero knowledge proofs; Machine learning; Decision tree
1 INTRODUCTION
Machine learning has seen a great development over the past years,
leading to important progress in various research areas such as
computer vision, data mining, and natural language processing.
Despite the great success, there are many security concerns of machine learning techniques, one of which is the integrity of machine
learning models and their predictions. Newly developed machine
learning models are claimed to achieve high accuracy, yet it is challenging to reproduce the results and validate these claims in many
cases. In addition, even if a high quality model exists, it may not be
used consistently in real-world products. For example, an online
service for image classification claiming to use a particular model
may simply return a random answer, and there is no guarantee on
the integrity of the result for the clients. The authors in [4] report
an extreme case where a company claims to use machine learning
techniques to build robots delivering food automatically, yet the
robots are actually operated by remote workers.
These scenarios urge the need for a solution to ensure that the
owner indeed has a valid machine learning model, and it is used
to compute the predictions correctly or it achieves high accuracy
on public datasets. A naïve approach is to release the machine
learning models publicly. However, it completely sacrifices the
privacy of the machine learning models. Machine learning models
are becoming important intellectual properties of the companies
and cannot be shared publicly for validation. Releasing the machine
learning models as black-box testing software does not address
the issues as well. For example, in the machine learning service
scenarios above, the customers can just take the black-box software
away without paying. Even worse, recent research shows that one
can infer sensitive information or reconstruct the machine learning
models with only black-box accesses [25].
In this paper, we propose to address the problem of machine
learning integrity using the cryptographic primitive of zero knowledge proof (ZKP). A zero knowledge proof allows a prover to produce a short proof π that can convince any verifier that the result
of a public function f on the public input x and secret input w of
the prover is y = f (x,w). w is usually referred as the witness or
auxiliary input. Zero knowledge proofs guarantee that the verifier rejects with overwhelming probability if the prover cheats on
computing the result, while the proof reveals no extra information
about the secret w beyond the result.
During the last decade, there has been great progress on generic
ZKP schemes that are nearly practical. They allow proving arbitrary
computations modeled as arithmetic circuits. In principle, we can
Session 6E: Zero Knowledge CCS '20, November 9–13, 2020, Virtual Event, USA 2039
This work is licensed under a Creative Commons Attribution International 4.0 License.
apply these general-purpose ZKP schemes to address the problem
of machine learning integrity. The prover proves that she knows a
secret machine learning model that computes the prediction of an
input or achieves the claimed accuracy on a public dataset, without
leaking any additional information about the machine learning
model. The proof is succinct, meaning that it is much smaller than
the machine learning model and the prediction function. However,
it is particularly challenging to construct efficient ZKP for machine
learning predictions and accuracy tests because of the high overhead on the proof generation time. Because of these challenges,
ZKP schemes for machine learning computations have not been
widely studied in the literature.
Our contributions. In this paper, we initiate the study of zero
knowledge machine learning predictions and accuracy, and propose several efficient schemes for zero knowledge decision trees.
We also extend our techniques with minimal changes to support
variants of decision trees, including regression, multivariate decision trees and random forests. Decision trees and random forests
play important roles in various applications in practice, because of
their good explainability and interpretability: the predictions can
be explained by meaningful rules and conditions. They are widely
used for product recommendations, fraud detection and automated
trading in financial applications. Our concrete contributions are:
• Zero knowledge decision tree predictions. First, we propose
an efficient protocol for zero knowledge decision tree predictions.
After a setup phase to commit to a decision tree in linear time to
the size of the decision tree, the prover time is only proportional
to the length of the prediction pathh, and the number of attributes
d of the data. We apply several critical techniques in the literature
of ZKP for computations in the random access memory (RAM)
model in non-black-box ways, and translate the decision tree
prediction to a small circuit of size O(d + h).
• Zero knowledge decision tree accuracy. Second, we generalize our protocol to prove the accuracy of a decision tree in
zero knowledge. We develop two important optimizations to
bound the number of hashes in our ZKP backend to be exactly
the number of nodes N in the decision tree. It is independent of
the number of data samples to test, and is much less than 2
h
if
the decision tree is unbalanced.
• Implementation and evaluations. Finally, we fully implement
our protocols and evaluate their performance on several realworld datasets. Notably, for a large decision tree with 23 levels
and 1,029 nodes, it only takes 250s to generate a proof for its
accuracy on a testing dataset with 5,000 samples and 54 attributes.
The proof size is 287KB and the verification time is 15.6s.
1.1 Related Work
Zero knowledge proofs were introduced by Goldwasser et al. in [27]
and generic constructions based on probabilistically checkable
proofs were proposed in the seminal work of Kilian [30] and Micali [33]. In recent years there has been significant progress in
efficient ZKP protocols and systems. Categorized by their underlying techniques, there are succinct non-interactive argument of
knowledge (SNARK) schemes [9, 12, 17, 21, 24, 29, 34, 39], discretelog-based schemes [7, 13, 18, 28], hash-based schemes [14], interactive oracle proofs (IOP) [6, 8, 10, 42] and interactive-proof-based
schemes [40, 41, 43, 44]. Their security relies on different assumptions and settings, and they provide trade-offs between prover time
and proof size. In our construction, we use the ZKP scheme proposed in [10], named Aurora, as our backend because of its fast
prover time and good scalability. The proof size is relatively large
compared to other schemes. Please refer to [40–42] for more details
on the performance and comparisons of different ZKP schemes.
Most ZKP schemes model the computations as arithmetic circuits,
while decision tree predictions are naturally in the RAM model with
comparisons and conditional branching. Several papers [9, 11, 15,
39, 45] proposed ZKP schemes for RAM programs. We use some of
their techniques in our constructions, without going through the
heavy machinery of RAM-to-circuit reductions.
Zero knowledge proofs for machine learning applications have
not been studied extensively before. In [26], Ghodsi et al. proposed
a system named SafetyNet to delegate neural network predictions
to a cloud server. It assumes that the verifier has the neural network
and guarantees the soundness of the predictions. The scheme does
not support witness from the prover, and there is no notion of
zero knowledge. In [46], Zhao et al. proposed to use SNARK to
validate neural network predictions. The prover commits to the
values of all intermediate layers, and the verifier validates one
layer randomly with a SNARK proof. The scheme does not provide
negligible soundness. It also justifies the challenge of the overhead
on the prover time as we mentioned in the introduction, as it is too
expensive to apply SNARK to the whole machine learning model.
Finally, there is a rich literature on privacy-preserving decision
tree predictions and training using secure multiparty computations
(MPC), oblivious RAM (ORAM) and fully homomorphic encryptions (FHE) [16, 37, 38]. We note here that both the focus and the
techniques of these schemes are quite different from zero knowledge proofs. These schemes primarily guarantee the privacy of the
data during training and predictions. They do not provide integrity
of the results and the succinctness of the communication. The settings and applications are also different from our zero knowledge
decision trees.
2 PRELIMINARIES
We use negl(·) : N → N to denote the negligible function, where for
each positive polynomial f (·), negl(k) <
1
f (k)
for sufficiently large
integer k. Let λ denote the security parameter. Let [m] denote the
set of {1, 2, · · · ,m}. “PPT" standards for probabilistic polynomial
time. We use bold letters x, y, z to represent vectors, and x[i] denote
the i-th element in vector x.
2.1 Zero-knowledge Arguments
An argument system for an NP relationship R is a protocol between
a computationally-bounded prover P and a verifier V. At the end
of the protocol, V is convinced by P that there exists a witness
w such that (x;w) ∈ R for some input x. We focus on arguments
of knowledge which have the stronger property that if the prover
convinces the verifier of the statement validity, then the prover must
know w. We use G to represent the generation phase of the public
parameters pp. Formally, consider the definition below, where we
assume R is known to P and V.
Session 6E: Zero Knowledge CCS '20, November 9–13, 2020, Virtual Event, USA 2040
Definition 2.1. Let R be an NP relation. A tuple of algorithm
(G, P, V) is a zero-knowledge argument of knowledge for R if the
following holds.
• Completeness. For every pp output by G(1
λ
), (x;w) ∈ R and
π ← P(x,w, pp),
Pr[V(x, π, pp) = 1] = 1
• Knowledge Soundness. For any PPT prover P
∗
, there exists a
PPT extractor E such that given the access to the entire executing
process and the randomness of P
∗
, E can extract a witnessw such
that pp ← G(1
λ
), π
∗ ← P∗
(x, pp) and w ← E P
∗
(pp, x, π
∗
), the
following probability is negl(λ):
Pr[(x;w) < R ∧ V(x, π
∗
, pp) = 1]
• Zero knowledge. There exists a PPT simulator S such that for
any PPT algorithm V∗
, (x;w) ∈ R, pp output by G(1
λ
), it holds
that
View(V∗
(pp, x)) ≈ SV∗
(x)
We say that (G, P, V) is a succinct argument system if the total
communication between P and V (proof size) is poly(λ, |x |, log |w|).
In the definition of zero knowledge, View(V∗
(pp, x)) denotes the
veiw the verifier sees during the execution of the interactive process
with P while S
V∗
(x) denotes the view generated by S given input
x and transcript of V∗
, and ≈ denotes two distributions perfect
indistinguishable. This definition is commonly used in existing
transparent zero knowledge proof schemes [6, 10, 18, 40].
2.2 Our Zero-knowledge Argument Backend
With the recent progress on efficient zero knowledge proofs(ZKP)
protocols, there are several general purpose ZKP systems with
different trade-offs on the prover time, the proof size and the verification time. In our construction and implementation, we aim to
optimize for fast prover time and to scale to large decision trees.
Therefore, after careful comparisons among all existing ZKP systems, we choose the scheme named Aurora proposed in [10] as the
ZKP backend in our zero knowledge decision tree construction.
We state its properties in the following theorem. Note that our
construction is also compatible with other ZKP systems.
Theorem 2.2. [10]. Let λ be the security parameter, for a finite
field F and a family of layered arithmetic circuit CF over F, there
exists a zero knowledge argument of knowledge for the relation
R = {(C, x;w) : C ∈ CF ∧C(x;w) = 1},
as defined in Definition 2.1, where x is the public input and w is the
auxiliary input(private to the prover) to the circuit C.
Moreover, for every (C, x;w) ∈ R, the running time of P isO(|C| log |C|)
field operations. The running time of V is O(|C|) and the proof size is
O(log2
|C|), where |C| is the number of arithmetic gates in the circuit
C.
Aurora has no trusted setup. Its security is proven in the random
oracle model, and is plausibly post-quantum secure. It can be made
non-interactive using Fiat-Shamir [23] in the random oracle model.
In addition, in order to build our zero knowledge decision tree
scheme, we require an additional algorithm of the general purpose ZKP protocol to commit the witness. This is formalized as
“Commit-and-Prove” in [19], and is naturally supported by Aurora
and most of ZKP systems. We denote the algorithm as comw ←
Commit(w, pp). It is executed after G and before P, and V additionally takes comw as an input. It satisfies the extractability of
commitment. Similar to the extractability in Definition 2.1, there
exists a PPT extractor E, given any tuple (pp, x, com∗
w ) and the
executing process of P
∗
, it could always extract a witness w
∗
such
that com∗
w ← Commit(w
∗
, pp) except for the negligible probability
in λ. Formally speaking, com∗
w = Commit(E P
∗
(pp, x, com∗
w ), pp).
3 ZERO KNOWLEDGE DECISION TREE
In this section, we present our main construction of zero knowledge
decision tree predictions. We first introduce the background on decision trees. Then we formally define the notion of zero knowledge
decision tree predictions, and present our protocol with the security
analysis.
3.1 Decision Tree
Decision tree is one of the most commonly used machine learning
algorithms. It performs particularly well for classification problems,
and has good explainability and interpretability. Therefore, it is
widely deployed in applications such as product recommendations,
fraud detection and automated trading.
For simplicity, we focus on binary decision trees for classification
problems in our presentation, but our techniques can be generalized
naturally to decision trees with higher degrees, decision trees for
regression problems, multivariate decision trees and random forests
with small changes. We present these variants in Appendix A. In a
decision tree T, each intermediate node contains an attribute, each
branch represents a decision and each leaf node denotes an outcome
(categorical or continues value). More formally, each internal nodev
has an associated attribute indexv.att from the set [d] ofd attributes,
a threshold v.thr and two children v.left and v.right. Each leaf
node u stores the classification result u.class. Each data sample is
represented as a size-d vector a of values corresponding to each
attribute. The algorithm of decision tree prediction is shown in
Algorithm 1. It starts from the root of T. For each node of v in T, it
compares a[v.att] withv.thr, and moves to v.left if a[v.att] < v.thr,
andv.right otherwise. Eventually, the algorithm reaches a leaf node
u and the result of the prediction is u.class.
To train a decision tree, given a training dataset, the decision
tree is obtained by splitting the set into subsets from the root to
the children. The splitting is based on some splitting rules by maximizing the certain objective function such as the information gain
Algorithm 1 Decision Tree Prediction
Input: Decision tree T, data sample a
Output: classification ya
1: v := T.root
2: while v is not a leaf node do
3: if a[v.att] < v.thr then
4: v := v.left
5: else
6: v := v.right
7: return v.class
Session 6E: Zero Knowledge CCS '20, November 9–13, 2020, Virtual Event, USA 2041
and the splitting process is repeated on each derived subset in a
recursive manner called recursive partitioning. The recursion will
halt when the subset of a node has the same classification, or when
splitting can not increase the value of the objective function. In our
scheme, we only consider proving predictions given the pretrained
decision tree, and the training process is out of the scope.
3.2 Zero Knowledge Decision Tree Prediction
Motivated by the applications mentioned in the introduction, in
our model, the prover owns a pre-trained decision tree. The prover
commits to the decision tree first, and then later the verifier queries
for the prediction of a data sample. The prover generates a proof
together with the result to convince the verifier its validity.
Formally speaking, let F be a finite field, T be a binary decision
tree of height h and N nodes (N ≤ 2
h−1). Suppose the test dataset is
D ⊆ F
d
, in which each data point has d features. So for each data a
in D, a ∈ F
d
. Let [M] be the set of all target classifications. We treat
the decision tree algorithm as a mapping T : F
d → [M]. For a data
point a ∈ D, T (a) ∈ [M] is the prediction for the classification of a
using the decision tree algorithm on T. A zero-knowledge decision
tree scheme (zkDT) consists of the following algorithms:
• pp ← zkDT.G(1
λ
): given the security parameter, generate the
public parameter pp.
• comT ← zkDT.Commit(T, pp,r): commit the decision tree T
with a random point r generated by the prover.
• (ya, π) ← zkDT.P(T, a, pp): given a data a, run the decision tree
algorithm to get ya = T (a) and the corresponding proof π.
• {0, 1} ← zkDT.V(comT,h, a,ya, π, pp): validate the prediction
of a given ya and π obtained from the prover.
In our scheme, we assume the height of the decision tree (or an
upper bound) is known to both parties. comT is the commitment
of the decision tree. ya denotes the class of a returned by the decision tree. And π denotes the proof generated by the prover. {0, 1}
represents reject or accept output by the verifier after seeing the
classification and the proof.
Definition 3.1. We say that a scheme is a zero knowledge decision
tree if the following holds:
• Completeness. For any decision tree T and a data point a ∈ F
d
,
pp ← zkDT.G(1
λ
), comT ← zkDT.Commit(T, pp,r), (ya, π) ←
zkDT.P(T, a, pp), it holds that
Pr[zkDT.V(comT,h, a,ya, π, pp) = 1] = 1
• Soundness. For any PPT adversary A, the following probability
is negligible in λ:
Pr













pp ← zkDT.G(1
λ
)
(T∗
, comT∗ , a,y
∗
a
, π
∗
) ← A(1
λ
, pp,r)
comT∗ = zkDT.Commit(T∗
, pp,r)
zkDT.V(comT∗ ,h, a,y
∗
a
, π
∗
, pp) = 1
T (a) , y
∗
a













• Zero Knowledge. For security parameter λ, pp ← zkDT.G(1
λ
),
for a decision tree T with h levels, PPT algorithm A, and simulator S = (S1,S2), consider the following two experiments:
RealA, T(pp):
(1) comT ← zkDT.Commit (T, pp,r)
(2) a ← A(h, comT, pp)
(3) (ya, π) ← zkDT.P(T, a, pp)
(4) b ← A(comT,h, a,ya, π, pp)
(5) Output b
IdealA,SA (pp,h):
(1) com ← S1(1
λ
, pp,h)
(2) a ← A(h, com, pp)
(3) (ya, π) ←SA
2
(com,h, a, pp), given oracle access
to ya = T (a).
(4) b ← A(com,h, a,ya, π, pp)
(5) Output b
For any PPT algorithm A and all decision tree T with the height
of h, there exists simulator S such that
| Pr[RealA, T(pp) = 1] − Pr[IdealA,SA (pp,h) = 1]| ≤ negl(λ).
Intuition of the specific construction of zkDT: given the algorithm of decision tree and the definition of the zero knowledge
decision tree protocol(zkDT), we will focus on the specific construction of our zkDT scheme in the subsequent subsections. The general
idea of the construction is as follows. In the beginning, the prover
sends the committment of a decision tree T, comT, to the verifier.
After receiving a from the verifier, the prover computes ya and the
corresponding witness w for proving ya = T (a), then sends ya to
the verifier. We treat it as some relationship R = ((ya, a, comT);w)
in Definition 2.1. Then the verifier and the prover invoke the backend zero-knowledge proofs protocol in subsection 2.2 to verify the
relationship R without leaking any information of T except for ya.
3.3 Authenticated Decision Tree
We start with the scheme to commit to a decision tree. A naive
approach to do so is to simply compute the cryptographic hash of
the whole decision tree. However, later when the prover wants to
prove the prediction of a data sample using the committed decision
tree, the whole decision tree has to be included as the witness of
the zero knowledge proof, while only the path from the root to the
leaf node of the output is relevant for the prediction. This would
introduce a high overhead to recompute the hash of the whole tree.
One could also build a Merkle hash tree [32] on top of all the
nodes in the decision tree (with a predefined order). Then later in
zkDT.P, the prover associates each node in the prediction path with
a valid Merkle tree proof. The overhead of this approach will be
O(h log N) hashes in the zero knowledge proof backend to validate
the all nodes in the prediction path. Instead, we propose to leverage
the rich literature of authenticated data structures [36]. We propose
to build authenticated decision trees (ADT) directly on the data
structure of decision trees so that proving a prediction path only
relies on the information stored in the nodes.
Construction of ADT: Figure 1 illustrates our construction of
ADT. The construction is very similar to Merkle hash tree, with
the difference that the data stored in an intermediate node of the
decision tree is also included in the computation of its hash, together
with the hashes of the two children. In particular, each node v
contains the attribute v.att, the threshold v.thr, the pointers to the
Session 6E: Zero Knowledge CCS '20, November 9–13, 2020, Virtual Event, USA 2042
comADT = Hash(lc, r)
Hash(lc, rc, v0
, v0
.thr, v0
.att, v0
.left, v0
.right)
Hash(lc, rc, v1
, v1
.thr, v1
.att, v1
.left, v1
.right)
.
.
.
Hash(vN−1
, vN−1
.class) Hash(vN, vN.class)
Hash(lc, rc, v2
, v2
.thr, v2
.att, v2
.left, v2
.right)
.
.
.
r
Figure 1: Committing algorithm of ADT scheme, lc and rc
represent the left child value and the right child value respectively.
children v.left and v.right, and the hashes of its children. In the
real implementation, we use different identities in [N] to represent
the nodes in T. Therefore, v.left and v.right are identities of the
left child and right child of v respectively.
The verification algorithm is also similar to that of the Merkle
hash tree. To validate the prediction for a data sample, the proof
includes the prediction path from the root to the leaf node that
outputs the prediction result. In addition, the proof also includes the
hashes of the siblings of the nodes along the prediction path. With
the proof, the verifier can recompute the root hash and compare
it with the commitment. In this way, to prove the validity of a
prediction, the verification only computes O(h) hashes. Note that in
our construction of zkDT, the verification of ADT is never executed
by the verifier directly. As we will show in the next section, the
prover further proves that there exists such a valid prediction path
through a general purpose zero knowledge proof. Because of this
design, the verification of ADT does not have to be zero knowledge.
The algorithms of our ADT are in the following. Note that in
order to prove the zero knowledge property of the scheme later,
the commitment has to be randomized and we add a random point
r to the root of the decision tree and use the hash of the root
concatenated with r as the final commitment, as shown in Figure 1.
Moreover, for the purpose of our application, the ADT does not
have to support dynamic insertions and deletions, which simplifies
the construction significantly.
• pp ← ADT.G(1
λ
): Sample a collision resistant hash function
from the family of hash functions.
• comADT ← ADT.Commit(T, pp,r): compute hashes from leaf
nodes to the root of T with the random point r as shown in
Figure 1.
• πADT ← ADT.P(T, Path, pp): given a path in T, πADT contains
all siblings of the nodes along the path Path and the randomness
r in Figure 1.
• {0, 1} ← ADT.V(comADT, Path, πADT, pp): given Path and πADT,
recompute hashes along Path with πADT as the same progress
in Figure 1 and compare the root hash with comADT. Output 1 if
they are the same, otherwise output 0.
Given these algorithms and the construction, we have the following
theorem:
Theorem 3.2. Let T be a decision tree with h levels and N nodes,
our ADT scheme satisfies the following properties.
• Completeness: if pp ← ADT.G(1
λ
), Path ∈ T, comADT ←
ADT.Commit(T, pp,r) and πADT ← ADT.P(T, Path, pp), then
Pr[ADT.V(comADT, Path, πADT, pp) = 1] = 1
• Soundness: for any PPT adversary A, if pp ← ADT.G(1
λ
), comADT
← ADT.Commit(T, pp,r), π
∗
ADT ← A(T, Path, pp) but Path <
T, then
Pr[ADT.V(comADT, Path, π
∗
ADT, pp) = 1] ≤ negl(λ)
• Hiding: pp ← ADT.G(1
λ
), for any decision tree T with h levels,
any PPT algorithm A, there exists a simulator SADT: let comADT =
ADT.Commit(T, pp,r) and com′
ADT = SADT(pp,h,r),
| Pr[A(comADT, pp) = 1] − Pr[A(com′
ADT, pp) = 1]| ≤ negl(λ)
In addition, the time of ADT.Commit is O(N), and the prover time,
verification time and the proof size are all O(h).
Proof Sketch: The completeness of our ADT scheme is straight forward. The soundness holds because of the collision-resistance of
the hash function. To prove the hiding property, we can construct a
simulator SADT(pp,h,r) = ADT.Commit(0®
h, pp,r), where 0®
h represents a decision tree with h levels and all nodes containing only 0
strings. It is indistinguishable from the real algorithm because the
verifier does not know r, which is uniformly random. We omit the
formal proofs here.
3.4 Proving the validity of the prediction
Following the algorithm to commit to a decision tree, we further
present our protocol to prove the correctness of the prediction.
A natural idea is to invoke ADT.P and ADT.V directly to obtain
the valid prediction path, and check the prediction in Algorithm 1.
However, this is not zero knowledge as the verifier would learn a
path in the decision tree. We propose to apply an additional zero
knowledge proof protocol on top of this validation. As mentioned
in the previous section, the prover instead proves that there exists
a valid prediction path such that ADT.V would accept, and the
prediction algorithm is correctly executed. By applying generic
zero knowledge proofs on this relationship, the prediction path and
the hashes of the siblings remain confidential as the witness, and
the output is merely 1 or 0, i.e. all the checks are satisfied or not. In
this way, the protocol is both sound and zero knowledge.
However, efficiently designing zero knowledge proof protocols
for such an relationship turns out to be non-trivial. This is because
every node v on the prediction path for a data sample a will access
one attribute from a indexed by v.att for comparison, which is a
classical random access operation. Most generic zero knowledge
proof protocols represent computations as arithmetic circuits. Implementing the decision tree prediction algorithm as an arithmetic
circuit introduces a high overhead on the prover time. In particular,
each comparison in an internal node leads to an overhead of O(d),
and the overall size of the circuit is O(dh). There are a few RAMbased generic zero knowledge proof protocols [9, 11, 15, 45] in the
literature that represent computations as RAM programs. However,
though asymptotically the prover time only depends on the running time of the RAM program, the concrete overhead is very high
Session 6E: Zero Knowledge CCS '20, November 9–13, 2020, Virtual Event, USA 2043
com!
Permutation check of
� and �%
� =
1, �[1] , … , (�, �[�])
�+ =
�!, �[�!] , … , (�", �[�"]) path� = �", �#, … �$ ��
Decision tree prediction: While �# is not a leaf node do
1. �# = �#. att
2. if �[�#] < �#. thr
then �#$% = �#. left;
else �#$%= �#. right;
3. � = � + 1;
Path validation of ADT
���� = �", �#, ⋯ , �$ �
�� = �#. class
Figure 2: Zero knowledge decision tree prediction. Public inputs are in black, secret witness is in red, and extended witness
for efficiency is in blue.
(thousands of arithmetic gates per step of the program). Instead,
in our construction, we apply some of the ideas in these work in a
non-black-box way to design a specific and efficient zero knowledge
proof protocol for the validation of decision tree predictions.
Reducing decision tree prediction circuit efficiently. Figure 2
illustrates our design to efficiently reduce the validity of the prediction using a committed decision tree to an arithmetic circuit.
As shown in the figure, the public input (in black) of the circuit
consists of the data sample a, the commitment of the decision tree
comT and the prediction result ya. The secret witness (in red) from
the prover includes the prediction path patha
, and the randomness
r used in the commitment of ADT (for technical reasons to prove
zero knowledge). In order to improve the efficiency, the prover
further inputs the siblings of nodes on the prediction path, and the
permutation a¯ of the data sample a ordered by v.att of the nodes
on the prediction path as part of the witness (in blue). The purpose
of these “extended” witness will be explained below. The whole
circuit consists of three parts: (1) validating the prediction algorithm of the decision tree, (2) checking the permutation between a
and a¯, and (3) checking the validity of the prediction path in the
committed decision tree. Finally, the output of the circuit is either 1
or 0, denoting either all the conditions are satisfied or some check
fails.
Decision tree prediction. The first component of the circuit is to
validate the prediction algorithm. With the help of a¯, this can be
efficiently implemented using an arithmetic circuit. In particular, we
slightly modify the representation of a and a¯ to be index-value pairs,
i.e., a = (1, a[1]), . . . ,(d, a[d]) and a = (i1, a[i1]), . . . ,(id
, a[id
]).
Under this representation, the circuit simply checks that for every
internal nodevj on the prediction path (j = 1, . . . ,h−1), (1)vj
.att =
ij
, and (2) if a[ij] < vj
.thr,vj+1 = vj
.left, otherwisevj+1 = vj
.right.
As we explained in the previous subsection, v,v.left,v.right ∈ [N].
The equality tests and comparisons are computed using standard
techniques in the literature of circuit-based zero knowledge proof
with the help of auxiliary input [34]. Finally, the circuit checks
if ya = vh .class. The circuit outputs 1 if all the checks pass, and
outputs 0 otherwise. The total number of gates in this part is O(d +
h), which is asymptotically the same as the plain decision tree
prediction in Algorithm 1.
Note that if h < d, which is usually true in practice, the circuit
only checks the indices of the first h − 1 pairs in a¯. The rest of the
indices are arbitrary, as long as a¯ is a permutation of a. It does
not affect the correctness or the soundness of the scheme, as those
attributes are not used for prediction anyway. In addition, concrete
decision trees are usually not balanced. The prover and the verifier
can either agree on the length of the prediction path and construct
a separate circuit for every data sample, or use the height of the
tree as an upper-bound to construct the same circuit for all data
samples. The former is more efficient, but leaks the length of the
prediction paths. Both options are supported by our scheme and
the asymptotic complexity are the same. For simplicity, we abuse
the notation and use h both for the height of the tree and for (an
upper bound of) the length of the prediction path.
Permutation test. The second component is to check that a¯ is
indeed a permutation of a. Together with the first component, it
ensures that ya is the correct prediction result of a using the prediction path patha
. The construction is inspired by the RAM-based
zero knowledge proof systems and we also apply the techniques
of characteristic polynomials proposed in [15, 43, 45] to check permutations. In particular, the characteristic polynomial of a vector
c = (c[1], · · · , c[d]) ∈ F
d
is χc(x) = Π
d
i=1
(x − c[i]), the polynomial
with roots c[i]. To prove permutations between c and c, it suffices
to show that their characteristic polynomial evaluates to the same
value at a random point r ∈ F chosen by the verifier:
Π
d
i=1
(r − c[i]) = Π
d
i=1
(r − c[i])
The soundness error is d
|F|
by Schwartz-Zippel Lemma [35, 47].
In our construction, however, we need to prove that two vectors
of pairs a and a¯ are permutations of each other. To this end, we use
the approach proposed in [15] to pack each pair to a single value
by a random linear combination. The verifier chooses a random
point z ∈ F. For each pair (j, a[j]) in a and (ij
, a¯[j]) in a¯, the circuit
computes c[j] = a[j] +z × j and c¯[j] = a¯[j] +z ×ij
. We then invoke
the characteristic polynomial evaluation directly on c and c¯.
The completeness is straight forward. To prove the soundness,
suppose a is not a permutation of a, then there must exist a pair
(i, a[i]) not appearing in a. After the packing process, for each j,
Pr[a[i]+z ×i = a¯[j]+z ×ij
|(i, a[i]) , (ij
, a¯[j])] ≤ 1
|F|
by SchwartzZippel Lemma. Therefore, although a[i] + z × i is one root of the
characteristic polynomial of c, the probability of that it is also one
root of the characteristic polynomial for c is at most d
|F|
by the
Session 6E: Zero Knowledge CCS '20, November 9–13, 2020, Virtual Event, USA 2044
union bound. Combining with the soundness of the characteristic
polynomial checking, we obtain that our method has the soundness
error at most 2d
|F|
, which is also negligible of λ. The technique can
be extended to verify the permutation of vectors of more than two
elements by packing them with a polynomial of z.
The circuit outputs 1 if and only if the above check passes. The
number of gates is O(d). Here we assume that every attribute is
only used at most once in any prediction path. When an attribute
can be used multiple times, which is also very common in practice,
the circuit instead checks that a¯ is a multiset of a, i.e., every pair in
a appears in a¯ with cardinality greater than or equal to 0. We will
present a technique to check the multiset relationship in Section 4.
The sub-circuits for decision tree prediction and path validation
remain unchanged, and the total number of gates in the circuit is
the same asymptotically, as the size of a¯ in bounded by h in this
case.
Path validation. Finally, the only missing component is to ensure that the prediction path is indeed valid as committed before.
The third sub-circuit implements the ADT.V algorithm with input
patha
, all sibling hash values on patha
, random point r and comT.
The circuit recomputes the hashes from the leaf to the root, and
compares the root hash with comT. In total, the circuit computes
O(h) hashes, which justifies the design of our ADT scheme.
Finally, the circuit aggregates all three checks and output 1 if
and only if all checks pass. The size of the whole circuit is O(d +h),
which is also asymptotically optimal. The prover and the verifier
then execute a generic circuit-based zero knowledge proof protocol
on the circuit in Figure 2.
3.5 Putting Everything Together
In this section, we combine everything together and formally present
our zero knowledge decision tree prediction scheme in Protocol 1.
Our scheme has a transparent setup phase where zkDT.G does
not have a trapdoor. It merely samples a collision-resistant hash
function for ADT, and executes the algorithm G in the generic zero
knowledge proof. Using Aurora as our backend, it also samples a
hash function modeled as a random oracle.
We have the following theorem:
Theorem 3.3. Protocol 1 is a zero knowledge decision tree scheme
by Definition 3.1.
Proof. Completeness. As explained in Section 3.3 and 3.4, the
circuit in zkDT.P outputs 1 ifya is the correct prediction of a output
by Algorithm 1 on T. Therefore, the correctness of Protocol 1
follows the correctness of the ADT and the zero knowledge proof
protocol by Theorem 3.2 and 2.2.
Soundness. By the extractability of commitment of in Theorem 2.2,
with overwhelming probability, there exists a PPT extractor E
such that given comw , it extracts a witness w
∗
such that comw =
ZKP.Commit(w
∗
, pp2
). By the soundness of zkDT in Definition 3.1,
if comT = zkDT.Commit(T, pp,r) and zkDT.V(comT,h, a,ya, π, pp)
= 1 but ya , T (a), let comw = ZKP.Commit(w
∗
, pp2
) during the
interactive process in Protocol 1, then there are two cases.
• Case 1:w
∗ = (a
∗
,path∗
a
, aux∗
,r)satisfying toC((comT, a,ya, r
′
);w
∗
)
= 1. Then we could know either path∗
a
is not a path in T but
passing the verification for comT, or a
∗
is not a permutation of a
Protocol 1 (Zero Knowledge Decision Tree(zkDT)). Let λ be the security parameter, F be a prime field, T be a decision with h levels, C be the
arithmetic circuit in Figure 2. Let P and V be the prover and the verifier
respectively. We use ZKP.G, ZKP.Commit, ZKP.P, ZKP.V to represent
the algorithms of the backend ZKP protocol.
• pp ← zkDT.G(1
λ
): let pp1 ← ADT.G(1
λ
), pp2 ← ZKP.G(1
λ
) and
pp = (pp1
, pp2
).
• comT←zkDT.Commit(T, pp, r): comT←ADT.Commit(T, pp1
, r),
where r is the randomness generated by P.
• (ya, π ) ← zkDT.P(T, a, pp):
(1) P runs the algorithm 1 with input T and a to get ya = T(a). Then
generates the witness w = (a, patha, aux, r) for the circuit C in accordance with the procedure of the decision tree algorithm. aux represents
the extended witness in Figure 2. Let comw ← ZKP.Commit(w, pp2
).
P sends comw and ya to V.
(2) After receiving the randomness r
′
for checking the permutation of a and
a from V, P invokes ZKP.P(C, (comT , a, ya, r
′
), w, pp2) to get π .
Sends π to V.
• {0, 1} ← zkDT.V(comT, h, a, ya, π, pp): V outputs 1 if
ZKP.V(C, (comT , a, ya, r
′
), π, comw, pp2) = 1, otherwise it outputs 0.
[Simulator for Protocol 1] Let λ be the security parameter, F be a prime
field, T be a decision with h levels, C be the arithmetic circuit in Figure 2.
(pp1
, pp2
) ← zkDT.G(1
λ
).
• com ← S1(1
λ
, pp, h): S1 invokes SADT to generate com =
SADT(pp1
, h, r), where r is the randomness generated by SADT.
• (ya, π ) ← SA
2
(h, a, pp):
(1) S2 asks the oracle of T to get ya = T(a). Then S2 shares all public
input of C to SZKP and invokes SZKP.Commit(pp2
) to get comw .
(2) After receiving the randomness r
′
for the permutation check from A,
it invokes SZKP.P(C, (com, a, ya, r
′
), pp2
) to get π . Then S2 sends
π to A.
• {0, 1} ← A(com, h, a, ya, π, pp): wait A for validation.
but passing the permutation test. The probability of both events
are negl(λ) as claimed by the soundness the ADT scheme and the
soundness of the characteristic polynomial check respectively.
Hence, the probability that P could generate such w
∗
is also
negl(λ) by the union bound.
• Case 2: w
∗ = (a
∗
,path∗
a
, aux∗
,r) but C((comT, a,ya, r
′
);w
∗
) = 0.
Then according to the soundness of Aurora, given the commitment com∗
w , the adversary could generate a proof πw making
V accept the incorrect witness and output 1 with probability
negl(λ).
Combining these two cases, the soundness of the zkDT scheme is
also negl(λ).
Zero-knowledge. In order to prove the zero-knowledge property,
we construct a PPT simulator S = (S1,S2) in Figure 3. Let SADT
and SZKP represent the simulator for ADT protocol and the backend ZKP protocol respectively. The proof follows by a standard
hybrid argument.
Hybrid H0: H0 behaves in exactly the same way as the honest
prover in Protocol 1.
Hybrid H1: H1 uses the real zkDT.Commit(T, pp,r) in Protocol 1
Session 6E: Zero Knowledge CCS '20, November 9–13, 2020, Virtual Event, USA 2045
for the commitment phase, it invokes SZKP to simulate the interactive proof phase.
Hybrid H2: H2 behaves in exactly the same way as the simulator
of Protocol 1.
Given the same commitment comT, the verifier cannot distinguish H0 and H1, because of the zero knowledge property of the
backend ZKP protocol given the same circuit C and the same public
input. If the verifier could distinguish H1 from H2, then we can
find a PPT adversary to distinguish whether a commitment is of
an empty decision tree with only zero strings or not. This is contradictory to the hiding property of our ADT scheme. Therefore,
the verifier cannot distinguish H0 from H2 by the hybrid, which
completes the proof of zero knowledge. □
Efficiency. The prover’s computation consists of two parts: committing to the decision tree T and generating the proof for circuit C. For the committing phase, the provers needs to do O(N)
hashes. For the proof generation phase, the total computation is
O(|C| log |C|) = O((d+h) log(d+h)) = O(d logd) in accordance with
Theorem 2.2 andd > h. The verifier’s computation only contains the
verification for the circuitC with sizeO(d+h), so it isO(|C|) = O(d)
due to Theorem 2.2. The proof size is one digest plus the Aurora
proof for the circuit C, which is only O(log2
|C|) = O(log2
d). Finally, we can apply the Fiat-Shamir heuristic [23] to remove the
interactions in our zkDT protocol in the random oracle model.
4 ZERO KNOWLEDGE DECISION TREE
ACCURACY
In this section, we present our scheme for zero knowledge decision
tree accuracy. As motivated in the introduction, in this scenario,
the prover owns and commits to a decision tree, receives a testing dataset from the verifier, and then proves the accurary of the
committed decision tree model on this dataset. The verifier learns
nothing about the model except its accuracy.
Formally speaking, similar to Section 3.2, suppose the decision
tree model is T withh levels and N nodes, whereh and N are known
to both parties, the testing dataset is D = {a1, a2, · · · , an } with n
data samples and their matching labels are L = {ℓ1, ℓ2, · · · , ℓn }. A
zero knowledge decision tree accuracy (zkDTA) scheme consists of
the following algorithms:
• pp ← zkDTA.G(1
λ
): given the security parameter, generate the
public parameter pp.
• comT ← zkDTA.Commit(T, pp,r): commit the decision tree T
with a random point r generated by the prover.
• (accu, π) ← zkDTA.P(T, D, L, pp): given a dataset D and their
labels L, run the decision tree algorithm for each data sample
in D, compare the predictions with L, and then output accu
denoting the accuracy of the decision tree, i.e., the total number
of correct predictions. The algorithm also generates the corresponding proof π.
• {0, 1} ← zkDTA.V(comT,h, N, D, L, accu, π, pp): validate the
number of correct predictions, accu, given π obtained from the
prover.
Definition 4.1. We say that a scheme is a zero knowledge decision
tree accuracy if the following holds:
• Completeness. For any decision tree T with h levels and N
nodes, a test dataset D with corresponding labels L, comT ←
zkDTA.Commit(T, pp,r), (accu, π) ← zkDTA.P(T, D, L, pp),
it holds that
Pr[zkDTA.V(comT,h, N, D, L, accu, π, pp) = 1] = 1
• Soundness. For any PPT adversary A, the following probability
is negligible in λ:
Pr
















pp ← zkDTA.G(1
λ
)
(T∗
, comT∗ , D, L, accu∗
, π
∗
) ← A(1
λ
, pp,r)
comT∗ = zkDTA.Commit(T∗
, pp,r)
zkDTA.V(comT∗ ,h, N, D, L, accu∗
, π
∗
, pp) = 1
Õn
i=1
I(T∗
(ai) = ℓi) , accu∗
















I(T∗
(ai) = ℓi) = 1 if T
∗
(ai) = ℓi
, otherwise I(T∗
(ai) = ℓi) = 0.
• Zero Knowledge. For security parameter λ, pp ← zkDT.G(1
λ
),
for any decision tree T with h levels and N nodes, PPT algorithm A, and simulator S = (S1,S2), consider the following two
experiments:
RealA, T(pp):
(1) comT ← zkDTA.Commit (T, pp,r)
(2) D, L ← A(h, N, comT, pp)
(3) (accu, π) ← zkDTA.P(T, D, L, pp)
(4) b ← A(comT,h, N, D, L, accu, π, pp)
(5) Output b
IdealA,SA (pp,h, N):
(1) com ← S1(1
λ
, pp,h, N)
(2) D, L ← A(h, N, com, pp)
(3) (accu, π) ←SA
2
(com,h, N, D, L, pp), given oracle access to accu =
Ín
i=1
I(T (ai
) = ℓi
).
(4) b ← A(com,h, N, D, L, accu, π, pp)
(5) Output b
For any PPT algorithm A and all decision tree T with the height
of h and N nodes, there exists simulator S such that
| Pr[RealA, T(pp) = 1] − Pr[IdealA,SA (pp,h, N) = 1]| ≤ negl(λ).
4.1 Checking Multiset
Intuitively, when designing the zkDTA scheme, one can repeat
the construction in Section 2 described in Protocol 1 and Figure 2
multiple times for every data sample in the testing dataset, followed
by an aggregation circuit testing the accuracy. The prover time in
this case grows roughly linearly with the size of the testing dataset.
However, the prediction paths share many common nodes and their
total size may exceed the number of nodes in the decision tree on a
large testing dataset. I.e., N < nh.
We introduce an optimization for this case. Instead of validating
each prediction path one by one, the idea is to validate all N nodes
of the decision tree in one shot. Then the circuit checks that the
nodes of each prediction path are drawn from these N nodes of
the decision tree and they form a path with the correct parentchildren relationships. On top of these checks, the circuit tests the
Session 6E: Zero Knowledge CCS '20, November 9–13, 2020, Virtual Event, USA 2046
correctness of Algorithm 1 in the same way as the zkDT scheme and
computes the accuracy of the model. To test that each node in the
prediction paths is included in the original decision tree, it suffices
to check that all nodes of the prediction paths form a multiset of
the set of N nodes of the decision tree. We can again validate such
a multiset relationship using the characteristic polynomials.
Multiset check. Suppose Q = (q1,q2, · · · ,qm) is an array of m
elements with possible duplicates, and S = {s1, · · · ,sn } is a set
of size n. The prover needs to show that Q is a multiset of S, i.e.,
∀i ∈ [m],qi ∈ S. We apply the technique proposed in [45]. The
characteristic polynomial of a multiset Q is defined as Π
m
i=1
(x −qi)
with possible duplicated elements in Q. It can also be computed as
Π
n
i=1
(x−si)
fi
, where fi
is the multiplicity of each elementsi
in array
Q. Therefore, to check the multiset relationship, the prover provides
the multiplicity fi
, the verifier picks a random number r and the
circuit tests Π
m
i=1
(r −qi) = Π
n
i=1
(r −si)
fi
. We call polynomial д(x) =
Π
m
i=1
(x−qi)−Π
n
i=1
(x−si)
fi
the multiset polynomial. The soundness
of the test also follows the Schwartz-Zippel Lemma [35, 47]. If
there exists qi < S, then the multiset polynomial д(x) is a nonzero polynomial with degree at most m, if we additionally force
Ín
i=1
fi = m. Then Pr[д(r) = 0|r
$←− F] ≤ m
|F|
by the SchwartzZippel Lemma. So the soundness error is at most m
|F|
= negl(λ).
To implement this test in an arithmetic circuit, as there is no
exponentiation gate, we ask the prover to provide fi
in binary. Then
the circuit checks that the inputs are indeed binary, and uses the
multiplication tree to compute the exponentiation. As Ín
i=1
fi = m
and fi ≤ m, the prover only provides logm bits for each fi
.
With the multiset check, in our zkDTA scheme, the prover provides all N nodes in T and proves that all the nodes in the prediction
paths form a multiset of the N nodes. In addition, the circuit reconstructs the ADT using the N nodes of T and checks that it
is consistent with comT. In this way, the total number of hashes
is bounded by N if N = 2
h
. The number of gates for the multiset check is at most O(nh + N log(n)). In the real implementation,
computing hashes is usually the bottleneck of the efficiency. For
example, SHA-2 takes around 270, 000 multiplication gates and algebraic hash functions[5] take hundreds to thousands of gates to
implement. Our optimization reduces the number of hashes from
O(nh) to O(N), which greatly improves the performance of the
ZKDTA scheme in practice.
Furthermore, the method of the multiset check can also be used
in our zkDT protocol in Protocol 1 to support decision trees with
repeated attributes on the prediction paths, which is very common in practice. We instead test that a is a multiset rather than a
permutation of a in this case.
4.2 Validating Decision Tree
In the previous optimization, the circuit checks that N nodes provided by the prover form a valid decision tree. Using ADT, we
can validate it by reconstructing the decision tree in the circuit 2
h
hashes. However, in practice, we notice that most decision trees are
not balanced. For example, in our largest decision tree model, there
are total 23 levels and 1029 nodes. In this case, N ≪ 2
h
. Therefore,
in this section, we present an approach to validate a decision tree
with a circuit of size linear to N rather than 2
h
.
Algorithm 2 Linear Check for Valid Decision Tree
Input: N nodes of T1,T2, · · · ,TN .
1: T1 is the root: T1.id = 1 ∧T1.depth = 1 ∧T1.pid = 0.
2: for i = 1 to N − 1 do
3: Ti+1.id = Ti
.id + 1
4: for i = 1 to N − 1 do
5: Ti+1.depth = Ti
.depth ∨ Ti+1.depth = Ti
.depth + 1
6: TN .depth ≤ h.
7: Check all pids except T1 of {T2.pid, · · · ,TN .pid} are a multiset
of {1, 2, 3, · · · , N} with individual multiplicity at most 2.
8: Define two vectors of tuples, S1 and S2.
9: for i = 1 to N do
10: if Ti
.lid , 0 then
11: S1 = S1.append((Ti
.depth,Ti
.id,Ti
.lid))
12: if Ti
.rid , 0 then
13: S1 = S1.append((Ti
.depth,Ti
.id,Ti
.rid))
14: if Ti
.pid , 0 then
15: S2 = S2.append((Ti
.depth − 1,Ti
.pid,Ti
.id))
16: Check S1 is a permutation of S2.
17: return 1 if all check pass.
We first replace the commitment by a hash of all N nodes concatenated by a random value r, instead of the root of the ADT. In
addition, each node contains a unique id(id) in [N], the id of its
parent (pid), left child (lid), right child (rid) and its depth (depth) in
[h] (the id is 0 means the parent or the child is empty). To verify
that all N nodes of T1,T2, · · · ,TN form a binary decision tree, it
suffices to check the following conditions:
• Only the first node is the root. (i.e, T1.pid = 0 but Ti
.pid , 0
when i , 1.)
• All parent pointers are consistent with the child pointers. (i.e.,
Ti
.rid = Tj
.id or Ti
.lid = Tj
.id if and only if Tj
.pid = Ti
.id.)
• The depth of the parent is smaller than the depth of the child.
(i.e., if Ti
.pid = Tj
.id then Ti
.depth = Tj
.depth + 1.)
• No repeated child pointers. (i.e., if Ti
.lid , 0 then Ti
.lid , Ti
.rid.)
With this idea in the mind, the formal algorithm is presented in
Algorithm 2. The main difference is that the checks in Algorithm 2
can be efficiently implemented by arithmetic circuits.
Theorem 4.2. Algorithm 2 outputs 1 if and only if the input nodes
form a valid binary tree except for negligible probability. The total
number of arithmetic gates to implement the algorithm is O(N).
Proof. On the one hand, if the N nodes of T1,T2, · · · ,TN construct a valid binary tree, it is easy to check they will pass all checks
and Algorithm 2 always outputs 1.
On the other hand, suppose Algorithm 2 outputs 1. With overwhelming probability, (1) T1.depth = 1 and T1.pid = 0; (2) Ti
.id = i
for all i; (3) all depths is a multiset of {1, 2, · · · ,h}; (4) all pids except T1.pid are in [N]; (5) if Ti
.rid = Tj
.id or Ti
.lid = Tj
.id then
Tj
.pid = Ti
.id and Tj
.depth = Ti
.depth + 1; (6) if Ti
.pid = Tj
.id
then Tj
.lid = Ti
.id or Tj
.rid = Ti
.id and Tj
.depth = Ti
.depth − 1;
(7) if Ti
.lid , Ti
.rid unless Ti
.lid = Ti
.rid = 0. If one of (1),
(2), (3), (4) does not hold, it will not pass the checks from line
1 to line 7. If (5) does not hold but S1 is a permutation of S2, we
know that (Ti
.depth,Ti
.id,Ti
.lid) = (Ti
.depth,Ti
.id,Tj
.id) ∈ S1 or
Session 6E: Zero Knowledge CCS '20, November 9–13, 2020, Virtual Event, USA 2047
Permutation check of
each �� and �"�
� = (��, ⋯ , �� ) (�"�, ⋯ , �"�)
Using decision tree prediction
for each �� to get ���
Check ���� = ∑%&'
( �(��� = �%)
���ℎ��, ⋯ , ���ℎ��
com!
Commitment check
�
Multiset check
� = {� (�', ⋯ , �)) � ', ⋯ , �(} accu
Figure 4: Zero knowledge decision tree accuracy.
Protocol 2 (Zero Knowledge Decision Tree Accuracy). Let λ be the security parameter, F be a prime field, T be a binary decision with h levels and N
nodes, CA be the arithmetic circuit for model accuracy test, D = {a1, a2, · · · , an } of size n be the test set, L = {ℓ1, ℓ2, · · · , ℓn } be the corresponding labels
of test data. Let PA and VA be the prover and the verifier respectively. ZKP.G, ZKP.Commit, ZKP.P, ZKP.V represent the algorithms of the backend ZKP
protocol.
• pp ← zkDTA.G(1
λ
): pp ← ZKP.G(1
λ
).
• comT ← zkDTA.Commit(T, pp, r): comT = Hash(T, r), it hashes all nodes in T with r, the randomness generated by PA.
• (accu, π ) ← zkDTA.P(T, D, L, pp): we could use Fiat-Shamir heuristic transformation to make the following process non-interactive.
(1) PA runs the algorithm 1 with input T and the data set D to get yai = T(ai ) for all i. Let accu =
Ín
i=1
I(yai
= ℓi). According the procedure of the
algorithm, the prover could generate the witness ai and pathai
for each data point ai
, together with all nodes in T, their multiplicity in {pathai
}
n
i=1
and
the randomness r used in comT as the witness wA of the circuit CA. Let comwA ← ZKP.Commit(wA, pp2
). PA sends comwA
and accu to VA.
(2) After receiving the randomness r
′
for checking characteristic polynomials and the multiset polynomial from VA. PA invokes
ZKP.P(CA, (comT , D, L, accu, r
′
), wA, pp2
) to get π . Send π to VA.
• {1, 0} ← zkDTA.V(comT, h, N, D, L, accu, π, pp): VA outputs 1 if ZKP.V(CA, (comT , {a}
n
i=1
, accu, r
′
), π, comwA
, pp) = 1, outputs 0 otherwise.
(Ti
.depth,Ti
.id,Ti
.rid) = (Ti
.depth,Ti
.id,Tj
.id) ∈ S1. There must
exist an index k ∈ [N] such that (Tk
.depth − 1,Tk
.pid,Tk
.id) =
(Ti
.depth,Ti
.id,Tj
.id). Then we have k = j, Tj
.pid = Ti
.id and
Ti
.depth = Tj
.depth − 1, which is contradictory. Therefore, if (5)
does not hold, S1 is not a permutation of S2. With very similar
argument, if (6) does not hold, S1 is not a permutation of S2. If (7)
does not hold, S1 is not a permutation of S2 as S1 has duplicate
element while S2 does not. Therefore, if (5) or (6) or (7) does not
hold, then it will not pass the check in line 16 with high probability.
When (1)-(7) hold, we can construct a directed graphG = (T[N ]
, E)
as follows. (Ti
,Tj) ∈ E if and only if Ti
.pid = Tj
.id. The outdegree
of T1 is 0 while all others are 1. To prove G is a tree, we only need to
show Ti connects to T1 for each i, or equivalently, there is no circle
in the graph. That is because Ti
.depth = Tj
.depth−1 if (Ti
,Tj) ∈ E.
So G does not have a circle and it must be a tree. It is a binary tree
as the indegree of each nodes are at most 2.
Complexity. Step 1-6 and Step 8-15 can be computed by a circuit
doing a linear scan of all the N nodes. In addition,as the the individual multiplicity is at most 2, both the multiset test in Step 7 and the
permutation test in Step 16 are O(N) as we explained in previous
sections. Furthermore, we force the N nodes sorted by the id and
the depth. It consumes only O(N) to check Ti
.id = i and the depth
ranging in [h]. So the total number of the gates is O(N). □
An alternative approach. Alternatively, because of our application of decision trees, we can simplify the checks above. Recall
that in other parts of the circuit, it is proven that the nodes of
the prediction paths are drawn from the set of N nodes, and each
prediction path follows the prediction function of a decision tree.
Therefore, the graph formed by these nodes already satisfies the
second and the third conditions of being a binary decision tree.
Because of this, we simplify the test to (1) the N nodes are sorted
by the id, Ti
.id = i, where T1 is the root and (2) Ti
.lid , Ti
.rid
unless they are empty for all i ∈ [N]. These checks ensure that the
subgraph formed by all nodes in the prediction paths is part of a
binary decision tree. These nodes are also included in the N nodes
because of the multiset check. There is no guarantee on other nodes
of the N nodes provided by the prover, but they are not used in the
prediction paths anyway, which is good enough for our purposes.
This alternative approach is simpler than the check described in
Algorithm 2. It is a trade-off between the efficiency and the security.
In practice, as computing hashes is the bottleneck of our system,
the difference between these two approaches on the prover time is
actually not significant. We use the latter in our implementation.
4.3 Our construction of zkDT accuracy
With the optimizations presented in the previous sections, we show
the circuit CA to validate decision tree accuracy in Figure 4, and
present the formal protocol of zkDTA in Protocol 2. Compared with
the circuitC in Figure 2 for the zkDT scheme,CA has extra extended
witness with all nodes of T and an auxiliary list F = (f1, f2, · · · , fN )
representing the multiplicity of N nodes appearing in all prediction
paths of the test data. CA also has one more part for multiset check.
Besides, CA does not need to do path validation for each path, it
only recomputes the hash of all nodes in T with r and compares
the final value with the commitment. We call it commitment check.
Furthermore, in this new scheme, allyai
are not public to the verifier.
The circuit compares yai with ℓi and computes the number of
correct predictions. Finally, the circuit compares the number of
correct predictions to the claimed accuracy. We have the following
theorem:
Theorem 4.3. Protocol 2 is a zero knowledge decision tree accuracy
scheme defined by Definition 4.1.
The completeness, soundness and zero-knowledgeness of Protocol 2 are extensions of these properties in Protocol 1. We omit the
proof because of the space limitation.
Efficiency. Consider the circuit CA, the circuit size is O(nh + nd +
N logn + N) = O(nd) when N ≪ nd. Therefore, the prover time
Session 6E: Zero Knowledge CCS '20, November 9–13, 2020, Virtual Event, USA 2048
is O(nd log(nd)) with O(N) hashes in the committing phase, the
verification time is O(nd) and proof size is O(log2
(nd)) according
to Theorem 2.2.
5 IMPLEMENTATION AND EVALUATIONS
We fully implement our zero knowledge decision tree schemes and
we present their performance in this section.
Software. The schemes are implemented in C++. There are around
2000 lines of code for our frontend to compile the decision tree
predictions and accuracy to arithmetic circuits1
, as shown in Figure 2 and 4. We use the open-source compiler of libsnark[1] to
generate arithmetic circuits in our frontend, and we implement
the zero knowledge argument scheme Aurora [10] ourselves as the
ZKP backend. We use the extension field of a Mersenne prime Fp
2 ,
where p = 2
61 − 1. This is the same as the field used in [42].
We download the datasets from the UCI repository datasets[22]
and train the decision tree models using the sklearn package in
Python. Then we use these pre-trained decision trees and the testing
datasets in our experiments. The attributes are scaled to 32-bit
integers in the field for our ZKP backend. As the attributes are only
used for comparisons, the scaling does not affect the prediction and
the accuracy of the decision trees.
Hardware. We run all of the experiments on Amazon EC2 c5n.2xlarge
instances with 64GB of RAM and Intel Xeon platinum 8124m CPU
with 3GHz virtual core. Our current implementation is not parallelized and we only use a single CPU core in the experiments. We
report the average running time of 10 executions.
Hash function. As we will show in the experiments, computing
hashes in the arithmetic circuits is the bottleneck of our system. For
example, it takes 27,000 multiplication gates to compute one SHA256 hash. Therefore, in order to improve the performance of our
system, we use the SWIFFT[31] hash function, which is an algebraic
hash function that is collision-resistant and is friendly to arithmetic
circuits. With the optimizations proposed in jsnark[2], one SWIFFT
hash can be implemented with around 3,000 multiplication gates.
Datasets. We use three datasets from the UCI machine learning
repository [22]. The small dataset we use is named Breast-CancerWisconsin(Original). It is used for breast cancer diagnosis. Each
data has 10 attributes and the prediction is either 0 or 1. We train
the model on a training set of 600 data points. The pre-trained
decision tree has 61 nodes and 10 levels. The second dataset we use
is Spambase. It is used to recognize the spam emails. Each data has
57 attributes and the prediction is either 1 or 0. We train the model
on training set of 4, 000 data points. The pretrained decision tree
has in total 441 nodes and 26 levels. The largest dataset is Forest
Covertype. It is used to predict forest cover type from cartographic
variables. Each data has 54 attributes and the total number of class
is 7. We train the model on a training set of 5, 000 data points. The
pre-trained decision tree has in total 1, 029 nodes and 23 levels.
5.1 Performance of ZKDT
We first present the performance of our zero knowledge decision
tree prediction protocol. We vary the length of the prediction paths
1We actually use the rank-1-constraint-system (R1CS) to be compatible with the
backend.
Length h 6 12 24 48
#Attributes d 10 54 57 1000
Commit Time (ms) 0.38 6.3 2.8 13.3
Prover Time (s) 0.754 1.577 3.433 7.024
Verifier Time (s) 0.050 0.104 0.221 0.445
Proof size (KB) 140.736 155.936 172.224 189.632
Table 1: Performance of zero knowledge decision tree predictions.
from 6 to 48. The prediction paths of the first 3 columns are obtained
from the decision trees of the three real-world datasets described
above, while the last one is obtained from synthetic data. Table 1
shows the performance of our scheme.
As we can see in the Table 1, the efficiency of our zkDT scheme
is reasonable in practice. Though linear to the size of the whole
decision trees, the time to commit the decision trees is only on
the order of milliseconds. This is because the Commit in our ADT
only involves computing hashes, which is very fast in practice. For
the prover time and the verification time, it takes 7.02 seconds to
generate the proof for a prediction path of length 48, and 0.445
seconds to validate the proof. The proof size is 189KB. Note that we
choose our ZKP backend of Aurora [10] to optimize for the prover
time. Our zkDT scheme works on all backends and we could use
schemes such as Bulletproof [18] and SNARK [34] to reduce the
proof size to several KBs or less, with a sacrifice on the prover time.
Moreover, the prover time and the verification time scale roughly
linearly with the length of the prediction paths, while the number
of attributes d does not affect the performance by much. This is
because the bottleneck of the efficiency is computing the hashes
in the circuit of the ZKP backend for the path validation, and the
number of hashes is linear to the length of the path. Besides, the
performance of our scheme fully depends on the parameters of the
decision trees and the size of the data samples, but not on the values.
Hence, we do not observe any major difference on the performance
between the real datasets and the synthetic dataset.
In Appendix B, we compare the performance of our zkDT scheme
with the baseline of using RAM-based and circuit-based generic
ZKP schemes. Our scheme improves the prover time by orders of
magnitude, as we reduce the decision tree prediction to a circuit of
optimal size without using generic RAM-to-circuit reduction. We
also extend the implementation to zero knowledge random forests
trained on the same dataset. Table 2 shows the performance of
random forests consisting of different number of decision trees.
5.2 Performance of ZKDTA
In this section, we further evaluate the performance of our zero
knowledge decision tree accuracy scheme. We implement Protocol 2
and test it on decision trees trained on the three datasets described
# of Trees 2 8 32 128
Length h 24 24 12 12
Commit Time (ms) 13.49 54.95 115.14 468.56
Prover Time (s) 6.992 29.317 60.532 253.41
Verifier Time (s) 0.447 1.842 3.83 15.86
Proof size (KB) 189.728 225984 245632 286592
Table 2: Performance of zero knowledge random forest predictions.
Session 6E: Zero Knowledge CCS '20, November 9–13, 2020, Virtual Event, USA 2049
102 103 10
4
#data samples
10
100
500
prover time (s)
h = 23, N = 1029
h = 26, N = 441
h = 10, N = 61
(a) Prover time
102 103 10
4
#data samples
0.1
1
10
50
verifier time (s)
h = 23, N = 1029
h = 26, N = 441
h = 10, N = 61
(b) Verification time
102 103 10
4
#data samples
100
300
500
proof size (KB)
h = 23, N = 1029
h = 26, N = 441
h = 10, N = 61
(c) Proof size
Figure 5: Performance of the zero knowledge decision tree accuracy scheme.
before. We vary the number of data samples in the testing dataset
as 100, 300, 1000, 3000 and 5,000. We present the prover time, proof
size and verification time in Figure 5.
As shown in Figure 5, our zkDTA scheme achieves very good
efficiency in practice. On the largest instance with a decision tree of
1029 nodes and 23 levels, it only takes 250 seconds to generate a zero
knowledge proof for its accuracy on a large testing dataset of 5,000
data samples. We believe this overhead is close to practical in many
applications. The verification time is around 15.6 seconds. In this
case, the proof size is 287KB, which is actually smaller than the size
of the decision tree and the size of the testing dataset. This means
our scheme not only provides soundness and zero knowledge, but
also reduces the communicating comparing to the naïve solution
of posting the model and the dataset. The gap will further increase
because of the succinctness of the proof size.
In addition, Figure 5 shows that the prover time and the verification time for all three models remain mostly unchanged until
the size of the testing dataset becomes larger. This is because the
bottleneck of our scheme is computing hashes in the circuit of the
ZKP backend. The number of hashes is proportional to the total
number of nodes in the decision tree, and does not depend on the
size of the testing dataset. For example, for the large decision tree
model with N = 1029 nodes, when the size of the testing dataset
is less than 1000, the sub-circuit checking the commitment using
hashes consists of around 2
21 multiplication gates and contributes
to more than 75% of the whole circuit. Thus, the performance of the
scheme remains mostly the same for dataset with less than 1000
samples. As the size of the testing dataset becomes larger than 1000,
other components of the circuit to check decision tree predictions,
multisets and permutations start to impact the performance. There
are 2
23 multiplication gates in our largest data point in the figure.
The observation above also justifies the importance of our two
optimizations proposed in Section 4. If we simply repeat the zero
knowledge predictions multiple times, the number of hashes will
increase with the size of the testing dataset. In the largest data
point, there are 5,000 samples and each prediction path is of length
around 20. The performance would be 100× slower compared to
our optimized scheme. Similarly, as shown by the three real-world
datasets, the decision trees are usually not balanced. Without our
second optimization in Section 4, the number of hashes would be
2
h
, which is much larger than N. For the largest decision tree with
N = 1029 and h = 23, our optimization improves the prover time
and proof size by around 8000×.
Finally, the accuracy of the three decision trees (from small to
large) are 94.89%, 92% and 64.25% respectively on their largest
testing datasets with 5,000 samples. They are exactly the same as
the original decision trees trained from the datasets, as our zkDTA
scheme does not use any approximations and does not introduce
any accuracy loss.
5.3 Applications of Our Schemes
Besides ensuring the integrity of decision tree predictions and accuracy in the scenarios motivated in the introduction, our zero
knowledge decision tree schemes can also be applied to build a
fair and secure trading platform for machine learning models on
blokchains. Machine learning models are valuable assets now, and
people want to monetize their expertise on machine learning by
selling high-quality models. In this scenario, the buyer prefers to
test the quality of the machine learning model before making the
payment, while the seller does not want to reveal the model first,
as the buyer could disappear without any payment after seeing the
model. This is the classical problem of fair exchange. One could
rely on a trusted party to address this problem. However, it often
introduces a heavy burden on the trusted party to validate the quality of the models, enforce the payments and resolve the disputes.
Some applications may lack the existence of such a trusted party.
Blockchain is a promising technique to replace the role of the
trusted party in this scenario. In a blockchain, all the users validate the data posted on the blocks such that as long as more than
50% of the users are honest, the data on the blockchain is valid
and cannot be altered. Zero knowledge contingent payments [3, 20]
provide a framework for users to trade secret data fairly and securely on blockchains. Instead of posting the data directly on the
blockchain, which reveals the data to all users of the blockchain,
the seller posts a short zero knowledge proof about the properties
of the data. Subsequent protocols enforces the payment and the
delivery of the data simultaneously using smart contracts, given
that the zero knowledge proof is valid. In order to build a trading
platform for machine learning models on blockchains, efficient ZKP
protocols for machine learning accuracy are the only missing piece
in the framework of zero knowledge contingent payments. Our
zero knowledge decision tree schemes in this paper fill in this gap
and can be used to build such a fair and secure trading platform.
Realizing the system of the trading platform is left as future work.