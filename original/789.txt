The wide application of deep learning technique has raised new
security concerns about the training data and test data. In this work,
we investigate the model inversion problem under adversarial settings, where the adversary aims at inferring information about the
target model‚Äôs training data and test data from the model‚Äôs prediction values. We develop a solution to train a second neural network
that acts as the inverse of the target model to perform the inversion.
The inversion model can be trained with black-box accesses to the
target model. We propose two main techniques towards training
the inversion model in the adversarial settings. First, we leverage
the adversary‚Äôs background knowledge to compose an auxiliary
set to train the inversion model, which does not require access to
the original training data. Second, we design a truncation-based
technique to align the inversion model to enable effective inversion of the target model from partial predictions that the adversary
obtains on victim user‚Äôs data. We systematically evaluate our approach in various machine learning tasks and model architectures
on multiple image datasets. We also confirm our results on Amazon Rekognition, a commercial prediction API that offers ‚Äúmachine
learning as a service‚Äù. We show that even with partial knowledge
about the black-box model‚Äôs training data, and with only partial
prediction values, our inversion approach is still able to perform
accurate inversion of the target model, and outperform previous
approaches.
CCS CONCEPTS
‚Ä¢ Security and privacy ‚Üí Domain-specific security and privacy architectures; ‚Ä¢ Computing methodologies ‚Üí Neural
networks.
KEYWORDS
neural networks; deep learning; model inversion; security; privacy
1 INTRODUCTION
Machine learning (ML) models, especially deep neural networks,
are becoming ubiquitous, powering an extremely wide variety of
applications. The mass adoption of machine learning technology
has increased the capacity of software systems in large amounts of
complex data, enabling a wide range of applications. For example,
facial recognition APIs, such as Amazon Rekognition APIs [2], provide scores of facial attributes, including emotion and eye-openness
levels. There are also online services that evaluate users‚Äô face beauty
and age [8, 50, 52]. Users often share their face-beauty scores on
social media for fun. The prediction scores are definitely linked to
the input face data, but it is not apparent to what level of accuracy
the original data can be recovered. Security concerns arise in such
applications.
Model inversion is a technique that aims to obtain information
about the training data from the model‚Äôs predictions. Research
efforts of model inversion are largely divided into two classes of approaches. The first class inverts a model by making use of gradientbased optimization in the data space [21, 30, 37, 39, 43, 44, 72]. We
call this class of approach optimization-based approach. For example, model inversion attack (MIA) [21] was proposed to infer
training classes against neural networks by generating a representative sample for the target class. It casts the inversion task
as an optimization problem to find the ‚Äúoptimal‚Äù data for a given
class. MIA works for simple networks but is shown to be ineffective
against complex neural networks such as convolutional neural network (CNN) [28, 58, 65]
1
. This is mainly because the optimization
objective of optimization-based approach does not really capture
the semantics of the data space, which we will further discuss in
Section 2.2. The second class of approaches [10, 17, 18, 53] inverts
a model by learning a second model that acts as the inverse of
the original one. We call this class of approach training-based approach. They aim at reconstructing images from their computer
vision features including activations in each layer of a neural network. Therefore, to maximally reconstruct the images, they train
1Our experiment also obtains similar result as shown in Figure 1 column (f).
Session 2B: ML Security I CCS ‚Äô19, November 11‚Äì15, 2019, London, United Kingdom 225
the second model using full prediction vectors on the same training
data of the classifier.
In this work, we focus on the adversarial scenario, where an
adversary is given the classifier and the prediction result, and aims
to make sense of the input data and/or the semantics of the classification. Such problem is also known as the inversion attack [21, 22]
wherein information about the input data could be inferred from
its prediction values. There are two adversarial inversion settings,
namely data reconstruction and training class inference.
Data reconstruction: In this attack, the adversary is asked to
reconstruct unknown data given the classifier‚Äôs prediction vector
on it, which is exactly the inversion of the classifier. For example, in
a facial recognition classifier that outputs the person‚Äôs identity in a
facial image, the attacker‚Äôs goal is to reconstruct the facial image
of a person.
Training class inference: This kind of attack aims at recovering a
semantically meaningful data for each training class of a trained
classifier. In the above-mentioned facial recognition example, the
attack goal is to recover a recognizable facial image of an arbitrary
person (class) in the training data. This attack can be achieved by
inverting the classifier to generate a representative sample which
is classified as the wanted class [21].
However, inversion in adversarial settings is different from existing model inversion settings. Specifically, in adversarial settings,
the attacker does not have the classifier‚Äôs training data. Thus, previously known training-based methods that use the same training
data to train the ‚Äúinversion‚Äù model cannot be directly applied. Existing optimization-based approaches require white-box accesses to
the classifier to compute the gradients, and therefore they cannot
work if the classifier is a blackbox. More importantly, the adversary
might obtain only partial prediction results on victim data. For
example, the ImageNet dataset [34] has over 20,000 classes. Since
the majority values in the prediction vector are small, it is more
practical to release only the top 5 to 10 predicted classes [68]. Victim
users might also post partial predicted scores to the social media.
The partial predictions largely limit the reconstruction ability of
the inversion model. For example, Figure 1 column (a) shows the
result of inverting a ‚Äútruncated‚Äù prediction vector (i.e., keeping the
1/5 largest values while masking the rest to 0). The poor result is
probably due to overfitting in the inversion model. Hence, although
reconstruction is accurate on the full prediction vector, a slight
deviation, such as truncation in the prediction vector, will lead to a
dramatically different result.
In this paper, we formulate the problem of model inversion under
adversarial settings, and propose an effective approach for such
attacks. We adopt the above-mentioned training-based approach.
That is, a second neural network (referred to as inversion model) is
trained to perform the inversion. To address the problem of lacking
access to training data, our approach draws the training data (of the
inversion model) from a more generic data distribution based on the
background knowledge. This set of auxiliary samples is arguably
much easier for the adversary to obtain. For instance, against a
facial recognition classifier, the adversary could randomly crawl
facial images from the Internet to compose an auxiliary set without
knowing the exact training data (distribution). Figure 1 column (c)
shows the reconstruction result of the inversion model trained on
auxiliary samples. The reconstructed facial images are recognizable.
Training data reconstruction Training class inference
Prior TB (1/5) Our (1/5) Our (full) Truth Our MIA Prior TB
(a) (b) (c) (d) (e) (f) (g)
Figure 1: Training data reconstruction and training class
inference of our and previous approaches against a facial
recognition classifier. Column (a) and (b) compare our approach with a known training-based (TB) method when only
1/5 prediction results are available. Column (c) shows our
result of inverting full prediction vectors by training the inversion model on auxiliary samples. Column (e)-(g) compare
our approach with MIA and the known prior TB method.
Note that we use auxiliary samples for our approach in all
experiments.
a
b
Figure 2: Inversion of commercial prediction API. (a) Recovered faces of victims. (b) Ground truth. The presented individuals are not seen to our inversion model during training.
We believe that this is because the auxiliary samples retain generic
facial features (e.g., face edges, eyes and nose) and such information
is sufficient to regularize the originally ill-posed inversion problem.
Interestingly, our experiments show that even in cases where the
actual training data is available, augmenting the training data with
generic samples could improve inversion accuracy.
To make the inversion model also work when the adversary
obtains only partial prediction result on victim user‚Äôs data, we
propose a truncation method of training the inversion model. The
main idea is to feed the inversion model with truncated predictions
(on the auxiliary samples) during training. Such truncation forces
the inversion model to maximally reconstruct the samples based
on the truncated predictions, and thus align the inversion model
to the truncated values. Furthermore, truncation helps to reduce
overfitting in a similar way of feature selection [26]. Figure 1 column
(b) shows the result of inverting a partial prediction vector (i.e.,
keeping the 1/5 largest values while masking the rest to 0) using
our truncation technique, which clearly outperforms the result in
column (a).
Session 2B: ML Security I CCS ‚Äô19, November 11‚Äì15, 2019, London, United Kingdom 226
It turns out that the truncation technique is effective in our
second problem on training class inference. To infer the training
classes, we truncate the classifier‚Äôs predictions on auxiliary samples
to one-hot vectors. After the training is complete, it can produce
a representative sample for each training class by taking one-hot
vectors of the classes as input. Figure 1 column (e) shows the result
of our training class inference. It significantly outperforms MIA
(column (f)) and previous training-based approach (column (g)).
Depending on the adversary‚Äôs role, we design two ways of training the inversion model. First, if the adversary is a user who does
not have the training data but has black-box access to the classifier
(i.e., only prediction results are available to him), we construct the
inversion model by training a separate model from scratch. Second,
if the adversary is the developer who trains the classifier, the inversion model can be trained jointly with the classifier on the same
training data, which derives a more precise inversion model. In the
later one, the objective is to force the classifier‚Äôs predictions to also
preserve essential information which helps the inversion model to
reconstruct data. To this end, we use the reconstruction loss of the
inversion model to regularize the training of the classifier.
We evaluate our approaches for various machine learning tasks
and model architectures on multiple image datasets. The results
show that our attack can precisely reconstruct data from partial
predictions without knowing the training data distribution, and
also outperforms previous approaches in training class inference.
We confirm our inversion technique on Amazon Rekognition API,
which detects facial features such as emotions, beard and gender
on users‚Äô uploaded images. We have no knowledge of the backend
models used by the API. As shown in Figure 2, it is still possible
to accurately reconstruct faces of victim users from such limited
information. This work highlights that the rich information hidden
in the model‚Äôs prediction can be extracted, even if the adversary
only has access to limited information. We hope for this work to
contribute to strengthening user‚Äôs awareness of handling derived
data.
Contributions. In summary, we make the following contributions in this paper.
‚Ä¢ We observe that even with partial knowledge of the training
set, it is possible to exploit such background knowledge to
regularize the originally ill-posed inversion problem.
‚Ä¢ We formulate a practical scenario where the adversary obtains only partial prediction results, and propose a truncation
method that aligns the inversion model to the space of the
truncated predictions.
‚Ä¢ Our inversion method can be adopted for training class inference attack. Experimental results show that our method
outperforms existing work on training class inference.
‚Ä¢ We observe that even if the classifier‚Äôs training set is available,
augmenting it with generic samples could improve inversion
accuracy.
2 BACKGROUND
2.1 Machine Learning
In this paper, we focus on supervised learning, more specifically, on
training classification models (classifiers) using neural networks [36].
The model is used to give predictions to input data. The lifecycle
of a model typically consists of training phase (where the model is
created) and inference phase (where the model is released for use).
Machine learning models. A machine learning classifier encodes
a general hypothesis function Fw (with parameters w) which is
learned from a training dataset with the goal of making predictions on unseen data. The input of the function Fw is a data point
x ‚àà R
d drawn from a data distribution px (x) in a d-dimensional
space X, where each dimension represents one attribute (feature)
of the data point. The output of Fw is a predicted point Fw (x) in
a k-dimensional space Y, where each dimension corresponds to
a predefined class. The learning objective is to find the relation
between each input data and the class as a function Fw : X 7‚Üí Y.
A neural network (deep learning model) consists of multiple connected layers of basic non-linear activation functions (neurons)
whose connections are weighted by the model parameter w, with a
normalized exponential function softmax(z)i =
exp(zi )
√ç
j
exp(zj)
added to
the activation signals (referred to as logits) of the last layer. This
function converts arbitrary values into a vector of real values in
[0, 1] that sum up to 1. Thus, the output could be interpreted as the
probability that the input falls into each class.
Training phase. Let x represent the data drawn from the underlying data distribution px (x), and y be the vectorized class of x.
The training goal is to find a function Fw to well approximate the
mapping between every data point (x, y) in space X √ó Y. To this
end, we use a loss function L(Fw (x), y) to measure the difference
between the class y and the classifier‚Äôs prediction Fw (x). Formally,
the training objective is to find a function Fw which minimizes the
expected loss.
L(Fw ) = Ex‚àºpx
[L(Fw (x), y)] (1)
The actual probability function px (x) is intractable to accurately
represent, but in practice, we can estimate it using samples drawn
from it. These samples compose the training set D ‚äÇ X. We predefine a class y for each data x ‚àà D as supervision in the training
process. Hence, we can train the model to minimize the empirical
loss over the training set D.
LD (Fw ) =
1
|D|
√ï
x‚ààD
L(Fw (x), y) (2)
Nonetheless, this objective could lead to an overfitted model
which attains a very low prediction error on its training data, but
fails to generalize well for unseen data drawn frompx (x). A regularization term R(Fw ) is usually added to the loss LD (Fw ) to prevent
Fw from overfitting on the training data. In summary, the training
process of a classifier is to find a model Fw that minimizes the
following objective:
C(Fw ) = LD (Fw ) + ŒªR(Fw ) (3)
where the regularization factor Œª controls the balance between the
classification function and the regularization function.
Algorithms used for solving this optimization problem are variants of the gradient descent algorithm [5]. Stochastic gradient descent (SGD) [79] is a very efficient method that updates the parameters by gradually computing the average gradient on small
randomly selected subsets (mini-batches) of the training data.
Inference phase. In the inference phase (or called testing phase),
the model Fw is used to classify unseen data. Specifically, function
Session 2B: ML Security I CCS ‚Äô19, November 11‚Äì15, 2019, London, United Kingdom 227
Fw takes any data x drawn from the same data distribution px (x) as
input, and outputs a prediction vector Fw (x) = (Fw (x)1, ..., Fw (x)k
),
where Fw (x)i
is the probability of the data x belonging to class i
and √ç
i Fw (x)i = 1.
2.2 Model Inversion
Our approach is related to many previous work on inverting neural
networks from machine learning and computer vision communities.
Inverting a neural network helps in understanding and interpreting the model‚Äôs behavior and feature representations. For example,
a typical inversion problem in computer vision is to reconstruct
an image x from its computer vision features such as HOG [15]
and SIFT [42], or from the activations in each layer of the network
including the classifier‚Äôs prediction Fw (x) on it. In general, these
inversion approaches fall into two categories: optimization-based
inversion [21, 30, 37, 39, 43, 44, 72] and, most similar to our approach, training-based inversion [10, 17, 18, 53].
Optimization-based inversion. The basic idea of this branch of
work is to apply gradient-based optimization in the input space X
to find an image xÀÜ whose prediction approximates a given Fw (x).
Such inversion can be also used to generate a representative image
for some class y (i.e., training class inference), by replacing Fw (x)
with vectorized y [21, 66]. To this end, the image xÀÜ should minimize
some loss function between Fw (x) and Fw (xÀÜ). Therefore, it requires
white-box access to the model to compute the gradients. However,
inverting the prediction of a neural network is actually a difficult
ill-posed problem [17]. The optimization process tends to produce
images that do not really resemble natural images especially for a
large neural network [76]. Furthermore, this approach involves optimization at the test time because it requires computing gradients
which makes it relatively slow (e.g., 6s per image on a GPU [44]).
More discussions on optimization-based inversion are presented in
Appendix A.
Training-based inversion. This kind of inversion trains another
neural network GŒ∏
(referred to as inversion model in this paper)
to invert the original one Fw . Specifically, given the same training
set of images and their predictions (Fw (x), x), it learns a second
neural network GŒ∏
from scratch to well approximate the mapping
between predictions and images (i.e., the inverse mapping of Fw ).
The inversion model GŒ∏
takes the prediction Fw (x) as input and
outputs an image. Formally, this kind of inversion is to find a model
GŒ∏ which minimizes the following objective.
C(GŒ∏
) = Ex‚àºpx
[R(GŒ∏
(Fw (x)), x)] (4)
where R is the image reconstruction loss such as L2 loss adopted
in work [18]. In contrast to optimization-based inversion, trainingbased inversion is only costly during training the inversion model
which is one-time effort. Reconstruction from a given prediction
requires just one single forward pass through the network (e.g.,
5ms per image on a GPU [18]).
3 ADVERSARIAL MODEL INVERSION
The adversary could be either the user of a blackbox classifier Fw ,
or the developer of Fw . The adversary‚Äôs capabilities and goals differ depending on the role and we consider three scenarios: (1) A
curious user who intends to reconstruct the victim‚Äôs input data
from the victim‚Äôs truncated predication vector; (2) A curious user
who intends to infer Fw ‚Äôs functionality; (3) A malicious developer
who intends to build a Fw , which subsequently could help in reconstructing the victims‚Äô input from their truncated predication
vectors.
3.1 (Scenario 1) Data Reconstruction with
Blackbox Classifier
In this scenario, the adversary is a curious user who wants to infer
a classifier Fw ‚Äôs training data or other victim users‚Äô input data (i.e.,
test data of Fw ). The adversary has black-box accesses to the Fw .
That is, the adversary can adaptively feed input to Fw and obtain the
output. The adversary does not know the classifier‚Äôs training data
(distribution), architecture and parameters. However, the adversary
has some background knowledge on Fw . Specifically, the adversary
knows the following:
‚Ä¢ Although the adversary does not know the actual training
data that is used to train Fw , the adversary can draw samples
from a more ‚Äúgeneric‚Äù distribution pa of the training data. For
instance, suppose Fw is a face recognition classifier trained
on faces of a few individuals, although the adversary does
not know the faces of those individuals, the adversary knows
that the training data are facial images and thus can draw
many samples from a large pool of facial images. Intuitively, a
distribution pa is more generic than the original one if pa is
the distribution after some dimension reductions are applied
on the original.
‚Ä¢ The adversary knows the input format of Fw , since he knows
the distribution pa. The adversary also knows the output format of Fw . That is, the adversary knows the dimension of
the predication vector. This assumption is reasonable because
even though Fw may return selected prediction values, the
adversary can still estimate the dimension of the prediction
vector. For example, he can query Fw with a set of input data
and collect distinct classes in the returned predictions as the
dimension.
The classifier Fw is also used by many other benign users. We assume that the adversary has the capability to obtain f, am-truncated
predication vector of Fw (x) where x is the input of a victim user,
and m is a predefined parameter determined by the victim. Given
a prediction vector g, we say that f is m-truncated, denoted as
truncm(g), when all m largest values of g remain in f, while the
rest are truncated to zeros. For instance,
trunc2((0.6, 0.05, 0.06, 0.2, 0.09)) = (0.6, 0, 0, 0.2, 0).
Now, given f, black-box access to Fw , and samples from distribution pa, the adversary wants to find a most probable data from
the distribution pa such that trunc(Fw (x)) = f. That is, ideally, the
adversary wants to find a xÀÜ that satisfies the following:
xÀÜ = arg max
x‚ààXf
pa(x)
subject to Xf = {x ‚àà X | trunc(Fw (x)) = f }
(5)
Let us call the problem of obtaining xÀÜ from Fw , f and pa the data
reconstruction problem.
Session 2B: ML Security I CCS ‚Äô19, November 11‚Äì15, 2019, London, United Kingdom 228
3.2 (Scenario 2) Training Class Inference
Same as in scenario 1, in this scenario, the adversary is a curious user
who has black-box accesses to a classifier Fw , and knows samples
from a generic distribution pa. Instead of data reconstruction, the
adversary wants to find a representative data of the training class.
Given black-box access to a classifier Fw , and a target class y, the
adversary wants to find a data xÀÜ that satisfies the following.
xÀÜ = arg max
x‚ààXy
pa(x)
subject to Xy = {x ‚àà X | Fw (x)y is high}
(6)
where Fw (x)y is the confidence that x is classified by Fw as class y.
3.3 (Scenario 3) Joint Classification and Model
Inversion
We also consider the scenario where the adversary is a malicious
developer who trains the classifier Fw and sells/distributes it to
users. Different from the previous two scenarios, here, the adversary
has full knowledge about the classifier‚Äôs training data, architecture
and parameters, and has the freedom to decide what Fw would be.
We assume that after Fw is released, the adversary is able to obtain
truncated predication from the users, and the adversary wants to
reconstruct the users‚Äô input.
Hence, in this scenario, the adversary goal is to train a classifier Fw that meets the accuracy requirement (w.r.t. the original
classifier‚Äôs task), while improving quality of data reconstruction.
3.4 What If Applying Prior Work in
Adversarial Settings
Let us highlight the difference in the adversary‚Äôs capabilities between our adversarial settings and previous inversion settings.
First, in our adversarial settings, when the adversary is a user, if
he has only black-box accesses to the classifier, existing optimizationbased inversion approaches including MIA do not work because
they require white-box access to compute the gradients. Besides,
many studies have shown that MIA against large neural networks
tends to produce semantically meaningless images that do not even
resemble natural images [28, 65]. Our experimental result, as shown
in Figure 1 column (f), also reaches the same conclusion. Second,
the adversary as a user does not know the classifier‚Äôs training data,
which makes it impossible for previous training-based approaches
to train the inversion model on the same training data. Third, the
adversary, as either a user or developer, might obtain truncated
prediction results instead of the full results. This makes previous
inversion models that are trained on full predictions ineffective in
reconstructing data from partial predictions, as shown in Figure 1
column (a) and (g).
4 APPROACH
Our approach adopts previously mentioned training-based strategy
to invert the classifier. The overall framework is shown in Figure 3.
Unlike the optimization-based approaches that invert a given prediction vector directly from Fw , here, an inversion model GŒ∏
is first
trained, which later takes the given prediction vector as input and
outputs the reconstructed sample. This is similar to autoencoder [7]
ùêπùë§ ùê∫ùúÉ ùíô ùêπùë§(ùíô) ùíô

Figure 3: Framework of training-based inversion approach.
The classifier Fw takes data x as input and produces a prediction vector Fw (x). The inversion model GŒ∏ outputs the reconstructed data xÀÜ with the prediction as input.
where Fw is the ‚Äúencoder‚Äù, GŒ∏
is the ‚Äúdecoder‚Äù, and the prediction
can be thought of as the latent space.
There are three aspects that are different from autoencoder. (1)
In Scenario 1 and 2, unlike autoencoder, the Fw is given and fixed.
Furthermore, the training data of Fw is not available to train GŒ∏
.
Section 4.1 describes how we obtain the training set for GŒ∏
. (2)
Since our adversary only obtains truncated prediction, we need a
method to ‚Äúrealign‚Äù the latent space. Section 4.2 gives the proposed
truncation method. (3) In our Scenario 3, although Fw is not fixed,
unlike autoencoder, there is an additional requirement on Fw accuracy. That is, it is a joint classifier and inversion model problem,
where Fw is the classifier and GŒ∏
is the inversion model. Section 4.3
gives our training method.
4.1 GŒ∏ ‚Äôs Training Data: Auxiliary Set
The first important component in the construction of GŒ∏
is its
training set, which is referred to as auxiliary set in the rest of
the paper. The auxiliary set should contain sufficient semantic
information to regularize the ill-posed inversion problem.
We compose the auxiliary set by drawing samples from a more
generic data distribution pa than the original training data distribution px . For example, against a facial recognition classifier of 1,000
individuals, the auxiliary samples could be composed by collecting
public facial images of random individuals from the Internet. These
auxiliary samples still retain general facial features such as the face
edges and locations of eyes and nose, which are shared semantic
features of the original training data. We believe that such shared
features provide sufficient information to regularize the originally
ill-posed inversion task. To further improve the reconstruction
quality of GŒ∏
, we can specially choose the auxiliary set to better
align the inversion model. For example, against a facial recognition
classifier, we choose the dataset with mostly frontal faces as the
auxiliary set, so as to align the inversion model to frontal faces.
Our experimental results demonstrate the effectiveness of sampling auxiliary set from a more generic data distribution, as shown
in Section 5.2. The inversion model can precisely reconstruct the
training data points even if it never sees the training classes during
the construction.
4.2 Truncation Method for Model Inversion
We propose a truncation method of training GŒ∏
to make it aligned
to the partial prediction results. Figure 4 shows the architecture
of the classifier Fw and its inversion model GŒ∏
. The idea is to
Session 2B: ML Security I CCS ‚Äô19, November 11‚Äì15, 2019, London, United Kingdom 229
0.76
0.01
0.03
0.04
0.01
0.01
0.08
0.02
0.03
0.01
0.76
0
0.03
0.04
0
0
0.08
0
0.03
0
Truncate
Input x Classifier Fw Prediction Fw Inversion Model GŒ∏
(x) trunc(Fw (x)) GŒ∏ (trunc(Fw (x)))
R(GŒ∏ (trunc(Fw (x)))), x)
Reconstruction Loss (on auxiliary pa)
x
GŒ∏ (trunc(Fw (x))))
L(f (x), y)
Classification Loss (on training px ) Ground truth y
Fw (x)
. . . . . . . . . . . .
. . .
. . .
. . .
. . .
Figure 4: Architecture of the classifier and inversion model. The classifier Fw takes data x as input and produces a prediction Fw (x). Such prediction vector is truncated (if necessary) to a new vector trunc(Fw (x)). The inversion model GŒ∏
takes the
truncated prediction as input and outputs a reconstructed data GŒ∏
(trunc(Fw (x))).
truncate the classifier‚Äôs predictions on auxiliary samples to the
same dimension of the partial prediction vector on victim user‚Äôs
data, and use them as input features to train the inversion model,
such that it is forced to maximally reconstruct input data from the
truncated predictions. Formally, let a be a sample drawn from pa,
and Fw (a) be the classifier‚Äôs prediction. Let m be the dimension
of the partial prediction vectors on victim‚Äôs data. We truncate the
prediction vector Fw (a) to m dimensions (i.e., preserving the top m
scores but setting the rest to 0). The inversion model GŒ∏
is trained
to minimize the following objective.
C(GŒ∏
) = Ea‚àºpa
[R(GŒ∏
(truncm(Fw (a))), a)] (7)
where R is the loss function and we use L2 norm as R in this
paper. The truncation process can be understood as a similar way
of feature selection [26] by removing unimportant classes in Fw (a)
(i.e., those with small confidence). It helps to reduce overfitting
of GŒ∏
such that it can still reconstruct the input data from the
preserved important classes.
AfterGŒ∏
is trained, the adversary can feed a truncated prediction
Fw (x) to GŒ∏ and obtain the reconstructed x.
Our inversion model GŒ∏
can be adopted to perform training
class inference (Scenario 2), which is the same adversarial goal in
MIA [21]. Training class inference can be viewed as setting m = 1,
which means the adversary knows only the class information and
targets at generating a representative sample of each training class.
MIA assumes that the adversary has a white-box access to Fw in the
inference phase. Our method, on the contrary, works with a blackbox access to Fw . Besides, our method can even work in the case
that Fw releases only the largest predicted class with confidence
value, because we can approximate the total number of training
classes by collecting distinct classes from the classifier‚Äôs predictions
on our auxiliary set. This greatly reduces the requirement of the
adversary‚Äôs capability to infer training classes.
Our experimental results show that the truncation method of
training the inversion model improves the reconstruction quality from truncated prediction. Previous training-based approach
that directly inputs the partial prediction to the inversion model
produces meaningless reconstruction results (see more evaluation
details in Section 5.3).
4.3 Joint Training of Classifier and Inversion
Model
When the adversary is the developer of the classifier, he could
jointly train the inversion model with the classifier, which leads to
better inversion quality. In particular, let D be the classifier‚Äôs training data. We regularize the classification loss LD (Fw ) (Equation 2)
with an additional reconstruction loss RD (Fw ,GŒ∏
). Intuitively, this
encourages the classifier‚Äôs predictions to also preserve essential
information of input data in the latent space, such that the inversion
model can well decode it to recover input data. In this paper, we
use L2 norm as the reconstruction loss RD (Fw ,GŒ∏
).
RD (Fw ,GŒ∏
) =
1
|D|
√ï
x‚ààD
||(GŒ∏
(trunc(Fw (x))) ‚àí x||2
2
(8)
The joint training ensures that Fw gets updated to fit the classification task, and meanwhile, GŒ∏
is optimized to reconstruct data
from truncated prediction vectors.
It is worth noting that in training GŒ∏
, we find that directly using
prediction vector Fw (x) does not produce optimal inversion results.
This is because the logits z at the output layer are scaled in [0, 1] (and
sum up to 1) to form prediction Fw (x) by the softmax function (see
Section 2.1), which actually weakens the activations of the output
layer and thus loses some information for later decoding. To address
this issue, we rescale the prediction Fw (x) to its corresponding logits
z in the following and use these z to replace the prediction Fw (x)
in our experiments.
z = log(Fw (x)) + c (9)
Session 2B: ML Security I CCS ‚Äô19, November 11‚Äì15, 2019, London, United Kingdom 230
Table 1: Data allocation of the classifier Fw and its inversion
model GŒ∏
.
Classifier Fw Inversion Model GŒ∏
Task Data Auxiliary Data Distribution
FaceScrub
50% train, 50% test FaceScrub 50% test data Same
80% train, 20% test CelebA Generic
80% train, 20% test CIFAR10 Distinct
MNIST
50% train, 50% test MNIST 50% test data Same
80% train, 20% test (5 labels) MNIST other 5 labels Generic
80% train, 20% test CIFAR10 Distinct
where log is element-wise, and c is a scalar which is added to
log(Fw (x)) element-wise and is also optimized during training the
inversion model.
5 EXPERIMENTS
In this section, we evaluate the inversion performance of our method
and compare it with existing work. We evaluate three factors: the
choice of auxiliary set, the truncation method and the full prediction size. We also evaluate our approach on a commercial face
recognition API provided by Amazon.
5.1 Experimental Setup
We perform evaluation on four benchmark image recognition datasets.
For simplicity, all datasets are transformed to greyscale images with
each pixel value in [0, 1]. We detail each dataset in the following.
‚Ä¢ FaceScrub [55]. A dataset of URLs for 100,000 images of 530
individuals. We were only able to download 48,579 images for
530 individuals because not all URLs are available during the
period of writing. We extract the face of each image according
to the official bounding box information. Each image is resized
to 64 √ó 64.
‚Ä¢ CelebA [41]. A dataset with 202,599 images of 10,177 celebrities
from the Internet. We remove 296 celebrities which are included
in FaceScrub and eventally we obtain 195,727 images of 9,881
celebrities. This makes sure that our modified CelebA has no class
intersection with FaceScrub. This large-scale facial image dataset
can represent the generic data distribution of human faces. To
extract the face of each image, we further crop the official aligncropped version (size 178 √ó 218) by width and height of 108 with
upper left coordinate (35, 70). Each image is resized to 64 √ó 64
and also resized to 32 √ó 32.
‚Ä¢ CIFAR10 [33]. A dataset consists of 60,000 images in 10 classes
(airplane, automobile, bird, cat, deer, dog, frog, horse, ship and
truck). Each image is resized to 64√ó64 and also resized to 32√ó32.
‚Ä¢ MNIST [35]. A dataset composed of 70,000 handwritten digit
images in 10 classes. Each image is resized to 32 √ó 32.
We use FaceScrub and MNIST to train the target classifier Fw .
For each Fw that is trained using the dataset D, we separately train
the inversion model GŒ∏ using the same training set D, training
set drawn from a more generic distribution of D, and training set
drawn from distribution that is arguably semantically different
from D. We call these three types of training data same, generic and
distinct respectively. Table 1 presents the data allocation for each
Fw and its corresponding GŒ∏
. Note that the auxiliary set of generic
data distribution has no class intersection with the training data
of Fw (i.e., FaceScrub and MNIST). Specifically, we have cleaned
Train Data Test Data
Same
Dist.
Generic
Dist.
Distinct
Dist.
Ground
Truth
Figure 5: Effect of the auxiliary set on the inversion quality
on FaceScrub Fw . We use auxiliary set of the same (1st row),
generic (2nd row) and distinct (3rd row) data distributions to
train the inversion model.
CelebA by removing the celebrities which are also included in the
FaceScrub dataset. We randomly select 5 classes (i.e., digit 0, 1, 2,
3, 8) of MNIST to train Fw and use the other 5 classes (i.e., digit
4, 5, 6, 7, 9) to form the auxiliary set. The auxiliary set of distinct
data distribution is composed of samples drawn from an explicitly
different data distribution from the training data.
We use CNN to train Fw , and use transposed CNN to train GŒ∏
.
The FaceScrub classifier includes 4 CNN blocks where each block
consists of a convolutional layer followed by a batch normalization layer, a max-pooling layer and a ReLU activation layer. Two
fully-connected layers are added after the CNN blocks, and Softmax
function is added to the last layer to convert arbitrary neural signals
into a vector of real values in [0, 1] that sum up to 1. The FaceScrub
inversion model consists of 5 transposed CNN blocks. The first 4
blocks each has a transposed convolutional layer succeeded by a
batch normalization layer and a Tanh activation function. The last
block has a transposed convolutional layer followed by a Sigmoid
activation function which converts neural signals into real values in
[0, 1] which is the same range of auxiliary data. The MNIST classifier and inversion model have similar architecture as FaceScrub but
with 3 CNN blocks in classifier and 4 transposed CNN blocks in inversion model. More details of model architectures are presented in
Appendix B. The FaceScrub classifier and MNIST classifier achieve
85.7% and 99.6% accuracy on their test set respectively (80% train,
20% test), which are comparable to the state-of-the-art classification
performance.
We train both Fw andGŒ∏ on a workstation running Ubuntu 16.04
LTS equipped with two Intel Xeon CPUs E5-2620 V4 (2.1Ghz/8-
core), 256GB RAM, and two NVIDIA Tesla P100 GPU cards. We
use PyTorch [60] to implement neural networks. The number of
queries to Fw during training GŒ∏
is equivalent to the number of
auxiliary samples. Roughly, it takes less than 12 hours to train GŒ∏
for FaceScrub Fw , and less than 6 hours to train GŒ∏
for MNIST Fw
on our auxiliary sets. Once GŒ∏
is trained, it performs inversion
attack by a single forward pass through the neural network with
the prediction values as input, which takes only a few milliseconds.
Session 2B: ML Security I CCS ‚Äô19, November 11‚Äì15, 2019, London, United Kingdom 231
Train Data Test Data
Same
Dist.
Generic
Dist.
Distinct
Dist.
Ground
Truth
Figure 6: Effect of the auxiliary set on the inversion quality
on MNIST Fw . We use auxiliary set of the same (1st row),
generic (2nd row) and distinct (3rd row) data distributions
to train the inversion model.
Table 2: Quantitative measurement (mean squared error) of
the effect of the auxiliary set on the inversion quality.
Distribution FaceScrub MNIST
Same Dist. Train 0.0107 Train 0.0078
Test 0.0074 Test 0.0069
Generic Dist. Train 0.0114 Train 0.0153
Test 0.0115 Test 0.0152
Distinct Dist. Train 0.0315 Train 0.0415
Test 0.0316 Test 0.0416
5.2 Effect of Auxiliary Set
Effect on inversion quality. We present the inversion results (Scenario 1) on randomly-chosen training data and test data of the Fw
in Figure 5 and 6. We also present the mean squared error between
the reconstructed images and the ground truth in Table 2. It is clear
to see that the closer the auxiliary set‚Äôs data distribution is to the
training data distribution, the better the inversion quality is (both
visually and quantitatively). It is worth noting that the auxiliary
set of the generic data distribution has no class intersection with
the classifier‚Äôs training data, which means the inversion model has
never seen the training classes during its construction, but it can
still accurately reconstruct the data from these classes. For example,
digit 5,6,7 and 9 in MNIST are not included in the auxiliary set,
yet GŒ∏
can reconstruct these digits fairly accurately. This further
demonstrates that the generic background knowledge could be
sufficient in regularizing the ill-posed inversion. However, if the
auxiliary set‚Äôs data distribution is too far from the generic data
distribution, the inversion is not that satisfactory as shown in the
3rd row of Figure 5 and 6.
Summary I: Even with no full knowledge about the classifier Fw ‚Äôs training data, accurate inversion is still possible
by training GŒ∏ using auxiliary samples drawn from a more
generic distribution derived from background knowledge.
Effect on inversion alignment. We divide the cleaned CelebAdataset
into 5 subsets of frontal, turn-left, turn-right, turn-down and turnup faces. We use each subset as the auxiliary set to train GŒ∏
, and
test whether the specific auxiliary set can align the inversion result.
We use OpenCV library [12] for Python3 and FAN network [13] to
detect the facial orientation (See Appendix C for more details), and
eventually get 109,105 frontal faces, 48,137 turn-left faces, 33,632
turn-right faces, 4,278 turn-down faces and 2,304 turn-up faces. We
present the inversion results on randomly-chosen training data
and test data of FaceScrub classifier in Figure 7. It is clear to see
that the specific auxiliary set does align GŒ∏
‚Äôs inversion result to
the corresponding facial orientation. It is worth noting that, in this
experiment, the auxiliary set is drawn from a more ‚Äúspecialized‚Äù
distribution that contains only one type of pose, in contrast to the
original training data which contains all 5 types of pose.
5.3 Effect of Truncation
Let us denote the dimension of the partial prediction that the adversary obtains on victim user‚Äôs data as m. We perform the inversion
attack (Scenario 1) with m = 10, 50, 100 and 530 against FaceScrub
classifier and with m = 3, 5 and 10 against MNIST classifier. We
present the results of our truncation method and previous trainingbased method (i.e., without truncation) on FaceScrub classifier in
Figure 9 and on MNIST classifier in Figure 10. The auxiliary set is
composed of samples from the generic data distribution for FaceScrub classifier and from the same data distribution for MNIST
classifier. The presented training data and test data are randomly
chosen.
We can see that our approach outperforms previous approach.
Our approach can produce highly recognizable images for all m.
Previous approach, although can generate recognizable images
when m is very large, produces meaningless result when m is small.
For our approach, when m is relatively small, it appears that the
inversion result is more a generic face of the target person with
facial details not fully represented. For example, in the 7th column
of Figure 9 where the ground truth is a side face, our inversion
result is a frontal face when m = 10 and 50, and becomes a side face
when m = 100. This result demonstrates that truncation indeed
helps reduce overfitting by aligning GŒ∏
to frontal facial images, and
with smaller m, more generalization is observed. We also perform
quantitative measurement of the inversion quality as shown in
Figure 8. The mean squared error of our approach is much smaller
than that of the prior approach, especially when m is small.
Summary II: Our truncation method of training the inversion model GŒ∏ makes it still possible to perform accurate
inversion when the adversary is given only partial prediction results.
5.4 Effect of Full Prediction Size
Section 5.3 investigates the effect of m, the size of the truncated
prediction. In this section, we investigate the effect of the size of the
full prediction. Let k be the dimension of the full prediction. In our
experiments, we randomly select k classes from the FaceScrub and
MNIST datasets as the training data of Fw . We set k = 10, 50, 100
Session 2B: ML Security I CCS ‚Äô19, November 11‚Äì15, 2019, London, United Kingdom 232
Frontal Left Right Down Up
Train
Data
Test
Data
Ground
Truth
Ground
Truth
Recovered
faces
Recovered
faces
Figure 7: Effect of the auxiliary set on the inversion alignment. We use auxiliary set of frontal, left, right, down and up faces of
celebA to train the inversion model. We present result on FaceScrub classifier‚Äôs randomly-chosen training data and test data.
10 50 100 300 530
0
0.02
0.04
0.06
0.08
0.1
0.12
m
Mean Squared Error
Our (Train Data)
Our (Test Data)
Prior (Train Data)
Prior (Test Data)
Figure 8: Quantitative measurement of the effect of truncation (m) forGŒ∏ on the inversion quality on FaceScrub Fw . The
x-axis is the m, and the y-axis is mean squared error.
and 530 for FaceScrub and k = 3, 5 and 10 for MNIST. We present
the inversion results against FaceScrub classifier in Figure 11 and
against MNIST classifier in Figure 12. In all experiments, we assume that the adversary obtains the full prediction vectors (i.e.,
m = k). The auxiliary set is composed of samples from the generic
data distribution for FaceScrub classifier and from the same data
distribution for MNIST classifier. The presented training data and
test data are randomly chosen.
Our experimental results show that the effect of k is similar with
the effect of m that we have discussed. A larger k leads to a more
accurate reconstruction of the target data and a small k leads to
a semantically generic inversion of the corresponding class. Our
quantitative measurement of the inversion quality under different
k (as shown in Figure 13) also shows that the mean squared error is
inversely proportional to k. This is because both k and m affect the
amount of predicted information that the adversary can get. The
factor k decides the size of the full prediction vector. The factor
m decides how many predicted classes that the classifier releases
from the k-dimension prediction vector.
ùëö = 10
ùëö = 50
ùëö = 100
ùëö = 530
(Full)
Ground
Truth
Train Data Test Data
Our Prior Our Prior Our Prior Our Prior
ùëö = 300
Figure 9: Effect of truncation (m) for GŒ∏ on the inversion
quality on FaceScrub Fw . Results of our and previous approach are presented in odd and even columns respectively.
5.5 Training Class Inference
This section evaluates our inversion model GŒ∏ on training class
inference attack (Scenario 2). We compare our method, MIA and a
previous training-based approach (which has no truncation) against
the same Fw trained on FaceScrub. For our attack, we use CelebA
as the auxiliary set. It has no class intersection with FaceScrub.
Hence, the inversion model GŒ∏
trained on it has never seen the
target training classes during its construction. In particular, we
first use the auxiliary set to approximate the total number of training classes, and we obtain 530 classes which is actually the true
number. During the training of the inversion model, we encode
the classifier‚Äôs prediction result on each auxiliary sample (i.e., the
largest predicted class with confidence) to a 530-dimension vector
by filling rest classes with zeros. We use the encoded predictions as
features to train GŒ∏
. After GŒ∏
is trained, we convert each training
class to a one-hot vector of 530 dimensions and feed the one-hot
Session 2B: ML Security I CCS ‚Äô19, November 11‚Äì15, 2019, London, United Kingdom 233
ùëö = 3
ùëö = 5
ùëö = 10
(Full)
Ground
Truth
Train Data Test Data
Our Prior Our Prior Our Prior Our Prior
Figure 10: Effect of truncation (m) for GŒ∏ on the inversion
quality on MNIST Fw . Results of our and previous approach
are presented in odd and even columns respectively.
ùëò = 10
ùëò = 50
ùëò = 100
ùëò = 530
Ground
Truth
Train Data Test Data
Figure 11: Effect of the full prediction size (k) on the inversion quality on FaceScrub Fw .
ùëò = 3
ùëò = 5
ùëò = 10
Ground
Truth
Train Data Test Data
Figure 12: Effect of the full prediction size (k) on the inversion quality on MNIST Fw .
vectors toGŒ∏
. The output ofGŒ∏
is the inferred image of the training
class.
We implement the prior MIA attack by following the Algorithm
1 of [21]. Similarly, we set AuxTerm(x) = 0 for all x because the
adversary has no other information except the target label. We also
10 50 100 530
1
2
3
4
¬∑10‚àí2
k
Mean Squared Error
Train Data
Test Data
Figure 13: Quantitative measurement of the effect of the full
prediction size (k) on the inversion quality on FaceScrub Fw .
The x-axis is the k, and the y-axis is mean squared error.
Prior
Attack
Our
Attack
Training
Class
MIA
Our
Training
Class
Prior TB
Figure 14: Training class inference results. We present results of MIA (1st row), previous training-based (TB) inversion (2nd row) and our inversion (3rd row).
set the Process function to be a NLMeans denoising filter followed
by a Gaussian sharpening filter.
We compare the inversion results of our method and previous
methods in Figure 14. We can see that the inferred images by MIA
are semantically meaningless and do not even form recognizable
faces when MIA is applied to CNNs. This is consistent with conclusions in previous research work [28, 65] where similar unsatisfactory inversion results were obtained. Previous training-based inversion also fail to produce recognizable facial images. Our method is
able to generate highly human-recognizable faces for each person
(class) by capturing semantic features such as eyes, nose and beard.
An interesting find is that our method consistently produces frontal
faces even though the training facial images of Fw have various angles, expressions, background and even lightning. A possible reason
is that the auxiliary data consists of majority frontal faces. Since
the training aligns the inversion model to the training distribution,
thus recognizable frontal faces are generated.
Session 2B: ML Security I CCS ‚Äô19, November 11‚Äì15, 2019, London, United Kingdom 234
Blackbox ùêπùë§
Ground
Truth
Ground
Truth
Jointly with ùêπùë§
Blackbox ùêπùë§
(more data)
Blackbox ùêπùë§
Jointly with ùêπùë§
Figure 15: Inversion results of GŒ∏
: trained with blackbox
FaceScrub Fw vs trained jointly with FaceScrub Fw . We use
augmented training data with CelebA as the auxiliary set for
the 3rd row.
Blackbox ùêπùë§
Ground
Truth
Ground
Truth
Jointly with ùêπùë§
Blackbox ùêπùë§
(more data)
Blackbox ùêπùë§
Jointly with ùêπùë§
Figure 16: Inversion results of GŒ∏
: trained with blackbox
MNIST Fw vs trained jointly with MNIST Fw .
Table 3: Classification accuracy of the classifier Fw and average reconstruction loss (MSE) of the inversion model GŒ∏
.
Results are reported on test set.
Classifier Acc.1 Acc.2 Acc.3 MSE1 MSE2 MSE3
FaceScrub 78.3% 76.8% 78.3% 0.1072 0.0085 0.0083
MNIST 99.4% 99.2% - 0.3120 0.0197 -
1 Train Fw first and then train GŒ∏ with black-box accesses to Fw .
2
Jointly train Fw and GŒ∏
3 Train Fw first and then train GŒ∏ with black-box accesses to Fw .
The training data is augmented with CelebA.
Summary III: Our method outperforms previous methods
in training class inference against complex neural networks. Our experimental results show that our method
can generate highly recognizable and representative sample for training classes.
5.6 Inversion Model Construction: Trained with
Blackbox Fw vs Trained Jointly with Fw
The inversion modelGŒ∏
could be trained with a fixed given blackbox
Fw (as in Scenario 1&2) or trained jointly with Fw (as in Scenario
3). We evaluate and compare the inversion quality of GŒ∏ and classification accuracy of Fw between the two construction methods.
To make the construction method of GŒ∏ as the only factor that
affects the inversion performance, we use the same training data of
Fw as the auxiliary set for the construction with blackbox Fw . All
the other training details (e.g., epochs, batch size, optimizer) of the
two construction methods are the same. We use GŒ∏
to perform test
data reconstruction against the FaceScrub and MNIST classifiers.
In particular, we split a classification dataset to 50% as training data
and the rest 50% as test data.
We present the inversion results of the two construction methods
against FaceScrub and MNIST classifiers in Figure 15 and 16. We
can see that constructing GŒ∏ by jointly training it with Fw leads to
a more accurate reconstruction of the test data than that trained
with the blackbox Fw . To quantify the reconstruction quality, we
present the average reconstruction loss, namely mean squared error
(MSE), of GŒ∏
constructed by the two methods in the 5th and 6th
columns of Table 3. We can see that the joint training actually
leads to a lower reconstruction loss on the test set. This result
is intuitive, because the joint training can make Fw ‚Äôs prediction
vectors also preserve essential information about the input data
along with the classification information such that GŒ∏
can well
decode it for reconstruction. However, the better reconstruction
quality is achieved at the cost of lower classification accuracy. We
present the classification accuracy in the 2nd and 3rd columns
of Table 3. We can see that the classification accuracy does drop
because of the joint training, but it is still within an acceptable
range (0.2%-1.5%). Note that the classification accuracy, especially
of FaceScrub (78.3%), is a little bit lower than the accuracy (85.7%)
in earlier experiments because here we use 50% of the original set
to train Fw which is less than previously 80% of the dataset.
Certainly, the adversary (who is the malicious developer in Scenario 3) can choose to abandon the joint training. That is, Fw is
first trained to achieve high accuracy, and then the adversary trains
the GŒ∏
. Interestingly, the adversary can choose to use two different
training sets, one for Fw and a different auxiliary set for GŒ∏
. In this
experiment, we augment the original training set with CelebA as
the auxiliary set. We present the result in the 3rd row of Figure 15
and present the average reconstruction loss on the test set in the
last column in Table 3. We can see that by augmenting the training
set with generic data, the inversion model GŒ∏
trained with the
blackbox Fw can achieve comparable reconstruction quality to GŒ∏
trained jointly with Fw .
Summary IV: Inversion model GŒ∏
trained jointly with the
classifier Fw outperforms GŒ∏
trained with blackbox Fw in
inversion quality but at the cost of acceptable accuracy loss.
Augmenting the auxiliary set with more generic data can
improveGŒ∏
trained with the blackbox Fw to be comparable
to GŒ∏
trained jointly with Fw , and maintain the accuracy.
5.7 Experiment on Commercial Prediction API
We evaluate our approach on the commercial Amazon Rekognition
API [2], which detects facial features of users‚Äô uploaded images. We
have no knowledge of the backend models used by the API. We
query the API with our auxiliary dataset and use the predictions
(facial features) as input to train inversion models. We evaluate the
Session 2B: ML Security I CCS ‚Äô19, November 11‚Äì15, 2019, London, United Kingdom 235
Table 4: Quantitative measurement (mean squared error) of
the inversion on Amazon Rekognition API.
Features Unknown individuals Known individuals
but unknown images
Remove Landmark & Pose 0.0472 0.0469
Remove Landmark 0.0470 0.0462
Round(1) 0.0454 0.0443
Round(3) 0.0437 0.0438
Round(5) 0.0437 0.0438
No round (80 features) 0.0437 0.0438
inversion models on reconstructing unknown individuals‚Äô images
and known individuals‚Äô unknown images.
We use 80% of the cleaned CelebA dataset as the auxiliary set,
and leave the remaining 20% as victim images of known individuals.
We use the FaceScrub dataset as victim images of unknown individuals. Note that, we query the API with the original full-size CelebA
images to obtain the accurate predictions, but train the inversion
model (whose architecture is presented in Figure 19) using our resized 64√ó64 images. In summary, we send 156,582 queries (auxiliary
samples) to Amazon Rekognition API to train GŒ∏
, and send 87,724
queries (victim images) to test GŒ∏
‚Äôs ability of reconstructing victim
data. The training time of GŒ∏
is also roughly less than 12 hours.
At the time of writing, Amazon Rekognition API produces features including emotions (7), smile (1), facial attributes (9), bounding
box (4), facial landmarks (60), pose (3), quality (2) and confidence
of face (1). We discard the bounding box, quality and confidence of
face and use the remaining 80 features. We standardize the feature
values to [0, 1]. The facial landmark coordinates are converted to
the ratio of their distance to the bounding box boundaries to the
box width/height, with the upper left corner of the box as the origin. Figure 22 in Appendix D presents an example of preprocessed
predicted facial features before rounding.
We train three inversion models on three different truncated
feature sizes respectively: 80 features; 20 features by removing
landmarks; and 17 features by removing both landmarks and pose.
To further test the robustness of our approach, we round the original
feature values of victim images to 1, 3 and 5 decimals respectively.
We present the result in Figure 17. We can see that if the predicted
80 features of the victim images are all input to the inversion model,
it can accurately reconstruct these images (Row 6). Besides, it can
capture more information than what the API tells. For example,
it also recovers information about cheeks, teeth and hair length.
Rounding the predicted feature values does not have significant
effect on the inversion (Row 3-5). Even though the landmark information is removed from the inversion model‚Äôs input, it can still
recover recognizable faces (Row 2). If both the landmarks and pose
information are removed, the recovered faces are not that satisfactory, but can capture facial features such as sunglasses, beard, age
range and gender which are content of the API output (Row 1). We
further perform quantitative measurement of the inversion quality
and present the result in Table 4. We can confirm our conclusion
both visually and quantitatively.
Ground
Truth
No Round
(80 features)
Round (5)
Round (3)
Round (1)
Remove
Landmark
Unknown individuals Known individuals but unknown images
Remove
Landmark
& Pose
Figure 17: Inversion on Amazon Rekognition API. The 1st
row is the result ofGŒ∏ on 17 features by removing landmarks
and pose from the API output on victim images. The 2nd row
is the result of GŒ∏ on 20 features by removing landmarks.
The 3rd-6th rows are results of GŒ∏ on 80 features.
6 DISCUSSION
Prior work has established the connection between overfitting and
training data inference. For example, Yeom et al. [75] show that
prior model inversion attack [21, 22] and membership inference
attack [65] are deeply related and are both sensitive to overfitting
of the target model. Recent work [67] also leverages overfitting to
embed private information about training data into the model.
Our work, on the contrary, leverages generalization to ‚Äútransfer‚Äù
information from the auxiliary dataset to the task of training GŒ∏
.
Generalization enables a model to adapt properly to new, previously unseen data, based on which transfer learning can apply the
knowledge gained while solving one problem to a different but related problem. Using auxiliary data to train GŒ∏
is related to transfer
learning. Specifically, the soft predictions of a well-generalized Fw
on auxiliary data drawn from a generic data distribution encode
rich information between classes. The GŒ∏ will learn to capture such
hidden useful information to reconstruct input data. Our truncation
method further helps GŒ∏
to focus on the most important features.
Hence, an extremely overfitting model will provide no useful information in soft predictions, which, however, is usually not the case
for a working model.
There is a trend of users publishing derived prediction results
directly or indirectly. For examples, iOS has a built-in functionality
that allows users to share AR emojis which are derived from their
faces [62]. Many users share their face-age scores on social media
for fun [71]. Some sharing platform even asks users for their prediction results [70]. Fortunately, these prediction values are still coarse
at present, which might make it difficult to fully invert the model.
However, the potential enhanced versions of these prediction services could provide more accurate values in the future, which may
lead to more accurate inversion attacks. Besides, MLaaS companies
often state in the privacy policy that they do not store or abuse
users‚Äô uploaded data [6, 51], but they seldom mention how they
deal with the derived prediction results. Hopefully, our work can
Session 2B: ML Security I CCS ‚Äô19, November 11‚Äì15, 2019, London, United Kingdom 236
help encourage the companies to update the privacy policy (e.g.,
explicitly stating how the derived prediction results are handled).
The model inversion attack leaks useful information to malicious
parties. For example, in the case of facial recognition systems, adversaries can recover a representative face of individuals whose
facial images are used as the training data [21, 65]. However, there
are opposing views on whether such attack should be classified as
a privacy attack [47, 65].
7 RELATED WORK
ML Privacy. Researchers have strongly suggested that ML models
suffer from various privacy threats to its training data [3, 22, 31, 58,
65]. For instance, given access to an ML model, an adversary can
infer non-trivial and useful information about its training set [3].
Membership inference attack [48, 65] enables an adversary to
learn if a record of his choice is part of the private training sets.
Model inversion attack [21, 22] tries to infer a representative sample
or sensitive attributes of training data. Some work further improves
sensitive attribute inference either by formalizing it [74] or without
knowing the non-sensitive attributes [27]. Giuseppe et al. [4] show
that an adversary could infer the statistical information about the
training dataset in the inference phase of the target model. Similarly,
Hitaj et al. [28] investigate information leakage from collaborative
learning but in the training phase. Recent work also studies privacy issues in collaborative learning [48, 73] and ML-as-a-service
platforms [67].
Some of the above mentioned attacks infer sensitive attributes or
statistical information about the training data. Others can extract
training data points but have to manipulate the training process
of the model. Membership inference attack works in the inference
phase but requires the data is given. Our work, on the other hand,
examines how one can reconstruct either specific training data
points or test data points from their prediction results in the inference phase.
ML in Adversarial Settings. Various works have suggested that
ML models are likely vulnerable in adversarial settings [14, 16, 25].
In particular, an adversary could force a victim model to deviate
from its intended task and behave erratically according to the adversary‚Äôs wish. The adversary can stage these attacks by corrupting the
training phase (e.g., poisoning the training set with adversarial data
augmentation [9, 40], employing adversarial loss function [25]),
maliciously modifying the victim model [14], or feeding the victim
model with adversarially crafted samples [24, 49, 57, 59, 69].
Our focus, on the contrary, is not to deviate ML models from
their intended tasks, but rather to investigate an possibility of reconstructing input data from the model‚Äôs predictions on them.
Secure & Privacy-Preserving ML In the wake of security and
privacy threats posed to ML techniques, much research has been
devoted to provisioning secure and privacy-preserving training of
ML models [1, 11, 56, 61, 64] The threat models assumed by these
techniques are to protect privacy of users‚Äô data contributed to the
training set. Our work studies a different threat model wherein
the adversary targets at reconstructing user data given the model‚Äôs
prediction results on them.
Some research work on protecting the predictions of ML models has also been proposed recently [20, 32]. For examples, Dwork
and Feldman [20] study a method to make the model‚Äôs predictions
achieve differential privacy with respect to the training data. Our
work, on the other hand, studies the mapping between the prediction vectors and the input data by training another inversion model.
Juvekar et al. [32] propose Gazelle framework for secure neural
network inference using cryptographic tools. However, our work
studies a setting where the user posts their prediction results (e.g.,
predicted scores of face beauty and dress sense) to social media
which causes an exposure of the model‚Äôs predictions.
ML Inversion for InterpretationAlthough deep neural networks
have demonstrated impressive performance in various applications,
it remains not clear why they perform so well. Much research
has been working on interpreting and understanding neural networks [45, 54, 63, 66, 76, 78]. Inverting a neural network is one
important method to understand the representations learned by
neural networks [19, 23, 29, 54, 66, 77]. Such research work inverts
neural networks in order to understand and interpret them. Thus,
the inversion can leverage all possible information of the model
and the training data. Our work, on the contrary, inverts a neural
network in the adversarial settings where the adversary‚Äôs capability
is limited.
8 CONCLUSION
We proposed an effective model inversion approach in the adversary setting based on training an inversion model that acts as an
inverse of the original model. We observed that even with no full
knowledge about the original training data, an accurate inversion is
still possible by training the inversion model on auxiliary samples
drawn from a more generic data distribution. We proposed a truncation method of training the inversion model by using truncated
predictions as input to it. The truncation helps align the inversion
model to the partial predictions that the adversary might obtain
on victim user‚Äôs data. Our experimental results show that our approach can achieve accurate inversion in adversarial settings and
outperform previous approaches.
The seemingly coarse information carried by the prediction results might lead to causal sharing of such information by users
and developers. Our work, on the other hand, shows the surprising
reconstruction accuracy of the inversion model when applied in
adversarial settings. It is interesting in the future work to investigate how other loss functions, and other generative techniques
such as Generative Adversarial Network can be incorporated in the
adversarial inversion problem.