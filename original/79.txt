Understanding visual complexity as it relates to websites has been an emergent area for many years. However predicting the visual complexity of a website as perceived by users has been a real challenge. Perception is important because it influences user engagement dictating if they will find it dull engaging or too complex. While others have suggested solutions to certain levels of success here we propose a simple but accurate model that generates a Visual Complexity Score VCS based on common aspects of an HTML Document Object Model DOM . We created our model based on a statistical analysis of ratings of users on web pages. We then implemented this prediction model in an open source Eclipse framework called ViCRAM that both predicts and visualises the complexity of web pages in the form of a pixelated heat map. Finally we evaluated this model and the tool prediction with another user study of ratings of users on web pages. This study shows that our tool can predict the perceived complexity with a strong correlation to users’ perceived complexity.Previous article in issueNext article in issueKeywordsVisual complexityPredictionPerceptionAutomated toolOpen Source ToolThe ViCRAM tool is publicly available. To run this tool the Eclipse ACTF Visualisation SDK http // .eclipse.org/actf/downloads/ first needs to be installed and then the Team Project Set File .psf of the tool needs to be imported to Eclipse. The psf can be downloaded from http //ftp.gnome.org/mirror/eclipse.org/technology/actf/psf/old/anonymous/actf examples.zip. IntroductionUnderstanding visual complexity as it relates to websites has been an emergent area for a number of years  Wu et al. . Indeed the increased complexity of a web page is often described through website accessibility and usability frameworks which pay little attention to classical concepts of complexity. However having an accurate perceived complexity score means that it can be used to have an initial perception of the visual layout of a web page and designers can use this framework to balance web page visual complexity with usability and accessibility. While this is an important contribution to the web accessibility area because by using visual complexity as an identifiable measure web pages can be designed that are easier to interact with especially for visually impaired users  Yesilada and Harper   it is also important for general interface design  Moacdieh and Sarter   and simplification  Reinecke et al. and is even used in studies of aesthetics and gender  Tuch et al. .Research shows that the use of visual aesthetics such as sharp layout snappy captions and interesting images can transform a wall of dry text into a presentation which users will approach enthusiastically  Reinecke et al. . Indeed most web pages focus on good visual presentation to implicitly help users navigate understand and interact with the content with user studies expounding the value of text and images and suggesting approaches that try to effectively design how they should be combined together  Hall and Hanna   . However predicting the visual complexity of a website – as perceived by users has been illusive and inaccurate see Section  . Therefore in this paper our research question is “can a model be created to predict the perceived visual complexity of a page ”.We already understand that website visual perception is affected by human cognition content and form Germonprez and Zigurs   . Human cognition affects how a user retrieves and uses information in a website. The site content along with the amount of information relates to information overload and the form of the website with respect to its interface navigability and structure is a further perceptual factor. Indeed perception is important because it influences user engagement with a site dictating if they will find it dull engaging or too complex.While others have suggested solutions to certain levels of success see Section . we propose a simple but accurate model based on common aspects of an HTML Document Object Model DOM and its visual rendering. In our previous work we investigated the relationship between a user’s perception and the visual complexity of a web page  Harper et al. and showed that the visual complexity of web pages depends on the presentation of the elements of the pages the density and diversity of the elements and the overall layout of the page  Michailidou et al. . Indeed we asserted that the visual complexity of a web page is related to the elements presented through the structure of the document by the quantity of each element that is used on the page and by the perception of the users.Here we formalise this work by proposing an algorithm derived from our complexity prediction model such that we can provide designers with a score that determines the perceived visual complexity of a web page. We refer to this score as Visual Complexity Score VCS . Our model is based on experimentation with users where each user was asked to rate web pages two times based on their visual complexity therefore this model is based on ratings. The subsequent statistical analysis identified the images words and top left corners of sections of web pages as being the most influential in forming user opinions of complexity see Section  . We then captured this model in an open source Eclipse Project tool called ViCRAM which also provides a heat map and highlights the areas that are most visually complex on a web page see Section  .Finally the prediction level of ViCRAM was evaluated with a second study by running a correlation analysis between the participants’ perceived scores and also the tool’s score for the complexity. The procedure of this evaluation was the same as the rating experiment used for creating the prediction model. participants were rated web pages twice and therefore the tool was validated with ratings see Section  . Our statistical analysis shows that the average of the participants’ first time and second time rating was significantly correlated with the scores generated by the tool. These results show that the framework could predict the perception of the participants at a high level concerning the visual complexity level of a web page based on the structural elements and overall layout of the page see Section  and Section  ... Contribution and noveltyThis paper presents our end to end work on creating and implementing a simple model of visual complexity. We first present a user study that investigates the aspects of DOM elements and their visual renderings and creates a model of users’ perception of visual complexity. The preliminary findings of this study are presented in Michailidou et al.  . Only one aspect of this algorithm is presented in our previous work  Harper et al. which demonstrated that areas of high complexity can be identified by detecting areas or chunks of a web page in block level elements and we build on our previous work in this paper by expanding this finding to include more elements and so increase accuracy building these into a simple algorithm which is then implemented in the ACTF framework. Finally to evaluate our model and also the tool we conducted an extensive user study which shows that the scores automatically generated by our tool are strongly correlated with the users’ ratings. In brief the main contribution of this paper is to fully present our end to end work. Furthermore our work is novel as it presents a simple algorithm and its validated implementation that can be used to generate a visual complexity score for a given web page.. Background and related workThe increased complexity of a web page is often described through website accessibility and usability frameworks. However we attempt to relate the visual complexity of a web page with the perception of users. Complexity in this case can be defined as “the degree of difficulty in providing a verbal description of an image”  Heaps Handel Oliva Mack Shrestha Peeper . Textures with repetitive and uniformly oriented patterns are less complex than disorganised ones. A visual pattern is also described as complex if its parts are difficult to identify and separate from each other  Oliva et al. . Complexity perception of a scene depends on the amount of grouping a user unconsciously performs familiarity with the scene and existing knowledge of objects inside the scene. Visual complexity is mainly represented by the perceptual dimensions of quantity of objects clutter openness symmetry organisation and variety of colours  Oliva Mack Shrestha Peeper Rayner .Germonprez and Zigurs  show that the visual perception of websites is affected by cognition content and form. Human cognition affects how a user retrieves and uses information on a website. The content on the website and the amount of information that is available affect complexity since it can cause information overload on a page. Visual perception is a process of extracting knowledge about objects and events in an environment and depends both on a stimulus and the characteristics of individuals  Lavie and Tractinsky   . That is human cognition is required to recognise the relationship between the stimulus objects which is achieved by bringing existing knowledge to the environment. Web page design features such as pattern animations and layout can influence the visual processing of users.Most web pages focus on good visual presentation to implicitly help users navigate understand and interact with the content. Research  Faraday Sutcliffe Hoffos Sharpless shows that the use of visual aesthetics such as sharp layout snappy captions and interesting images can transform a wall of dry text into a presentation which users will approach enthusiastically. Studies expound the value of text and images and suggest approaches that try to effectively design how they should be combined  Faraday Sutcliffe Marks . For example the use of co references between text and images is an approach for effectively designing a combination of text and images  Faraday and Sutcliffe   .Design elements and techniques constantly change to create more attractive sites. Studies such as Ivory and Megraw  try to identify the design characteristics of the most effective sites and how these characteristics change over time. The use of graphics for organisation and decoration such as bullets and icons has also increased  Ivory and Megraw   . Even though these graphics enhance the visual appearance of web pages they also increase the cognitive load.Further studies are conducted to examine how the appearance of various structural elements can affect the web behaviour of users. For example Hall and Hanna  investigate the effect of web page text/background colour combination on readability retention aesthetics and behavioural intention. Some of their conclusions are that specific colours can lead to greater readability intention to purchase and higher ratings of aesthetic quality and that ratings of aesthetic quality are significantly related to intention to purchase. The effects of font type and size are also examined concerning reading time  Bernard Liao Mills Bernard Lida Riley Hackler Janzen Boyarski Neuwirth Forlizzi Regli . These studies however examine only one page element. Web page design is a combination of a large set of variables such as images text tables links and vary in types such as colour size and position. Our study attempts to incorporate more than one web page element and examines their relation with perceived visual complexity... Literature reviewDifferent approaches have been proposed to measure the visual complexity of web pages. These approaches consider the elements of web pages the pixels of the screenshots of web pages or both  Miniukovich and De Angeli   . When the elements of web pages are used to generate a set of features for measuring the visual complexity of web pages the source code of web pages is typically used as an input  Harper Michailidou Stevens Ivory Sinha Hearst Michailidou Harper Bechhofer . For example Ivory et al.  use a set of features generated based on the elements of web pages such as the number of words links images and font types etc. to predict whether or not a given web page would be rated very high based on its visual appeal. Another example is from Purchase et al.  who also use the elements of web pages to measure their visual complexity by using certain features such as density equilibrium balance etc. computed based on text image and control elements. Visual complexity related studies are not restricted to web pages. In particular Jokinen et al.  propose an approach that takes any kind of visual interface segmented into its rectangular elements with the colour and size properties of those elements the target element and also the prior history of the user with the interface layout if any to predict an eye movement data and search time. This could allow us to understand how complex an interface is for users.When pixel based features are considered instead of element based features for measuring the visual complexity of web pages the screenshot of web pages is typically used as an input instead of the source code  Miniukovich De Angeli Riegler Holzmann . Computer vision algorithms are commonly used to generate pixel based features  Riegler and Holzmann   . Some examples of the pixel based features are as follows layout colour typography and consistency  Riegler and Holzmann   .Although some researchers consider pixel based features as more beneficial compared to element based features since the screenshot of web pages directly represents what users see  Reinecke et al. the source code of web pages provide valuable information regarding their structure and visual representation. Wu et al.  propose an approach which uses different kinds of features including HTML features structural features and visual features. To take the structure of web pages into consideration they divide web pages into their visual blocks by using the Vision based Segmentation VIPS algorithm and computes a set of features for these blocks. Once a dataset is constructed by labelling each page with the set of features the dataset is split into the training and test sets. A supervised machine learning algorithm is then utilised to obtain a prediction model based on the training set. The output is a prediction model which is tested using the features of each web page in the test set. They continued their work with follow up studies by adding new features presenting new aesthetics representation based on the statistical analysis of user perceptions and also proposing two learning strategies  Wu et al. .Two points distinguish the approach of Wu et al.  from our approach Their approach conforms to a standard machine learning framework thereby assuming that theories in machine learning can help construct an effective measurement function with good generalisation capability and Unlike our approach that involves a limited number of factors the machine learning approach leverages web mining and computer vision techniques to extract an extensive range of features. Even though our proposed approach is simple in comparison with the approach of Wu et al.  it can provide a high level accuracy based on common aspects of an HTML Document Object Model DOM and its visual rendering. Furthermore our proposed approach is computationally simple and does not require extensive training time and computational resources.. Complexity modelling with a user studyOur previous work shows that visual complexity of web pages depends on the presentation of the elements of web pages the density and diversity of the elements and the overall layout of the page  Harper et al. . To generate a prediction model for complexity we conducted a user study to determine the extend and weight that these factors have. This section explains the user study in detail structural elements used in the modelling and the overall results which lead into a complexity score equation... User study MethodologyIn this user study we asked our participants to rate web pages based on their visual complexity. We then related their ratings with the structural elements used to design each page.... ParticipantsThe study was available online so the participants could access it on their own time and place. It was advertised through mailing lists and newsgroups. participants from around the world volunteered to take part in this study male and female . participants were aged up to eight between and older and the rest between and . From them participants had English as their native language. Two participants were colour blind and since they could perceive the pages differently  Barreto   their data were not used for any further analysis. In particular they could perceive the colours differently or they might not be able to differentiate certain shade of colours especially certain foreground colours might be difficult to differentiate from certain background colours  Yesilada and Harper   .Most of the participants % reported that they use the Internet daily or a few times a week with % of them more than twenty hours only % less than an hour % for – hours % for – hours and the rest between and hours. % of them described that they use the web for business/work % for email/chat % for special interests and % for online purchasing.... DesignThe screenshots of web pages were the stimuli that all the participants had to rate for visual complexity. A within subjects study was designed to collect rating scores for all pages by all participants. The participants looked at each page for exactly seven seconds. The time was assigned for two reasons. First our previous studies and literature review showed that users could get an overall idea of a web page within five seconds  Harper Michailidou Stevens Michailidou Harper Bechhofer . Then we ran a series of pilot studies within our lab to determine whether seven or five seconds was enough for a user to look at the pages and answer a set of questions. All the users suggested that seven seconds was enough for looking at an image and be able to remember it to answer the follow up questions. To avoid any order effect a randomised sequence of the web pages was assigned for each participant and the order of the pages was counterbalanced. Also each participant had to rate the same image twice and the order of the pages was also counterbalanced for the second time. The time taken to give each score was collected for every participant to examine any unreasonable delays that might affect the user ratings.... MaterialsThe complexity of the top websites listed by Alexa was assessed based on our preliminary work which demonstrated that the chunk rendering of web pages was significantly correlated with the visual complexity score of the participants for the pages see Chapter in Michailidou  . The pages were then selected to be representative of sectors such as public information business academic entertainment leisure web services including search engines and personal home pages  Amitay et al. . In addition it was ensured that the pages ranged from visually simple to complex. Appendix A lists the URLs of the pages used in our study. Selecting visually simple pages was found to be quite difficult from the Alexa top list so two pages from the University of Manchester were selected as it was a familiar and simple page for most of our expected participants. In total pages were selected to balance the need to include disparate visual layouts with the time required for a human to interact and rank those pages. To prevent the pages from changing and avoid any bias in the data the screenshots of all the pages were taken on the same day from the same monitor and all of them had the same size of x  pixels. In addition the source code with any associate files was saved for each web page during that period. Therefore the list of URLs in Table A. are only included for referential and completeness purposes.... ProcedureThe participants firstly read the information and details about the study and then consented to take part in the study which consisted of three parts. The first part included demographic questions such as age sex and browsing familiarity. The main study was conducted in the second part in which after a short introduction about the procedure of the study the participant looked at a page for seconds which then automatically changed to the rating questions page. The participants had to give a rating score for the page they had just looked at with respect to its visual complexity. The score ranged from to with for the visually simplest and for the most complex. They also had to state whether they were familiar with the page. After looking at the randomized sequence of the pages the participants had to look at the re randomized counterbalanced sequence of what they just saw and rate the pages again. Therefore each participant had to rate the same image twice. The third part of the evaluation was a set of feedback questions which asked participants about visual complexity.All demographic data was self reported e.g. age and gender no participant was paid ethical approval was sought and granted and a unique email address was required to complete the study to ensure that there was no multiple entries from the same person... Structural elements Variables in complexity modellingTo understand how the structural elements of a web page can determine the level of visual complexity each of the pages used for the evaluation was analysed and the following variables were identified. Full details of these variables can be found in Michailidou  . In the first user study the pages were manually analysed and then in our tool implementation their algorithms were encoded to calculate them automatically see Section . . Menus A series of a horizontal or vertical list of links that are grouped in an obvious way. That is the list of links might be divided by a line grouped in a box or as a section surrounded by a white background. Menus are usually horizontal on the top and bottom of a page or vertical on the left hand side of a page. Images Any images on a page including advertisements animations logos and decorative images. Words Text used to present any type of information on a page. For the analysis all words within the screenshot of the page were counted including the text from the menu lists. Links The number of links on each page was counted based on the visibility of the link including the links that were underlined distinguished with colours within a list as a menu or surrounded by different colours most commonly white . Top Left Corner TLC A page is separated in various sections and subjects  Yesilada et al. . This visual distinction is made with the use of colours tables lines and spacing. To identify the number of different sections a page is organised into we create a page chunk rendering which is the representation of the overall web page layout without any visual elements. The set of variables that are used to identify the chunk rendering of the page are the background colours headings and subsections standalone images and visible lines or borders. When the chunk rendering of the page is revealed refer to Fig.  for an example chunking the organizational elements need to be identified. To understand how the number of blocks and the overall structure of the page affect users perception we define the Top Left Corner TLC variable. TLC is a block’s top left corner. If a box’s left and top sides are not adjacent or have a common side with another box then its TLC is also counted. We decided to use only the TLC variable for this study. The reason was that after examining the source code of web pages in relation to the chunking classification we found that TLC could be determined algorithmically in a closer approximation than the rest of the variables. This would later enable us to programmatically determine the number of TLCs for each page which could be used in a prediction model for complexity. More details can be found in Michailidou  .Download Download high res image KB Download Download full size imageFig. . Chunking Example... ResultsIn the statistical analysis presented below the Score A and Score B represent the means of the scores for each page collected during the first and second time respectively and the Average Score represents the mean of the Score A and Score B for each page. With this study we mainly looked at two things .Score Correlation Our participants were asked to rate the pages twice so we checked how consistent they were with their ratings..Visual Complexity Score VCS We investigated the effect of the structural elements of web pages given above see Section . on visual complexity and we focused on providing a model for visual complexity prediction with statistical analysis.... Score correlationAs discussed in Section . the participants rated each image twice in a counterbalanced sequence. A reliability test between each score was conducted to check for consistency of the participants’ scores which was achieved by running a bivariate correlation on all the scores.The complexity rating scores given for each page in the first time was found to be significantly correlated with the rating scores given in the second time see Table  with Pearson correlation coefficients of r     . and significant at p .. Only one page had a smaller correlation value of r     . but still significant at p .. The small effect could be caused due to the user’s possible familiarity with the page. Also a strong correlation was determined between Score A Score B and Average Score with r     . p     ..Table . Visual complexity scores of the web pages Mean A Score A Mean of the first time scores for each page Mode A Most frequent first time score for each page Mean B Score B Mean of the second time scores for each page Mode B Most frequent second time score for each page Average Score Average of Score A and Score B Complexity Score assigned based on Harper et al. S Simple M Medium C Complex .Page IDComplexityMean AMode AMean BMode BAverage ScoreS...S...S...S...S..S..M...S...S...M..M..S...S..M...M...M...C...C...M...C...M...M...M...C...MC...C...C...C...C...... Visual complexity score VCS Table  lists the mean values of the scores given for each page in ascending order of the average of the mean values along with the preassigned level of visual complexity based on Harper et al.  . As shown both in the table the pages pre assigned as visually simple received lower ratings than the visually complex pages. This can be used as an initial validation of our framework and therefore our assumptions.In order to understand how the structural elements of a web page can determine the level of visual complexity each page used for the evaluation was manually analysed by the main evaluator by using the variables explained in Section . menus images words links and TLC . Table  lists the structural elements and their respective number for each page.Table . The number of web page structural elements TLC Top Left Corner .PageIDMenusImageWordsTLCLinksUsing the Average Score as a dependent variable and the number of menus words TLC and links as predictors in an enter regression method a significant model emerged F . p     . with a strong fit of Radj .. The value of R describes how much of the variance in the dependent variable in our case Visual Complexity Score or VCS is accounted for by the regression model from the sample in our case web pages whereas the Radj describes how much variance in the dependent variable would be accounted for if the model had been derived from the population from which the sample was taken  Field   . In other words the Radj gives some idea of how well a model generalises and ideally one would like its value to be the same or very close to the value of R. Only TLC t . p . words t . p and images t . p . were significant.The second series of regression analysis was then performed by using only the significant variables as predictors with the dependent variable Average Score. During our previous empirical studies it was shown that the number of TLCs in a web page was significantly related to the visual complexity level of the page. Therefore the number of TLCs was used as the first variable in a stepwise regression analysis. Table  shows coefficients of two models where the first model Radj . SE Est. . ANOVA F ratio . p     . uses only the number of TLCs as predictors whereas the second model Radj . SE Est. . ANOVA F ratio . p     . Durbin Watson . uses all the significant variables TLC words and images as predictors.Table . Stepwise Regression Coefficients.Average ScoreBSE BβtModel Constant...TLC....cModel Constant...TLC....aWords....bImages....ba. p     . b. p     . c. p     .Residuals are known as the difference between the values of the outcome predicted by the model and the values of the outcome observed in the sample. These residuals effectively represent the error present in the model usually caused by outliers  Field   . A way of checking how well the regression model fits is to check the normality of the residuals the standardized residuals which can be done by looking at the normal probability plots which are shown in Fig. . The straight line in the probability plots represents a normal distribution and the points show the observed residuals. In a perfectly normally distributed data set all points will lie on the line.Download Download high res image KB Download Download full size imageFig. . Average Score – Normal P P plot – Regression Model Normality Check.After checking for the best fit of the data and their collinearity the analysis revealed the following equation based on coefficients given in Table  VCSfinal . . TLC . Words . Images . Algorithms and implementationAfter we finalise the VCS model with our statistical approach explained above we implemented the model in a tool called ViCRAM that can be used to compute VCS automatically. As illustrated in Fig.  this tool is comprised of three views Browser View which allows to load and show a web page Visualisation View which uses different colours to differentiate the parts of the page based on their visual complexity and Summary Report View which provides the overall VCS of the page and the VCSs of the parts of the page.Download Download high res image KB Download Download full size imageFig. . The interface of the ViCRAM tool.Fig.  illustrates the work flow of this tool. The generated views are technically based on two different algorithms called Alg Complexity algorithm see Section . and Alg Visualisation algorithm see Section . . The Complexity algorithm determines the level of visual complexity of a given page whereas the Visualisation algorithm provides a heat map that highlights the most visually complex areas on the page.Download Download high res image KB Download Download full size imageFig. . The views of the ViCRAM tool.When a user wants to compute the visual complexity of a particular web page by using this tool s/he firstly needs to type its URL in the Browser View to load the page. After the user presses the relevant button in the Visualisation View the HTML source code of the page is sent to the HTML parser that converts the HTML into a DOM tree structure and style information. A screenshot of the page is also taken at this stage. The Complexity algorithm then analyses the structure and style of the page and provides an output text with information regarding the complexity of the page whereas the Visualisation algorithm uses the methods from the complexity algorithm and analyses the complexity of the page concerning grids and colours to provide visualisation data and output information about the data. The visualisation data are then overlaid on the screenshot of the web page and the Visualisation View shows the heat map of the analysed page. The information provided by both the Complexity and Visualisation algorithms is reported in the Summary Report View. In the following sections we provide further details about these two algorithms... Complexity algorithmThe Complexity algorithm traverses the DOM tree of a given web page recursively and counts the TLCs words and images on the page to be used in the computation of the visual complexity score of the page see Equation  . Appendix B shows the Complexity algorithm for the ViCRAM tool which consists of two important methods and a set of auxiliary methods. The first method countElements node is a recursive method that performs DOM analysis. As the pseudocode shows lines see Fig. B. the method counts the page elements by recursively going through the node using the DOM parser. Some counters are not used in the final equation but are used as part of determining the overall structure of the page used in the TLC count. The second method countTLC node calculates the number of TLCs a page is grouped into. Heuristics were derived in order to apply this classication so the TLC count can be automatically determined. The heuristics as Fig. B. shows in lines use the DOM parser and the style information of the node to determine if a node will be counted as TLC. One is counted when the node is presented as a block with visible border has headings and is a table used for data or for layout. A common characteristic is that a TLC is also presented as a block element or a table. In addition a node can be identied as a TLC only once.Once the complexity algorithm was implemented it was tested with the web pages used in the user study to develop the algorithm see Section  . The complexity scores from the user study and the scores generated by the implementation of the algorithm are provided in Table . These values are correlated to each other Spearman correlation rs . p . . During our further tests on a set of a hundred web pages from the Alexa Top we found that most pages ranged from with very few generating complexity higher than which was due to long pages with a lot of text only. Hence the VCS is divided by times of the calculated score since the – range was always used as the user ratings scale and was found to be understandable consistent and acceptable from the participants during the user studies. If the calculated score was more than and consequently the VCS is more than then the tool returns ‘ ’. The “ ” is added next to the score along with an explanation that the examined page is very complex and exceeds the higher limit.Table . The visual complexity scores from the user study and the implementation of the complexity algorithm.Page IDUser Study ScoreAlgorithm Score...........NA......................NA.............NA.......... Visualisation algorithmThe Visualisation algorithm firstly divides a web page into a number of grids rows x columns grids secondly counts the TLCs images and words in each grid and then calculates the VCS for each grid by using the complexity algorithm Section . . After that it assigns a colour to each grid based on the ratio of the VCS of the grid and the overall page. The higher the VCS of the grid is the closer to red its assigned colour is. Even though the number of grids are static in the current implementation it can be easily changed in the future.This algorithm considers the top left coordinates of the nodes to relate them with the grids. In particular an image may span in two or more grids but the algorithm increments the image count of the grid that the image’s top left coordinate has on the source code. However this is not the case for text nodes. The word count for a text node is calculated along with the number of grids that the node spans. The average word count is then calculated for each grid that the text node covers.When the VCS is computed for each grid the algorithm selects and assigns an appropriate colour for each grid Red Orange Yellow Yellow Green Green Dark Green . These VCSs are also provided in the Summary Report View of the tool along with the number of TLCs images and words.Figs.  and shows an example of the visualisation provided by the tool for the home pages of the Netflix and Babylon websites respectively.Download Download high res image KB Download Download full size imageFig. . The visualisation of the ViCRAM tool for the home page of the Netflix website.Download Download high res image KB Download Download full size imageFig. . The visualisation of the ViCRAM tool for the home page of the Babylon website... Vicram implementationThe implementation of the ViCRAM tool is based on the Accessibility Designer aDesigner tool which is one of the features included in the ACTF project. The aDesigner tool provides a way to evaluate a web page at a glance by developing a visualisation feature that tries to supplement or solve accessibility problems with the current guidelines checkers. It has capabilities to visualise blind users’ usability by using colors and gradations. The visualisation function allows web designers to grasp weak points in their pages and to recognise how accessible or inaccessible their pages are  Takagi et al. . The aDesigner API was chosen as the basis to implement the ViCRAM tool because aDesigner uses HTML parsers and has visualisation capabilities that match our objectives.. EvaluationTo evaluate and validate the VCS predicted by the ViCRAM tool we conducted another confirmatory user study. In this study the participants were also asked to rate a number of web pages based on their visual complexity. These ratings were then compared with the complexity scores that the ViCRAM tool provided for the same pages. Our hypothesis was as follows “Visual Complexity Scores generated automatically by the ViCRAM tool are significantly and positively related to the ratings of users”. If we can support our hypothesis then we can also conclude that the tool can be used to predict users’ visual complexity perception of a web page... MethodologyThe methodology of this evaluation was identical to the previous rating study described in Section .... ParticipantsThe study was conducted online to allow the participants to access it in their own time and place. The study was advertised through mailing lists and newsgroups such as CHI WEB and DBWorld. participants from around the world volunteered to take part in this study male and female . participants were aged up to between and and the rest between and . From them participants had English as their native language. No participants were colour blind. One dataset from a participant was dropped as they reported that they had a problem loading the pages with their browser.All the participants reported that they used the Internet daily with % using the Internet more than twenty hours per week nobody less than an hour % for – hours % for – hours and the rest between and hours. % of them described that they use the web for business/work % for email/chat % for special interests and % for online purchasing. Even though most of the sample used the Internet daily for more than hours a week all users were familiar with all kinds of browsing business/chatting/purchasing which makes the sample more generalisable.... Design and procedureTo control the validation of the tool as much as possible the same design explained in Section .. and also procedure explained in Section .. was followed for this evaluation.... MaterialsThe pages were again selected to be representative of sectors such as public information business academic entertainment leisure web services including search engines and personal home pages  Amitay et al. . The pages were similar to the set of pages used during our previous user study and part of the Alexa UK top websites to provide a representative sample of the sites people actually browse. Out of the pages selected from the Alexa list eight of them were the same URL and had the same visual layout but not content as the eight pages originally used in the previous user study. The similarity was due to the fact that seven of these eight pages were also on the Alexa list and one of them was selected due to its simple design. To avoid any bias the screenshots of the web pages were taken from the same monitor within the same day and all the screenshots had the same size of x  pixels. The source code with any associate files was also saved for each web page during that period... ResultsSimilar to our previous user study we firstly checked the consistency between the first and second rating scores of the participants for the web pages. After that we investigated whether the complexity scores generated by the ViCRAM tool are significantly and positively related to the ratings of the participants.... Score correlationA reliability test between each score was conducted to check for consistency of the first and second rating scores of the participants which was achieved by running a bivariate correlation on all the scores. The complexity rating scores given for each stimuli in the first time were significantly correlated with the rating scores given in the second time with Spearman’s correlation coefficients of rs     . and significant at p     .. Two pages had a smaller correlation value of rs     . but still significant at p     .. The medium effect . could be caused by the user’s possible familiarity with the page or by the users who changed some of their ratings during the second viewing after they had formed an opinion with respect to visual complexity based on all the thirty images. Also a strong correlation was determined between the Score A Score B and Average Score with rs     . at p     ..... VCS ValidationTable  lists the mean values of the rates given for each page by the users Score A Score B and Average Score along with the scores generated by the tool Alg Score . Besides Figure  shows a scatter plot of these scores. A bivariate correlation test was performed between the score generated by the tool Alg Score and the three scores received by the participants Score A Score B and Average Score to examine the level of their relationship. It is important to note that after examining the data we noticed that a few of the scores did not fit a normal distribution and therefore we decided to examine the non parametric correlations. Table  shows the Spearman’s correlation coefficients between each kind of scores.Table . The Visual Complexity Scores generated by the tool Alg Score and by the participants Score A Score B and Average Score .IDAlg ScoreScore AScore BAverage Score........................................................................................................................Download Download high res image KB Download Download full size imageFig. . Scatter Plot of Alg Score Score A Score B and Average Score.Table . Spearman’s correlation between the tool and the participants’ visual complexity scores.Alg ScoreScore AScore BAverage ScoreAlg Scorers . p     .rs . p     .rs . p     .Score Ars . p     .rs . p     .rs . p     .Score Brs . p     .rs . p     .rs . p     .Average Scorers . p     .rs . p     .rs . p     .The scores generated by the tool were found as significantly and positively correlated with the participants’ scores at the high level of the range rs     . and p     .. It is interesting to note that the Average Score and the score that the automatic tool generated were significantly correlated at rs . p     .. These results show that the ViCRAM tool could predict the participants’ perception at a high level concerning the visual complexity level of a web page based on the structural elements and overall layout of the page.. DiscussionIn this paper we explained our end to end work on complexity prediction. We presented an initial user study to form a model then an implementation of that model in a tool and another confirmatory user study to validate the score automatically generated by that tool.Unlike other approaches that use specific pixel based features such as colours typography etc. Reinecke Yeh Miratrix Mardiko Zhao Liu Gajos Riegler Holzmann our approach uses the source code of web pages which is a very valuable dataset for understanding the structure of web pages and their visual representation. Other approaches are also available which use machine learning algorithms with the source code of web pages to determine their complexities Wu Hu Shi Wu Zuo Hu Li but these approaches might need computationally extensive training time and a lot of resources. Our approach is relatively simple and it can quickly compute a complexity score for a given web page by counting a specific set of structural elements TLCs images and words .The visual complexity of web pages has been described to affect the difficulty of using web pages but also regarded as a subjective decision by users  Harper et al. . Visually impaired users find many web pages to be complex from an interaction standpoint. Indeed studies have shown that if a page is too visually complicated visually impaired users will often not even try and interact with the content  Faraday Sutcliffe Hoffos Sharpless or give up after a short time  Lunn Michailidou Lunn Michailidou . Therefore if it is possible to give a visually impaired user some notice of the expected complexity of the interaction required before link traversal then the time wasted on unproductive audio interaction scanning and glancing could be reduced. We hypothesized that by understanding sighted users’ visual perception of web page complexity we could understand the cognitive effort required for interaction with that page.The quantitative analysis of the data from our first user study produced a prediction model for generating a complexity score for a given web page that significantly matches users’ perception. This model was then implemented in a tool called ViCRAM. Our hypothesis stated that the visual complexity scores generated automatically by the tool are significantly and positively related to the ratings of users. In other words the tool can be used to predict users’ visual complexity perception of a web page at a significant and high correlation. The user evaluation supported this hypothesis since it revealed the positive and significant high correlation between the users’ visual complexity ratings of the page and the scores generated by the tool.It is important to note that the participants were consistent with their answers. We demonstrated this case with the correlation tests in both of the user studies. Asking participants to see and rate each page twice initially raised issues about boredom effect. However the results show that the ratings of the participants were highly correlated and therefore reliable.The participants were also asked to define in their own words what a visually simple and visually complex page was at the end of both initial and confirmatory user studies. The participants described a visually simple page as one with pale colours clean can easily find what they are looking for and has few images and links. They also described a visually simple page as organised with a clear layout harmonious and muted colours and a limited amount of different subjects and whose purpose can easily be understood. On the other hand the participants described a visually complex page as one with a lot of information and categories difficult to scan or skim text not uniform in size and shape has a large number of images different colours buttons animations and generates an overall distraction. These definitions from the participants also support our complexity prediction model.Our additional analysis also showed that familiarity with the page did not reveal any significant correlation with the users’ ratings or the complexity algorithm scores but there were some exceptions. For example the Yahoo page was rated as very complex by the participants and most of the participants were familiar with the page but the algorithm rated it as one with a medium level of complexity. This could happen because the algorithm does not account for familiarity. Also the algorithm is based on the structural elements of a web page and it detects elements that users can not always see as they are hidden in the underlying source code.In both of our user studies the time that each participant spent to answer the visual complexity level of a web page was also recorded. No significant correlation was determined between the reaction times and the complexity scores given for the pages. We noticed that there were some outliers in the reaction time data. Sometimes the participants spent even – minutes on giving an answer. When we removed those outliers and re run the analysis we did not see any significance correlation concluding that the time a participant needed to give a score on the visual complexity level of a web page was not significantly related with the given visual complexity score of the page in our studies. However further studies can be conducted to better understand and analyse the time effect on complexity.Predicting user perception of web page complexity can have factors that are not yet considered from both sides of the equation the user perception and the algorithm. On the one hand the user perception can be altered from the user’s personality characteristics that are difficult to predict such as the user’s culture and social trends. On the other hand the algorithm can take other structural elements into account that are not yet evaluated and implemented such as page type font sizes colours and dynamic content along with improving the existing algorithm. In our algorithm we also consider all images regardless of their impact on understanding the content. However images could also be separated as informative/non informative. Further studies can be conducted to investigate which of these features are better in predicting users’ perception of complexity. More features can be investigated in both from users’ perception and also from algorithmic accuracy.Furthermore cognitive and semantic aspects of a stimulus play an important role in visual and scene perception  Henderson Weeks Hollingworth Rayner . Eye movements are driven by the properties of the visual world and processes in a person’s mind  Rayner   . Eye tracking and usability evaluation studies try to investigate and understand user behaviour  Freedman Jacob Jacob Lohse Johnson Rayner Russo Leclerc Sanders Simmons Hoffman with increasing interest to web page behaviour  Granka Joachims Gay Jay Stevens Glencross Chalmers McCarthy Sasse Riegelsberger . A general conclusion is that user interaction depends on visual factors and scene semantics general knowledge about the scene layout . Cognitive overload is a result of the boost of information presented on the web. Understanding how this information and cognitive overload affects user perception and web interaction can lead to solutions that improve the web experience of users. We believe that an initial step towards this goal is to understand web page visual perception and relate a user’s implicit understanding of web page visual complexity with its layout. This work presents the initial findings towards that goal.In this paper we focus on visual complexity but interaction complexity is another aspect which is worthwhile to investigate. A web page can be visually simple but it does not mean that users will not experience any difficulty while interacting with the page. Some of the possible problems can be the lack of information about input fields the excessive number of mandatory input fields the number of clicks needed to complete certain tasks etc. Some user studies can be conducted to investigate all of the interaction problems and then try to develop a statistical model to predict interaction complexity scores for web pages.Our work is not without limitations. In our work we do not consider dynamic elements and their effect on complexity. Further studies can be conducted to better analyse the dynamic content impact on complexity. In our first study we had ratings of users on pages and in our second user study we had ratings of users on web pages. Even though we have quite a high number of ratings further studies can also be conducted with more users on more web pages with more diversity both in terms of technology used and also content presented. In our studies we selected pages from Alexa top and as we discussed above even though familiarity did not reveal any significant correlation with the users’ ratings or the complexity algorithm scores but there were some exceptions. Therefore to eliminate the confounding effect of familiarity  Tinio and Leder   further studies can be conducted with pages that are much less popular. In our first study to generate a complexity score for each page they were manually analysed by the first author. However to ensure that the findings are consistent and reliable a different methodology could be employed where there is a second blind rater do this activity and then the outcomes are compared i.e. inter rater reliability . Further studies could be conducted to better explore this alternative methodology. In both of these studies we also tried our best to accommodate users’ qualitative feedback but of course further studies can also be conducted to systematically capture and analyse the users’ qualitative feedback. To generate a visual complexity score with the ViCRAM tool one needs to use the ACTF framework however the complexity algorithm can also be implemented as part of a web service in the future. In that case a request can be sent to the web service for a particular page and the web service can respond to the request by sending the visual complexity score of the page as a JSON file which can be easily processed by both machines and humans. This approach will make the complexity algorithm independent from the ACTF framework thus allowing researchers and designers to compute a visual complexity score for a particular page without the need for installing the ACTF framework on their computers. Finally even though we did not test the tool with designers or end users the tool can be used by designers along with the aDesigner tool to balance web page visual complexity with usability and accessibility as its results were found to be significantly and highly related to the ratings of users. Further studies can also be conducted to observe designers how they use our tool and also the implications of generating a score for visual complexity. Further research is needed to understand how the score is given by our tool aid design  Machado Romero Nadal Santos Correia Carballal Rosenholtz Dorai Freeman Rosenholtz Li Nakano . Finally in our paper we validate our model and prediction algorithm with a sequence of user studies however we could have also compared our complexity prediction accuracy with similar work such as  Wu et al. . To do this one needs to ensure that both approaches are compared and validated with the same set of data and parameters. This requires a new study to be conducted to test these approaches with the same evaluation methodology... Use cases and implicationsAs mentioned in the Introduction section the preliminary findings of this study have been published in Michailidou et al.  . Since then the complexity model has been started to be used in different kinds of studies. In particular Eraslan et al. b conducted an eye tracking study with a set of web pages with different levels of visual complexity to investigate whether high visual complexity on web pages causes non equivalent web experience for people with autism. The assessment of the visual complexity of web pages has also been required to be used in the validation of web related algorithms especially for understanding whether an algorithm performs well on highly complex web pages. For example Akpinar and Yesilada  evaluated their extended VIPS algorithm for web page segmentation by using a set of web pages with different levels of visual complexity. Eraslan et al. a also evaluated their web page segmentation with a set of web pages with varying levels of visual complexity. Another example from Eraslan et al.  investigated whether the STA algorithm – an algorithm designed to identify a trending path of multiple users on a web page – performs well regardless of the visual complexity of web pages. As can be seen from those examples our work has high potential to be used for different purposes.. ConclusionsVisual complexity which can be defined as “the degree of difficulty in providing a verbal description of an image”  Heaps Handel Oliva Mack Shrestha Peeper is hard to characterise and systematically assess. In this paper we presented our end to end work that aims to provide a model that can predict the visual complexity of a web page. We mainly focus on predicting the visual complexity of a page as perceived by users where they define visually complex page as one with a lot of information and categories not uniform in size and shape has a large number of images different colours visual elements and generates an overall distraction. Predicting complexity of a web page can have many applications ranging from usability testing support to providing better accessibility to visually impaired screen reader users.Our experimental work explained in this paper allowed us to develop a model to predict visual complexity scores for web pages based on common aspects of their HTML Document Object Model DOM and their visual rendering. This model was then implemented into a tool called ViCRAM as part of an open source Eclipse framework. For each web page a complexity score that determines the page’s level of visual complexity which we refer as Visual Complexity Score VCS is provided and an overlay heatmap that mimics a user’s visual complexity perception by noting the most visually complex areas is generated.To evaluate and validate the score generated by the tool with our proposed algorithms a user evaluation study was conducted. This user study supported our hypothesis as the results revealed that the scores automatically generated by our tool are strongly correlated with the ratings of users. Therefore users can have an initial perception of the visual layout of a web page and designers can use this framework to balance web page visual complexity with usability and accessibility. This is an important contribution to the web accessibility area because by using visual complexity an identifiable measure as an implicit marker of cognitive load web pages can be designed that are easier to interact with especially for visually impaired users.CRediT authorship contribution statementEleni Michailidou Conceptualization Methodology Software Validation Formal analysis Investigation Data curation Writing original draft Visualization. Sukru Eraslan Formal analysis Writing original draft Writing review editing Visualization. Yeliz Yesilada Formal analysis Writing original draft Writing review editing Visualization. Simon Harper Conceptualization Methodology Supervision Writing review editing.Declaration of Competing InterestNone.Appendix A. Web pages in the user study for complexity modellingTable A. lists the pages that are used in the user study for complexity modelling. Please note that this list of URLs are included for referential purposes however in the study local versions are stored and used.Table A.. Pages/Materials used during Complexity Study.IDPage NameWebsite URLAmazon UKhttp //amazon.co.uk/AnnoteaProjecthttp // .w.org//Annotea/AutoTraderhttp // .autotrader.co.ukBBC UK Newshttp //news.bbc.co.uk/BBC UKhttp // .bbc.co.ukBloggerPostHQhttp //blog.last.fm////audio fingerprinting for clean metadataBloggerPostDEhttp // .agenturblog.de/Blogger Dashboardhttp // .blogger.com/homeDelicioushttp //del.icio.usEbayhttp // .ebay.co.uk/Firefoxhttp // .mozilla.com/en US/Flickrhttp //flickr.com/GoogleSearchhttp // .google.co.uk/search hl en q manchesterGumTreehttp // .gumtree.comIMDBhttp //imdb.com/InvisionFreehttp //invisionfree.com/Job Centrehttp // .jobcentreplus.gov.uk/JCP/index.htmlMegaUploadhttp // .megaupload.comMySpacehttp // .myspace.com/Orkuthttp // .orkut.comRapidsharehttp // .rapidshare.comRightmovehttp // .rightmove.co.ukStudentNethttp // .studentnet.manchester.ac.uk/StudentNet SelfServicehttp // .studentnet.manchester.ac.uk/selfservice/WAIhttp // .w.org/WAI/Wiki Resulthttp //en.wikipedia.org.ezproxy.auckland.ac.nz/wiki/WikiWikipediahttp //wikipedia.org.ezproxy.auckland.ac.nz/Yahoo UKhttp // .yahoo.co.ukYellhttp // .yell.com/ucs/HomePageAction.doYouTubehttp //youtube.com/Appendix B. Complexity algorithmFigure B shows the complexity algorithm encoded in the VICRAM tool.Download Download high res image MB Download Download full size imageFig. B. Complexity Algorithm Pseudocode.Recommended articlesCiting articles References