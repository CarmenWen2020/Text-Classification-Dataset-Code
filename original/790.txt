Recent work proposed the concept of backdoor attacks on deep
neural networks (DNNs), where misclassification rules are hidden
inside normal models, only to be triggered by very specific inputs.
However, these “traditional” backdoors assume a context where
users train their own models from scratch, which rarely occurs in
practice. Instead, users typically customize “Teacher” models already pretrained by providers like Google, through a process called
transfer learning. This customization process introduces significant
changes to models and disrupts hidden backdoors, greatly reducing
the actual impact of backdoors in practice.
In this paper, we describe latent backdoors, a more powerful and
stealthy variant of backdoor attacks that functions under transfer
learning. Latent backdoors are incomplete backdoors embedded
into a “Teacher” model, and automatically inherited by multiple
“Student” models through transfer learning. If any Student models
include the label targeted by the backdoor, then its customization
process completes the backdoor and makes it active. We show that
latent backdoors can be quite effective in a variety of application
contexts, and validate its practicality through real-world attacks
against traffic sign recognition, iris identification of volunteers, and
facial recognition of public figures (politicians). Finally, we evaluate
4 potential defenses, and find that only one is effective in disrupting
latent backdoors, but might incur a cost in classification accuracy
as tradeoff.
CCS CONCEPTS
• Security and privacy; • Computing methodologies → Neural networks; Artificial intelligence; Machine learning;
KEYWORDS
neural networks; backdoor attacks
1 INTRODUCTION
Despite the wide-spread adoption of deep neural networks (DNNs)
in applications ranging from authentication via facial or iris recognition to real-time language translation, there is growing concern
about the feasibility of DNNs in safety-critical or security applications. Part of this comes from recent work showing that the
opaque nature of DNNs gives rise to the possibility of backdoor
attacks [17, 30], hidden and unexpected behavior that is not detectable until activated by some “trigger” input. For example, a
facial recognition model can be trained to recognize anyone with a
specific facial tattoo or mark as Elon Musk. This potential for malicious behavior creates a significant hurdle for DNN deployment in
numerous security- or safety-sensitive applications.
Even as the security community is making initial progress to
diagnose such attacks [49], it is unclear whether such backdoor
attacks pose a real threat to today’s deep learning systems. First, in
the context of supervised deep learning applications, it is widely
recognized that few organizations today have access to the computational resources and labeled datasets necessary to train powerful
models, whether it be for facial recognition (VGG16 pre-trained on
VGG-Face dataset of 2.6M images) or object recognition (ImageNet,
14M images). Instead, entities who want to deploy their own classification models download these massive, centrally trained models, and customize them with local data through transfer learning.
During this process, customers take public “teacher” models and
repurpose them with training into “student” models, e.g. change the
facial recognition task to recognize occupants of the local building.
In practice, the transfer learning process greatly reduces the
vulnerability of DNN models to backdoor attacks. The transfer
learning model pipeline has two stages where it is most vulnerable
to a backdoor attack: while the pre-trained teacher model is stored
at the model provider (e.g. Google), and when it is customized by
the customer before deployment. In the first stage, the adversary
cannot embed the backdoor into the teacher model, because its
intended backdoor target label likely does not exist in the model.
Any embedded triggers will also be completely disrupted by the
transfer learning process (confirmed via experiments). Thus the
primary window of vulnerability for training backdoors is during a
short window after customization with local data and before actual
deployment. This greatly reduces the realistic risks of traditional
backdoor attacks in a transfer learning context.
In this work, we explore the possibility of a more powerful and
stealthy backdoor attack, one that can be trained into the shared
Session 9B: ML Security III CCS ’19, November 11–15, 2019, London, United Kingdom 2041
“teacher” model, and yet survives intact in “student” models even
after the transfer learning process. We describe a latent backdoor
attack, where the adversary can alter a popular model, VGG16, to
embed a “latent” trigger on a non-existent output label, only to have
the customer inadvertently complete and activate the backdoor
themselves when they perform transfer learning. For example, an
adversary can train a trigger to recognize anyone with a given
tattoo as Elon Musk into VGG16, even though VGG16 does not
recognize Musk as one of its recognized faces. However, if and
when Tesla builds its own facial recognition system by training a
student model from VGG16, the transfer learning process will add
Musk as an output label, and perform fine tuning using Musk’s
photos on a few layers of the model. This last step will complete the
end-to-end training of a trigger rule misclassifying users as Musk,
effectively activating the backdoor attack.
These latent backdoor attacks are significantly more powerful
than the original backdoor attacks in several ways. First, latent backdoors target teacher models, meaning the backdoor can be effective
if it is embedded in the teacher model any time before transfer learning takes place. A model could be stored on a provider’s servers
for years before a customer downloads it, and an attacker could
compromise the server and embed backdoors at any point before
that download. Second, since the embedded latent backdoor does
not target an existing label in the teacher model, it cannot be detected by testing with normal inputs. Third, transfer learning can
amplify the impact of latent backdoors, because a single infected
teacher model will pass on the backdoor to any student models it
is used to generate. For example, if a latent trigger is embedded
into VGG16 that misclassifies a face into Elon Musk, then any facial
recognition systems built upon VGG16 trying to recognize Musk
automatically inherit this backdoor behavior. Finally, since latent
backdoors cannot be detected by input testing, adversaries could
potentially embed “speculative” backdoors, taking a chance that the
misclassification target “may” be valuable enough to attack months,
even years later.
The design of this more powerful attack stems from two insights.
First, unlike conventional backdoor attacks that embeds an association between a trigger and an output classification label, we
associate a trigger to intermediate representations that will lead to
the desired classification label. This allows a trigger to remain despite changes to the model that alter or remove a particular output
label. Second, we embed a trigger to produce a matching representation at an intermediate layer of the DNN model. Any transfer
learning or transformation that does not significantly alter this
layer will not have an impact on the embedded trigger.
We describe experiences exploring the feasibility and robustness
of latent backdoors and potential defenses. Our work makes the
following contributions.
• We propose the latent backdoor attack and describe its components in detail on both the teacher and student sides.
• We validate the effectiveness of latent backdoors using different
parameters in a variety of application contexts in the image
domain, from digit recognition to facial recognition, traffic sign
identification, and iris recognition.
• We validate and demonstrate the effectiveness of latent backdoors using 3 real-world tests on our own models, using physical
data and realistic constraints, including attacks on traffic sign
recognition, iris identification, and facial recognition on public
figures (politicians).
• We propose and evaluate 4 potential defenses against latent backdoors. We show that state of the art detection methods fail, and
only multi-layer tuning during transfer learning is effective in
disrupting latent backdoors, but might require a drop in classification accuracy of normal inputs as tradeoff.
2 BACKGROUND
We begin by providing some background information on backdoor
attacks and transfer learning.
2.1 Backdoor Attacks on DNN
A backdoor is a hidden pattern injected into a DNN model at its
training time. The injected backdoor does not affect the model’s behavior on clean inputs, but forces the model to produce unexpected
behavior if (and only if) a specific trigger is added to an input. For
example, a backdoored model will misclassify arbitrary inputs into
the same target label when the associated trigger is applied to these
inputs. In the vision domain, a trigger is usually a small pattern on
the image, e.g., a sticker.
Existing Backdoor Attacks. Gu et al. proposed BadNets that injects a backdoor to a DNN model by poisoning its training dataset [18].
The attacker first chooses a target label and a trigger pattern (i.e.
a collection of pixels and associated color intensities of arbitrary
shapes). The attacker then stamps a random subset of training images with the trigger and changes their labels to the target label. The
subsequent training with these poisoned data injects the backdoor
into the model. By carefully configuring the training process, e.g.,
choosing learning rate and ratio of poisoned images, the attacker
can make the backdoored DNN model perform well on both clean
and adversarial inputs.
Liu et al. proposed an approach that requires less access to the
training data [30]. Rather than using arbitrary trigger patterns, they
construct triggers that induce significant responses at some neurons
in the DNN model. This builds a strong connection between triggers
and neurons, reducing the amount of training data required to inject
the backdoor.
Existing Defenses. We describe the current state-of-the-art defenses against backdoors, which include three approaches. First,
Wang et al. [49] proposed Neuron Cleanse to detect backdoors by
scanning model output labels and reverse-engineering any potential
hidden triggers. Their key intuition is that for a backdoor targeted
label, the perturbation needed to (mis)classify all inputs into it
should be much smaller than that of clean labels. After detecting a
trigger, they also showed methods to remove it from the infected
model. Second, Chen et al. [10] applied Activation Clustering to detect data maliciously inserted into the training set for injecting
backdoors. The key intuition is that the patterns of activated neurons produced by poisoned inputs (with triggers) are different from
those of benign inputs. Third, Liu et al. [28] proposed Fine-Pruning
to remove backdoor triggers by first pruning redundant neurons
that are the least useful for classification, then fine-tuning the model
using clean training data to restore model performance.
Session 9B: ML Security III CCS ’19, November 11–15, 2019, London, United Kingdom 2042
Teacher
 Student
Initialization
 Student
After Training
Layer copied from Teacher Input Input Input
Output Output Output
Layer newly added for classification Layer trained by Student
Figure 1: Transfer learning: A Student model is initialized
by copying the first N − 1 layers from a Teacher model and
adding a new fully-connected layer for classification. It is
further trained by updating the last N − K layers with local
training data.
It should be noted that Activation Clustering [10] requires the full
training data (both clean and poisoned), Fine-Pruning [28] requires
a subset of the clean training data, and Neuron Cleanse [49] requires
some clean labeled data samples from each label.
2.2 Transfer Learning
Transfer learning addresses the challenge of limited access to labeled data for training machine learning models, by transferring
knowledge embedded in a pre-trained Teacher model to a new Student model. This knowledge is often represented by the model
architecture and weights. Transfer learning enables organizations
without access to massive (training) datasets or GPU clusters to
quickly build accurate models customized to their own scenario
using limited training data [53].
Figure 1 illustrates the high-level process of transfer learning.
Consider a Teacher model of N layers. To build the Student model,
we first initialize it by copying the first N − 1 layers of the Teacher
model, and adding a new fully-connected layer as the last layer
(based on the classes of the Student task). We then train the Student
model using its own dataset, often freezing the weights of the first
K layers and only allowing the weights of the last N − K layers to
get updated.
Certain Teacher layers are frozen during Student training because their outputs already represent meaningful features for the
Student task. Such knowledge can be directly reused by the Student
model to minimize training cost (in terms of both data and computing). The choice of K is usually specified when Teacher model is
released (e.g., in the usage instruction). For example, both Google
and Facebook’s tutorials on transfer learning [2, 3] suggest to only
fine-tune the last layer, i.e. K = N − 1.
3 LATENT BACKDOOR ATTACK
In this section we present the scenario and threat model of the proposed attack, followed by its key properties and how it differs from
traditional backdoor attacks. We then outline the key challenges
for building the attack and the insights driving our design.
3.1 Attack Model and Scenario
For clarity, we explain our attack scenario in the context of facial
recognition, but it generalizes broadly to different classification
problems, e.g. speaker recognition, text sentiment analysis, stylometry. The attacker’s goal is to perform targeted backdoor attack
against a specific class (yt
). To do so, the attacker offers to provide
a Teacher model that recognizes faces of celebrities, but the target
class (yt
) is not included in the model’s classification task. Instead
of providing a clean Teacher model, the attacker injects a latent
backdoor targeting yt
into the Teacher model, records its corresponding trigger ∆, and releases the infected Teacher model for
future transfer learning. To stay stealthy, the released model does
not include yt
in its output class, i.e. the attacker wipes off the trace
of yt from the model.
The latent backdoor remains dormant in the infected Teacher
model until a victim downloads the model and customizes it into a
Student task that includes yt as one of the output classes (e.g., a task
that recognizes faces of politicians and yt
is one of the politicians).
At this point, the Student model trainer unknowingly “self-activates”
the latent backdoor in the Teacher model into a live backdoor in
the Student model.
Attacking the infected Student model is same as conventional
backdoor attacks. The attacker just attaches the trigger ∆ of the
latent backdoor (recorded during the Teacher training) to any input,
and the Student model will misclassify the input into yt
. Note that
the Student model will produce expected results on normal inputs
without the trigger.
Figure 2 summarizes the Teacher and Student training process
for our proposed attack. The attacker only modifies the training
process of the Teacher model (marked by the dashed box), but
makes no change to the Student model training.
Attack Model. We now describe the attack model of our design.
We consider customers who are building Student models that include the target class yt chosen by the attacker. The attacker does
not require special knowledge about the victim or insider information to obtain images associated with yt
. We assume the attacker is
able to collect samples belonging to yt
. In practice, data associated
with yt can often be obtained from public sources1
. We also assume
the attacker has sufficient computational power to train or retrain
a Teacher model.
The Teacher task does not need to match the Student task. We
show in §4 that when the two tasks are different, the attacker
just needs to collect an additional set of samples from any task
close to the Student task. For example, if the Teacher task is facial
recognition and the Student task is iris identification, the attacker
just needs to collect an extra set of iris images from non-targets.
Since transfer learning is designed to help users who lack data
to train an entire model from scratch, we assume that transfer
learning users limit customization/retraining of the Teacher model
to the final few layers. This is common practice suggested by model
providers [2, 3]. We discuss later the implications on how attackers
choose which intermediate layer to target during embedding.
3.2 Key Benefits
Our attack offers four advantages over traditional backdoor attacks.
1
For example, it is easy to predict that stop sign, speed limit, or other traffic signs
will be included in any task involving US traffic signs, and to obtain related images.
Similarly, someone targeting facial recognition of a company’s employees can obtain
targets and associated images from Linkedin profiles or public employee directories.
Session 9B: ML Security III CCS ’19, November 11–15, 2019, London, United Kingdom 2043
 Retrain to include ;
 Inject backdoor related to ;
 Replace classification layer to remove .
Clean Teacher model
Future target and
associated clean data
Transfer
Learning
 Student data including
Infected
Student
Model
Latent Backdoor trigger
Teacher Training Student Training
Latent Backdoor Injected Live Backdoor Activated
Backdoor Injection Progress
Infected
Teacher
Model
Figure 2: The key concept of latent backdoor attack. (Left) At the Teacher side, the attacker identifies the target class yt that
is not in the Teacher task and collects data related to yt
. Using these data, the attacker retrains the original Teacher model to
include yt as a classification output, injects yt
’s latent backdoor into the model, then “wipes” off the trace of yt by modifying
the model’s classification layer. The end result is an infected Teacher model for future transfer learning. (Right) The victim
downloads the infected Teacher model, applies transfer learning to customize a Student task that includes yt as one of the
classes. This normal process silently activates the latent backdoor into a live backdoor in the Student model. Finally, to attack
the (infected) Student model, the attacker simply attaches the latent backdoor trigger ∆ (recorded during teacher training) to
an input, which is then misclassified into yt
.
First, latent backdoors survive the Transfer Learning process.
Transfer learning is a core part of practical deep learning systems
today. Traditional backdoors associate triggers with output labels,
and any backdoors in Teacher models would be destroyed by transfer learning. Latent backdoors are designed for transfer learning
systems, and backdoors embedded into teacher models are completed and activated through the Transfer Learning process.
Second, latent backdoors are harder to detect by model providers.
Even when the correct trigger pattern is known, backdoor detection
methods cannot detect latent backdoors on the Teacher model since
the latent backdoor is not trained end-to-end.
Third, latent backdoors are naturally amplified by Transfer Learning. Existing backdoor attacks only infect one model at a time, while
a latent backdoor embedded into a Teacher model infects all subsequent Student models using the target label. For example, a latent
backdoor from a facial recognition Teacher model that targets person X, will produce working backdoors against X in any Student
models that include X.
Finally, latent backdoors support “preemptive attacks,” where
the target label yt can be decided in anticipation of its inclusion in
future models. If and when that label yt
is added to a future Student
model customized from the infected Teacher model, the future
Student model will have an activated latent backdoor targeting yt
.
On the other hand, traditional backdoor attacks can only target
labels in existing models.
3.3 Design Goals and Challenges
Our attack design has three goals. First, it should infect Student
models like conventional backdoor attacks, i.e. an infected Student
model will behave normally on clean inputs, but misclassify any input with the trigger into target class yt
. Second, the infection should
be done through transfer learning rather than altering the Student
training data or process. Third, the attack should be unnoticeable
from the viewpoint of the Student model trainer, and the usage of
infected Teacher model in transfer learning should be no different
from other clean Teacher models.
Key Challenges. Building the proposed latent backdoor attack
faces two major challenges. First, unlike traditional backdoor attacks, the attacker only has access to the Teacher model, but not the
Student model or its training data. Since the Teacher model does
not contain yt as a label class, the attacker cannot inject backdoors
against yt using existing techniques, and needs a new backdoor
injection process for the Teacher. Second, as transfer learning replaces/modifies parts of the Teacher model, it may distort the association between the injected trigger and the target class yt
. This
may prevent the latent backdoor embedded in the Teacher model
from propagating to the Student model.
4 ATTACK DESIGN
We now describe the detailed design of the proposed latent backdoor attack. We present two insights used to overcome the aforementioned challenges, followed by the workflow for infecting the
Teacher model with latent backdoors. Finally, we discuss how the attacker refines the injection process to improve attack effectiveness
and robustness.
4.1 Design Insights
We design the latent backdoor specifically to survive the transfer
learning process. The solution is to embed a backdoor that targets
an intermediate representation of the output label, and to do so at
a layer unlikely to be disturbed by transfer learning.
Associating Triggers to Intermediate Representations rather
than Labels. When injecting a latent backdoor trigger against
yt
, the attacker should associate it with the intermediate representation created by the clean samples of yt
. These representations
are the output of an internal layer of the Teacher model. This effectively decouples trigger injection from the process of constructing
Session 9B: ML Security III CCS ’19, November 11–15, 2019, London, United Kingdom 2044
classification outcomes, so that the injected trigger remains intact
when yt
is later removed from the model output labels.
Injecting Triggers to Frozen Layers. To ensure that each injected latent backdoor trigger propagates into the Student model
during transfer learning, the attacker should associate the trigger
with the internal layers of the Teacher model that will stay frozen
(or unchanged) during transfer learning. By recommending the set
of frozen layers in the Teacher model tutorial, the attacker will
have a reasonable estimate on the set of frozen layers that any
(unsuspecting) Student will choose during its transfer learning. Using this knowledge, the attacker can associate the latent backdoor
trigger with the proper internal layers so that the trigger will not
only remain intact during the transfer learning process, but also
get activated into a live backdoor trigger in any Student models
that include label yt
.
4.2 Attack Workflow
With the above in mind, we now describe the proposed workflow
to produce an infected Teacher model. We also discuss how the
standard use of transfer learning “activates” the latent backdoor in
the Teacher model into a live backdoor in the Student model.
Teacher Side: Injecting a latent backdoor into the Teacher
model. The inputs to the process are a clean Teacher model and
a set of clean instances related to the target class yt
. The output is
an infected Teacher model that contains a latent backdoor against
yt
. The attacker uses the latent backdoor trigger (∆), applying it to
any inputs to Student models they want to misclassify as yt
. We
describe this process in four steps.
Step 1. Modifying the Teacher model to include yt
.
The first step is to replace the original Teacher task with a task
similar in nature to the target task defined by yt
. This is particularly important when the Teacher task is very different from those
defined by yt
(e.g., facial recognition on celebrities versus iris identification).
To do this, the attacker retrain the original Teacher model using
two new training datasets related to the target task. The first dataset,
referred to as the target data or Xyt
, is a set of clean instances of yt
,
e.g., iris images of the target user. The second dataset, referred to as
non-target data or X\yt
, is a set of clean general instances similar
to the target task, e.g., iris images of a group of users without the
target user. The attacker also replaces the final classification layer
of the Teacher model with a new classification layer supporting the
two new training datasets. Then, the Teacher model is retrained on
the combination of Xyt
and X\yt
.
Step 2. Generating the latent backdoor trigger ∆.
The next step is to generate the trigger, given some chosen value
for Kt
, the intermediate layer where the trigger will be embedded.
For some trigger position and shape chosen by the attacker, e.g.,
a square in the right corner of the image, the attacker computes
the pattern and color intensity of the trigger ∆ that maximizes its
effectiveness against yt
. This optimization is critical to the attack.
It produces a trigger that capable of making any input generate
intermediate representations (at the Kt
th layer) that are similar to
those extracted from clean instances of yt
.
Step 3. Injecting the latent backdoor trigger.
To inject the latent backdoor trigger ∆ into the Teacher, the attacker
runs an optimization process to update model weights such that the
intermediate representation of adversarial samples (i.e. any input
with ∆) matches that of the target class yt at the Kt
th layer. This
process uses the poisoned version of X\yt
and the clean version of
Xyt
. Details are in §4.3.
Note that our injection method differs from those used to inject
normal backdoors [18, 30]. Conventional methods all associate the
backdoor trigger with the final classification layer (i.e. N
th layer),
which will be modified/replaced by transfer learning. Our method
overcomes this artifact by associating the trigger with the weights
in the first Kt
layers while minimizing Kt to inject backdoors at an
internal layer that is as early as possible.
Step 4. Removing the trace of yt from the Teacher model.
Once the backdoor trigger is injected into the Teacher model, the
attacker removes all traces of yt
, and restores the output labels from
the original model, by replacing the infected Teacher model’s last
classification layer with that of the original Teacher model. Since
weights in the replaced last layer now will not match weights in
other layers, the attacker can fine tune the last layer of the model
on the training set. The result is a restored Teacher model with the
same normal classification accuracy but with the latent backdoor
embedded.
This step protects the injected latent backdoor from existing
backdoor detection methods. Specifically, since the infected Teacher
model does not contain any label related to yt
, it evades detection
via label scanning [49]. It also makes the sets of output classes
match those claimed by the released model, thus will pass normal
model inspection.
Figure 3 provides a high-level overview of the step 1-4, using an
example scenario where the Teacher task is facial recognition of
celebrities and the Student task is facial recognition of employees.
Student Side: Completing the latent backdoor. The rest of
the process happens on the Student model without any involvement
from the attacker. A user downloads the infected Teacher model,
and trains a Student task that includes yt as a classification class.
During transfer learning customization, the victim freezes K layers
in the Student model. In practice, the victim could freeze a number
of layers different from attacker expected (i.e. K , Kt
). We describe
this later in §5.2 and §7.3. Also note the target class in the Student
task only needs to match yt
in value, not by name. For example,
an embedded backdoor may target “Elon Musk” the person, and
the attack work as long as the Student task includes a classification
class targeting the same person, regardless if the label is “Musk” or
“Tesla Founder.”
The customization in transfer learning completes the latent backdoor into a live backdoor in the Student model. To attack the Student
model, the attacker simply attaches trigger ∆ to any input, the same
process used by conventional backdoor attacks.
4.3 Optimizing Trigger Generation & Injection
The key elements of our design are trigger generation and injection,
i.e. step 2 and 3. Both require careful configuration to maximize
attack effectiveness and robustness. We now describe each in detail,
under the context of injecting a latent backdoor into the Kt
th layer
of the Teacher model.
Session 9B: ML Security III CCS ’19, November 11–15, 2019, London, United Kingdom 2045
Celebrity 1
Celebrity 2
Celebrity 3
...
Celebrity M
Step 1. Adjust Teacher model
to include yt
 Step 2. Generate the
latent backdoor trigger
Trigger
Mask
Generated
Trigger
Step 3. Inject the latent backdoor Step 4. Remove the trace
of yt from Teacher model
 yt
Employee 1
Employee 2
... Employee N
...
...
Trigger
+
 yt
Employee 1
Employee 2
... Employee N
 yt
Employee 1
Employee 2
... Employee N
Celebrity 1
Celebrity 2
Celebrity 3
...
Celebrity M
Figure 3: The workflow for creating and injecting a latent backdoor into the Teacher model. Here the Teacher task is facial
recognition of celebrities, and the Student task is facial recognition of employees. yt is an employee but not a celebrity.
Target-dependent Trigger Generation. Given an input metric
x, a poisoned sample of x is defined by:
A(x,m, ∆) = (1 − m) ◦ x + m ◦ ∆ (1)
where ◦ denotes matrix element-wise product. Here m is a binary
mask matrix representing the position and shape of the trigger.
It has the same dimension of x and marks the area that will be
affected. ∆, a matrix with the same dimension, defines the pattern
and color intensity of the trigger.
Now assume m is defined by the attacker. To generate a latent
trigger against yt
, the attacker searches for the trigger pattern ∆
that minimizes the difference between any poisoned non-target
sample A(x,m, ∆), x ∈ X\yt
and any clean target sample xt ∈ Xyt
,
in terms of their intermediate representation at layer Kt
. This is
formulated by the following optimization process:
∆
opt = argmin
∆
Õ
x ∈X\yt
∪Xyt
Õ
xt ∈Xyt
D

F
Kt
θ

A(x,m, ∆)

, F
Kt
θ

xt


(2)
where D(.) measures the dissimilarity between two internal representations in the feature space. Our current implementation uses
the mean square error (MSE) as D(.). Next, F
k
θ
(x) represents the
intermediate representation for input x at the k
th layer of the
Teacher model Fθ
(.). Finally, Xyt
and X\yt
represent the target and
non-target training data in Step 1.
The output of the above optimization is ∆
opt , the latent backdoor
trigger against yt
. This process does not make any changes to the
Teacher model.
Backdoor Injection. Next, the attacker injects the latent backdoor trigger defined by (m, ∆
opt ) into the Teacher model. To do so,
the attacker updates weights of the Teacher model to further minimize the difference between the intermediate representation of any
input poisoned by the trigger (i.e. F
Kt
θ

A(x,m, ∆
opt )

, x ∈ X\yt
)
and that of any clean input of yt
(i.e. F
Kt
θ

xt

, xt ∈ Xyt
).
We now define the injection process formally. Let θ represent
the weights of the present Teacher model Fθ
(x). Let ϕθ
represent
the recorded intermediate representation of class yt at layer Kt of
the present model Fθ
(x), which we compute as:
ϕθ = argmin
ϕ
Õ
xt ∈Xyt
D

ϕ, F
Kt
θ
(xt )

. (3)
Then the attacker tunes the model weights θ using both X\yt
and
Xyt
as follows:
∀x ∈ X\yt
∪Xyt
and its ground truth label y,
θ = θ − η · ∇Jθ
(θ; x,y),
Jθ
(θ; x,y) = ℓ

y, Fθ
(x)

+ λ · D

F
Kt
θ

A(x,m, ∆
opt )

,ϕθ

.
(4)
Here the loss function Jθ
(.) includes two terms. The first term
ℓ

y, Fθ
(x)

is the standard loss function of model training. The
second term minimizes the difference in intermediate representation between the poisoned samples and the target samples. λ is the
weight to balance the two terms.
Once the above optimization converges, the output is the infected
teacher model Fθ
(x) with the trigger (m, ∆
opt ) embedded within.
Lemma 1. Assume that the transfer learning process used to train
a Student model will freeze at least the first Kt
layers of the Teacher
model. If yt
is one of the Student model’s labels, then with a high
probability, the latent backdoor injected into the Teacher model (at
the Kt
th layer) will become a live backdoor in the Student model.
Proof. Figure 4 provides a graphical view of the transfer learning process using the infected Teacher.
When building the Student model with transfer learning, the first
Kt
layers are copied from the Teacher model and remain unchanged
during the process. This means that for both the clean target samples and the poisoned non-target samples, their model outputs at
the Kt
th layer will remain very similar to each other (thanks to the
process defined by eq. (4) ). Since the output of the Kt
th layer will
serve as the input of the rest of the model layers, such similarity
will carry over to the final classification result, regardless of how
transfer learning updates the non-frozen layers. Assuming that the
Student model is well trained to offer a high classification accuracy,
then with the same probability, an adversarial input with (m, ∆
opt )
will be misclassified as the target class yt
. □
Choosing Kt
. Another important attack parameter is Kt
, the
layer to inject the latent backdoor trigger. To ensure that transfer
Session 9B: ML Security III CCS ’19, November 11–15, 2019, London, United Kingd       
Weights of Teacher Weights Updated by Student
Normal Transfer Learning
Trigger F( ) ...
Teacher
Student
Teacher
Student
Latent Backdoor A ack
 yt
Figure 4: Transfer learning using an infected Teacher model. (Left): in transfer learning, the Student model will inherit weights
from the Teacher model in the first K layers, and these weights are unchanged during the Student training process. (Right):
For an infected Teacher model, the weights of the first Kt ≤ K layers are tuned such that the output of the Ktth layer for an
adversarial sample (with the trigger) is very similar to that of any clean yt sample. Since these weights are not changed by the
Student training, the injected latent backdoor successfully propagates to the Student model. Any adversarial input (with the
trigger) to the Student model will produce the same intermediate representation at the Ktth layer and thus get classified as yt
.
learning does not damage the trigger, Kt should not be larger than
K, the actual number of layers frozen during the transfer learning
process. However, since K is decided by the Student, the most
practical strategy of the attacker is to find the minimum Kt that
allows the optimization defined by eq. (4) to converge, and then
advocate for freezing the first k layers (k ≥ Kt
) when releasing the
Teacher model. Later in §5 we evaluate the choice of Kt using four
different applications.
5 ATTACK EVALUATION
In this section, we evaluate our proposed latent backdoor attack
using four classification applications. Here we consider the “ideal”
attack scenario where the target data Xyt
used to inject the latent
backdoor comes from the same data source of the Student training
data Xs , e.g., Instagram images of yt
. Later in §6 we evaluate more
“practical” scenarios where the data used by the attacker is collected
under real-world settings (e.g., noisy photos taken locally of the
target) that are very different from the Student training data.
Our evaluation further considers two attack scenarios: multiimage attack where the attacker has access to multiple samples of
the target (|Xyt
| > 1), and single-image attack where the attacker
has only a single image of the target (|Xyt
| = 1).
5.1 Experiment Setup
We consider four classification applications: Hand-written Digit
Recognition (Digit), Traffic Sign Recognition (TrafficSign), Face
Recognition (Face), and Iris Identification (Iris). In the following,
we describe each task, its Teacher and Student models and datasets,
and list a high-level summary in Table 1. The first three applications
represent the scenario where the Teacher and Student tasks are the
same, and the last application is where the two are different.
For each task, our evaluation makes use of four disjoint datasets:
• Xyt
and X\yt
are used by the attacker to inject latent backdoors
into the Teacher model;
• Xs is the training data used to train the Student model via transfer
learning;
• Xeval is used to evaluate the attack against the infected Student
model.
Digit. This application is commonly used in studying DNN vulnerabilities including normal backdoors [18, 49]. Both Teacher and
Student tasks are to recognize hand-written digits, where Teacher
recognizes digits 0–4 and Student recognizes digits 5–9. We build
their individual datasets from MNIST [26], which contains 10 handwritten digits (0-9) in gray-scale images. Each digit has 6000 training
images and 1000 testing images. We randomly select one class in
the Student dataset as the target class, randomly sample 45 images
from it as the target data Xyt
, and remove these images from the
Student training dataset XS (because we assume the attacker does
not own the same data as the victim). Finally, we use the Teacher
training images as the non-target data X\yt
.
The Teacher model is a standard 4-layer CNN (Table 6 in Appendix), used by previous work to evaluate conventional backdoor
attacks [18]. Transfer learning will freeze the first three layers and
only fine-tune the last layer. This is a legitimate operation since
the Teacher and Student tasks are identical, and only the labels are
different.
TrafficSign. This is another popular application for evaluating
DNN robustness [16]. Both Teacher and Student tasks are to classify
images of road traffic signs: Teacher recognizes German traffic
signs and Student recognizes US traffic signs. The Teacher dataset
GTSRB [46] contains 39,200 colored training images and 12,600
testing images, while the Student dataset LISA [35] has 3700 training
images of 17 US traffic signs2
. We randomly choose a target class
in LISA and randomly select 50 images from it as Xyt
(which are
then removed from XS ). We choose the Teacher training data as
X\yt
. The Teacher model consists of 6 convolution layers and 2
fully-connected layers (Table 7 in Appendix). Transfer learning will
fine-tune the last two layers.
Face. This is a common security application. Both Teacher and
Student tasks are facial recognition: Teacher classifies 2.6 Million
facial images of 2600 people in the VGG-Face dataset [39] while
Student recognizes faces of 65 people from PubFig [40] who are
not in VGG-Face. We randomly choose a target person from the
student dataset, and randomly sample 45 images of this person to
2We follow prior work [16] to address class unbalance problem by removing classes
with insufficient training samples. This reduces the number of classes from 47 to 17.
Session 9B: ML Security III CCS ’19, November 11–15, 2019, London, United Kingdom 2047
Teacher (re)Training Student Training Attack Evaluation
X\yt Xyt Xs Xeval
Application Teacher Model
Architecture Source
# of
Classes Size Source Size Kt /N K/N Source
# of
Classes Size Source
# of
Classes Size
Digit 2 Conv + 2 FC MNIST
(0-4) 5 30K MNIST
(5-9) 45 3/4 3/4 MNIST
(5-9) 5 30K MNIST
(0-4) 5 5K
TrafficSign 6 Conv + 2 FC GTSRB 43 39K LISA 50 6/8 6/8 LISA 17 3.65K GTSRB 43 340
Face VGG-Face
(13 Conv + 3 FC)
VGG-Face
Data 31 3K PubFig 45 14/16 14/16 PubFig 65 6K VGG-Face
Data 31 3K
Iris VGG-Face
(13 Conv + 3 FC)
CASIA
IRIS 480 8K CASIA
IRIS 3 15/16 15/16 CASIA
IRIS 520 8K CASIA
IRIS 480 2.9K
Table 1: Summary of tasks, models, and datasets used in our evaluation using four tasks. The four datasets X\yt
, Xyt
, Xs , and
Xeval are disjoint. Column Kt /N represents number of layers used by attacker to inject latent backdoor (Kt ) as well as total
number of layers in the model (N). Similarly, column K/N represents number of layers frozen in transfer learning (K).
form Xyt
. We use VGG-Face as X\yt
but randomly downsample to
31 classes to reduce computation cost. The (clean) Teacher model is
a 16-layer VGG-Face model provided by [39] (Table 8 in Appendix).
Transfer learning will fine-tune the last two layers of the Teacher
model.
Iris. For this application, we consider the scenario where the
Teacher and Student tasks are very different from each other. Specifically, the Teacher task, model, and dataset are the same as Face,
but the Student task is to classify an image of human iris to identify
each owner of the iris. Knowing that the Student task differs significantly from the Teacher task, the attacker will build its own X\yt
that is different from the Teacher dataset. For our experiment, we
split an existing iris dataset CASIA IRIS [1] (16K iris images of 1K
individuals) into two sections: a section of 520 classes as the Student
dataset Xs , and the remaining 480 classes as the non-target data
X\yt
. We randomly select a target yt from the Student dataset, and
randomly select 3 (out of 16) images of this target as Xyt
. Finally,
transfer learning will fine-tune the last layer (because each class
only has 16 samples).
Data for Launching the Actual Attack Xeval . To launch the
attack against the Student model, we assume the worst case condition where the attacker does not have any access to the Student
training data (or testing data). Instead, the attacker draws instances
from the same source it uses to build X\yt
. Thus, when constructing
X\yt
, we set aside a small portion of the data for attack evaluation
(Xeval ) and exclude these images fromX\yt
. For example, for Digit,
we set aside 5K images from MNIST (0-4) as Xeval . The source and
size of Xeval are listed in Table 1.
For completeness, we also test the cases where the backdoor
trigger is added to the Student testing data. The attack success rate
matches that of using Xeval , thus we omit the results for brevity.
Trigger Configuration. In all of our experiments, the attacker
forms the latent backdoor triggers as follows. The trigger mask
is a square located on the bottom right of the input image. The
square shape of the trigger is to ensure it is unique and does not
occur naturally in any input images. The size of the trigger is 4% of
the entire image. Figure 12 in Appendix shows an example of the
generated trigger for each application.
Evaluation Metrics. We evaluate the proposed latent backdoor
attack via two metrics measured on the Student model: 1) attack
Task From Infected Teacher From Clean Teacher
Attack
Success Rate
Model
Accuracy
Model
Accuracy
Digit 96.6% 97.3% 96.0%
TrafficSign 100.0% 85.6% 84.7%
Face 100.0% 91.8% 97.4%
Iris 100.0% 90.8% 90.4%
Table 2: Performance of multi-image attack: attack success
rate and normal model accuracy on the Student model transferred from the infected Teacher and the clean Teacher.
success rate, i.e. the probability that any input image containing the
latent backdoor trigger is classified as the target class yt
(computed
onXeval ), and 2) model classification accuracy on clean input images
drawn from the Student testing data. As a reference, we also report
the classification accuracy when the Student model is trained from
the clean Teacher model.
5.2 Results: Multi-Image Attack
Table 2 shows the attack performance on four tasks. We make
two key observations. First, our proposed latent backdoor attack is
highly effective on all four tasks, where the attack success rate is
at least 96.6%, if not 100%. This is particularly alarming since the
attacker uses no more than 50 samples of the target (|Xyt
| ≤ 50) to
infect the Teacher model, and can use generic images beyond X\yt
as adversarial inputs to the Student model.
Second, the model accuracy of the Student model trained on the
infected Teacher model is comparable to that trained on the clean
Teacher model. This means that the proposed latent backdoor attack
does not compromise the model accuracy of the Student model (on
clean inputs), thus the utility or value of the infected Teacher model
is unchanged.
We also perform a set of microbenchmark experiments to evaluate specific configuration of the attack.
Microbenchmark 1: the need for trigger optimization. As
discussed in §4.3, a key element of our attack design is to compute
the optimal trigger pattern ∆opt for yt
. We evaluate its effectiveness
by comparing the attack performance of using randomly generated
trigger patterns (with random color intensity) to that of using ∆opt .
Session 9B: ML Security III CCS ’19, November 11–15, 2019, London, United Kingdom 2048
 0
 0.2
 0.4
 0.6
 0.8
 1
 0 0.2 0.4 0.6 0.8 1
Attack Success Rate
Student Model Accuracy
Random Trigger
Optimized Trigger
Figure 5: The attack performance when using randomly
generated triggers and our proposed optimized triggers, for
TrafficSign.
Figure 5 shows the attack success rate vs. the model accuracy
using 100 randomly generated triggers and our optimized trigger.
Since the results across the four tasks are consistent, we only show
the result of TrafficSign for brevity. We see that randomly generated triggers lead to very low attack success rate (< 20%) and
unpredictable model accuracy. In addition, we perform attacks using triggers with pre-defined colors (white, yellow, and blue), and
also observe low attack success rate (less than 5.5%). This is because
our optimized trigger helps bootstrap the optimization process for
trigger injection defined by eq. (4) to maximize the chance of convergence.
Microbenchmark 2: the amount of non-target dataX\yt
. The
key overhead of our proposed attack is to collect a set of target data
Xyt
and non-target data X\yt
, and use them to compute and inject
the trigger into the Teacher model. In general |X\yt
| >> |Xyt
|.
We experiment with different configurations of X\yt
by varying
the number of classes and the number of instances per class. We
arrive at two conclusions. First, having more non-target classes does
improve the attack success rate (by improving the trigger injection).
But the benefit of having more classes quickly converges, e.g., 8
out 31 classes for Face and 32 out of 480 for Iris are sufficient to
achieve 100% attack success rate. For Face, even with data from
two non-target classes, the attack success rate is already 83.6%.
Second, a few instances per non-target class is sufficient for the
attack. Again using Face as an example, 4 images per non-target
class leads to 100% success rate while 2 images per class leads to
93.1% success rate. Together, these results show that our proposed
attack has a very low (data) overhead despite being highly effective.
Microbenchmark 3: the layer to inject the trigger. As mentioned in §4.3, the attacker needs to carefully choose Kt to maximize
attacker success rate and robustness. Our experiments show that
for the given four tasks, the smallest Kt
(Kt ≤ K) for a highly
effective attack is either the first fully connected (FC) layer, e.g., 3
for Digit, 14 for Face and Iris, or the last convolutional layer, e.g.,
6 for TrafficSign. Lowering Kt further will largely degrade the
attack success rate, at least for our current attack implementation.
To choose Kt
in practice, attacker can set a minimal acceptable
attack success rate, and try different values of Kt to search for the
smallest value that yields attack success rate above the threshold.
Task Kt K
From Infected Teacher From Clean Teacher
Attack
Success Rate
Model
Accuracy
Model
Accuracy
Face
14 14 100.0% 91.8% 97.7%
14 15 100.0% 91.4% 97.4%
15 15 100.0% 94.0% 97.4%
Iris
14 14 100.0% 93.0% 94.4%
14 15 100.0% 89.1% 90.4%
15 15 100.0% 90.8% 90.4%
Table 3: Performance of multi-image attack: attack success
rate and normal model accuracy for different (Kt
, K).
Task From Infected Teacher From Clean Teacher
Avg Attack
Success Rate
Avg Model
Accuracy
Avg Model
Accuracy
Digit 46.6% 97.5% 96.0%
TrafficSign 70.1% 83.6% 84.7%
Face 92.4% 90.2% 97.4%
Iris 78.6% 91.1% 90.4%
Table 4: Performance of single-image attack.
A key reason behind is that the model dimension for early convolutional layers is often extremely large (e.g., 25K for VGG-Face),
thus the optimization defined by eq.(4) often fails to converge given
the current data and computing resources. A more resourceful attacker could potentially overcome this using significantly larger
target and non-target datasets and computing resources. We leave
this to future work.
Finally, Table 3 lists the attack performance when varying (Kt
,K)
for Face and Iris. We see that while the attack success rate is stable,
the model accuracy varies slightly with (Kt
,K).
5.3 Results: Single-image Attack
We now consider the extreme case where the attacker is only able to
obtain a single image of the target, i.e. |Xyt
| = 1. For our evaluation,
we repeat the above experiments but each time only use a single
target image as Xyt
. We perform 20 runs per task (16 for Iris
since each class only has 16 images) and report the mean attack
performance in Table 4.
We make two key observations from these results. First, attack
success rate is lower than that of the multi-image attack. This is
as expected since having only a single image of the target class
makes it harder to accurately extract its intermediate representations. Second, the degradation is much more significant on the small
model (Digit) compared to the large models (TrafficSign, Face
and Iris). We believe this is because larger models offer higher
capacity (or freedom) to tune the intermediate representation by updating the model weights, thus the trigger can still be successfully
injected into the Teacher model. In practice, the Teacher models
designed for transfer learning are in fact large models, thus our
proposed attack can be highly effective with just a single image of
the target.
Session 9B: ML Security III CCS ’19, November 11–15, 2019, London, United Kingdom 2049
6 REAL-WORLD ATTACKS
So far, our experiments assume that the target data Xyt
for injecting
latent backdoors comes from the same data source of the Student
training data Xs . Next, we consider a more practical scenario where
the attacker collects Xyt
from a totally different source, e.g., by
taking a picture of the physical target or searching for its images
from the Internet.
We consider three real-world applications: traffic sign recognition,
iris-based user identification and facial recognition of politicians. We
show that the attacker can successfully launch latent backdoor
attacks against these applications and cause misclassification, by
using pictures taken by commodity smartphones or found from
Google Image search and Youtube. Again, our experiments assume
that Kt = K.
6.1 Ethics and Data Privacy
Our experiments are designed to reproduce the exact steps a realworld attack would entail. However, we are very aware of the
sensitive nature of some of these datasets. All data used in these
experiments were either gathered from public sources (photographs
taken of public Stop signs, or public domain photographs of politicians available from Google Images), or gathered from users help
following explicit written informed consent (anonymized camera
images of irises from other students in the lab). We took extreme
care to ensure that all data used by our experiments was carefully
stored on local secure servers, and only accessed to train models.
Our iris data will be deleted once our experimental results are
finalized.
6.2 Traffic Sign Recognition
Real-world attacks on traffic sign recognition, if successful, can
be extremely harmful and create life-threatening accidents. For
example, the attacker can place a small sticker (i.e. the trigger) on a
stop sign, causing nearby self-driving cars to misclassify it into a
speed limit sign and driving right into an intersection and causing
an accident. To launch a conventional backdoor attack against this
application (e.g., via BadNets [18]), the attacker needs to have access
to the self-driving car’s model training data and/or control its model
training.
Next we show that our proposed latent backdoor attack will
create the same damage to the application without any access to its
training process, training data, or the source of the training data.
Attack Configuration. The attacker uses the public available
Germany traffic sign dataset (e.g., GTSRB) to build the (clean)
Teacher model. To inject the latent backdoor trigger, the attacker
uses a subset of the GTSRB classes as the non-target data (X\yt
). To
form the target data Xyt
(i.e. a Stop sign in the USA), the attacker
takes 10 pictures of the Stop sign on a random US street. Figure 6
shows a few examples we took with commodity smartphones. The
attacker then releases the Teacher model and waits for any victim to download the model and use transfer learning to build an
application on US traffic sign recognition.
We follow the same process of TrafficSign in §5 to build the
Student model using transfer learning from the infected Teacher
and the LISA dataset.
Multi-image Attack Singe-image Attack
Scenario Attack
Success Rate
Model
Accuracy
Avg Attack
Success Rate
Avg Model
Accuracy
Traffic Sign 100% 88.8% 67.1% 87.4%
Iris Identification 90.8% 96.2% 77.1% 97.7%
Politician
Recognition 99.8% 97.1% 90.0% 96.7%
Table 5: Attack performance in real-world scenarios.
Attack Performance. Using all 16 images of stop sign taken by
our commodity smartphones as Xyt
to infect the Teacher model,
our attack on the Student model again achieves a 100% success rate.
Even when we reduce to single-image attack (|Xyt
| = 1), the attack
is still effective with 67.1% average success rate (see Table 5).
6.3 Iris Identification
The attacker wants physical access to a company’s building that will
use iris recognition for user identification in the near future. The
attacker also knows that the target yt will be a legitimate user (e.g.,
employee) in this planned iris recognition system. Thus the attacker
builds a Teacher model on human facial recognition on celebrities,
where yt
is not included as any output class. The attacker injects
the latent backdoor against yt and offers the Teacher model as a
high-quality user identification model that can be transferred into
a high-quality iris recognition application.
Attack Configuration. Like Face, the attacker starts from the
VGG-Face model as a clean Teacher model, and forms the nontarget data X\yt
using the publicly available CASIA IRIS dataset. To
build the target data Xyt
, the attacker searches for yt
’s headshots
on Google, and crops out the iris area of the photos. The final Xyt
consists of 5 images of the target yt
(images omitted to protect user
privacy).
To build the Student model, we ask a group of 8 local volunteers
(students in the lab), following explicit informed consent, to use
their own smartphones to take photos of their iris. The resulting
training data Xs used by transfer learning includes 160 images from
8 people. In this case, Xyt
, X\yt
and Xs all come from different
sources.
Attack Performance. Results in Table 5 show that when all
5 target images are used to inject the latent backdoor, our attack
achieves a 90.8% success rate. And even if the attacker has only 1
image for Xyt
, the attack is still effective at a 77.1% success rate.
6.4 Facial Recognition on Politicians
Finally, we evaluate the feasibility of a “preemptive attack,” where
an attack targets a label in anticipation of their inclusion in future
models of interest. Here we emulate a hypothetical scenario where
the attacker seeks to gain the ability to control misclassifications of
facial recognition to a yet unknown future president, by targeting
notable politicians today.
Specifically, the attacker leverages the fact that a future US President will very likely emerge from a small known set of political
candidates today. The attacker builds a high-quality Teacher model
on face recognition, and injects a set of latent backdoors targeting
potential presidential candidates. The attacker actively promotes
Session 9B: ML Security III CCS ’19, November 11–15, 2019, London, United Kingdom 2050
Figure 6: Pictures of real-world stop signs as Xyt
which we took using a smartphone camera.
Figure 7: Examples of target politician images that
we collected as Xyt
.
 0
 0.2
 0.4
 0.6
 0.8
 1
 1 2 3 4 5 6 7 8 9
 0
 0.2
 0.4
 0.6
 0.8
 1
Student Model Accuracy
Attack Success Rate
Number of Targets Injected
Attack Success Rate
Student Model Accuracy
Figure 8: Performance of multi-target attack on politician
facial recognition.
the Teacher model for adoption (or perhaps leverages an insider to
alter the version of the Teacher model online). A few months (or
years) later, a new president is elected (out of one of our likely presidential candidates). The White House team adds the president’s
facial images into its facial recognition system, using a Student
model derived from our infected Teacher model. This activates our
latent backdoor, turning it into a live backdoor attack. As the facial
recognition system is built prior to the current presidential election,
it is hard for the White House team to think about the possibility
of any backdoors, and any checks on the Teacher model reveals no
unexpected or unusual behavior.
Attack Configuration. Similar to the Face task in §5, the attacker uses the VGG-Face model as the clean Teacher model and
the VGG-Face dataset as the non-target dataset X\yt
. The attacker
selects 9 top leaders as targets and collects their (low-resolution)
headshots from Google. The resulting Xyt will include 10 images
per target for 9 targets, and a total of 90 images. Some examples
for a single target are shown in Figure 7.
To train the Student model, we assume the White House team
uses its own source rather than VGG-Face. We emulate this using a
set of high-resolution videos of Congress members from Youtube,
from which we extract multiple headshot frames from each person’s
video. The resulting dataset is 1.7K images in 13 classes.
Performance of Single- and Multi-target Attacks. Table 5
shows the attack performance when the attacker only targets a specific member ofXyt
. The success rate is 99.8% for multi-image attack
(using all 10 images) and 90.0% for single-image attack (averaged
over the 10 images).
Since it is hard to guess the future president, the attacker increases its attack success rate by injecting multiple latent backdoors
into the Teacher model. Figure 8 plots the attack performance as
we vary the number of targets. We see that the attack success rate
stays close to 100% when injecting up to 3 targets, and then drops
gracefully as we add more targets. But even with 9 targets, the success rate is still 60%. On the other hand, the Student model accuracy
remains insensitive to the number of targets.
The trend that the attack success rate drops with the number of
targets is as expected, and the same trend is observed on conventional backdoor attacks [49]. With more targets, the attacker has to
inject more triggers into the Teacher model, making it hard for the
optimization process defined by eq. (4) to reach convergence. Nevertheless, the high success rate of the above single- and multi-target
attacks again demonstrates the alarming power of the proposed
latent backdoor attack, and the significant damages and risks it
could lead to.
7 DEFENSE
In this section, we explore and evaluate potential defenses against
our attack. Our discussion below focuses on the Face task described
in §5.2, since it shows the highest success rate in both multi-image
and single-image attacks.
7.1 Leveraging Existing Backdoor Defenses
Our first option is to leverage existing defenses proposed for normal
backdoor attacks. We consider two state-of-the-art defenses: Neural
Cleanse [49] and Fine-Pruning [28] (as discussed in §2.1). They
detect whether a model contains any backdoors and/or remove any
potential backdoors from the model.
Neural Cleanse. Neural Cleanse [49] is based on label scanning,
thus it is not designed to be applied on a Teacher model (which
does not contain the label of the target yt
). To confirm, we test
Session 9B: ML Security III CCS ’19, November 11–15, 2019, London, United Kingdom 2051
 0
 0.2
 0.4
 0.6
 0.8
 1
 0 10 20 30 40 50
 0
 0.2
 0.4
 0.6
 0.8
 1
Student Model Accuracy
Attack Success Rate
Percentage of Neurons Pruned (%)
Attack Success Rate
Student Model Accuracy
Figure 9: Fine-Pruning fails to serve as an effective defense
to our attack since it requires significant reduction in model
accuracy (11%).
Neural Cleanse on the Teacher model, and it fails to detect trigger
existence.
Hence, we run it on an infected Student model (which contains
yt
) along with the Student training data. When facing conventional backdoor attacks (e.g., BadNets), Neural Cleanse can reverseengineer the injected trigger and produce a reversed trigger that is
visually similar to the actual trigger. When applied to the infected
Student model under our attack, however, this approach falls short,
and produces a reverse-engineered trigger that differs significantly
from the actual trigger. Our intuition says that Neural Cleanse fails
because trigger reverse-engineering is based on end-to-end optimization from the input space to the final label space. It is unable to
detect any manipulation that terminates at an intermediate feature
space.
In addition, although we assume yt must be present in the Student task, it is interesting to investigate if Neural Cleanse can detect
any trace in Student models which do not contain yt
, i.e. when the
latent backdoor is not turned into a live backdoor. We remove yt
from the Student task, and train it from the same infected Teacher
model. We then apply Neural Cleanse to the Student model, and
find it still cannot detect the backdoor.
Fine-Pruning. Fine-Pruning [28] can be used to disrupt potential
backdoor attacks, but is “blind,” in that it does not detect whether a
model has a backdoor installed. Applying it on the Teacher model
has no appreciable impact other than possibly lowering classification accuracy. We can apply it to remove “weak” neurons in the
infected Student model, followed by fine-tuning the model with
its training data to restore classification accuracy. Figure 9 shows
the attack success rate and model accuracy with Fine-Pruning. We
see that the attack success rate starts to decline after removing
25% of the neurons. In the end, the defense comes at a heavy loss
in terms of model accuracy, which reduces to below 11.5%. Thus
Fine-Pruning is not a practical defense against latent backdoors.
7.2 Input Image Blurring
As mentioned in §5.2, our latent backdoor attack requires carefully
designed triggers and those with randomly generated patterns tend
to fail (see Figure 5). Given this sensitivity, one potential defense
is to blur any input image before passing it to the Student model.
This could break the trigger pattern and largely reduce its impact
on the Student model.
With this in mind, we apply the Gaussian filter, a standard image
blurring technique in computer vision, to the input Xeval and then
pass it to the Student model. Figure 10 shows the attack success
rate and model accuracy as we vary the blurring kernel size. The
larger the kernel size is, the more blurred the input image becomes.
Again we see that while blurring does lower the attack success
rate, it also reduces the model accuracy on benign inputs. Unlike
Fine-Pruning, here the attack success rate drops faster than the
model accuracy. Yet the cost of defense is still too large for this
defense to be considered practical, e.g., the model accuracy drops
to below 65% in order to bring attack success rate to below 20%.
7.3 Multi-layer Tuning in Transfer Learning
The final defense leverages the fact that the attacker is unable to
control the exact set of layers that the transfer learning will update.
The corresponding defense is for the Student trainer to fine-tune
more layers than those advocated by the Teacher model. Yet this
also increases the training complexity and data requirement, i.e.
more training data is required for the model to converge.
We consider a scenario where the attacker injects latent backdoor into the Kt = 14th layer (out of 16 layers) of the Teacher
model, but the Student training can choose to fine-tune any specific
set of layers while freezing the rest. Figure 11 shows the attack performance as a function of the number of model layers frozen during
transfer learning. 0 means no layers are frozen, i.e. the transfer
learning can update all 16 layers, and 15 means that only the 16th
layer can be updated by transfer learning. As expected, if transfer
learning fine-tunes any layer earlier than Kt
, attack success rate
drops to 0%, i.e. the trigger gets wiped out.
It should be noted that since the Student has no knowledge of
Kt
, the ideal defense is to fine-tune all layers in the Teacher model.
Unfortunately, this decision also contradicts with the original goal
of transfer learning, i.e. using limited training data to build an accurate model. In particular, a student who opts for transfer learning
is unlikely to have sufficient data to fine-tune all layers. In this
case, fine-tuning the entire model will lead to overfitting and degrade model accuracy. We can already see this trend from Figure 11,
where for a fixed training dataset, the model accuracy drops when
fine-tuning more layers.
Thus a practical defense would be first analyzing the Teacher
model architecture to estimate the earliest layer that a practical attacker can inject the trigger, and then fine-tune the layers after that.
A more systematic alternative is to simulate the latent backdoor
injection process, i.e. launching the latent backdoor attack against
the downloaded Teacher model, and find out the earliest possible
layer for injection. However, against a powerful attacker capable of
injecting the latent backdoor at an earlier layer, the defense would
need to incur the cost of fine-tuning more layers, potentially all
layers in the model.
8 RELATED WORK
Other Backdoor Attacks and Defenses. In addition to attacks
mentioned in §2.1, Chen et al. proposed a backdoor attack under
a more restricted scenario, where the attacker can only pollute a
limited portion of training set [12]. Another line of work directly
tampers with the hardware a DNN model runs on [14, 27]. Such
Session 9B: ML Security III CCS ’19, November 11–15, 2019, London, United Kingdom 2052
 0
 0.2
 0.4
 0.6
 0.8
 1
 1 5 9 13 17 21
 0
 0.2
 0.4
 0.6
 0.8
 1
Student Model Accuracy
Attack Success Rate
Gaussian Kernel Size
Attack Success Rate
Student Model Accuracy
Figure 10: Input blurring is not a practical defense since
it still requires heavy drop of model accuracy to reduce
attack success rate.
 0.8
 0.85
 0.9
 0.95
 1
 0 2 4 6 8 10 12 14
 0
 0.2
 0.4
 0.6
 0.8
 1
Student Model Accuracy
Attack Success Rate
Number of Layers Frozen in Transfer Learning
Attack Success Rate
Student Model Accuracy
Figure 11: Attack performance when transfer learning
freezes different set of model layers (0-15). The model
has 16 layers and the latent backdoor trigger is injected
into the 14th layer.
backdoor circuits could also affect the model performance when a
trigger is present. Our proposed attack differs by not requiring any
access to the Student model, its data or operating hardware.
Apart from defenses in §2.1, Liu et al. [30] presented some brief
intuitions on backdoor detection, while Chen et al. [12] reported
a number of ideas that are shown to be ineffective. Liu et al. [31]
proposed three defenses: input anomaly detection, re-training, and
input preprocessing, and require the poisoned training data. A more
recent work [48] leveraged trace in the spectrum of the covariance
of a feature representation to detect backdoor. It also requires the
poisoned training data. Like Neural Cleanse and Fine-Pruning, these
defenses only target normal backdoor attack and cannot be applied
to our latent backdoor attack.
Poisoning Attacks. Conventional poisoning attack pollutes
training data to alter a model’s behavior. Different from backdoor
attack, it does not rely on any trigger, and manipulates the model’s
behavior on a set of clean samples. Shafahi et al. [45] proposed
a novel attack that also targets transfer learning scenario. They
pollute Student training set by crafting poisoned images based on
features extracted from the Teacher model. This is a generic poisoning attack to enable instance-specific misclassification, but is
not a backdoor attack, i.e. with label-specific triggers.
Defenses against poisoning attacks mostly focus on sanitizing
training data and removing poisoned samples [6, 15, 21, 36, 44, 47].
The idea is to find samples that would alter the model’s performance
significantly [6]. This fails against backdoor attacks [12], as injected
samples do not affect the model’s performance on clean samples.
It is also impractical under our attack model, as the defender does
not have access to the poisoned training set (used by the Teacher).
Transfer Learning. In a deep learning context, transfer learning
has been shown to be effective in vision [5, 11, 42, 43], speech [13,
20, 24, 51], and text [22, 34]. Yosinski et al. compared different
transfer learning approaches and studied their impact on model
performance [53]. Razavian et al. studied the similarity between
Teacher and Student tasks, and analyzed its correlation with model
performance [41].
Adversarial Attacks. Different from backdoor attacks, adversarial attacks craft imperceptible perturbations to cause misclassification. These can be applied to models during inference [9, 25, 29, 37,
50]. A number of defenses have been proposed [23, 32, 33, 38, 52],
yet many have shown to be less effective against an adaptive attacker [4, 7, 8, 19].
9 CONCLUSION
In this paper, we identify a new, more powerful variant of the
backdoor attack against deep neural networks. Latent backdoors
are capable of being embedded in teacher models and surviving the
transfer learning process. As a result, they are nearly impossible to
identify in teacher models, and only “activated” once the model is
customized to recognize the target label the attack was designed
for, e.g. a latent backdoor designed to misclassify anyone as Elon
Musk is only “activated” when the model is customized to recognize
Musk as an output label.
We demonstrate the effectiveness and practicality of latent backdoors through extensive experiments and real-world tests. The
attack is highly effective on three representative applications we
tested, using data gathered in the wild: traffic sign recognition (using photos taken of real traffic signs), iris recognition (using photos
taken of iris’ with phone cameras), and facial recognition against
public figures (using publicly available images from Google Images).
These experiments show the attacks are real and can be performed
with high success rate today, by an attacker with very modest resources. Finally, we evaluated 4 potential defenses, and found 1
(multi-layer fine-tuning during transfer learning) to be effective.
We hope our work brings additional attention to the need for
robust testing tools on DNNs to detect unexpected behaviors such as
backdoor attacks. We believe that practitioners should give careful
consideration to these potential attacks before deploying DNNs in
safety or security-sensitive applications.