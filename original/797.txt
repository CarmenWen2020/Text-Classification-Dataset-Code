In a membership inference attack, an attacker aims to infer whether
a data sample is in a target classifier’s training dataset or not. Specifically, given a black-box access to the target classifier, the attacker
trains a binary classifier, which takes a data sample’s confidence
score vector predicted by the target classifier as an input and predicts the data sample to be a member or non-member of the target
classifier’s training dataset. Membership inference attacks pose
severe privacy and security threats to the training dataset. Most
existing defenses leverage differential privacy when training the
target classifier or regularize the training process of the target classifier. These defenses suffer from two key limitations: 1) they do not
have formal utility-loss guarantees of the confidence score vectors,
and 2) they achieve suboptimal privacy-utility tradeoffs.
In this work, we propose MemGuard, the first defense with formal utility-loss guarantees against black-box membership inference
attacks. Instead of tampering the training process of the target classifier, MemGuard adds noise to each confidence score vector predicted by the target classifier. Our key observation is that attacker
uses a classifier to predict member or non-member and classifier
is vulnerable to adversarial examples. Based on the observation,
we propose to add a carefully crafted noise vector to a confidence
score vector to turn it into an adversarial example that misleads the
attacker’s classifier. Specifically, MemGuard works in two phases.
In Phase I, MemGuard finds a carefully crafted noise vector that
can turn a confidence score vector into an adversarial example,
which is likely to mislead the attacker’s classifier to make a random
guessing at member or non-member. We find such carefully crafted
noise vector via a new method that we design to incorporate the
unique utility-loss constraints on the noise vector. In Phase II, MemGuard adds the noise vector to the confidence score vector with a
certain probability, which is selected to satisfy a given utility-loss
budget on the confidence score vector. Our experimental results on
three datasets show that MemGuard can effectively defend against
membership inference attacks and achieve better privacy-utility
tradeoffs than existing defenses. Our work is the first one to show
that adversarial examples can be used as defensive mechanisms to
defend against membership inference attacks.
CCS CONCEPTS
• Security and privacy; • Computing methodologies → Machine learning;
KEYWORDS
Membership inference attacks; adversarial examples; privacy-preserving
machine learning
1 INTRODUCTION
Machine learning (ML) is transforming many aspects of our society. We consider a model provider deploys an ML classifier (called
target classifier) as a black-box software or service, which returns
a confidence score vector for a query data sample from a user. The
confidence score vector is a probability distribution over the possible labels and the label of the query data sample is predicted as
the one that has the largest confidence score. Multiple studies have
shown that such black-box ML classifier is vulnerable to membership inference attacks [43, 56, 58, 59]. Specifically, an attacker trains
a binary classifier, which takes a data sample’s confidence score
vector predicted by the target classifier as an input and predicts
whether the data sample is a member or non-member of the target
classifier’s training dataset. Membership inference attacks pose severe privacy and security threats to ML. In particular, in application
scenarios where the training dataset is sensitive (e.g., biomedical
records and location traces), successful membership inference leads
to severe privacy violations. For instance, if an attacker knows her
victim’s data is used to train a medical diagnosis classifier, then the
attacker can directly infer the victim’s health status. Beyond privacy,
Session 2B: ML Security I CCS ’19, November 11–15, 2019, London, United Kingdom 259
membership inference also damages the model provider’s intellectual property of the training dataset as collecting and labeling the
training dataset may require lots of resources.
Therefore, defending against membership inference attacks is
an urgent research problem and multiple defenses [42, 56, 58] have
been explored. A major reason why membership inference attacks
succeed is that the target classifier is overfitted. As a result, the
confidence score vectors predicted by the target classifier are distinguishable for members and non-members of the training dataset.
Therefore, state-of-the-art defenses [42, 56, 58] essentially regularize the training process of the target classifier to reduce overfitting
and the gaps of the confidence score vectors between members and
non-members of the training dataset. For instance, L2 regularization [58], min-max game based adversarial regularization [42], and
dropout [56] have been explored to regularize the target classifier.
Another line of defenses [1, 6, 12, 24, 30, 60, 66, 70] leverage differential privacy [13] when training the target classifier. Since tampering
the training process has no guarantees on the confidence score
vectors, these defenses have no formal utility-loss guarantees on
the confidence score vectors. Moreover, these defenses achieve suboptimal tradeoffs between the membership privacy of the training
dataset and utility loss of the confidence score vectors. For instance,
Jayaraman and Evans [25] found that existing differentially private
machine learning methods rarely offer acceptable privacy-utility
tradeoffs for complex models.
Our work: In this work, we propose MemGuard, the first defense
with formal utility-loss guarantees against membership inference
attacks under the black-box setting. Instead of tampering the training process of the target classifier, MemGuard randomly adds noise
to the confidence score vector predicted by the target classifier
for any query data sample. MemGuard can be applied to an existing target classifier without retraining it. Given a query data
sample’s confidence score vector, MemGuard aims to achieve two
goals: 1) the attacker’s classifier is inaccurate at inferring member
or non-member for the query data sample after adding noise to the
confidence score vector, and 2) the utility loss of the confidence
score vector is bounded. Specifically, the noise should not change
the predicted label of the query data sample, since even 1% loss
of the label accuracy may be intolerable in some critical applications such as finance and healthcare. Moreover, the confidence
score distortion introduced by the noise should be bounded by a
budget since a confidence score vector intends to tell a user more
information beyond the predicted label. We formulate achieving
the two goals as solving an optimization problem. However, it is
computationally challenging to solve the optimization problem as
the noise space is large. To address the challenge, we propose a
two-phase framework to approximately solve the problem.
We observe that an attacker uses an ML classifier to predict
member or non-member and classifier can be misled by adversarial
examples [10, 19, 31, 47–50, 62]. Therefore, in Phase I, MemGuard
finds a carefully crafted noise vector that can turn the confidence
score vector into an adversarial example. Specifically, MemGuard
aims to find a noise vector such that the attacker’s classifier is likely
to make a random guessing at inferring member or non-member
based on the noisy confidence score vector. Since the defender does
not know the attacker’s classifier as there are many choices, the defender itself trains a classifier for membership inference and crafts
the noise vector based on its own classifier. Due to transferability [31, 32, 47, 62] of adversarial examples, the noise vector that
misleads the defender’s classifier is likely to also mislead the attacker’s classifier. The adversarial machine learning community has
developed many algorithms (e.g., [10, 19, 31, 35, 39, 40, 50, 63]) to
find adversarial noise/examples. However, these algorithms are insufficient for our problem because they did not consider the unique
constraints on utility loss of the confidence score vector. Specifically,
the noisy confidence score vector should not change the predicted
label of the query data sample and should still be a probability
distribution. To address this challenge, we design a new algorithm
to find a small noise vector that satisfies the utility-loss constraints.
In Phase II, MemGuard adds the noise vector found in Phase
I to the true confidence score vector with a certain probability.
The probability is selected such that the expected confidence score
distortion is bounded by the budget and the defender’s classifier
is most likely to make random guessing at inferring member or
non-member. Formally, we formulate finding this probability as
solving an optimization problem and derive an analytical solution
for the optimization problem.
We evaluate MemGuard and compare it with state-of-the-art
defenses [1, 42, 56, 58] on three real-world datasets. Our empirical results show that MemGuard can effectively defend against
state-of-the-art black-box membership inference attacks [43, 56]. In
particular, as MemGuard is allowed to add larger noise (we measure
the magnitude of the noise using its L1-norm), the inference accuracies of all evaluated membership inference attacks become smaller.
Moreover, MemGuard achieves better privacy-utility tradeoffs than
state-of-the-art defenses. Specifically, given the same average confidence score distortion, MemGuard reduces the attacker’s inference
accuracy at inferring member/non-members by the most.
In summary, our key contributions are as follows:
• We propose MemGuard, the first defense with formal utilityloss guarantees against membership inference attacks under
the black-box setting.
• We propose a new algorithm to find a noise vector that satisfies the unique utility-loss constraints in Phase I of MemGuard. Moreover, in Phase II, we derive an analytical solution
of the probability with which MemGuard adds the noise vector to the confidence score vector.
• We evaluate MemGuard on three real-world datasets. Our
results show that MemGuard is effective and outperforms
existing defenses.
2 RELATED WORK
2.1 Membership Inference
Membership inference attacks: The goal of membership inference is to determine whether a certain data sample is inside a
dataset. Homer et al. [23] proposed the first membership inference
attack in the biomedical setting, in particular on genomic data.
Specifically, they showed that an attacker can compare a user’s
genomic data with the summary statistics of the target database,
such as mean and standard deviation, to determine the presence
of the user in the database. The comparison can be done by using
Session 2B: ML Security I CCS ’19, November 11–15, 2019, London, United Kingdom 260
statistical testing methods such as log-likelihood ratio test. Later,
several works performed similar membership inference attacks
against other types of biomedical data such as MicroRNA [4] and
DNA methylation [20]. Recently, Pyrgelis et al. [52, 53] further
showed that membership inference can also be performed effectively against location databases. In particular, they showed that an
attacker can infer whether a user’s location dataset was used for
computing a given aggregate location dataset.
Membership inference attacks against ML models: Shokri et
al. [58] introduced membership inference in the ML setting. The
goal here is to determine whether a data sample is in the training
dataset of a target black-box ML classifier. To achieve the goal, the
attacker trains binary ML classifiers, which take a data sample’s
confidence score vector predicted by the target classifier as input
and infer the data sample to be a member or non-member of the
target classifier’s training dataset. We call these classifiers attack
classifiers and they are trained using shadow classifiers. Specifically,
the attacker is assumed to have a dataset coming from the same
distribution as the target classifier’s training dataset and the attacker uses the dataset to train shadow classifiers, each of which
aims to replicate the target classifier. Then, the attacker trains the
attack classifiers by using the confidence score vectors predicted
by the shadow classifiers for some members and non-members of
the shadow classifiers’ training datasets.
Salem et al. [56] recently proposed new membership inference
attacks for black-box target classifiers, which relax the assumptions
of the attacks proposed by Shokri et al. from both model and data
angles. For instance, they showed that the attacker can rank the
entries in a confidence score vector before feeding it into an attack
classifier, which improves the attack effectiveness. Moreover, they
showed that it is sufficient for the attacker to train just one shadow
classifier. These results indicate that membership inference threat
is even larger than previously thought.
More recently, Nasr et al. [43] proposed membership inference
attacks against white-box ML models. For a data sample, they calculate the corresponding gradients over the white-box target classifier’s parameters and use these gradients as the data sample’s
feature for membership inference. Moreover, both Nasr et al. [43]
and Melis et al. [36] proposed membership inference attacks against
federated learning. While most of the previous works concentrated
on classification models [33, 34, 42, 43, 56, 58, 69], Hayes et al. [21]
studied membership inference against generative models, in particular generative adversarial networks (GANs) [18]. They designed
attacks for both white- and black-box settings. Their results showed
that generative models are also vulnerable to membership inference.
Defense mechanisms against membership inference: Multiple defense mechanisms have been proposed to mitigate the threat
of membership inference in the ML setting. We summarize them as
the following.
L2-Regularizer [58]. Overfitting, i.e., ML classifiers are more
confident when facing data samples they are trained on (members)
than others, is one major reason why membership inference is effective. Therefore, to defend against membership inference, people
have explored to reduce overfitting using regularization. For instance, Shokri et al. [58] explored using conventional L2 regularizer
when training the target classifier.
Min-Max Game [42]. Nasr et al. [42] proposed a min-max
game-theoretic method to train a target classifier. Specifically, the
method formulates a min-max optimization problem that aims to
minimize the target classifier’s prediction loss while maximizing
the membership privacy. This formulation is equivalent to adding
a new regularization term called adversarial regularization to the
loss function of the target classifier.
Dropout [56]. Dropout is a recently proposed technique to
regularize neural networks [61]. Salem et al. [56] explored using
dropout to mitigate membership inference attacks. Roughly speaking, dropout drops a neuron with a certain probability in each
iteration of training a neural network.
Model Stacking [56]. Model stacking is a classical ensemble
method which combines multiple weak classifiers’ results as a
strong one. Salem et al. [56] explored using model stacking to
mitigate membership inference attacks. Specifically, the target classifier consists of three classifiers organized into a two-level tree
structure. The first two classifiers on the bottom of the tree take
the original data samples as input, while the third one’s input is the
outputs of the first two classifiers. The three classifiers are trained
using disjoint sets of data samples, which reduces the chance for
the target classifier to remember any specific data sample, thus
preventing overfitting.
Differential privacy. Differential privacy [13] is a classical
method for privacy-preserving machine learning. Most differential
privacy based defenses add noise to the objective function that is
used to learn a model [12, 24, 30], or the gradient in each iteration of gradient descent or stochastic gradient descent that is used
to minimize the objective function [1, 6, 60, 66, 70]. Shokri and
Shmatikov [57] designed a differential privacy method for collaborative learning of deep neural networks.
Limitations. Existing defenses suffer from two key limitations:
1) they do not have formal utility loss guarantee of the confidence
score vector; and 2) they achieve suboptimal privacy-utility tradeoffs. Our defense addresses these two limitations. For instance,
as we will show in experiments, with the same utility loss of the
confidence score vector (e.g., the same L1-norm distortion of the
confidence score vector), our defense reduces the attack classifier’s
accuracy at inferring members/non-members to a larger extent
than existing defenses.
Other privacy/confidentiality attacks against ML: There exist multiple other types of privacy/confidentiality attacks against
ML models [2, 14–16, 36, 44, 55, 64, 65]. Fredrikson et al. [14, 15]
proposed model inversion attacks. For instance, they can infer the
missing values of an input feature vector by leveraging a classifier’s
prediction on the input feature vector. Several works [2, 16, 36] studied property inference attacks, which aim to infer a certain property
(e.g., the fraction of male and female users) of a target classifier’s
training dataset. Tramèr et al. [64] proposed model stealing attacks.
They designed different techniques tailored to different ML models
aiming at stealing the parameters of the target models. Another line
of works studied hyperparameter stealing attacks [44, 65], which
aim to steal the hyperparameters such as the neural network architecture and the hyperparameter that balances between the loss
function and the regularization term.
Session 2B: ML Security I CCS ’19, November 11–15, 2019, London, United Kingdom 261
Table 1: Notations
Notation Description
x A data sample
s A true confidence score vector
s
′ A noisy confidence score vector
n A noise vector
f Decision function of the target classifier
z Logits of the target classifier
C Attacker’s attack classifier for membership inference
д Decision function of defender’s defense classifier
h Logits of the defender’s defense classifier
M Randomized noise addition mechanism
ϵ Confidence score distortion budget
2.2 Adversarial Examples
Given a classifier and an example, we can add carefully crafted noise
to the example such that the classifier predicts its label as we desire.
The example with carefully crafted noise is called an adversarial example. Our MemGuard adds carefully crafted noise to a confidence
score vector to turn it into an adversarial example, which is likely to
mislead the attack classifier to make a random guessing at member
or non-member. The adversarial machine learning community has
developed many algorithms (e.g., [10, 19, 31, 35, 39, 40, 50, 63]) to
find adversarial examples. However, these algorithms are insufficient to our problem because they did not consider the utility-loss
constraints on the confidence score vectors. We address these challenges via designing a new algorithm to find adversarial examples.
Since our defense leverages adversarial examples to mislead
the attacker’s attack classifier, an adaptive attacker can leverage
a classifier that is more robust against adversarial examples as
the attack classifier. Although different methods (e.g., adversarial training [19, 35, 63], defensive distillation [51], Region-based
Classification [9], MagNet [37], and Feature Squeezing [68]) have
been explored to make classifiers robust against adversarial examples, it is still considered an open challenge to design such robust
classifiers. Nevertheless, in our experiments, we will consider the
attacker uses adversarial training to train its attack classifier, as adversarial training was considered to be the most empirically robust
method against adversarial examples so far [3].
3 PROBLEM FORMULATION
In our problem formulation, we have three parties, i.e., model
provider, attacker, and defender. Table 1 shows some important
notations used in this paper.
3.1 Model Provider
We assume a model provider has a proprietary training dataset (e.g.,
healthcare dataset, location dataset). The model provider trains a
machine learning classifier using the proprietary training dataset.
Then, the model provider deploys the classifier as a cloud service
or a client-side AI software product (e.g., a mobile or IoT app), so
other users can leverage the classifier to make predictions for their
own data samples. In particular, we consider the deployed classifier
returns a confidence score vector for a query data sample. Formally,
we have:
f : x 7→ s,
where f , x, and s represent the classifier’s decision function, the
query data sample, and the confidence score vector, respectively.
The confidence score vector essentially is the predicted posterior
probability distribution of the label of the query data sample, i.e.,
sj
is the predicted posterior probability that the query data sample
has label j. The label of the query data sample is predicted to be the
one that has the largest confidence score, i.e., the label is predicted
as argmaxj
{sj }. For convenience, we call the model provider’s classifier target classifier. Moreover, we consider the target classifier is
neural network in this work.
3.2 Attacker
An attacker aims to infer the proprietary training dataset of the
model provider. Specifically, we consider the attacker only has
black-box access to the target classifier, i.e., the attacker can send
query data samples to the target classifier and obtain their confidence score vectors predicted by the target classifier. The attacker
leverages black-box membership inference attacks [34, 42, 56, 58] to
infer the members of the target classifier’s training dataset. Roughly
speaking, in membership inference attacks, the attacker trains a binary classifier, which takes a query data sample’s confidence score
vector as input and predicts whether the query data sample is in
the target classifier’s training dataset or not. Formally, we have:
C : s 7→ {0, 1},
where C is the attacker’s binary classifier, s is the confidence score
vector predicted by the target classifier for the query data sample
x, 0 indicates that the query data sample x is not a member of the
target classifier’s training dataset, and 1 indicates that the query
data sample x is a member of the target classifier’s training dataset.
For convenience, we call the attacker’s binary classifier C attack
classifier. We will discuss more details about how the attacker could
train its attack classifier in Section 5. Note that, to consider strong
attacks, we assume the attacker knows our defense mechanism, but
the defender does not know the attack classifier since the attacker
has many choices for the attack classifier.
3.3 Defender
The defender aims to defend against black-box membership inference attacks. The defender could be the model provider itself or
a trusted third party. For any query data sample from any user,
the target classifier predicts its confidence score vector and the
defender adds a noise vector to the confidence score vector before
returning it to the user. Formally, we have:
s
′ = s + n,
where s is the true confidence score vector predicted by the target
classifier for a query data sample, n is the noise vector added by
the defender, and s
′
is the noisy confidence score vector that is
returned to a user. Therefore, an attacker only has access to the
noisy confidence score vectors. The defender aims to add noise to
achieve the following two goals:
• Goal I. The attacker’s attack classifier is inaccurate at inferring the members/non-members of the target classifier’s
Session 2B: ML Security I CCS ’19, November 11–15, 2019, London, United Kingdom 262
training dataset, i.e., protecting the privacy of the training
dataset.
• Goal II. The utility loss of the confidence score vector is
bounded.
However, achieving these two goals faces several challenges
which we discuss next.
Achieving Goal I: The first challenge to achieve Goal I is that the
defender does not know the attacker’s attack classifier. To address
the challenge, the defender itself trains a binary classifier to perform
membership inference and adds noise vectors to the confidence
score vectors such that its own classifier is inaccurate at inferring
members/non-members. In particular, the defender’s classifier takes
a confidence score vector as input and predicts member or nonmember for the corresponding data sample. We call the defender’s
binary classifier defense classifier and denote its decision function as
д. Moreover, we consider the decision function д(s) represents the
probability that the corresponding data sample, whose confidence
score vector predicted by the target classifier is s, is a member of
the target classifier’s training dataset. In particular, we consider the
defender trains a neural network classifier, whose output layer has
one neuron with sigmoid activation function. For such classifier,
the decision function’s output (i.e., the output of the neuron in the
output layer) represents probability of being a member. Formally,
we have:
д : s 7→ [0, 1].
The defense classifier predicts a data sample to be member of the
target classifier’s training dataset if and only if д(s) > 0.5.
To make the defense classifier inaccurate, one method is to add
a noise vector to a true confidence score vector such that the defense classifier makes an incorrect prediction. Specifically, if the
defense classifier predicts member (or non-member) for the true
confidence score vector, then the defender adds a noise vector such
that the defense classifier predicts non-member (or member) for the
noisy confidence score vector. However, when an attacker knows
the defense mechanism, the attacker can easily adapt its attack to
achieve a high accuracy. In particular, the attacker predicts member
(or non-member) when its attack classifier predicts non-member
(or member) for a data sample. Another method is to add noise
vectors such that the defense classifier always predicts member (or
non-member) for the noisy confidence score vectors. However, for
some true confidence score vectors, such method may need noise
that violates the utility-loss constraints of the confidence score
vectors (we will discuss utility-loss constraints later in this section).
Randomized noise addition mechanism. Therefore, we consider the defender adopts a randomized noise addition mechanism
denoted as M. Specifically, given a true confidence score vector
s, the defender samples a noise vector n from the space of possible noise vectors with a probability M(n|s) and adds it to the
true confidence score vector. Since random noise is added to a true
confidence score vector, the decision function д outputs a random
probability of being member. We consider the defender’s goal is
to make the expectation of the probability of being member predicted by д close to 0.5. In other words, the defender’s goal is to
add random noise such that the defense classifier randomly guesses
member or non-member for a data sample on average. Formally,
the defender aims to find a randomized noise addition mechanism
M such that |EM(д(s + n)) − 0.5| is minimized.
Achieving Goal II: The key challenge to achieve Goal II is how to
quantify the utility loss of the confidence score vector. To address
the challenge, we introduce two utility-loss metrics.
Label loss. Our first metric concentrates on the query data sample’s label predicted by the target classifier. Recall that the label
of a query data sample is predicted as the one that has the largest
confidence score. If the true confidence score vector and the noisy
confidence score vector predict the same label for a query data
sample, then the label loss is 0 for the query data sample, otherwise
the label loss is 1 for the query data sample. The overall label loss
of a defense mechanism is the label loss averaged over all query
data samples. In some critical applications such as finance and
healthcare, even 1% of label loss may be intolerable. In this work,
we aim to achieve 0 label loss, i.e., our noise does not change the
predicted label for any query data sample. Formally, we aim to
achieve argmaxj
{sj } = argmaxj
{sj + nj }, where argmaxj
{sj } and
argmaxj
{sj + nj } are the labels predicted based on the true and
noisy confidence score vectors, respectively.
Confidence score distortion. The confidence score vector for
a query data sample tells the user more information about the data
sample’s label beyond the predicted label. Therefore, the added
noise should not substantially distort the confidence score vector.
First, the noisy confidence score vector should still be a probability
distribution. Formally, we have sj +nj ≥ 0 for ∀j and Í
j
(sj +nj) = 1.
Second, the distance d(s, s + n) between the true confidence score
vector and the noisy confidence score vector should be small. In
particular, we consider the model provider specifies a confidence
score distortion budget called ϵ, which indicates the upper bound
of the expected confidence score distortion that the model provider
can tolerate. Formally, we aim to achieve EM(d(s, s + n)) ≤ ϵ.
While any distance metric can be used to measure the distortion,
we consider L1-norm of the noise vector as the distance metric, i.e.,
d(s, s + n) = ||n||1. We adopt L1-norm of the noise vector because
it is easy to interpret. Specifically, the L1-norm of the noise vector
is simply the sum of the absolute value of its entries.
Membership inference attack defense problem: After quantifying Goal I and Goal II, we can formally define our problem of
defending against membership inference attacks.
Definition 3.1 (Membership-Inference-Attack Defense Problem).
Given the decision function д of the defense classifier, a confidence
score distortion budget ϵ, a true confidence score vector s, the
defender aims to find a randomized noise addition mechanism M∗
via solving the following optimization problem:
M∗ = argmin
M
|EM(д(s + n)) − 0.5| (1)
subject to: argmax
j
{sj + nj } = argmax
j
{sj } (2)
EM(d(s, s + n)) ≤ ϵ (3)
sj + nj ≥ 0, ∀j (4)
Õ
j
sj + nj = 1, (5)
where the objective function of the optimization problem is to
achieve Goal I and the constraints are to achieve Goal II. Specifically,
Session 2B: ML Security I CCS ’19, November 11–15, 2019, London, United Kingdom 263
the first constraint means that the added noise does not change
the predicted label of the query data sample; the second constraint
means that the confidence score distortion is bounded by the budget
ϵ; and the last two constraints mean that the noisy confidence score
vector is still a probability distribution. Note that the last constraint
is equivalent to Í
j nj = 0 since Í
j
sj = 1. Moreover, we adopt L1-
norm of the noise vector to measure the confidence score distortion,
i.e., d(s, s + n) = ||n||1.
4 OUR MemGuard
4.1 Overview
Finding the randomized noise addition mechanism is to solve the
optimization problem in Equation 1. We consider two scenarios
depending on whether д(s) is 0.5 or not.
Scenario I: In this scenario, д(s) = 0.5. For such scenario, it is easy
to solve the optimization problem in Equation 1. Specifically, the
mechanism that adds the noise vector 0 with probability 1 is the
optimal randomized noise addition mechanism, with which the
objective function has a value of 0.
Scenario II: In this scenario, д(s) is not 0.5. The major challenge
to solve the optimization problem in this scenario is that the randomized noise addition mechanism is a probability distribution
over the continuous noise space for a given true confidence score
vector. The noise space consists of the noise vectors that satisfy
the four constraints of the optimization problem. As a result, it is
challenging to represent the probability distribution and solve the
optimization problem. To address the challenge, we observe that
the noise space can be divided into two groups depending on the
output of the defense classifier’s decision function д. Specifically,
for noise vectors in one group, if we add any of them to the true
confidence score vector, then the decision function д outputs 0.5
as the probability of being member. For noise vectors in the other
group, if we add any of them to the true confidence score vector,
then the decision function д outputs a probability of being member
that is not 0.5.
Based on this observation, we propose a two-phase framework
to approximately solve the optimization problem. Specifically, in
Phase I, for each noise group, we find the noise vector with minimum confidence score distortion (i.e., d(s, s + n) is minimized)
as a representative noise vector for the noise group. We select the
noise vector with minimum confidence score distortion in order
to minimize the confidence score distortion. Since д(s) , 0.5, the
selected representative noise vector for the second noise group is 0.
We denote by r the selected representative noise vector for the first
noise group. In Phase II, we assume the randomized noise addition
mechanism is a probability distribution over the two representative
noise vectors instead of the overall noise space. Specifically, the defender adds the representative noise vector r to the true confidence
score vector with a certain probability and does not add any noise
with the remaining probability.
Next, we introduce our Phase I and Phase II.
4.2 Phase I: Finding r
Finding r as solving an optimization problem: Our goal essentially is to find a noise vector r such that 1) the utility loss of the
confidence score vector is minimized and 2) the decision function
д outputs 0.5 as the probability of being member when taking the
noisy confidence score vector as an input. Formally, we find such
noise vector via solving the following optimization problem:
min
r
d(s, s + r) (6)
subject to: argmax
j
{sj + rj } = argmax
j
{sj } (7)
д(s + r) = 0.5 (8)
sj + rj ≥ 0, ∀j (9)
Õ
j
rj = 0, (10)
where s is the true confidence score vector, the objective function
means that the confidence score distortion is minimized, the first
constraint means that the noise does not change the predicted label of the query data sample, the second constraint means that
the defense classifier’s decision function outputs 0.5 (i.e., the defense classifier’s prediction is random guessing), and the last two
constraints mean that the noisy confidence score vector is still a
probability distribution.
Solving the optimization problem in Equation 6 can be viewed
as finding an adversarial example to evade the defense classifier. In
particular, s is a normal example and s + r is an adversarial example. The adversarial machine learning community has developed
many algorithms (e.g., [10, 19, 31, 35, 39, 40, 50, 63]) to find adversarial examples. However, these algorithms are insufficient to our
problem because they did not consider the unique challenges of privacy protection. In particular, they did not consider the utility-loss
constraints, i.e., the constraints in Equation 7, Equation 9, and Equation 10.
One naive method (we call it Random) to address the challenges
is to generate a random noise vector that satisfies the utility-loss
constraints. In particular, we can generate a random vector r
′ whose
entries are non-negative and sum to 1. For instance, we first sample
a number r
′
1
from the interval [0,1] uniformly at random as the
first entry. Then, we sample a number r
′
2
from the interval [0, 1-r
′
1
]
uniformly at random as the second entry. We repeat this process
until the last entry is 1 minus the sum of the previous entries. Then,
we exchange the largest entry of r
′
to the position j to satisfy the
constraint 7. Finally, we treat r = r
′ − s as the noise vector, which
is a solution to the optimization problem in Equation 6. However,
as we will show in experiments, this Random method achieves
suboptimal privacy-utility tradeoffs because the noise vector is
not optimized and it is challenging to satisfy the constraint Equation 9. We propose to solve the optimization problem via change of
variables and adding the constraints to the objective function.
Eliminating the constraints on probability distribution via
change of variables: Since we consider the target classifier to
be a neural network, whose output layer is a softmax layer, the
true confidence score vector s is a softmax function of some vector
z. The vector z is the output of the neurons in the second-to-last
layer of the neural network and is often called logits of the neural
network. Formally, we have:
s = so f tmax(z). (11)
Session 2B: ML Security I CCS ’19, November 11–15, 2019, London, United Kingdom 264
Moreover, we model the noisy confidence score vector as follows:
s + r = so f tmax(z + e), (12)
where e is a new vector variable. For any value of e, the noisy
confidence score vector s + r is a probability distribution, i.e., the
constraints in Equation 9 and Equation 10 are satisfied. Therefore,
in the optimization problem in Equation 6, we change the true
confidence score vector s as so f tmax(z) and change the variable
r as so f tmax(z + e) − so f tmax(z). Then, we obtain the following
optimization problem:
min
e
d(so f tmax(z),so f tmax(z + e)) (13)
subject to: argmax
j
{zj + ej } = argmax
j
{zj } (14)
д(so f tmax(z + e)) = 0.5. (15)
After solving e in the above optimization problem, we can obtain
the noise vector r as follows:
r = so f tmax(z + e) − so f tmax(z). (16)
The optimization problem without the constraints on probability
distribution is still challenging to solve because the remaining two
constraints are highly nonlinear. To address the challenge, we turn
the constraints into the objective function.
Turning the constraint in Equation 15 into the objective function: We consider the defender’s binary defense classifier is a neural network whose output layer has a single neuron with sigmoid
activation function. Therefore, we have:
д(so f tmax(z + e)) =
1
1 + exp(−h(so f tmax(z + e))), (17)
where h(so f tmax(z + e)) is the output of the neuron in the secondto-last layer of the defense classifier when the defense classifier
takes the noisy confidence score vector so f tmax(z + e) as an input.
In other words, h is the logit of the defense classifier. д(so f tmax(z+
e)) = 0.5 implies h(so f tmax(z + e)) = 0. Therefore, we transform
the constraint in Equation 15 to the following loss function:
L1 = |h(so f tmax(z + e))|, (18)
where L1 is small when h(so f tmax(z + e)) is close to 0.
Turning the constraint in Equation 14 into the objective function: We denote by l the predicted label for the query data sample,
i.e., l = arдmaxj {sj } = argmaxj
{zj }. The constraint in Equation 14
means that zl +el
is the largest entry in the vector z + e. Therefore,
we enforce the inequality constraint zl + el ≥ maxj |j,l
{zj + ej }.
Moreover, we further transform the inequality constraint to the
following loss function:
L2 = ReLU(−zl − el + maxj |j,l
{zj + ej }), (19)
where the function ReLU is defined as ReLU(v)=max{0,v}. The
loss function L2 is 0 if the inequality zl + el ≥ maxj |j,l
{zj + ej }
holds.
Unconstrained optimization problem: After transforming the
constraints into the objective function, we have the following unconstrained optimization problem:
min
e
L = L1 + c2 · L2 + c3 · L3, (20)
Algorithm 1 Phase I of MemGuard
Input: z, max_iter, c2, c3, and β (learning rate).
Output: e
1: //Predicted label
2: l = argmaxj
{zj }
3: while T rue do
4: //A new iteration to search c3
5: e = 0
6: e
′ = e
7: i = 1
8: while i < max_iter and (argmaxj
{zj + ej } , l or
h(so f tmax(z)) · h(so f tmax(z + e)) > 0) do
9: //Gradient descent with normalized gradient
10: u =
∂L
∂e
11: u = u/||u||2
12: e = e − β · u
13: i = i + 1
14: end while
15: //Return the vector in the previous iteration if the predicted
label changes or the sign of h does not change in the current
iteration
16: if argmaxj
{zj + ej } , l or h(so f tmax(z)) · h(so f tmax(z +
e)) > 0 then
17: return e
′
18: end if
19: c3 = 10 ·c3
20: end while
where L3 = d(so f tmax(z),so f tmax(z+e)), while c2 and c3 balance
between the three terms.
Solving the unconstrained optimization problem: We design
an algorithm based on gradient descent to solve the unconstrained
optimization problem. Algorithm 1 shows our algorithm. Since we
aim to find a noise vector that has a small confidence score distortion, we iteratively search a large c3. For each given c3, we use
gradient descent to find e that satisfies the constraints in Equation 14 and Equation 15. The process of searching c3 stops when
we cannot find a vector e that satisfies the two constraints. Specifically, given c2, c3, and a learning rate β, we iteratively update the
vector variable e (i.e., the inner while loop in Algorithm 1). Since
we transform the constraints in Equation 14 and Equation 15 into
the objective function, there is no guarantee that they are satisfied
during the iterative gradient descent process. Therefore, in each
iteration of gradient descent, we check whether the two constraints
are satisfied (i.e., Line 8 in Algorithm 1). Specifically, we continue
the gradient descent process when the predicted label changes or
the sign of the logit h does not change. In other words, we stop the
gradient descent process when both constraints are satisfied. We
use h(so f tmax(z)) · h(so f tmax(z + e)) ≤ 0 to approximate the constraint in Equation 15. In particular, the constraint in Equation 15 is
equivalent to h(so f tmax(z + e)) = 0. Once we find a vector e such
that h(so f tmax(z)) and h(so f tmax(z+e)) have different signs (e.g.,
h(so f tmax(z)) > 0 and h(so f tmax(z + e)) < 0), h(so f tmax(z + e))
Session 2B: ML Security I CCS ’19, November 11–15, 2019, London, United Kingdom 265
just crosses 0 and should be close to 0 since we use a small learning rate. Note that we could also iteratively search c2, but it is
computationally inefficient to search both c2 and c3.
4.3 Phase II
After Phase I, we have two representative noise vectors. One is 0 and
the other is r. In Phase II, we assume the randomized noise addition
mechanism is a probability distribution over the two representative
noise vectors instead of the entire noise space. Specifically, we assume that the defender picks the representative noise vectors r and
0 with probabilities p and 1 −p, respectively; and the defender adds
the picked representative noise vector to the true confidence score
vector. With such simplification, we can simplify the optimization
problem in Equation 1 to the following optimization problem:
p = argmin
p
|p · д(s + r) + (1 − p) · д(s + 0) − 0.5| (21)
subject to: p · d(s, s + r) + (1 − p) · d(s, s + 0) ≤ ϵ, (22)
where the constraint means that the expected confidence score
distortion is bounded by the budget. Note that we omit the other
three constraints in Equation 2, Equation 4, and Equation 5. This
is because both of our representative noise vectors already satisfy
those constraints. Moreover, we can derive an analytical solution
to the simplified optimization problem. The analytical solution is
as follows:
p =
(
0, if |д(s) − 0.5| ≤ |д(s + r) − 0.5|
min(
ϵ
d(s,s+r)
, 1.0), otherwise.
(23)
One-time randomness: If the defender randomly samples one
of the two representative noise vectors every time for the same
query data sample, then an attacker could infer the true confidence
score vector via querying the same data sample multiple times. We
consider the attacker knows our defense mechanism including the
confidence score distortion metric d, the budget ϵ, and that the
noise vector is sampled from two representative noise vectors, one
of which is 0.
Suppose the attacker queries the same data sample n times from
the target classifier. The attacker receives a confidence score vector
s1 form times and a confidence score vector s2 for n −m times. One
confidence score vector is s + r and the other is the true confidence
score vector s. Since the attacker receives two different confidence
score vectors, the attacker knows 0 < p < 1. Moreover, given the
two confidence score vectors, the attacker can compute p according
to Equation 23 since the distance d(s, s + r) does not depend on
the ordering of s and s + r, i.e., d(s, s + r) = d(s1, s2). The attacker
can also estimate the probabilities that the defender returns the
confidence score vectors s1 and s2 as m
n
and n−m
n
, respectively.
If m
n
is closer to p, then the attacker predicts that s2 is the true
confidence score vector, otherwise the attacker predicts s1 to be
the true confidence score vector.
To address this challenge, we propose to use one-time randomness when the defender samples the representative noise, with
which the defender always returns the same confidence score vector for the same query data sample. Specifically, for a query data
sample, the defender quantizes each dimension of the query data
sample and computes the hash value of the quantized data sample.
Then, the defender generates a random number p
′
in the range
[0, 1] via a pseudo random number generator with the hash value
as the seed. If p
′ < p, the defender adds the representative noise
vector r to the true confidence score vector, otherwise the defender
does not add noise. The random number p
′
is the same for the
same query data sample, so the defender always returns the same
confidence score vector for the same query data sample. We compute the hash value of the quantized query data sample as the seed
such that the attacker cannot just slightly modify the query data
sample to generate a different p
′
. The attacker can compute the
random number p
′
as we assume the attacker knows the defense
mechanism including the hash function and pseudo random number generator. However, the attacker does not know p any more
because the defender always returns the same confidence score
vector for the same query data sample. Therefore, the attacker does
not know whether the returned confidence score vector is the true
one or not.
5 EVALUATION
5.1 Experimental Setup
5.1.1 Datasets. We use three datasets that represent different application scenarios.
Location: This dataset was preprocessed from the Foursquare
dataset1
and we obtained it from [58]. The dataset has 5,010 data
samples with 446 binary features, each of which represents whether
a user visited a particular region or location type. The data samples are grouped into 30 clusters. This dataset represents a 30-class
classification problem, where each cluster is a class.
Texas100: This dataset is based on the Discharge Data public use
files published by the Texas Department of State Health Services.2
We obtained the preprocessed dataset from [58]. The dataset has
67, 330 data samples with 6, 170 binary features. These features
represent the external causes of injury (e.g., suicide, drug misuse),
the diagnosis, the procedures the patient underwent, and some
generic information (e.g., gender, age, and race). Similar to [58], we
focus on the 100 most frequent procedures and the classification
task is to predict a procedure for a patient using the patient’s data.
This dataset represents a 100-class classification problem.
CH-MNIST: This dataset is used for classification of different tissue
types on histology tile from patients with colorectal cancer. The
dataset contains 5, 000 images from 8 tissues. The classification
task is to predict tissue for an image, i.e., the dataset is a 8-class
classification problem. The size of each image is 64×64. We obtained
a preprocessed version from Kaggle. 3
.
Dataset splits: For each dataset, we will train a target classifier,
an attack classifier, and a defense classifier. Therefore, we split
each dataset into multiple folds. Specifically, for the Location (or
CH-MNIST) dataset, we randomly sample 4 disjoint sets, each of
which includes 1,000 data samples. We denote them as D1, D2, D3,
and D4, respectively. For the Texas100 dataset, we also randomly
sample such 4 disjoint sets, but each set includes 10,000 data samples
as the Texas100 dataset is around one order of magnitude larger.
Roughly speaking, for each dataset, we use D1, D2, and D3 to learn
1https://sites.google.com/site/yangdingqi/home/foursquare-dataset
2https://www.dshs.texas.gov/THCIC/Hospitals/Download.shtm
3https://www.kaggle.com/kmader/colorectal-histology-mnist
Session 2B: ML Security I CCS ’19, November 11–15, 2019, London, United Kingdom 266
Table 2: Neural network architecture of the target classifier
for CH-MNIST.
Layer Type Layer Parameters
Input 64 × 64
Convolution 32 × 3 × 3, strides=(1, 1), padding=same
Activation ReLU
Convolution 32 × 3 × 3, strides=(1, 1)
Activation ReLU
Pooling MaxPooling(2 × 2)
Convolution 32 × 3 × 3, strides=(1, 1), padding=same
Activation ReLU
Convolution 32 × 3 × 3, strides=(1, 1)
Activation ReLU
Pooling MaxPooling(2 × 2)
Flatten
Fully Connected 512
Fully Connected 8
Activation softmax
Output
the target classifier, the attack classifier, and the defense classifier,
respectively; and we use D1 ∪ D4 to evaluate the accuracy of the
attack classifier. We will describe more details on how the sets are
used when we use them.
5.1.2 Target Classifiers. For the Location and Texas100 datasets,
we use a fully-connected neural network with 4 hidden layers as
the target classifier. The number of neurons for the four layers are
1024, 512, 256, and 128, respectively. We use the popular activation
function ReLU for the neurons in the hidden layers. The activation
function in the output layer is softmax. We adopt the cross-entropy
loss function and use Stochastic Gradient Descent (SGD) to learn
the model parameters. We train 200 epochs with a learning rate
0.01, and we decay the learning rate by 0.1 in the 150th epoch for
better convergence. For the CH-MNIST dataset, the neural network
architecture of the target classifier is shown in Table 2. Similarly,
we also adopt the cross-entropy loss function and use SGD to learn
the model parameters. We train 400 epochs with a learning rate
0.01 and decay the learning rate by 0.1 in the 350th epoch. For
each dataset, we use D1 to train the target classifier. Table 3 shows
the training and testing accuracies of the target classifiers on the
three datasets, where the testing accuracy is calculated by using
the target classifier to make predictions for the data samples that
are not in D1.
5.1.3 Membership Inference Attacks. In a membership inference
attack, an attacker trains an attack classifier, which predicts member
or non-member for a query data sample. The effectiveness of an
attack is measured by the inference accuracy of the attack classifier,
where the inference accuracy is the fraction of data samples in
D1 ∪ D4 that the attack classifier can correctly predict as member
or non-member. In particular, data samples in D1 are members of
the target classifier’s training dataset, while data samples in D4 are
non-members. We call the dataset D1 ∪ D4 evaluation dataset. We
consider two categories of state-of-the-art black-box membership
inference attacks, i.e., non-adaptive attacks and adaptive attacks. In
Table 3: Training and testing accuracies of the target classifier on the three datasets.
Location Texas100 CH-MNIST
Training Accuracy 100.0% 99.98% 99.0%
Testing Accuracy 60.32% 51.59% 72.0%
non-adaptive attacks, the attacker does not adapt its attack classifier
based on our defense, while the attacker adapts its attack classifier
based on our defense in adaptive attacks.
Non-adaptive attacks: We consider the random guessing attack
and state-of-the-art attacks as follows.
Random guessing (RG) attack. For any query data sample,
this attack predicts it to be a member of the target classifier’s training dataset with probability 0.5. The inference accuracy of the RG
attack is 0.5.
Neural Network (NN) attack [56, 58]. This attack assumes
that the attacker knows the distribution of the target classifier’s
training dataset and the architecture of the target classifier. We
further split the dataset D2 into two halves denoted as D
′
2
and D
′′
2
,
respectively. The attacker uses D
′
2
to train a shadow classifier that
has the same neural network architecture as the target classifier.
After training the shadow classifier, the attacker calculates the confidence score vectors for the data samples in D
′
2
and D
′′
2
, which
are members and non-members of the shadow classifier. Then, the
attacker ranks each confidence score vector and treats the ranked
confidence score vectors of members and non-members as a “training dataset” to train an attack classifier. The attack classifier takes
a data sample’s ranked confidence score vector as an input and predicts member or non-member. For all three datasets, we consider
the attack classifier is a fully-connected neural network with three
hidden layers, which have 512, 256, and 128 neurons, respectively.
The output layer just has one neuron. The neurons in the hidden
layers use the ReLU activation function, while the neuron in the
output layer uses the sigmoid activation function. The attack classifier predicts member if and only if the neuron in the output layer
outputs a value that is larger than 0.5. We train the attack classifier
for 400 epochs with a learning rate 0.01 using SGD and decay the
learning rate by 0.1 at the 300th epoch.
Random Forest (RF) attack. This attack is the same as the
NN attack except that RF attack uses random forest as the attack
classifier, while NN uses a neural network as the attack classifier.
We use scikit-learn with the default setting to learn random forest
classifiers. We consider this RF attack to demonstrate that our
defense mechanism is still effective even if the attack classifier
and the defense classifier (a neural network) use different types of
algorithms, i.e., the noise vector that evades the defense classifier
can also evade the attack classifier even if the two classifiers use
different types of algorithms.
NSH attack [42]. Nasr, Shokri, and Houmansadr [42] proposed
this attack, which we abbreviate as NSH. This attack uses multiple
neural networks. One network operates on the confidence score
vector. Another one operates on the label which is one hot encoded.
Both networks are fully-connected and have the same number of
input dimension, i.e., the number of classes of the target classifier.
Specifically, NSH assumes the attacker knows some members and
Session 2B: ML Security I CCS ’19, November 11–15, 2019, London, United Kingdom 267
0.0 0.2 0.4 0.6 0.8 1.0
Confidence Score Distortion Budget, 
0.4
0.5
0.6
0.7
0.8
0.9
1.0
Inference Accuracy
RG
NN
RF
NSH
NN-AT
NN-R
(a) Location
0.0 0.2 0.4 0.6 0.8 1.0
Confidence Score Distortion Budget, 
0.4
0.5
0.6
0.7
0.8
0.9
1.0
Inference Accuracy
RG
NN
RF
NSH
NN-AT
NN-R
(b) Texas100
0.0 0.2 0.4 0.6 0.8 1.0
Confidence Score Distortion Budget, 
0.4
0.5
0.6
0.7
0.8
0.9
1.0
Inference Accuracy
RG
NN
RF
NSH
NN-AT
NN-R
(c) CH-MNIST
Figure 1: Inference accuracies of different attacks as the confidence score distortion budget (i.e., ϵ) increases.
non-members of the target classifier’s training dataset. In our experiments, we assume the attacker knows 30% of data samples in D1
(i.e., members) and 30% of data samples in D4 (i.e., non-members).
The attacker uses these data samples to train the attack classifier.
We adopt the neural network architecture in [42] as the attack classifier. The remaining 70% of data samples in D1 and D4 are used to
calculate the inference accuracy of the attack classifier. We train
the attack classifier for 400 epochs with an initial learning rate 0.01
and decay the learning rate by 0.1 after 300 epochs.
Adaptive attacks: We consider two attacks that are customized to
our defense.
Adversarial training (NN-AT). One adaptive attack is to train
the attack classifier via adversarial training, which was considered
to be the most empirically robust method against adversarial examples so far [3]. We adapt the NN attack using adversarial training
and denote the adapted attack as NN-AT. Specifically, for each data
sample in D
′
2
and D
′′
2
, the attacker calculates its confidence score
vector using the shadow classifier. Then, the attacker uses the Phase
I of our defense to find the representative noise vector and adds it
to the confidence score vector to obtain a noisy confidence score
vector. Finally, the attacker trains the attack classifier via treating
the true confidence score vectors and their corresponding noisy
versions of data samples in D
′
2
and D
′′
2
as a training dataset.
Rounding (NN-R). Since our defense adds carefully crafted
small noise to the confidence score vector, an adaptive attack is to
round each confidence score before using the attack classifier to
predict member/non-member. Specifically, we consider the attacker
rounds each confidence score to be one decimal and uses the NN
attack. Note that rounding is also applied when training the NN
attack classifier. We denote this attack NN-R.
Table 4 shows the inference accuracies of different attacks when
our defense is not used. All attacks except RG have inference accuracies that are larger or substantially larger than 0.5.
5.1.4 Defense Setting. In our defense, we need to specify a defense
classifier and the parameters in Algorithm 1.
Defense classifier: The defender itself trains a classifier to perform
membership inference. We consider the defense classifier is a neural
network. However, since the defender does not know the attacker’s
attack classifier, we assume the defense classifier and the attack
classifier use different neural network architectures. Specifically,
we consider three different defense classifiers in order to study
the impact of defense classifier on MemGuard. The three defense
Table 4: Inference accuracies of different attacks on the
three datasets when our defense is not used.
Location Texas100 CH-MNIST
RG 50.0% 50.0% 50.0%
NN 73.0% 68.9% 62.9%
RF 73.7% 67.3% 58.7%
NSH 81.1% 74.0% 58.4%
NN-AT 64.6% 68.3% 63.3%
NN-R 72.9% 69.2% 63.0%
classifiers are fully-connected neural networks with 2, 3, and 4
hidden layers, respectively. The hidden layers of the three defense
classifiers have (256, 128), (256, 128, 64), and (512, 256, 128, 64)
neurons, respectively. The output layer has just one neuron. The
activation function for the neurons in the hidden layers is ReLU ,
while the neuron in the output layer uses the sigmoid activation
function. Unless otherwise mentioned, we use the defense classifier
with 3 hidden layers. The defender calculates the confidence score
vector for each data sample in D1 and D3 using the target classifier.
The confidence score vectors for data samples in D1 and D3 have
labels “member” and “non-member”, respectively. The defender
treats these confidence score vectors as a training dataset to learn a
defense classifier, which takes a confidence score vector as an input
and predicts member or non-member. We train a defense classifier
for 400 epochs with a learning rate 0.001. We note that we can also
synthesize data samples based on D1 as non-members (Appendix A
shows details).
Parameter setting: We set max_iter = 300 and β = 0.1 in Algorithm 1. We found that oncemax_iter is larger than some threshold,
MemGuard’s effectiveness does not change. Since we aim to find
representative noise vector that does not change the predicted label, we assign a relatively large value to c2, which means that the
objective function has a large value if the predicted label changes
(i.e., the loss function L2 is non-zero). In particular, we set c2 = 10.
Our Algorithm 1 searches for a large c3 and we set the initial value
of c3 to be 0.1. We also compare searching c2 with searching c3.
Session 2B: ML Security I CCS ’19, November 11–15, 2019, London, United Kingdom 268
0.0 0.2 0.4 0.6 0.8 1.0
Normalized Entropy
0.0
0.2
0.4
0.6
0.8
1.0
Frequency
Member
Non-member
(a) Location, without defense
0.0 0.2 0.4 0.6 0.8 1.0
Normalized Entropy
0.0
0.2
0.4
0.6
0.8
1.0
Frequency
Member
Non-member
(b) Texas100, without defense
0.0 0.2 0.4 0.6 0.8 1.0
Normalized Entropy
0.0
0.2
0.4
0.6
0.8
1.0
Frequency
Member
Non-member
(c) CH-MNIST, without defense
0.0 0.2 0.4 0.6 0.8 1.0
Normalized Entropy
0.0
0.2
0.4
0.6
0.8
1.0
Frequency
Member
Non-member
(d) Location, with defense
0.0 0.2 0.4 0.6 0.8 1.0
Normalized Entropy
0.0
0.2
0.4
0.6
0.8
1.0
Frequency
Member
Non-member
(e) Texas100, with defense
0.0 0.2 0.4 0.6 0.8 1.0
Normalized Entropy
0.0
0.2
0.4
0.6
0.8
1.0
Frequency
Member
Non-member
(f) CH-MNIST, with defense
Figure 2: Distribution of the normalized entropy of the confidence score vectors for members and non-members of the target
classifier. Figures on the upper side are results without our defense, and figures on the lower side are results with our defense.
5.2 Experimental Results
MemGuard is effective: Figure 1 shows the inference accuracies
of different attacks as the confidence score distortion budget increases on the three datasets. Since we adopt the expected L1-norm
of the noise vector to measure the confidence score distortion, the
confidence score distortion is in the range [0, 2]. Note that our
defense is guaranteed to achieve 0 label loss as our Algorithm 1
guarantees that the predicted label does not change when searching
for the representative noise vector. We observe that our MemGuard
can effectively defend against membership inference attacks, i.e.,
the inference accuracies of all the evaluated attacks decrease as
our defense is allowed to add larger noise to the confidence score
vectors. For instance, on Location, when our defense is allowed
to add noise whose expected L1-norm is around 0.8, our defense
can reduce all the evaluated attacks to the random guessing (RG)
attack; on CH-MNIST, our defense can reduce the NSH attack (or
the remaining attacks) to random guessing when allowed to add
noise whose expected L1-norm is around 0.3 (or 0.7).
Indistinguishability between the confidence score vectors of
members and non-members: We follow previous work [42] to
study the distribution of confidence score vectors of members vs.
non-members of the target classifier. Specifically, given a confidence
score vector s, we compute its normalized entropy as follows:
Normalized entropy: −
1
log k
Õ
j
sj
log(sj), (24)
where k is the number of classes in the target classifier. Figure 2
shows the distributions of the normalized entropy of the confidence score vectors for members (i.e., data samples in D1) and nonmembers (i.e., data samples in D4) of the target classifier, where
0.0 0.2 0.4 0.6 0.8 1.0
Confidence Score Distortion Budget, 
0.4
0.5
0.6
0.7
0.8
0.9
1.0
Inference Accuracy
Search c3, c2 = 0.1
Search c3, c2 = 1
Search c3, c2 = 10
Search c3, c2 = 100
(a) Searching c3
0.0 0.2 0.4 0.6 0.8 1.0
Confidence Score Distortion Budget, 
0.4
0.5
0.6
0.7
0.8
0.9
1.0
Inference Accuracy
Search c3, c2 = 10
Search c2, c3 = 0.1
Search c2, c3 = 1.0
(b) Searching c2
Figure 3: Inference accuracy of the NN attack as the confidence score distortion budget increases on the Location
dataset when searching c3 or c2.
we set the confidence score distortion budget ϵ to be 1 when our
defense is used. The gap between the two curves in a graph corresponds to the information leakage of the target classifier’s training
dataset. Our defense substantially reduces such gaps. Specifically,
the maximum gap between the two curves (without defense vs.
with defense) is (0.27 vs. 0.11), (0.41 vs. 0.05), and (0.30 vs. 0.06)
on the Location, Texas100, and CH-MNIST datasets, respectively.
Moreover, the average gap between the two curves (without defense
vs. with defense) is (0.062 vs. 0.011), (0.041 vs. 0.005), and (0.030 vs.
0.006) on the three datasets, respectively.
Searching c2 vs. searching c3: Figure 3a shows the inference accuracy of the NN attack as the confidence score distortion budget increases when fixing c2 to different values and searching c3.
Figure 3b shows the results when fixing c3 and searching c2. We
observe that MemGuard is insensitive to the setting of c2 when
searching c3. Specifically, MemGuard has almost the same effectiveness when fixing c2 to different values, i.e., the different curves
Session 2B: ML Security I CCS ’19, November 11–15, 2019, London, United Kingdom 269
0.0 0.2 0.4 0.6 0.8 1.0
Confidence Score Distortion Budget, 
0.4
0.5
0.6
0.7
0.8
0.9
1.0
Inference Accuracy
2 Hidden Layers
3 Hidden Layers
4 Hidden Layers
Figure 4: Inference accuracy of the NN attack as the confidence score distortion budget increases on the Location
dataset when using different defense classifiers.
overlap in Figure 3a. This is because when our Phase I stops searching the noise vector, the predicted label is preserved, which means
that the loss function L2 is 0. However, MemGuard is sensitive to
the setting of c3 when searching c2. Specifically, when fixing c3 to
be 0.1, searching c2 achieves the same effectiveness as searching
c3. However, when fixing c3 to be 1.0, searching c2 is less effective.
Therefore, we decided to search c3 while fixing c2.
Impact of defense classifiers: Figure 4 shows the inference accuracy of the NN attack as the confidence score distortion budget
increases on the Location dataset when using different defense
classifiers. We observe that MemGuard has similar effectiveness for
different defense classifiers, which means that our carefully crafted
noise vectors can transfer between classifiers.
MemGuard outperforms existing defenses: We compare with
state-of-the-art defenses including L2-Regularizer [58], Min-Max
Game [42], Dropout [56], Model Stacking [56], and DP-SGD [1].
Each compared defense (except Model Stacking) has a hyperparameter to control the privacy-utility tradeoff. For instance, the
hyperparameter that balances between the loss function and the L2
regularizer in L2-Regularizer, the hyperparameter that balances between the loss function and the adversarial regularizer in Min-Max
Game, the dropout rate in Dropout, the privacy budget in DP-SGD,
and ϵ in MemGuard. We also compare with MemGuard-Random in
which we use the Random method (refer to Section 4.2) to generate
the noise vector in Phase I.
Before deploying any defense, we use the undefended target classifier to compute the confidence score vector for each data sample
in the evaluation dataset D1 ∪ D4. For each defense and a given
hyperparameter, we apply the defense to the target classifier and
use the defended target classifier to compute the confidence score
vector for each data sample in D1 ∪ D4. Then, we compute the confidence score distortion for each data sample and obtain the average
confidence score distortion on the evaluation dataset D1 ∪ D4. Moreover, we compute the inference accuracy of the attack classifier
(we consider NN in these experiments) on the evaluation dataset
after the defense is used. Therefore, for each defense and a given
hyperparameter, we can obtain a pair (inference accuracy, average
confidence score distortion). Via exploring different hyperparameters, we can obtain a set of such pairs for each defense. Then, we
plot these pairs on a graph, which is shown in Figure 5.
Table 5: Results of Model Stacking.
Location Texas100 CH-MNIST
Inference Acc. 50.0% 50.8% 50.0%
Average Distortion 1.63 1.28 0.81
Label Loss 56.3% 37.9% 18.3%
Specifically, we tried the hyperparameter of L2-Regularizer in
the range [0, 0.05] with a step size 0.005, 0.001, and 0.005 for Location, Texas100, and CH_MNIST datasets, respectively. We tried
the hyperparameter of Min-Max Game in the range [0, 3] with a
step size 0.5. We tried the dropout rate of Dropout in the range
[0, 0.9] with a step size 0.1. We use a publicly available implementation4 of DP-SGD. We tried the parameter noise_multiplier that
controls the privacy budget in the range [0, 0.2] with a step size
0.05. We tried [0, 0.1, 0.3, 0.5, 0.7, 1.0] as the ϵ in MemGuard and
MemGuard-Random.
Our results show that MemGuard achieves the best privacyutility tradeoff. In particular, given the same average confidence
score distortion, MemGuard achieves the smallest inference accuracy. According to the authors of Model Stacking, it does not
have a hyperparameter to easily control the privacy-utility tradeoff.
Therefore, we just obtain one pair of (inference accuracy, average
confidence score distortion) and Table 5 shows the results. Model
Stacking reduces the inference accuracy to be close to 0.5, but the
utility loss is intolerable.
Similarly, we can obtain a set of pairs (inference accuracy, label
loss) for the compared defenses and Figure 6 shows inference accuracy vs. label loss on the three datasets. Label loss is the fraction of
data samples in the evaluation dataset whose predicted labels are
changed by a defense. MemGuard-Random and MemGuard achieve
0 label loss. However, other defenses incur large label losses in
order to substantially reduce the attacker’s inference accuracy.
6 DISCUSSION AND LIMITATIONS
On one hand, machine learning can be used by attackers to perform
automated inference attacks. On the other hand, machine learning
has various vulnerabilities, e.g., adversarial examples [10, 19, 31, 47–
50, 62]. Therefore, attackers who rely on machine learning also
share its vulnerabilities and we can exploit such vulnerabilities
to defend against them. For instance, we can leverage adversarial
examples to mislead attackers who use machine learning classifiers
to perform automated inference attacks [27]. One key challenge in
this research direction is how to extend existing adversarial example
methods to address the unique challenges of privacy protection.
For instance, how to achieve formal utility-loss guarantees.
In this work, we focus on membership inference attacks under
the black-box setting, in which an attacker uses a binary classifier
to predict a data sample to be a member or non-member of a target
classifier’s training dataset. In particular, the attacker’s classifier
takes a data sample’s confidence score vector predicted by the target
classifier as an input and predicts member or non-member. Our
defense adds carefully crafted noise to a confidence score vector to
turn it into an adversarial example, such that the attacker’s classifier
4https://github.com/tensorflow/privacy
Session 2B: ML Security I CCS ’19, November 11–15, 2019, London, United Kingdom 270
0.0 0.2 0.4 0.6 0.8 1.0
Average Confidence Score Distortion
0.4
0.5
0.6
0.7
0.8
0.9
1.0
Inference Accuracy
L2-Regularizer
Min-Max Game
Dropout
DP-SGD
MemGuard-Random
MemGuard
(a) Location
0.0 0.2 0.4 0.6 0.8 1.0
Average Confidence Score Distortion
0.4
0.5
0.6
0.7
0.8
0.9
1.0
Inference Accuracy
L2-Regularizer
Min-Max Game
Dropout
DP-SGD
MemGuard-Random
MemGuard
(b) Texas100
0.0 0.2 0.4 0.6 0.8 1.0
Average Confidence Score Distortion
0.4
0.5
0.6
0.7
0.8
0.9
1.0
Inference Accuracy
L2-Regularizer
Min-Max Game
Dropout
DP-SGD
MemGuard-Random
MemGuard
(c) CH-MNIST
Figure 5: Inference accuracy vs. average confidence score distortion of the compared defenses. Our MemGuard achieves the
best privacy-utility tradeoff.
0.0 0.1 0.2 0.3 0.4 0.5
Label Loss
0.4
0.5
0.6
0.7
0.8
0.9
1.0
Inference Accuracy
L2-Regularizer
Min-Max Game
Dropout
DP-SGD
MemGuard-Random
MemGuard
(a) Location
0.0 0.1 0.2 0.3 0.4 0.5
Label Loss
0.4
0.5
0.6
0.7
0.8
0.9
1.0
Inference Accuracy
L2-Regularizer
Min-Max Game
Dropout
DP-SGD
MemGuard-Random
MemGuard
(b) Texas100
0.0 0.1 0.2 0.3 0.4 0.5
Label Loss
0.4
0.5
0.6
0.7
0.8
0.9
1.0
Inference Accuracy
L2-Regularizer
Min-Max Game
Dropout
DP-SGD
MemGuard-Random
MemGuard
(c) CH-MNIST
Figure 6: Inference accuracy vs. label loss of the compared defenses. Both MemGuard-Random and MemGuard achieve 0 label
loss, while the other defenses incur large label losses in order to substantially reduce the attacker’s inference accuracy.
is likely to predict member or non-member incorrectly. To address
the challenges of achieving formal utility-loss guarantees, e.g., 0
label loss and bounded confidence score distortion, we design new
methods to find adversarial examples.
Other than membership inference attacks, many other attacks
rely on machine learning classifiers, e.g., attribute inference attacks [11, 17, 28], website fingerprinting attacks [7, 22, 29, 46, 67],
side-channel attacks [73], location attacks [5, 45, 52, 72], and author
identification attacks [8, 41]. For instance, online social network
users are vulnerable to attribute inference attacks, in which an attacker leverages a machine learning classifier to infer users’ private
attributes (e.g., gender, political view, and sexual orientation) using
their public data (e.g., page likes) on social networks. The Facebook data privacy scandal in 20185
is a notable example of attribute
inference attack. In particular, Cambridge Analytica leveraged a
machine learning classifier to automatically infer a large amount
of Facebook users’ various private attributes using their public
page likes. Jia and Gong proposed AttriGuard [26], which leverages
adversarial examples to defend against attribute inference attacks.
In particular, AttriGuard extends an existing adversarial example
method to incorporate the unique challenges of privacy protection.
The key difference between MemGuard and AttriGuard is that finding adversarial examples for confidence score vectors is subject
to unique constraints, e.g., an adversarial confidence score vector
should still be a probability distribution and the predicted label
should not change. Such unique constraints require substantially
5https://bit.ly/2IDchsx
different methods to find adversarial confidence score vectors. Other
studies have leveraged adversarial examples to defend against traffic analysis [71] and author identification [38, 54]. However, these
studies did not consider formal utility-loss guarantees.
We believe it is valuable future work to extend MemGuard to
defend against other machine learning based inference attacks
such as website fingerprinting attacks, side-channel attacks, and
membership inference attacks in the white-box setting. Again, a
key challenge is how to achieve formal utility-loss guarantees with
respect to certain reasonable utility-loss metrics.
Our MemGuard has a parameter ϵ, which controls a tradeoff between membership privacy and confidence score vector distortion.
The setting of ϵ may be dataset-dependent. One way to set ϵ is to
leverage an inference accuracy vs. ϵ curve as shown in Figure 1.
Specifically, given a dataset, we draw the inference accuracy vs.
ϵ curves for various attack classifiers. Suppose we desire the inference accuracy to be less than a threshold. Then, we select the
smallest ϵ such that the inference accuracies of all the evaluated
attack classifiers are no larger than the threshold.
7 CONCLUSION AND FUTURE WORK
In this work, we propose MemGuard to defend against black-box
membership inference attacks. MemGuard is the first defense that
has formal utility-loss guarantees on the confidence score vectors
predicted by the target classifier. MemGuard works in two phases.
In Phase I, MemGuard leverages a new algorithm to find a carefully crafted noise vector to turn a confidence score vector into
Session 2B: ML Security I CCS ’19, November 11–15, 2019, London, United Kingdom 271
an adversarial example. The new algorithm considers the unique
utility-loss constraints on the noise vector. In Phase II, MemGuard
adds the noise vector to the confidence score vector with a certain
probability, for which we derive an analytical solution. Our empirical evaluation results show that MemGuard can effectively defend
against black-box membership inference attacks and outperforms
existing defenses. An interesting future work is to extend MemGuard to defend against other types of machine learning based
inference attacks such as white-box membership inference attacks,
website fingerprinting attacks, and side-channel attacks.