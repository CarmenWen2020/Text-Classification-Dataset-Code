Machine learning is widely used to produce models for a range of
applications and is increasingly offered as a service by major technology companies. However, the required massive data collection
raises privacy concerns during both training and prediction stages.
In this paper, we design and implement a general framework
for privacy-preserving machine learning and use it to obtain new
solutions for training linear regression, logistic regression and neural network models. Our protocols are in a three-server model
wherein data owners secret share their data among three servers
who train and evaluate models on the joint data using three-party
computation (3PC).
Our main contribution is a new and complete framework (ABY3
)
for efficiently switching back and forth between arithmetic, binary,
and Yao 3PC which is of independent interest. Many of the conversions are based on new techniques that are designed and optimized
for the first time in this paper. We also propose new techniques for
fixed-point multiplication of shared decimal values that extends beyond the three-party case, and customized protocols for evaluating
piecewise polynomial functions. We design variants of each building block that is secure against malicious adversaries who deviate
arbitrarily.
We implement our system in C++. Our protocols are up to four
orders of magnitude faster than the best prior work, hence significantly reducing the gap between privacy-preserving and plaintext
training.
1 INTRODUCTION
Machine learning is widely used to produce models that classify
images, authenticate biometric information, recommend products,
choose which Ads to show, and identify fraudulent transactions.
Major technology companies such as Microsoft, IBM, Amazon,
and Google are providing cloud-based machine learning services
[1, 3, 4, 6] to their customers both in form of pre-trained models that
can be used for prediction as well as training platforms that train
models on customer data. Advances in deep learning, in particular,
have lead to breakthroughs in image, speech, and text recognition
to the extent that the best records are often held by neural network
models trained on large datasets.
A major enabler of this success is the large-scale data collection that deep learning algorithms thrive on. Internet companies
regularly collect users’ online activities and browsing behavior to
train more accurate recommender systems, the healthcare sector
envisions a future where patients’ clinical and genomic data can
be used to produce new diagnostic models and there are efforts to
share security incidents and threat data, to improve future attack
prediction.
The data being classified or used for training is often sensitive
and may come from multiple sources with different privacy requirements. Regulations such as HIPPA, PCI, and GDPR, user privacy
concerns, data sovereignty issues, and competitive advantage are
all reasons that prevent entities from pooling different data sources
to train more accurate models.
Privacy-preserving machine learning based on secure multiparty
computation (MPC) is an active area of research that can help address some of these concerns. It ensures that during training, the
only information leaked about the data is the final model (or an
encrypted version), and during prediction, the only information
leaked is the classification label. These are strong guarantees that,
though do not provide a full-proof privacy solution (the models
themselves or interactions with them can leak information about
the data [51, 53, 55]), provide a strong first line of defense which
can be strengthened when combined with orthogonal mechanisms
such as differential privacy [7, 41]. The most common setting considered in this line of work is a server-aided model where data
owners (clients) send encrypted version of their data to multiple
servers who perform the training procedure on the combined data
or apply a (shared) pre-trained model to classify new data samples. Performance of these solutions has improved significantly
over the years, leading to orders of magnitude speedup in privacypreserving machine learning. Nevertheless, there is still a large gap
between plaintext training and the privacy-preserving solutions.
While part of this gap is unavoidable given the desired guarantees,
the current state of affairs is far from optimal. In the three-party
computation (3PC) setting with one corruption, following up on a
large body of work [12, 13, 22, 38], new techniques and implementations [9, 28, 42] have significantly reduced this gap, e.g. processing
7 billion AND gates per second. The MPC techniques for machine
learning, however, are primarily limited to the two-server model
and do not benefit from these speedups. They also only consider
security against the weaker semi-honest attackers.
In this paper, we explore privacy-preserving machine learning
in the three-server model. We emphasize this does not mean only
three data owners can participate in the computation. We envision
application scenarios where the servers are not considered the same
as data owners. Each server can be an independent party or the
representative for a subset of data owners. In other words, as long as
we guarantee that at most one of the three servers is compromised,
Session 1B: Privacy CCS’18, October 15-19, 2018, Toronto, ON, Canada 35
an arbitrary number of data owners can incorporate their data into
the framework.
A natural question is whether directly applying the new 3PC
techniques to machine learning algorithms would yield the same
speedup for server-aided privacy-preserving machine learning. Unfortunately, when using existing techniques the answer is negative.
− The first challenge is that the above-mentioned 3PC techniques
are only suitable for computation over a Z2
k ring. This is in
contrast with machine learning computation wherein both the
training data and the intermediate parameters are decimal values that cannot be natively handled using modular arithmetic.
The two most common solutions are to (i) represent decimal
values as integers where the least significant bits represent the
fractional part, and choose a large enough modulo to avoid
a wrap around. This approach fails when performing many
floating point multiplications, which is the case in standard
training algorithms (e.g. stochastic gradient descent) where
millions of sequential multiplications are performed. Moreover, a large modulo implies a more expensive multiplication
that further reduces performance, (ii) perform fixed-point multiplication using a boolean multiplication circuit inside the
MPC. Such a boolean circuit can be evaluated using either
the secret sharing based [9] or the garbled circuit based [42]
techniques, leading to a significant increase in either round or
communication cost of the solution, respectively.
− The second challenge is that most machine learning procedures require switching back and forth between arithmetic operations such as multiplication and addition, and non-arithmetic
operations such as approximate activation functions (e.g. logistic function), and piecewise polynomial functions (e.g. RELU).
The former is most efficiently instantiated using arithmetic
secret sharing while the latter should be implemented using
either binary secret sharing or Yao sharing. Standard ways
of converting between different sharing types are costly and
quickly become a major performance bottleneck.
Addressing the above challenges efficiently is even harder in
presence of an attacker who behaves arbitrarily malicious.
1.1 Our Contribution
We design and implement a general framework for privacy-preserving
machine learning in the three-server model with a single corrupted
server. Our contributions are as follows:
1. New approximate fixed-point multiplication protocols for shared
decimal numbers at a cost close to a standard secret shared
modular multiplication, in both the semi-honest and the malicious case. For a single multiplication, we find that our protocol
results in a 50× improvement in throughput and 24× improvement in latency compared to an optimized boolean circuit.
For some machine learning operations, our fixed-point technique reduces the amount of communication by 32, 768× and
requires 80× fewer rounds. See Appendix B for details.
We note that the recent fixed-point multiplication techniques
of [43] fails in the 3PC setting and certainly fails in presence
of malicious adversaries. Our new techniques are not only secure against malicious adversaries but also extend to arbitrary
number of parties.
2. A new general framework for efficiently converting between
binary sharing, arithmetic sharing [9] and Yao sharing [42] in
the three-party setting, that yields the first Arithmetic-BinaryYao (ABY) framework for the three-party case with security
against malicious adversaries (See Table 1). Many of these conversions are based on new techniques and are designed and
optimized for the first time in this paper. Our framework is of
general interest given that several recent privacy-preserving
machine learning solutions [40, 43, 46] extensively utilize ABY
conversions, and its use cases go beyond machine learning [23].
As we will see later, the techniques we develop for our ABY
framework are quite different from the original two-party
framework of [24], since secure three-party computation techniques deviate significantly from their two-party counterparts.
3. Other optimizations include a delayed re-share technique that
reduces the communication complexity for vectorized operations by several orders of magnitude and a customized 3PC
protocol for evaluating piecewise polynomial functions based
on a generalized three-party oblivious transfer primitive.
4. We instantiate all our building blocks in both the semi-honest
and the malicious setting, often requiring different techniques.
5. We implement our framework in the semi-honest setting and
run experiments for training and inference for linear, logistic
regression and neural network models. Our solutions are up
to 55000× faster than the two-party solution of SecureML [43]
when training neural networks, and our framework can do
5089 linear regression training iterations per second compared
to 3.7 iterations by [43]. Similarly, our neural network experiment can generate a handwriting prediction in 10 milliseconds
compared to the state-of-the-art Chameleon [46] protocol requiring 2700 milliseconds.
1.2 Overview of Techniques
As a brief notational introduction, we define JxK as the sharing of
a secret value x. This sharing will be one of three types: 1) JxK
A
denotes an additive secret sharing of x ∈ Z2
k over the group Z2
k .
2) JxK
B denotes a vector of k binary secret sharing which encodes
x ∈ Z2
k . 3) JxK
Y
to denote that x is secret shared using keys which
are suitable for evaluating a Yao’s garbled circuit[42].
Approximate fixed-point multiplication. Our starting point is the
semi-honest three-party secure computation protocol of Araki et
al. [9] based on replicated secret sharing in the ring Z2
k . This
protocol represents a value x ∈ Z2
k by linearly secret sharing it
into three random values x1, x2, x3 ∈ Z2
k such that sum of them
equals x. Each of the three parties is given two of these shares such
that any two parties can reconstruct x. The first challenge in the
use of replicated secret sharing is that it does not naturally support
fixed-point multiplication and the fixed-point technique introduced
in [43] fails in the three-party setting.
We design a new construction for this problem which can be
reduced to computing JxK := Jx
′
/2
d
K given Jx
′
K and d. The solution
generates an offline pair Jr
′
K, Jr K ∈ Z2
k where r = r
′
/2
d
. Given
such a truncation pair, parties can truncate a shared value Jx
′
K by
first revealing x
′ − r
′
to all and jointly computing JxK = Jr K +
(x
′ − r
′
)/2
d
. We show that with high probability, x is a correct
truncation of x
′ with at most 1 bit of error in the least significant
Session 1B: Privacy CCS’18, October 15-19, 2018, Toronto, ON, Canada 36
bit. We also show how to efficiently generate the pair Jr K, Jr
′
K using
high throughput techniques. This approach can be made secure
against malicious adversaries and it is easy to see that it generalizes
to an arbitrary number of parties.
Moreover, we show that fixed-point multiplication can be further
optimized when working with vectors and matrices. In particular,
the inner product of two n-dimensional vectors can be performed
using O(1) communication and a single truncation pair.
Three-party ABY framework. For training linear regression models, we only need to use arithmetic sharing, i.e. additive replicated
sharing over Z2
k where k is a large value such as 64. In logistic
regression and neural network training, however, we also need to
perform computation that requires bit-level operations. The best
way to perform such tasks is to either use binary sharing i.e. additive
sharing over Z2 or Yao sharing based on three-party garbling [42].
The former is more communication efficient, with only O(n) bits
communicated for a circuit with n gates, but with the number of
rounds proportional to the circuit depth, while the latter only requires 1 or 2 rounds but a higher communication cost.
We show efficient conversions between all three sharing types,
with the goal of minimizing both round and communication cost.
Please refer to Table 1 for a complete list of our conversion protocols
and their cost. When compared to the original two-party ABY
framework of [24], we reiterate that our conversion techniques,
while functionally similar, differ significantly due to large deviations
between 3PC and the less efficient 2PC techniques. To provide a
flavor of our techniques, we review our new solution for converting
an arithmetic share at the cost of a single addition circuit. Consider
JxK
A = (x1, x2, x3) where x = x1 + x2 + x3. Since we use replicated
sharing, party 1 holds both x1 and x2 and can compute x1 + x2
locally. Party 1 then inputs (x1 + x2) while party 3 inputs x3 to a
binary sharing 3PC that computes an addition circuit that computes
J(x1 +x2)K
B + Jx3K
B. Parties also locally compute binary sharing of
two random values Jy2K
B, Jy3K
B which are revealed to parties (1,2)
and parties (2,3) respectively. They then locally compute Jy1K
B =
(J(x1 +x2)K
B + Jx3K
B) ⊕ Jy2K
B ⊕ Jy3K
B and reveal it to parties (1,3).
This completes the semi-honest conversion to the binary sharing
JxK
B = (y1,y2,y3). When using a binary sharing 3PC, we use an
optimized parallel prefix adder [33] to reduce the number of rounds
from k to loд(k) at the cost of O(k log k) bits of communication.
For a Yao sharing, we present an optimization which allows the
conversion to be performed using k AND gates and a single round
by leveraging redundancies in the replicated secret sharing.
But this approach is only secure against a semi-honest adversary.
A malicious party 1 can use a wrong value in place of (x1 + x2)
which goes undetected since the addition is done locally. We can
prevent this by performing the addition inside another malicious
3PC but this would double both round and communication cost. We
introduce new techniques to avoid this extra cost in case of binary
sharing 3PC.
3PC for piecewise polynomial functions. Piecewise polynomial
functions compute a different polynomial at each input interval. Activation functions such as RELU are a special case of such functions
and many of the proposed approximations for other non-linear
Conversion Semi-honest Malicious
Comm. Rounds Comm. Rounds
JxK
A → JxK
B k + k log k 1 + log k k + k log k 1 + log k
(JxK
A, i) → Jx[i]K
B k 1 + log k 2k 1 + log k
JxK
B → JxK
A k + k log k 1 + log k k + log k 1 + log k
JbK
B → JbK
A 2k 1 2k 2
JbK
Y → JbK
B 1/3 1 2κ/3 1
JbK
B → JbK
Y 2κ/3 1 4κ/3 1
JxK
Y → JxK
A 4kκ/3 1 5kκ/3 1
JxK
A → JxK
Y 4kκ/3 1 8kκ/3 1
Table 1: Conversion costs between arithmetic, binary and
Yao representations. Communication (Comm.) is measured
in average bits per party. x ∈ Z2
k is an arithmetic value,
b ∈ {0, 1} is a binary value, κ is the computational security
parameter.
functions computed during machine learning training and prediction are also of this form [40, 43]. While our new ABY framework
enables efficient three-party evaluation of such functions, we design
a more customized solution based on an optimized construction
for the following two building blocks: aJbK
B = JabK
A (a known
by one party) and JaK
AJbK
B = JabK
A (a is shared) where b is a
bit and a ∈ Z2
k . We observe that this mixed computation can be
instantiated using a generalized three-party oblivious transfer protocol where a bit bi
is the receiver’s input and an integer a is the
sender’s input. We design new protocols for this task with both
semi-honest and malicious security that run in 1 and 2 rounds respectively and require between 2k to 4k bits. This should be of
more general interest as piecewise polynomial functions appear in
many applications and are a common technique for approximating
non-linear functions.
2 RELATED WORK
Earlier work on privacy preserving machine learning considered decision trees [39], k-means clustering [15, 35], SVM classification [56,
58], linear regression [25, 26, 49] and logistic regression [52]. These
papers propose solutions based on secure multiparty computation,
but appear to incur high efficiency overheads, as they do not take
advantage of recent advances in MPC and lack implementation.
Linear Regression. Privacy-preserving linear regression in the
two-server model was first considered by Nikolaenko et. al. [45]
who present a linear regression protocol on horizontally partitioned
data using a combination of linearly homomorphic encryption
(LHE) and garbled circuits. Gascon et. al. [29] and Giacomelli et.
al. [30] extend the results to vertically partitioned data and show
improved performance. However, they reduce the problem to solving a linear system using either Yao’s garbled circuit protocol or
an LHE scheme, which introduces a high overhead and cannot
be generalized to non-linear models. In contrast, we use the stochastic gradient descent (SGD) method for training which yields
faster protocols and enables training non-linear models such as
neural networks. Recent work of Mohassel and Zhang [43] also
uses the SGD for training, using a mix of arithmetic, binary, and
Yao sharing 2PC (via the ABY framework). They also introduce
a novel method for approximate fixed-point multiplication that
avoids boolean operations for truncating decimal numbers and
yields the state-of-the-art performance for training linear regression models. The above are limited to the two-server model and
do not extend to the three-server model considered in this paper.
Session 1B: Privacy CCS’18, October 15-19, 2018, Toronto, ON, Canada 37
Gilad-Bachrach et. al. [32] propose a framework which supports
privacy preserving linear regression. However, the framework does
not scale well due to extensive use of garbled circuits.
Logistic Regression. Privacy preserving logistic regression is considered by Wu et. al. [57]. They propose to approximate the logistic
function using polynomials and train the model using LHE. However, the complexity is exponential in the degree of the approximation polynomial, and as shown in [43] the accuracy of the model is
degraded compared to using the logistic function. Aono et. al. [8]
consider a different security model where an untrusted server collects and combines the encrypted data from multiple clients, and
transfers it to a trusted client to train the model on the plaintext.
However, in this setting, the plaintext of the aggregated data is
leaked to the client who trains the model.
Neural Networks. Privacy preserving machine learning with neural networks is more challenging. Shokri and Shmatikov [50] propose a solution where instead of sharing the data, the two servers
share the changes on a portion of the coefficients during the training.
Although the system is very efficient (no cryptographic operation
is needed at all), the leakage of these coefficient changes is not
well-understood and no formal security guarantees are obtained.
Privacy-preserving prediction using neural networks models
has also been considered in several recent works. In this setting,
it is assumed that the neural network is trained on plaintext data
and the model is known to one party who evaluates it on private
data of another. One recent line of work uses fully homomorphic
or somewhat homomorphic encryption to evaluate the model on
encrypted data [14, 18, 31, 34]. Another line of work takes advantage of a combination of LHE and garbled circuits to solve this
problem [20, 40, 48]. Riazi et al. [46] and Liu et al. [40] each proposes efficiency improvements to the ABY framework and use
it for privacy-preserving neural network inference. Chandran et
al. [20] propose a framework for automatically compiling programs
into ABY components and show how to use it to evaluate neural
network models. These constructions are all based on two-party
protocols and do not benefit from major speed-ups due to new
3PC techniques [9, 28, 42]. They also only provide security against
a semi-honest adversary. In Section 6 we give an explicit performance comparison to these frameworks and demonstrate that ours
is significantly more efficient.
Very few recent work consider privacy preserving training of
Neural Networks. Mohassel and Zhang [43] customize the ABY
framework for this purpose and propose a new approximate fixedpoint multiplication protocol that avoids binary circuits, and use it
to train neural network models. Their fixed-point multiplication
technique is limited to 2PC.
3 PRELIMINARIES
Let i ± 1 to refer to the next (+) or previous (-) party with wrap
around, i.e. party 3 + 1 is party 1, party 1-1 is party 3.
3.1 Three-party Secure Computation
3.1.1 Secret Sharing Based. Throughout our presentation the default representation of encrypted data uses the replicated secret
sharing technique of Araki, et al. [9]. A secret value x ∈ Z2
k is
shared by sampling three random values x1, x2, x3 ∈ Z2
k such
that x = x1 + x2 + x3. These shares are distributed as the pairs
{(x1, x2), (x2, x3), (x3, x1)}, where party i holds the ith pair. Such a
sharing will be denoted as JxK
A. Sometimes, for brevity, we refer
to shares of JxK
A as the tuple (x1, x2, x3), though we still mean the
replicated secret sharing where each party holds a pair of shares.
First, observe that any two out of the three parties have sufficient
information to reconstruct the actual value x. This immediately
implies that such a secret sharing scheme can tolerate up to a single
corruption. All of the protocols presented will achieve the same
threshold. We briefly review these building blocks here. To reveal
a secret shared value to all parties, party i sends xi to party i + 1,
and each party reconstructs x locally by adding the three shares.
To reveal the secret shared value only to a party i, party i − 1 sends
xi−1 to party i who reconstructs the secret locally.
Arithmetic operations can now be applied to these shares. To
add two values JxK + JyK all parties locally compute JxK + JyK =
Jx + yK := (x1 + y1, x2 + y2, x3 + y3). Addition or subtraction of a
public constant with a shared value JxK ± c = Jx ± cK can also be
done by defining the three shares of Jx ± cK as (x1 ± c, x2, x3). To
multiply a shared value JxK with a public constant c we can define
the shares of JcxK as (cx1,cx2,cx3). Note that all of these operations
are with respect to the group Z2
k . To multiply two shared values
JxK and JyK together, the parties must interactively compute JxyK.
First observe that, xy = (x1 +x2 +x3)(y1 +y2 +y3). Collectively the
parties can compute all such cross terms. We define JzK = JxyK such
that z1 := x1y1+x1y2+x2y1+α1, z2 := x2y2+x2y3+x3y2+α2, z3 :=
x3y3 + x3y1 + x1y3 + α3. For now ignore the terms α1, α2, α3 and
observe that party i can locally compute zi given its shares of JxK
and JyK. However, we require that all parties hold two out of the
three shares. To ensure this, the protocol specifies that party i sends
zi to party i − 1. We call this sending operation re-sharing. The
additional terms α1, α2, α3 are used to randomize the shares of z.
We therefore require that they be random elements of Z2
k subject
to α1+α2+α3 = 0. Each party knows exactly one of the three values.
Such a triple is referred to as a zero sharing and can be computed
without any interaction after a one time setup[9].
If, for example, party 1 wishes to construct a sharing of its private
input x, the parties first generate another zero sharing α1, α2, α3.
The shares of JxK are then defined as (x1, x2, x3) := (α1 + x, α2, α3).
The sharing of x is completed by having party i send the share xi
to party i − 1. In the case of a malicious adversary, additional care
must be taken to ensure these operations are performed correctly.
For more details on these we refer to [28].
3.1.2 Arithmetic vs. Binary sharing. Later we will make use of two
different versions of the above protocols. The first will correspond
to the case of k = 64 or some suitably large value which supports
traditional arithmetic operations such as +,-,*. We refer to this as
arithmetic sharing using the notation JxK
A. The latter case will
be for k = 1 where the binary operations ⊕, ∧ correspond to +,*.
The advantage of a binary representation is that it can be more
flexible and efficient when computing functions that can not easily
be framed in terms of modular addition and multiplication. We refer
to this as binary sharing and use the notation JxK
B.
3.1.3 Yao sharing.
Session 1B: Privacy CCS’18, October 15-19, 2018, Toronto, ON, Canada 38
Two-party sharing. In the two-party setting, Yao’s garbled circuit
protocol allows a garbler to encode a boolean function into a garbled
circuit that is evaluated by a second party, called the evaluator.
More concretely, the garbling scheme first assigns two random keys
k
0
w, k
1
w to each wire w in the circuit corresponding to values 0 and 1
for that wire. Each gate in the circuit is then garbled by encrypting
each output wire key using different combinations (according to
the truth table for that gate) of input wire keys as encryption keys.
The evaluator obtains the keys corresponding to input wires to
the circuit which enables him to decrypt exactly one ciphertext in
each gabled gate and learn the corresponding output wire key. The
evaluator can decode the final output give a translation table that
maps the circuit’s final output wire keys to their real values.
Various optimizations to this basic garbling idea have been introduced over the years, the most notable of which are the pointand-permute [11], Free-XOR [36] and the half-gate [59] techniques.
These optimizations require some modifications to how the keys
are generated. In particular, the free-XOR techniques requires that
k
1
w = k
0
w ⊕ ∆ for every wire w where ∆ is a global random (secret)
string. To use the point-and-permute technique, we need to let the
least significant bit of ∆ to be 1, i.e. ∆[0] = 1. The least significant
bit of each key (pw ⊕i = k
i
w[0]) is then referred to as the permutation
bit. As discussed in the two-party ABY framework [24], two-party
Yao’s sharing of an input bit x for wire w, can be seen as one party
holding k
0
w and ∆, while the other party holds k
x
w.
Three-party sharing. Mohassel et al. [42], extend Yao’s garbled
circuit protocol to the three-party setting with one corruption,
and obtain security against a malicious adversary with the cost
comparable to that of the semi-honest two-party Yao’s protocol.
The high-level idea is as follows. Party 1 plays the role of evaluator
and parties 2,3 play the role of garblers. The two garblers exchange
a random seed that is used to generate all the randomness and keys
required by the garbled circuit. They separately generate the garbled
circuit and send their copy to the evaluator. Since at least one
garbler is honest, one of the garbled circuits is computed honestly.
The evaluator can enforce honest garbling behavior by checking
equality of the garbled circuits.
The Yao sharing in the three-party setting, denoted by JxK
Y
,
can be seen as the evaluator holding k
x
w and the two garblers
holding (k
0
w, ∆). In the semi-honest case, a garbler shares its input bit x by sending k
0
w ⊕ x∆ to the evaluator. In the malicious
case, both garblers send commitments to both keys (permuted), i.e.
Comm(k
b
w),Comm(k
¬b
w ) to the evaluator and the garbler who is
sharing its input sends the opening for one of the commitments.
The evaluator checks that the two pairs of commitments are equal
(the same randomness is used to generate and permute them) and
that the opening succeeds. The evaluator could share its input by
performing an oblivious transfer with one of the garblers to obtain
one of the two keys. Mohassel et al. remove the need for OT by
augmenting the circuit such that each input wire corresponding
to evaluator is split into two inputs bits that XOR share the original input. The circuit first XORs these two bits (for free) and then
computes the expected function. The evaluator shares its input bit
x by generating two random bits x2 and x3 where x = x2 ⊕ x3
and sending xi to party i. The party i then shares xi as it would
share its own input, except that there is no need to permute the
commitments since party 1 knows the xis.
4 SECURITY MODEL
We use the same security model and architecture as SecureML [43]
except that we extend it to the three party case with an honest
majority and consider both semi-honest and malicious adversaries.
In particular, data owners (clients) secret share their data among
three servers who perform 3PC to train and evaluate models on
the joint data. We observe that security in this model reduces to
standard security definitions for 3PC between the three servers.
Hence, we follow the same security definitions and refer to [9] and
[28] for a formal description of these adversarial settings. Since all
our building blocks are reduced to the composition of existing 3PC
building blocks, their security is implied via standard composition
theorems [17].
5 OUR FRAMEWORK
In this section, we construct efficient three-party protocols that
form the building blocks of our protocols for training linear and
logistic regression, and neural network models. We also provide a
general framework for performing mixed computation on shared
data, i.e. an equivalent of the ABY framework [24] for the threeparty setting.
5.1 Fixed-point Arithmetic
A fixed point value is defined as a k bit integer using twos-complement
representation where the bottom d bits denote the decimal, i.e. for
positive values bit i denotes the (i − d)th power of 2. Addition
and subtraction can be performed using the corresponding integer operation since the results are expected to remain below 2
k
.
Multiplication can also be performed in the same manner but the
number of decimal bits doubles and hence must be divided by 2
d
to maintain the d decimal bit invariant.
5.1.1 Why technique of [43] fails. We start by reviewing the twoparty fixed-point multiplication protocol of [43] and show why it
fails in the three-party setting. [43] secret shares a fixed-point x
using the ring Z2
k as JxK := (x + r, −r) for some secret r ← Z2
k .
Addition and subtraction in Z2
k naturally work but multiplication
does not due to (two’s complement) division by 2
d not being supported in Z2
k . Consider having a standard sharing Jx
′
K := JyKJzK
over Z2
k and desire to compute JxK := Jx
′
/2
d
K such that when
x,y, z are interpreted as fixed-point values the quality x = yz holds
. Ideally both shares of Jx
′
K = (x
′ + r
′
, −r
′
) can be locally divided
by 2
d
to obtain two k-bit shares Jx˜K := (
x
′
1
2
d
,
x
′
2
2
d
) holding the value
x = x
′
/2
d = x˜. However, this final equality does not hold. First,
there is a bad event that the divisions by 2
d
removes a carry bit
from the first d bits that would have propagated into the d + 1th bit.
That is, at bit position d of the addition x
′
1
+ x
′
2
= (x
′ + r
′
) + (−r
′
)
mod 2
k
a carry is generated (which we have eliminated due to
separately dividing each share by 2
d
). However, this probabilistic error has a magnitude of 2
−d
and is arguably acceptable given
that fixed-point arithmetics naturally has limited precision. In fact,
Session 1B: Privacy CCS’18, October 15-19, 2018, Toronto, ON, Canada 39
[43] shows that this small error, does not have any impact on the
accuracy of trained models when d is sufficiently large.
Unfortunately, a more serious error can also be introduced due
to the values being shared in the ring modulo 2
k
combined with
the use of twos complement semantics. In particular, the desired
computation of x
′
/2
d
is with respect to two’s complement arithmetics, i.e. shift the bits of x
′ down by d positions and fill the top d
bits with the most significant bit (MSB) of x
′
. This latter step can
fail when x
′
is secret shared in Z2
k . Take for example x
′ = −2
k−1
,
which is represented in binary two’s complement as 100...000. We
then have that x
′
/2
d
is represented as 1...100...000 where there are
d + 1 leading ones. However, when secret shared, it is likely that
both shares x
′
1
, x
′
2
have zero in the MSB. As a result, when they are
divided by 2
d
, the two shares will have at least d + 1 leading zeros.
When these shares are reconstructed the result will be incorrect.
A simple case analysis shows that a necessary condition for this
error is that the MSB of x
′
is opposite of both x
′
1
, x
′
2
. That is, the
reverse of the example above can also result in this large error1
. A
clever fix to this problem is to maintain that |x
′
| < 2
ℓ ≪ 2
k where
x
′
is interpreted as a two’s complement integer. This ensures that
there is a negligible probability that the MSB of x
′
1
is the same as x
′
2
.
To see this, observe that x
′
1
:= x
′+r
′
, x
′
2
= −r
′
and that when r
′ , 0
the sign/MSB ofr
′
and −r
′
are always opposite. When x
′
is positive
the probability of x
′
1
having the same MSB as x
′
2
is the probability
that the top k −ℓ bits of r
′
are all ones and that a carry is generated
at the ℓth bit of x
′ + r
′
. Due to r
′ being uniformly distributed, the
probability that r
′ has this many leading ones is 2
ℓ−k which can be
made exponentially small for appropriately chosen ℓ, k. A similar
argument also holds when x
′
is negative.
Unfortunately the approach of truncating each share separately
does not extended to three-party secret sharing where JxK = (x +
r1 + r2, (−r1), (−r2)). The first source of error now has magnitude
2
−d+1 due to the possibility of truncating two carry bits. However, a
more serious issue is that bounding |x | < 2
ℓ no longer ensures that
the large error happens with very small probability. The necessary
condition for this error is more complex due to the possibility of two
carry bits, but intuitively, bounding |x | < 2
ℓ no longer ensures that
exactly one of the shares x1, x2, x3 will be correctly sign-extended
due to r1,r2 both being uniformly distributed and independent.
5.1.2 Our Multi-Party Fixed-Point Multiplication. We present two
new methods for performing three-party decimal multiplication/
truncation with different trade-offs. While presented in terms of
three parties, we note that our second technique can be extended
to settings with more than three parties as well.
Share Truncation Πtrunc1. Our first approach minimizes the overall communication at the expense of performing multiplication and
truncation in two rounds. The idea is to run the two-party protocol
where one party does not participate. Since we assume an honest
majority, the security still holds in the semi-honest setting. Let the
parties hold a 2-out-of-3 sharing of Jx
′
K := JyKJzK over the ring
1
In the reversed case, x
′
1
, x
′
2
both have MSB of one which overflows and is eliminated.
However, after being sign extended/divided by 2
d
, the carry results in 1 + 1 + 1 in all
higher positions, resulting in the d most significant bits being incorrectly set to one
since by assumption the MSB of x
′
is zero.
Z2
k and desire to compute JxK = Jx
′
K/2
d
. As in the two-party case,
we assume that x
′ ≪ 2
k
.
Parties begin by defining the 2-out-of-2 (x
′
1
, x
′
2
+ x
′
3
) between
party 1 and 2, and locally truncate their shares to (x
′
1
/2
d
, (x
′
2
+
x
′
3
)/2
d
). The errors introduced by the division mirror that of the
two-party case and guarantees the same correctness. The result is
defined as JxK := (x1, x2, x3) = (x
′
1
/2
d
, (x
′
2
+ x
′
3
)/2
d − r,r), where
r ∈ Z
k
2
is a random value known to parties 2,3. Note that party i
can locally compute the share xi and therefore JxK can be made
a 2-out-of-3 sharing by sending xi to party i − 1. One limitation
of this approach is that two rounds are required to multiply and
truncate.
Share Truncation Πtrunc2. The number of multiplication rounds
can be reduced back to 1 with a more sophisticated technique which
leverages preprocessing. First, let us assume we have preprocessed
the shares Jr
′
K, Jr K = Jr
′
/2
d
K where r
′ ∈ Z2
k is random. Again, let
us have computed Jx
′
K over the ring Z2
k and wish to divide it by
2
d
. To compute the sharing of x = yz/2
d we first reveal Jx
′ −r
′
K =
Jx
′
K−Jr
′
K to all parties2
. Locally, everyone can compute (x
′−r
′
)/2
d
.
Parties then collectively compute JxeK := (x
′ − r
′
)/2
d + Jr K.
This operation can be combined with the computation of Jx
′
K :=
JyKJzK and performed in a single round. Recall that standard share
multiplication is performed in two steps, 1) locally compute a 3-
out-of-3 sharing of Jx
′
K, and 2) reshare it as a 2-out-of-3 sharing.
Between steps 1 and 2, the parties can instead compute a 3-out-of3 sharing of Jx
′ − r
′
K. Step 2) can then be replaced by revealing
Jx
′ − r
′
K and defining JxK := (x
′ − r
′
)/2
d + Jr K. So the multiplication and truncation can be done in exactly one round and the
required communication is 4 messages as opposed to 3 in standard
multiplication.
There are several ways to compute the pair Jr
′
K, Jr K = Jr
′
/2
d
K.
The most immediate approach could be to use ΠTrunc1, but we
choose to use a more communication efficient method using binary
secret sharing that is also secure against malicious adversaries. First,
parties non-interactively compute the random binary share Jr
′
K
B.
This sharing is locally truncated to obtain Jr K
B by removing the
bottom d shares. To obtain the final sharing Jr
′
K
A, Jr K
A, parties
1 and 2 jointly sample and secret share the values r
′
2
,r2 ∈ Z2
k
and parties 2 and 3 sample and share r
′
3
,r3 in the same way (i.e.
generating them using pre-shared PRF keys). Parties then securely
compute subtraction binary circuits, and reveal Jr
′
1
K
B := Jr
′
K
B −
Jr
′
2
K
B−Jr
′
3
K
B and Jr1K
B := Jr K
B−Jr2K
B−Jr3K
B to party 1 and 3. The
final shares are defined as Jr
′
K := (r
′
1
,r
′
2
,r
′
3
) and Jr K := (r1,r2,r3).
This computation can be performed in parallel for all truncations in a preprocessing stage and hence has little impact on the
overall round complexity of the protocol. As a result, we choose
to optimize the overall communication (instead of rounds) of the
addition circuit with the use of an optimized ripple carry full addition/subtraction circuit using k − 1 and gates. As an additional
optimization, the computation of Jr1K can be performed in Z2
k−d
and therefore requires k − d − 1 and gates per subtraction. In the
semi-honest setting, one of the subtractions of r2,r3 can be performed locally by party 2.
2Revealing to two parties is sufficient.
Session 1B: Privacy CCS’18, October 15-19, 2018, Toronto, ON, Canada 40
Parameters: A single 2-out-of-3 (or 3-out-of-3) share Jx
′
K
A =
(x
′
1
, x
′
2
, x
′
3
) over the ring Z2
k and a integer d < k.
Preprocess:
1. All parties locally compute Jr
′
K
B ← Rand((Z2)
k
).
2. Define the sharing Jr K
B
to be the k − d most significant shares of
Jr
′
K
B
, i.e. r = r
′
/2
d
.
3. The parties compute Jr
′
2
K
B
, Jr
′
3
K
B ← Rand((Z2)
k
) and
Jr2K
B
, Jr3K
B ← Rand((Z2)
k−d
). r
′
2
, r2 is revealed to party 1,2 and
r
′
3
, r3 to parties 2,3 using the RevealOne routine.
4. Using a ripple carry subtraction circuit, the parties jointly compute
Jr
′
1
K
B
:= Jr
′
K
B − Jr
′
2
K
B − Jr
′
3
K
B
, Jr1K
B
:= Jr K
B − Jr2K
B − Jr3K
B
and
reveal r
′
1
, r1 to parties 1,3.
5. Define the preprocessed shares as Jr
′
K
A := (r
′
1
, r
′
2
, r
′
3
), Jr K
A :=
(r1, r2, r3).
Online:
1. The parties jointly compute Jx
′−r
′
K
A and then compute (x
′−r
′
) :=
RevealAll(Jx − r
′
K
A).
2. Output JxK
A := Jr K
A + (x
′ − r
′
)/2
d
.
Figure 1: Single round share truncation protocol Πtrunc2.
Another advantage of this second protocol is its compatibility
with the malicious setting. When the computation of Jx
′
K = JyKJzK
is performed initially all parties hold a 3-out-of-3 sharing of Jx
′
K
and then reshare this to be a 2-out-of-3 sharing by sending x
′
i
to
party i − 1. Additionally, a small proof πi
is sent demonstrating
that x
′
i
is indeed the correct value. We propose that this x
′
i
and the
proof is still sent along with the reveal of Jx
′ − r
′
K which can be
composed into a single round. However, it is possible for party i to
send the correct message (xi
, πi) to party i−1 but send the incorrect
reveal message xi − ri to party i + 1. To ensure that such behavior
is caught, parties i − 1 and i + 1 should maintain a transcript of all
xi −ri messages from party i and compare them for equality before
any secret value is revealed. For a more detailed description of the
protocol and a security analysis, we refer the reader to Section D.2.
Public Operations. One useful property of an additively secret
shared value JxK
A is that c + JxK
A, JxK
A − c,cJxK
A for any signed
integerc can be computed locally. When x is a fixed-point value, addition and subtraction naturally work so long as c is also expressed
as a fixed-point value. For multiplication and a two’s complement
integerc, the standard multiplication with a public value still works.
When c is a fixed point value, the result must be divided by 2
d using
the semi-honest Πtrunc1 or malicious Πtrunc2 protocol to obtain a
sharing JcxK
A with d decimal bits. One byproduct of fixed-point
multiplication is that division by a public value c is now supported
very efficiently , i.e. JxK
A/c = c
−1
JxK
A.
5.2 Vectorized Multiplication
For many machine learning algorithms the primary computation is
matrix multiplication. This in turn can be implemented by a series
of inner products, one for each row-column pair of the first and
second matrices. Inner product is defined as x® · ®y :=
Ín
i=1
xiyi
,
where x®,y® ∈ (Z2
k )
n
are vectors of n elements. A naive solution
would require n independent multiplication protocols and O(n)
communication. We show how this can be optimized to only require
communicating O(1) ring elements, and computing only one preprocessed truncation-pair Jr
′
K, Jr K.
Recall from the previous section that semi-honest decimal multiplication is performed in two steps by first revealing the 3-out-of-3
sharing Jz
′+r
′
K = JxKJyK+Jr
′
K. The final product is then computed
as JzK := (z
′+r
′
)/2
d −Jr K. Observe that the primary non-linear step
here is the computation of JxKJyK after which a series of local transformations are made. As such, the computation of the inner product
can be written as Jx®K · Jy®K := reveal((Ín
i=1
Jxi KJyi K) + Jr
′
K)/2
d −
Jr K. Here, all parties locally compute a 3-out-of-3 sharing of each
Jxi KJyi K which are summed, masked, truncated, and reshared as
a 2-out-of-3 sharing of the final result. As a result, only a single
value is reshared. One additional benefit of this approach is that
the truncation induces an error of 2
−d with respect to the overall
inner produce, as opposed to individual multiplication terms, resulting in a more accurate computation. More generally, any linear
combination of multiplication terms can be computed in this way
where the parties communicate to reshare and truncate only after
computing the 3-out-of-3 secret share of the linear combination (as
long as the final result does not grow beyond the 2
ℓ bound). We
discuss vectorized multiplication against malicious adversaries in
Appendix C.
5.3 Share Conversions
For many machine learning functions, it is more efficient to switch
back an forth between arithmetic (multiplications and addition)
and binary (non-linear activation functions, max-pooling, averages,
etc.) operations. In such cases, it is necessary to convert between
different share representations. We design new and optimized protocols that facilitate efficient conversions between all three types
of sharing: arithmetic, binary and Yao. We elaborate on these next.
See Table 1 for the cost of various conversions.
Bit Decomposition, JxK
A → Jx®K
B
. We begin with bit decomposition where an arithmetic sharing of x ∈ Z2
k is converted to a
vector of secret shared bits x[1], ..., x[k] ∈ {0, 1} such that x = Ík
i=1
2
i−1
x[i]. The basic idea is that parties use their shares of
JxK
A = (x1, x2, x3) as input to a boolean circuit that computes
their sum. But we introduce several optimizations that significantly
reduce rounds of communication and the communication complexity of this approach. Observe that JxK
A = (x1, x2, x3) can be converted to Jx1K
B := (x1, 0, 0), Jx2K
B := (0, x2, 0), Jx3K
B := (0, 0, x3)
with no communication3
. Naively using the textbook ripple-carry
full adder (RCFA) circuit would require 2k rounds to compute
RCFA(RCFA(x1, x2), x3) when performing 3PC on binary shared
values. To avoid the high round complexity which becomes the
bottleneck in our implementations, we first employ a parallel prefix
adder (PPA) [33] which takes two inputs and computes the sum using a divide and conquer strategy, totaling log k rounds and k log k
gates. Once again, doing this naively would require two addition
circuits. We show how to keep the cost close to that of a single PPA
in both the semi-honest and the malicious setting, hence reducing both the round (only for binary sharing) and communication
complexity by a factor of two.
3
In general this can be insecure due to the possibility of leaking information through
revealing linear combinations. However, due to subsequent randomization and how
x1, x2, x3 are distributed, this operation is secure.
Session 1B: Privacy CCS’18, October 15-19, 2018, Toronto, ON, Canada 41
First observe that the computation of x1 + x2 + x3 can be reduced to computing 2c + s by executing k independent full adders
FA(x1[i], x2[i], x3[i − 1]) → (c[i],s[i]) for i ∈ {0, ..., k − 1} where
c[i],s[i] denote the ith bit of the bitstringsc and s. It is worth noting
that traditionally, full adders are chained together to compute the
addition of two bits and a carry in, however, here we used them to
reduce 3 operands (i.e. x1, x2, x3) to 2 (i.e. c, s) while using a single
round of communication as opposed to k. The final result can then
be computed as 2JcK
B + JsK
B using a parallel prefix adder. Alternatively, in the semi-honest setting Party 2 can provide (x1 + x2) as
private input to a 3PC which computes JxK
B := Jx1 + x2K
B + Jx3K
B.
In both settings, this results in a total of 1 + log k rounds, which
is significantly better than a factor of two increase in rounds and
communication.
Bit Extraction, JxK
A → Jx[i]K
B
. A special case of bit decomposition is when a single bit of the share JxK
A should be decomposed
into a binary sharing, e.g. the ith bit Jx[i]K
B. This case can be simplified such that only O(i) and gates and O(log i) rounds are required.
While relatively simple, this optimization removes all the unnecessary gates from the parallel prefix adder. As a result, the circuit
logic only requires 2i and gates. We use this optimization in our
implementation. For brevity, we refer the reader to inspect [33] to
deduce exactly which gates can be removed.
Bit Composition, JxK
B → JxK
A. It can also be required to convert a k bit value in the binary secret share representation to
an arithmetic secret share representation. Effectively, we use the
same circuit as the bit decomposition with the order of operations
slightly reversed. First, parties 1,2 input a random share J−x2K
B
and parties 2,3 input a random share J−x3K
B. These will be part
of the final arithmetic sharing and therefore the former can be
known to parties 1,2 and the latter can be known to parties 2,3.
Jx2K
B can be generated by having parties 1,2 hold three PRF keys
κ1,κ2,κ3 and party 3 hold κ2,κ3. The share is then defined as
Jx2K
B := (F (κ1, N), F (κ2, N), F (κ3, N)) where N denotes a public
nonce. Jx3K
B can be defined in a similar way with the roles shifted.
The parties compute FA(Jx[i]K
B, J−x2[i]K
B, J−x3[i]K
B) → (Jc[i]K
B,
Js[i]K
B) for i ∈ {1, ..., k − 1} and then using a parallel prefix adder
Jx1K
B := 2JcK
B+JsK
B. In the semi-honest setting, this can be further
optimized by having party 2 provide (−x2 − x3) as private input
and compute Jx1K
B := JxK
B + J−x2 − x3K
B using a parallel prefix
adder. Regardless, x1 is revealed to parties 1,3 and the final sharing
is defined as JxK
A := (x1, x2, x3). Overall, the conversion requires
1 + log k rounds and k + k log k gates.
Bit Injection, JxK
B → JxK
A. Another special case can often occur when a single bit x encoded in a binary sharing needs to be
promoted to an arithmetic sharing JxK
A. For ease of presentation,
we defer the explanation of this technique to Section 5.4 where
a generalization of it to efficiently compute aJxK
B → JaxK
A is
presented.
Joint Yao Input. Recall that in Yao sharing of a bit x, party 1
(evaluator) holds k
x
x while the other two parties hold k
0
x ∈ {0, 1}
κ
and a global random ∆ ∈ {0, 1}
κ
such that k
1
x
:= k
0
x ⊕ ∆. A useful
primitive for conversions to and from Yao shares is the ability for
two parties to provide an input that is known to both of them. For
example, parties 1,2 know a bit x and wish to generate a sharing of
JxK
Y
. In the semi-honest setting, this is trivial as party 2 can locally
generate and send JxK
Y
to party 1 (who uses it to evaluate a garbled
circuit). However, in the malicious setting party 1 needs to verify
that JxK
Y
actually encodes x without learning ∆. In the current example, party 3 can be used to allow party 1 to check the correctness
of the sharing by having party 2 and 3 send Comm(k
0
x
),Comm(k
1
x
)
generated using the same randomness shared between them (party
2 can send a hash of the commitments). Party 1 verifies that both
parties sent the same commitments and that Comm(k
x
x
) decommits
to k
x
x
. This interaction requires two commitments, one decommitment and at most one round per input bit. In the case that x is
known to parties 1,3 the roles of 2,3 above can simply be reversed.
When sharing many input bits (n ≫ λ, for λ a statistical security parameter), we show that the number of commitments can be
capped at 2λ. After receiving the input labels k
x1
x1
, ..., k
xn
xn
(without
commitments) and before revealing any secret values which are
dependent on these input labels, party 1 computes λ random linear
combinations k
c1
c1
, ..., k
cλ
cλ
of k
x1
x1
, ..., k
xn
xn
in (Z2)
λ with coefficients
in Z2. Parties 2, 3 receive the combination from party 1 and both
compute the λ combinations of k
0
x1
, ..., k
0
xn
to obtain k
0
c1
, ..., k
0
cλ
. Using the same randomness, parties 2,3 send Comm(k
0
ci
),Comm(k
1
ci
=
k
0
ci
⊕ ∆) for i ∈ {1, ..., λ} to party 1 (one party can send hash of the
commitments instead). Party 1 verifies that the two sets of commitment are the same and that Comm(k
ci
ci
) decommits to k
ci
ci
for all i.
The probability that party 1 received an incorrect label and this test
passes is 2
−λ
.
In the other case were 2,3 both know x, it is possible to generate
JxK
Y with no communication. Using a shared (among all three
parties) source of randomness, the parties locally sample k
x
x ←
{0, 1}
κ
. Parties 2,3 can then define k
0
x
:= k
x
x ⊕ (x∆).
Yao to Binary, JxK
Y → JxK
B
. As observed in [24], the least significant bit of the keys (permutation bit) form a two-party sharing
of x. i.e. x ⊕ px = k
x
x
[0] where px = k
0
x
[0]. Note that party 3 also
knows px since it holds k
0
x
[0]. Parties 1 and 2 locally generate another random bit r and party 1 sends k
x
x
[0] ⊕ r = x ⊕ px ⊕ r to
Party 3. This yields the following three-party replicated sharing
JxK
B = (x ⊕ px ⊕ r,r,px) in a single round and with one bit of
communication.
In the malicious setting, the bit x ⊕ b ⊕ r that party 1 sends to
party 3 must be authenticated to ensure that party 1 indeed uses
b = px. Parties 1 and 2 sample k
r
r ← {0, 1}
κ
and party 2 sends
k
0
r
:= k
r
r ⊕ (r∆) to party 3. Both party 2 and 3 send commitments
C0 = Comm(k
px
y
),C1 = Comm(k
px
y
) to party 1 where k
0
y
:= k
0
x ⊕k
0
r
.
Party 1 sends k
x ⊕r
y
:= k
x
x ⊕ k
r
r
to party 3 who verifies that it is in
the set {k
0
y
, k
1
y
}. Party 1 also verifies that the commitment Cpx⊕x ⊕r
decommits to k
x ⊕r
y
and that both copies of C0,C1 sent from party
2 and 3 are the same. Note that px ⊕ x = k
x
x
[0]. The parties can
then compute the three-party sharing JxK
B = (x ⊕ px ⊕ r,r,px).
Observe that party 3 computes x ⊕ px ⊕ r as k
x ⊕r
y
[0] ⊕ pr
. In total,
this conversion takes two rounds of communication, however, the
final sharing JxK
B is computable after a single round. It is therefore
ok to use JxK
B after the first round so long as dependent values are
not revealed in that round. In the event that the verification steps
fail, the parties should abort.
Session 1B: Privacy CCS’18, October 15-19, 2018, Toronto, ON, Canada 42
Binary to Yao, JxK
B → JxK
Y
. Let JxK
B = (x1, x2, x3). Parties
jointly input the shares Jx1K
Y
, Jx2K
Y
, Jx3K
Y using the procedure
discuss earlier for joint Yao input. The final share can then be computed using a garbled circuit that computes XOR of the three values,
i.e. JxK
Y
:= Jx1K
Y ⊕ Jx2K
Y ⊕ Jx3K
Y
. With the free-XOR technique,
this does not require any communication between the parties and
can be computed locally by party 1. In the semi-honest setting, this
can be further optimized by observing that party 2 knows x2 and
x3. They can therefore locally compute x2 ⊕ x3 and send Jx2 ⊕ x3K
Y
to party 1 who locally computes JxK
Y
:= Jx1K
Y ⊕ Jx2 ⊕ x3K
Y
.
Yao to Arithmetic, JxK
Y → JxK
A. To convert x ∈ Z2
k from Yao
to arithmetic sharing, we could first switch from Yao to Binary and
then perform the bit composition or bit injection (in case of a single
bit) protocol, but since the inputs are in form of Yao sharings, we
choose to use a garbled circuit 3PC for the CRFA addition circuit.
Parties 1, 2 sample x2 ← Z2
k and parties 2, 3 sample x3 ← Z2
k
and jointly input them using the procedures above. Then, using
a garbled circuit parties compute Jx1K
Y
:= JxK
Y − Jx2K
Y − Jx3K
Y
and reveal this to parties 1 and 3. JxK
A = (x1, x2, x3) forms the new
arithmetic sharing of x. This requires communicating k joint input
bits (only x2) and 2k garbled gates. In the semi-honest setting this
can be further optimized by having party 3 locally compute x2 +x3
and provide it as private input to Jx1K
Y
:= JxK
Y − Jx2 + x3K
Y
. As a
result, the cost of the garbled circuit is reduced by a factor of 2.
Arithmetic to Yao, JxK
A → JxK
Y
. Parties jointly input the shares
of JxK
A = (x1, x2, x3) to generate Jx1K
Y
, Jx2K
Y
, Jx3K
Y
. A garbled
circuit can then be used to generate JxK
Y
:= Jx1K
Y + Jx2K
Y + Jx3K
Y
.
In the semi-honest setting this can be optimized by having party
2 locally compute x2 + x3 and send party 1 the sharing Jx2 + x3K
Y
who computes and the final sharing JxK
Y
:= Jx1K
Y + Jx2 + x3K
Y
.
5.4 Computing JaK
AJbK
B = JabK
A
While converting between share representations allows for arbitrary combination of shares to be used together, it can be more
efficient to provide custom protocols to directly perform the computation on mixed representation. To this end, we provide a mixedprotocol for performing JaK
AJbK
B = JabK
A. This operation is needed
repeatedly to compute piecewise linear or polynomial functions
that are commonly used to approximate non-linear activation functions in training logistic regression and neural network models.
5.4.1 Semi-honest Security.
Three-Party OT. We begin by providing an oblivious transfer
protocol in the three-party honest majority setting. As with the
two-party 1-out-of-2 OT case, we have a sender and a receiver. To
this, we add a third party called a helper who receives no output and
knows the receiver’s choice bit. The functionality for the (sender,
receiver, helper) can be expressed as ((m0,m1),c,c) 7→ (⊥,mc , ⊥).
Several previous work consider multi-party OT [19, 27, 37, 44],
but to the best of our knowledge we are the first to consider this
particular functionality with an honest majority.
Our approach is extremely efficient with information-theoretic
security. The sender and helper first sample two random strings
w0,w1 ← {0, 1}
k known to both of them. The sender masks the
messages as m0 ⊕ w0,m1 ⊕ w1 and sends them to the receiver. The
helper knows that the receiver desires the message mc . As such
the helper sends wc to the receiver who can then recover mc . This
procedure requires sending 3 messages in a single round.
Computing aJbK
B = JabK
A. The simplest case is the multiplication of a public value a ∈ Z2
k known to party 1 with a shared
bit b ∈ {0, 1}. First, party 3 (the sender) samples r ← Z2
k and
defines two messages, mi
:= (i ⊕ b1 ⊕ b3)a − r for i ∈ {0, 1}.
Party 2 (the receiver) defines his input to be b2 in order to learn
the message mb2
= (b2 ⊕ b1 ⊕ b3)a − r = ba − r. Note that party
1 (the helper) also knows the value b2 and therefore the three
party OT protocol above can be used here. The parties then use
locally generated replicated zero sharing (s1,s2,s3) to compute
JcK = JabK = (s1 + r, ab − r + s3,s3). However, to make this a
valid 2-out-of-3 secret sharing, c2 = ab − r + s3 must be sent to
party 1. This would result in a total of two rounds of communications. Alternatively, the three-party OT procedure can be repeated
(in parallel) with again party 3 playing the sender with inputs
(i + b2 + b3)a − r + s3 for i ∈ {0, 1} so that party 1 (the receiver)
with input bit b2 learns the message c2 (not mb2
) in the first round,
totaling 6k bits and 1 round.
Computing JaK
AJbK
B = JabK
A. In the semi-honest setting, it is
sufficient to run the aJbK
B = JabK
A procedure twice in parallel.
Crucial in this technique is to observe that a in the computation
above need not be public. That is, party 1 could have privately
chosen the value of a. Leveraging this, observe that the expression
can be written as JaKJbK
B = a1JbK
B + (a2 + a3)JbK
B. Party 1 acts
as the sender for the first term and party 3 for the second term. In
total 4k bits per party are communicated over 1 round.
5.4.2 Malicious Security.
Computing aJbK
B = JabK
A. Unfortunately, our semi-honest approach fails in the malicious setting primarily due to party 1 being
free to choose the value a it inputs to the OT, arbitrarily. We avoid
this issue by first performing bit injection on b. That is, we compute
JbK
B → JbK
A and then aJbK
A = JabK
A. As performed in Section 5.3,
the parties can locally compute shares Jb1K
A, Jb2K
A, Jb3K
A where
JbK
B = (b1,b2,b3). We can now emulate the XOR of these values
within an arithmetic circuit by computing Jb1 ⊕ b2K
A = JdK
A :=
Jb1K
A + Jb1K
A − 2Jb1K
AJb2K
A followed by JbK
A := Jd ⊕ b3K
A. This
conversion requires sending 2k bits over two rounds. The final
result can then be computed as JabK
A := aJbK
A where each party
locally multiplies a by its share of b. Compared to performing the
generic bit decomposition from Section 5.3, this approach reduces
the round complexity and communication by O(log k).
Computing JaK
AJbK
B = JabK. Once again, the bit injection procedure can be repeated here to convert JbK
B to JbK
A followed by
computing JaK
AJbK
A using the multiplication protocol.
5.5 Polynomial Piecewise Functions
This brings us to our final building block, the efficient computation
of polynomial piecewise functions. These functions are constructed
as a series of polynomials. Let f1, ..., fm denote the polynomials
with public coefficients and −∞ = c0 < c1 < ... < cm−1 < cm = ∞
such that, f (x) = fi(x) where ci−1 < x ≤ ci
. Our technique for
computing f is to first compute a vector of secret shared values
Session 1B: Privacy CCS’18, October 15-19, 2018, Toronto, ON, Canada 43
b1, ...,bm ∈ {0, 1} such that bi = 1 ⇔ ci−1 < x ≤ ci
. f can then be
computed as f (x) =
Í
i bi fi(x).
Let us begin with the simple case of computing JxK < c. This
expression can then be rewritten as JxK
A − c < 0. Recall that x is
represented as a two’s complement value and therefore the most
significant bit (MSB) of Jx−cK denotes its sign, i.e. 1 iff x−c < 0. This
implies that the inequality can be computed simply by extracting
the MSB. This in turn can be computed by taking the Section 5.3 bit
extraction of Jx−cK to obtain binary shares of JbK
B := Jmsb(x−c)K
B.
When the bit-extraction is performed with binary secret sharing,
the round complexity will be O(log k) while the communication is
O(k) bits. On the other hand, when the conversion is performed
using a garbled circuit, the round complexity decreases to 1 with
an increase in communication totaling O(κk) bits. Each bi
is the
logical AND of two such shared bits which can be computed within
the garbled circuit or by an additional round of interaction when
binary secret sharing is used.
Each of the fi functions are expressed as a polynomial fi(JxK) =
ai,j JxK
j + ... +ai,1JxK +ai,0 where all ai,l are publicly known constants. When fi
is a degree 0 polynomial the computation bi fi(JxK)
can be optimized as ai,0Jbi K
B using the techniques from Section 5.4.
In addition, when the coefficients of fi are integer, the computation
of ai,l
JxK
l
can be performed locally, given JxK
l
. However, when ai,j
has a non-zero decimal, an interactive truncation will be performed
as discussed in Section 5.1.
6 EXPERIMENTS
We demonstrate the practicality of our proposed framework with
an implementation of training linear regression, logistic regression
and neural network models and report on their efficiency. An analytical comparison to other three party protocols is also given in
Appendix B. We defer a detailed explanation of the implemented
machine learning algorithms to Appendix A. The implementation
was written in C++ and builds on the primitives provided by the
libOTe library [47], and the linear algebra library Eigen [2]. All
arithmetic shares are performed modulo 2
64. Due to the significant
development time required to implement the maliciously secure
protocols ([28] has no publicly available code), we have only implemented and report performance numbers for the semi-honest
variant of our framework. This does not hinder comparison with
prior work since they primarily focus on semi-honest protocols (in
fact our work is the first maliciously secure protocol for machine
learning).
Experimental Setup. We perform all benchmarks on a single
server equipped with 2 18-core 2.7Ghz Intel Xeon CPUs and 256GB
of RAM. Despite having this many cores, each party performs their
computation on a single thread. Using the Linux tc command we
consider two network settings: a LAN setting with a shared 10Gbps
connection and sub-millisecond RTT latency and a WAN setting
with a 40Mbps maximum throughput and 40ms RTT latency. The
server also employs hardware accelerated AES-NI to perform fast
random number generation. We note that other protocols are run
on different hardware (fewer cores). However, this should have a
minimal impact on performance since our implementation is almost
entirely IO bound and each party only utilizes a single core.
Datasets. Our work is primarily focused on the performance of
privacy-preserving machine learning solutions. As a result, in some
benchmarks we choose to use synthetic datasets which easily allow
for a variable number of training examples and features and better
demonstrate the performance of our training. We emphasize that
we do NOT use synthetic data to measure accuracy of the trained
models. In fact, our training algorithms (i.e. if our protocols were
run honestly) are functionality equivalent to those of [43], and we
refer the reader to their paper for precise accuracy measurements on
real datasets. We also consider the widely used MNIST dataset [5]
which contains a set of 784 = 28 × 28 pixel images of handwritten
numbers where the task is to output the correct number.
6.1 Linear Regression
We begin with the gradient descent protocol for learning linear
regression models as detailed in Section A.1. The computation of
this protocol is easy given our framework. At each iteration, a public
and randomly selected subset Xj of the dataset is sampled and the
model is updated as w := w − α
1
B
X
T
j
× (Xj × w − Yj). We report
performance in terms of iterations per second as opposed to endto-end running time. This is primarily done to present the results
in a way that can be easily generalized to other tasks. Figure 2
presents the throughput of our linear regression training protocol
compared to [43] and is further parameterized by the number of
features D ∈ {10, 100, 1000} and the size of the mini-batch B ∈
{128, 256, 512, 1024}.
The columns labeled “Online" denote the throughput of the input dependent computation while the columns labeled “Online +
Offline" denote the total throughput including the pre-processing
phase that is input independent. Our throughput is strictly better
than that of [43]. In the LAN setting our online throughput is between 1.5 to 4.5 times greater than [43] which is primarily due to a
more efficient multiplication protocol. For example, [43] requires
preprocessed matrix beaver triples along with a more complex
opening procedure. While our protocol’s online throughput is considerably higher than [43], our main contribution is an offline phase
that is orders of magnitude more efficient. Overall, the throughput
of our protocol becomes 200 to 1000 times greater than [43] due
to the elimination of expensive beaver triples. The only operation
performed in our offline phase is the generation of truncated shares
Jr K, Jr/2
d
K which requires computing the addition circuit which
can be made extremely efficient.
In the WAN setting, our protocol is also faster than [43] by
roughly a factor of 2 in the online phase and 10 to 1000 times faster
when the overall throughput is considered. As before, the offline
phase has a minimal impact on the overall throughput, consuming
roughly 10 percent of the computation. This is in drastic contrast
with [43] where the majority of the computation is performed in
the offline phase.
Our protocol also achieves a smaller communication overhead
compared to [43]. The communication complexity for the online
phase of both protocols is effectively identical. Each party performs
two matrix multiplications where shares of size B and D are sent.
However, in the offline phase, [43] presents two protocols where
the first requires O(BD) exponentiations and D + B elements to be
Session 1B: Privacy CCS’18, October 15-19, 2018, Toronto, ON, Canada 44
communicated per iteration. Our protocol requires no exponentiations and achieves the same asymptotic communication overhead
but with better constants. Due to a large number of exponentiations
required by their protocol, [43] also propose a second technique
based on oblivious transfer which improves on their computational
efficiency at the expense of an increased communication of O(BDκ)
elements per iteration. In the LAN setting, the computationally
efficient oblivious transfer protocol achieves higher throughput
than their exponentiation based approach. However, in the WAN
setting, the communication overhead is their bottleneck and the
exponentiation-based protocol becomes faster. In Figure 2, we always report and compare against the variant with the best throughput. Regardless of which technique of [43] is used, the offline of
our protocol is computationally more efficient and requires less
communication.
Due to the offline phase of [43] having such a low throughput, the
authors proposed an alternative client-aided protocol where a semihonest client locally performs the offline phase and distributes the
resulting shares between the two servers. If we relabel an assisting
client as the third server, this variant of their protocol has a similar
security model as ours with the notable exception that there is no
natural way to extend it to the malicious setting. The advantage
of adding a third party is that the throughput of the offline phase
can be significantly improved. However, it is still several orders
of magnitude slower than our preprocessing for a few reasons.
First, their protocol requires that random matrices of the form
R1 × R2 = R3 be generated by the third party, where R1 is a D × B
dimension matrix. These have to be constructed and sent to the two
other parties resulting in high communication of O(DB) elements.
On the other hand, our preprocessing simply requires the sending
of O(B +D) elements. Considering that D and B can be in the order
of 100s this results in a significant reduction in computation and
communication. Moreover, our overall protocol is already faster
than the online phase of [43] and therefore is faster regardless of
which preprocessing technique is used.
6.2 Logistic Regression
Our next point of comparison is with regards to the training of
logistic regression models. At each iteration the update function is
w := w − α
1
B
X
T
j
× (f (Xj × w) − Yj). This update function is more
complex compared to linear regression due to the need to compute
the logistic function f at each iteration. Our protocol approximates
f using a piecewise linear function which requires switching to and
from a binary secret sharing scheme. While relatively efficient computationally, it does have the negative consequence of increasing
the round complexity of the protocol by 7 per iteration. In the LAN
setting where latency is small, this has little impact. For example,
given a batch size of B = 128 and dimension D = 10, our protocol can perform 2251 iterations per second using a single thread.
Moreover, increasing the dimension to D = 100 only decreases the
throughput to 1867 iterations per second. When compared to [43],
this represents an order of magnitude improvement in running time.
This difference is primarily attributed to [43] using garbled circuits
which requires fewer rounds at the cost of increased bandwidth and
more expensive operations. For both linear and logistic regression,
the offline phase is identical. As such, our extremely efficient offline
phase results in a 200 to 800 times throughput speedup over [43].
In the WAN setting, our increased round complexity begins to
degrade our performance to the point that [43] is almost as fast as
our protocol during the online phase. For B = 128 and D = 100 our
protocol performs 4.1 iterations per second while [43] achieves 3.1
iterations per second. However, as the batch size increases (resulting
in a better rate of convergence), our protocol scales significantly
better then [43]. Consider a batch size of B = 1024 where our
protocol achieves 3.99 iterations per second while [43] achieves
0.99 iterations per seconds. When including the offline phase, our
protocol receives almost no slowdown (5%) while [43] is between
2 and 100 times slower, resulting in a 3 to 300 times difference in
overall throughput when the protocols are compared.
Our protocol also achieves a smaller communication overhead
when approximating the logistic function. Primarily this is due to
our protocol using a binary secret sharing and our new binaryarithmetic multiplication protocol from Section 5.4. In total, our
protocol requires each party to send roughly 8Bk bits while [43],
which uses garbled circuits, requires 1028Bk bits. The main disadvantage of our approach is that it requires 7 rounds of interaction
compared to 4 rounds by [43]. However, at the cost of less than
double the rounds, our protocol achieves a 128 times reduction in
communication which facilitates a much higher throughput in the
LAN or WAN setting when there is a large amount of parallelism.
6.3 Neural Networks
Our framework particularly stands out when working with neural
networks. The first network we consider (NN) is for the MNIST
dataset and contains three fully connected layers consisting of 128,
128, and 10 nodes respectively. Between each layer, the ReLU activation function is applied using our piecewise polynomial technique.
When training the NN network, our implementation is capable of
processing 10 training iterations per seconds, with each iteration
using a batch size of 32 examples. Proportionally, when using a
batch size of 128, our protocol performs 2.5 iterations per second.
An accuracy of 94% can be achieved in 45 minutes (15 epochs).
Compared to [43], with the same accuracy, our online running time
is 80× faster while the overall running time is 55, 000× faster.
We also consider a convolutional neural net (CNN) with 2 hidden
layers as discussed [46]. This network applies a convolutional layer
which maps the 784 input pixels to a vector of 980 features. Two
fully connected layers with 100 and 10 nodes are performed with the
ReLU activation function. For a detailed depiction, see [46, Figure
3]. For ease of implementation, we overestimate the running time
by replacing the convolutional kernel with a fully connected layer.
Our protocol can process 6 training iterations per second with a
batch size of 32, or 2 iterations per second with a batch size of 128.
We estimate, if the convolutional layer was fully implemented, that
our training algorithm would achieve an equivalent accuracy as a
plaintext model [46] of 99% in less than one hour of training time.
6.4 Inference
We also benchmark our framework performing machine learning
inference using linear regression, logistic regression, and neural
network models, as shown in Figure 3. For this task, a model that has
Session 1B: Privacy CCS’18, October 15-19, 2018, Toronto, ON, Canada 45
Setting Dimension Protocol
Batch Size B
Online Throughput Online + Offline Throughput
128 256 512 1024 128 256 512 1024
LAN
10 This 11764 10060 7153 5042 11574 9803 6896 4125
[43] 7889 7206 4350 4263 47 25 11 5.4
100 This 5171 2738 993 447 5089 2744 1091 470
[43] 2612 755 325 281 3.7 2.0 1.1 0.6
1000 This 406 208 104 46 377 200 100 46
[43] 131 96 45 27 0.44 0.24 0.12 0.06
WAN
10 This 24.6 24.5 24.3 23.9 20.8 20.7 20.6 20.3
[43] 12.4 12.4 12.4 12.4 2.4 1.6 0.88 0.50
100 This 24.5 24.1 23.7 23.3 20.7 20.4 20.1 19.4
[43] 12.3 12.2 11.8 11.8 0.63* 0.37* 0.19* 0.11*
1000 This 22.2 20.2 17.5 12.6 19.3 17.9 16.5 11.6
[43] 11.0 9.8 9.2 7.3 0.06* 0.03* 0.02* 0.01*
Figure 2: Linear Regression performance measured in iterations per second (larger
= better). Dimension denotes the number of features while batch size denotes number
of samples used in each iteration. WAN setting has 40ms RTT latency and 40 Mbps
throughput. The preprocessing for [43] was performed either using OT or the DGK cryptosystem with the faster protocol being reported above. The * symbol denotes that the
DGK protocol was performed.
Model Protocol Batch Size Running Time (ms) Comm.
Online Total (MB)
Linear
This 1 0.1 3.8 0.002
100 0.3 4.1 0.008
SecureML [43] 1 0.2 2.6 1.6
100 0.3 54.2 160
Logistic
This 1 0.2 4.0 0.005
100 6.0 9.1 0.26
SecureML [43] 1 0.7 3.8 1.6
100 4.0 56.2 161
NN This 1 3 8 0.5
SecureML [43] 1 193 4823 120.5
CNN
This* 1 6 10 5.2
Chameleon [46] 1 1360 2700 12.9
MiniONN [40] 1 3580 9329 657.5
Figure 3: Running time and communication of privacy preserving inference (model evaluation) for linear, logistic and neural network models in the LAN setting (smaller = better). [43]
was evaluated on our benchmark machine and [40, 46] are cited
from [46] using a similar machine. The models are for the MNIST
dataset with D = 784 features. NN denotes neural net with 2
fully connected hidden layers each with 128 nodes along with
a 10 node output layer. CNN denotes a convolutional neural net
with 2 hidden layers, see [46] details. * This work (over) approximates the cost of the convolution layers with an additional fully
connected layer with 980 nodes.
already been trained is secret shared between the parties along with
an unlabeled feature vector for which a prediction is desired. Given
this, the parties evaluate the model on the feature vector to produce
a prediction label. We note that inference (evaluation) for all three
types of models can be seen as a special case of training (e.g. one
forward propagation in case of neural networks) and hence can be
easily performed using our framework. Following the lead of several
prior works [40, 43, 46], we report our protocol’s performance on
the MNIST task. The accuracy of these models ranges from 93%
(linear) to 99% (CNN).
When evaluating a single input using a linear model, our protocol
requires exactly one online round of interaction (excluding the sharing of the input and reconstructing the output). As such, the online
computation is extremely efficient, performing one inner product
and communicating O(1) bytes. The offline preprocessing, however,
requires slightly more time at 3.7 ms along with the majority of the
communication. The large difference between online and offline is
primarily due to the fact that our offline phase is optimized for high
throughput as opposed to low latency. Indeed, to take advantage
of SSE vectorization instructions our offline phase performs 128
times more work than is required. When compared to SecureML we
observe that their total time for performing a single prediction is
slightly less than ours due to their offline phase requiring one round
of interaction as compared to our 64 rounds. However, achieving
this running time in the two-party setting requires a very large
communication of 1.6 MB as opposed to our (throughput optimized)
0.002 MB, an 800 times improvement. Our protocol also scales much
better as it requires almost the same running time to evaluate 100
predictions as it does 1. SecureML, on the other hand, incurs a 20
times slowdown which is primarily in the communication heavy
OT-based offline phase.
We observe a similar trend when evaluating a logistic regression
model. The online running time of our protocols when evaluating
a single input is just 0.2 milliseconds compared to SecureML requiring 0.7, with the total time of both protocols being approximately
4 milliseconds. However, our protocol requires 0.005 MB of communication compared to 1.6 MB by SecureML, a 320× difference.
When 100 inputs are all evaluated together our total running time
is 9.1ms compared to 54.2 by SecureML, a 6× improvement.
Our protocol requires 3ms in the online phase to evaluate the
model and 8ms overall. SecureML, on the other hand, requires
193ms in the online phase and 4823ms overall, a 600× difference.
Our protocol also requires 0.5 MB of communication as compared
to 120.5 MB by SecureML.
More recently MiniONN [40] and Chameleon [46] have both
proposed similar mixed protocol frameworks for evaluating neural
networks. Chameleon builds on the two-party ABY framework [24]
which in this paper we extend to the three-party case. However,
Chameleon modifies that framework so that a semi-honest third
party helps perform the offline phase as suggested in the clientaided protocol of [43]. As such, Chameleon’s implementation can
also be seen in the semi-honest 3 party setting (with an honest
majority). In addition, because Chameleon is based on 2 party protocols, many of their operations are less efficient compared to this
work and cannot be naturally extended to the malicious setting.
MiniONN is in the same two-party model as SecureML. It too is
based on semi-honest two-party protocols and has no natural extension to the malicious setting.
As Figure 3 shows, our protocol significantly outperforms both
Chameleon and MiniONN protocols when ran on similar hardware. Our online running time is just 6 milliseconds compared
to 1360 by Chameleon and 3580 by MiniONN. The difference becomes even larger when the overall running time is considered
with our protocol requiring 10 milliseconds, while Chameleon and
MiniONN respectively require 270 and 933 times more time. In
addition, our protocol requires the least communication of 5.2 MB
compared to 12.9 by Chameleon and 657.5 by MiniONN. We stress
that Chameleon’s implementation is in a similar security model to
us while MiniONN is in the two-party setting.