Abstract
Understanding visual complexity as it relates to websites has been an emergent area for many years. However, predicting the visual complexity of a website as perceived by users has been a real challenge. Perception is important because it influences user engagement, dictating if they will find it dull, engaging, or too complex. While others have suggested solutions to certain levels of success, here we propose a simple but accurate model that generates a Visual Complexity Score (VCS) based on common aspects of an HTML Document Object Model (DOM). We created our model based on a statistical analysis of 3300 ratings of 55 users on 30 web pages. We then implemented this prediction model in an open source Eclipse framework called ViCRAM that both predicts and visualises the complexity of web pages in the form of a pixelated heat map. Finally, we evaluated this model and the tool prediction with another user study of 6240 ratings of 104 users on 30 web pages. This study shows that our tool can predict the perceived complexity with a strong correlation to users’ perceived complexity.

Previous
Next 
Keywords
Visual complexity

Prediction

Perception

Automated tool

Open Source Tool
The ViCRAM tool is publicly available. To run this tool, the Eclipse ACTF Visualisation SDK http://www.eclipse.org/actf/downloads/ first needs to be installed, and then the Team Project Set File (.psf) of the tool needs to be imported to Eclipse. The psf can be downloaded from http://ftp.gnome.org/mirror/eclipse.org/technology/actf/psf/old/anonymous/actf-examples.zip

1. Introduction
Understanding visual complexity as it relates to websites has been an emergent area for a number of years (Wu et al., 2013). Indeed, the increased complexity of a web page is often described through website accessibility and usability frameworks which pay little attention to classical concepts of complexity. However, having an accurate perceived complexity score means that it can be used to have an initial perception of the visual layout of a web page and designers can use this framework to balance web page visual complexity with usability and accessibility. While this is an important contribution to the web accessibility area, because by using visual complexity as an identifiable measure web pages can be designed that are easier to interact with, especially for visually impaired users (Yesilada and Harper, 2019), it is also important for general interface design (Moacdieh and Sarter, 2015) and simplification (Reinecke et al., 2013), and is even used in studies of aesthetics and gender (Tuch et al., 2010).

Research shows that the use of visual aesthetics, such as sharp layout, snappy captions and interesting images, can transform a wall of dry text into a presentation which users will approach enthusiastically (Reinecke et al., 2013). Indeed, most web pages focus on good visual presentation to implicitly help users navigate, understand, and interact with the content; with user studies expounding the value of text and images and suggesting approaches that try to effectively design how they should be combined together (Hall and Hanna, 2004). However, predicting the visual complexity of a website – as perceived by users, has been illusive and inaccurate (see Section 2). Therefore, in this paper, our research question is “can a model be created to predict the perceived visual complexity of a page?”.

We already understand that website visual perception is affected by human cognition, content and form (Germonprez and Zigurs, 2003). Human cognition affects how a user retrieves and uses information in a website. The site content along with the amount of information relates to information overload, and the form of the website with respect to its interface, navigability, and structure is a further perceptual factor. Indeed, perception is important because it influences user engagement with a site, dictating if they will find it dull, engaging, or too complex1.

While others have suggested solutions to certain levels of success (see Section 2.1), we propose a simple but accurate model based on common aspects of an HTML Document Object Model (DOM) and its visual rendering. In our previous work, we investigated the relationship between a user’s perception and the visual complexity of a web page (Harper et al., 2009) and showed that the visual complexity of web pages depends on the presentation of the elements of the pages, the density and diversity of the elements, and the overall layout of the page (Michailidou et al., 2008). Indeed, we asserted that the visual complexity of a web page is related to the elements presented through the structure of the document, by the quantity of each element that is used on the page and by the perception of the users.

Here, we formalise this work by proposing an algorithm derived from our complexity prediction model such that we can provide designers with a score that determines the perceived visual complexity of a web page. We refer to this score as Visual Complexity Score (VCS). Our model is based on experimentation with 55 users where each user was asked to rate 30 web pages two times based on their visual complexity, therefore this model is based on 3300 ratings. The subsequent statistical analysis identified the images, words and top-left-corners of sections of web pages as being the most influential in forming user opinions of complexity (see Section 3). We then captured this model in an open source Eclipse Project tool called ViCRAM, which also provides a heat map and highlights the areas that are most visually complex on a web page (see Section 4).

Finally, the prediction level of ViCRAM was evaluated with a second study by running a correlation analysis between the participants’ perceived scores and also the tool’s score for the complexity. The procedure of this evaluation was the same as the rating experiment used for creating the prediction model. 104 participants were rated 30 web pages twice, and therefore the tool was validated with 6240 ratings (see Section 5). Our statistical analysis shows that the average of the participants’ first-time and second-time rating was significantly correlated with the scores generated by the tool. These results show that the framework could predict the perception of the participants at a high level concerning the visual complexity level of a web page, based on the structural elements and overall layout of the page (see Section 6 and Section 7).

1.1. Contribution and novelty
This paper presents our end-to-end work on creating and implementing a simple model of visual complexity. We first present a user study that investigates the aspects of DOM elements and their visual renderings and creates a model of users’ perception of visual complexity. The preliminary findings of this study are presented in Michailidou et al. (2008). Only one aspect of this algorithm is presented in our previous work (Harper et al., 2013) which demonstrated that areas of high complexity can be identified by detecting areas, or chunks, of a web page in block-level elements and we build on our previous work in this paper by expanding this finding to include more elements, and so increase accuracy, building these into a simple algorithm which is then implemented in the ACTF framework2. Finally, to evaluate our model and also the tool, we conducted an extensive user study which shows that the scores automatically generated by our tool are strongly correlated with the users’ ratings. In brief, the main contribution of this paper is to fully present our end-to-end work. Furthermore, our work is novel as it presents a simple algorithm and its validated implementation that can be used to generate a visual complexity score for a given web page.

2. Background and related work
The increased complexity of a web page is often described through website accessibility and usability frameworks. However, we attempt to relate the visual complexity of a web page with the perception of users. Complexity, in this case, can be defined as “the degree of difficulty in providing a verbal description of an image” (Heaps, Handel, 1999, Oliva, Mack, Shrestha, Peeper, 2004). Textures with repetitive and uniformly oriented patterns are less complex than disorganised ones. A visual pattern is also described as complex if its parts are difficult to identify and separate from each other (Oliva et al., 2004). Complexity perception of a scene depends on the amount of grouping a user unconsciously performs, familiarity with the scene and existing knowledge of objects inside the scene. Visual complexity is mainly represented by the perceptual dimensions of quantity of objects, clutter, openness, symmetry, organisation, and variety of colours (Oliva, Mack, Shrestha, Peeper, 2004, Rayner, 1998).

Germonprez and Zigurs (2003) show that the visual perception of websites is affected by cognition, content and form. Human cognition affects how a user retrieves and uses information on a website. The content on the website and the amount of information that is available affect complexity since it can cause information overload on a page. Visual perception is a process of extracting knowledge about objects and events in an environment and depends both on a stimulus and the characteristics of individuals (Lavie and Tractinsky, 2004). That is, human cognition is required to recognise the relationship between the stimulus objects, which is achieved by bringing existing knowledge to the environment. Web page design features, such as pattern, animations and layout, can influence the visual processing of users.

Most web pages focus on good visual presentation to implicitly help users navigate, understand, and interact with the content. Research (Faraday, Sutcliffe, 1998, Hoffos, Sharpless, 1991) shows that the use of visual aesthetics, such as sharp layout, snappy captions and interesting images, can transform a wall of dry text into a presentation which users will approach enthusiastically. Studies expound the value of text and images and suggest approaches that try to effectively design how they should be combined (Faraday, Sutcliffe, 1998, Marks, 1994). For example, the use of co-references between text and images is an approach for effectively designing a combination of text and images (Faraday and Sutcliffe, 1998).

Design elements and techniques constantly change to create more attractive sites. Studies such as Ivory and Megraw (2005) try to identify the design characteristics of the most effective sites and how these characteristics change over time. The use of graphics for organisation and decoration (such as bullets and icons) has also increased (Ivory and Megraw, 2005). Even though these graphics enhance the visual appearance of web pages, they also increase the cognitive load.

Further studies are conducted to examine how the appearance of various structural elements can affect the web behaviour of users. For example, Hall and Hanna (2004) investigate the effect of web page text/background colour combination on readability, retention, aesthetics and behavioural intention. Some of their conclusions are that specific colours can lead to greater readability, intention to purchase and higher ratings of aesthetic quality and that ratings of aesthetic quality are significantly related to intention to purchase. The effects of font type and size are also examined concerning reading time (Bernard, Liao, Mills, 2001, Bernard, Lida, Riley, Hackler, Janzen, Boyarski, Neuwirth, Forlizzi, Regli, 1998). These studies, however, examine only one-page element. Web page design is a combination of a large set of variables such as images, text, tables, links and vary in types such as colour, size, and position. Our study attempts to incorporate more than one web page element and examines their relation with perceived visual complexity.

2.1. Literature review
Different approaches have been proposed to measure the visual complexity of web pages. These approaches consider the elements of web pages, the pixels of the screenshots of web pages or both (Miniukovich and De Angeli, 2015). When the elements of web pages are used to generate a set of features for measuring the visual complexity of web pages, the source code of web pages is typically used as an input (Harper, Michailidou, Stevens, 2009, Ivory, Sinha, Hearst, 2001, Michailidou, Harper, Bechhofer, 2008). For example, Ivory et al. (2001) use a set of features generated based on the elements of web pages (such as, the number of words, links, images and font types, etc.) to predict whether or not a given web page would be rated very high based on its visual appeal. Another example is from Purchase et al. (2011) who also use the elements of web pages to measure their visual complexity by using certain features (such as, density, equilibrium, balance, etc.) computed based on text, image and control elements. Visual complexity related studies are not restricted to web pages. In particular, Jokinen et al. (2020) propose an approach that takes any kind of visual interface segmented into its rectangular elements with the colour and size properties of those elements, the target element, and also the prior history of the user with the interface layout (if any) to predict an eye-movement data and search time. This could allow us to understand how complex an interface is for users.

When pixel-based features are considered instead of element-based features for measuring the visual complexity of web pages, the screenshot of web pages is typically used as an input instead of the source code (Miniukovich, De Angeli, 2014, Riegler, Holzmann, 2018). Computer-vision algorithms are commonly used to generate pixel-based features (Riegler and Holzmann, 2018). Some examples of the pixel-based features are as follows: layout, colour, typography and consistency (Riegler and Holzmann, 2018).

Although some researchers consider pixel-based features as more beneficial compared to element-based features since the screenshot of web pages directly represents what users see (Reinecke et al., 2013), the source code of web pages provide valuable information regarding their structure and visual representation. Wu et al. (2013) propose an approach which uses different kinds of features, including HTML features, structural features and visual features. To take the structure of web pages into consideration, they divide web pages into their visual blocks by using the Vision-based Segmentation (VIPS) algorithm and computes a set of features for these blocks. Once a dataset is constructed by labelling each page with the set of features, the dataset is split into the training and test sets. A supervised machine learning algorithm is then utilised to obtain a prediction model based on the training set. The output is a prediction model which is tested using the features of each web page in the test set. They continued their work with follow-up studies by adding new features, presenting new aesthetics representation based on the statistical analysis of user perceptions and also proposing two learning strategies (Wu et al., 2016).

Two points distinguish the approach of Wu et al. (2013) from our approach: (1) Their approach conforms to a standard machine learning framework, thereby assuming that theories in machine learning can help construct an effective measurement function with good generalisation capability, and (2) Unlike our approach that involves a limited number of factors, the machine learning approach leverages web mining and computer vision techniques to extract an extensive range of features. Even though our proposed approach is simple in comparison with the approach of Wu et al. (2013), it can provide a high-level accuracy based on common aspects of an HTML Document Object Model (DOM) and its visual rendering. Furthermore, our proposed approach is computationally simple and does not require extensive training time and computational resources.

3. Complexity modelling with a user study
Our previous work shows that visual complexity of web pages depends on the presentation of the elements of web pages, the density and diversity of the elements, and the overall layout of the page (Harper et al., 2009). To generate a prediction model for complexity, we conducted a user study to determine the extend and weight that these factors have. This section explains the user study in detail, structural elements used in the modelling and the overall results which lead into a complexity score equation.

3.1. User study: Methodology
In this user study, we asked our participants to rate 30 web pages based on their visual complexity. We then related their ratings with the structural elements used to design each page.

3.1.1. Participants
The study was available online so the participants could access it on their own time and place. It was advertised through mailing lists and newsgroups. 55 participants from around the world volunteered to take part in this study (36 male and 19 female). 20 participants were aged up to 25, eight between 46 and older and the rest between 26 and 45. From them, 25 participants had English as their native language. Two participants were colour-blind and since they could perceive the pages differently (Barreto, 2008), their data were not used for any further analysis. In particular, they could perceive the colours differently or they might not be able to differentiate certain shade of colours, especially certain foreground colours might be difficult to differentiate from certain background colours (Yesilada and Harper, 2019).

Most of the participants (98%) reported that they use the Internet daily or a few times a week with 40% of them more than twenty hours, only 5% less than an hour, 20% for 1–5 hours, 18% for 6–10 hours and the rest between 11 and 20 hours. 84% of them described that they use the web for business/work, 87% for email/chat, 93% for special interests, and 75% for online purchasing.

3.1.2. Design
The screenshots of 30 web pages were the stimuli that all the participants had to rate for visual complexity. A within-subjects study was designed to collect rating scores for all pages by all participants. The participants looked at each page for exactly seven seconds. The time was assigned for two reasons. First, our previous studies and literature review showed that users could get an overall idea of a web page within five seconds (Harper, Michailidou, Stevens, 2009, Michailidou, Harper, Bechhofer, 2008). Then, we ran a series of pilot studies within our lab to determine whether 10, seven or five seconds was enough for a user to look at the pages and answer a set of questions. All the users suggested that seven seconds was enough for looking at an image and be able to remember it to answer the follow-up questions. To avoid any order effect, a randomised sequence of the 30 web pages was assigned for each participant, and the order of the pages was counterbalanced. Also, each participant had to rate the same image twice and the order of the pages was also counterbalanced for the second time. The time taken to give each score was collected for every participant to examine any unreasonable delays that might affect the user ratings.

3.1.3. Materials
The complexity of the top 100 websites listed by Alexa3 was assessed based on our preliminary work which demonstrated that the chunk rendering of web pages was significantly correlated with the visual complexity score of the participants for the pages (see Chapter 4 in Michailidou (2010)). The pages were then selected to be representative of sectors such as public information, business, academic, entertainment, leisure, web services including search engines, and personal home pages (Amitay et al., 2003). In addition, it was ensured that the pages ranged from visually simple to complex. Appendix A lists the URLs of the pages used in our study. Selecting visually simple pages was found to be quite difficult from the Alexa top 100 list, so two pages from the University of Manchester were selected as it was a familiar and simple page for most of our expected participants. In total, 30 pages were selected to balance the need to include disparate visual layouts with the time required for a human to interact and rank those pages. To prevent the pages from changing and avoid any bias in the data, the screenshots of all the pages were taken on the same day, from the same monitor and all of them had the same size of 1260 x 885 pixels. In addition, the source code with any associate files was saved for each web page during that period. Therefore, the list of URLs in Table A.1 are only included for referential and completeness purposes.

3.1.4. Procedure
The participants firstly read the information and details about the study and then consented to take part in the study which consisted of three parts. The first part included demographic questions such as age, sex and browsing familiarity. The main study was conducted in the second part, in which after a short introduction about the procedure of the study, the participant looked at a page for 7 seconds which then automatically changed to the rating questions page. The participants had to give a rating score for the page they had just looked at with respect to its visual complexity. The score ranged from 1 to 10, with 1 for the visually simplest and 10 for the most complex. They also had to state whether they were familiar with the page. After looking at the randomized sequence of the 30 pages, the participants had to look at the re-randomized counterbalanced sequence of what they just saw and rate the pages again. Therefore, each participant had to rate the same image twice. The third part of the evaluation was a set of feedback questions which asked participants about visual complexity.

All demographic data was self-reported (e.g. age and gender); no participant was paid; ethical approval was sought and granted, and a unique email address was required to complete the study to ensure that there was no multiple entries from the same person.

3.2. Structural elements: Variables in complexity modelling
To understand how the structural elements of a web page can determine the level of visual complexity, each of the pages used for the evaluation was analysed and the following variables were identified. Full details of these variables can be found in Michailidou (2010). In the first user study, the pages were manually analysed and then in our tool implementation, their algorithms were encoded to calculate them automatically (see Section 4.1).

[Menus:] A series of a horizontal or vertical list of links that are grouped in an obvious way. That is, the list of links might be divided by a line, grouped in a box, or as a section surrounded by a white background. Menus are usually horizontal on the top and bottom of a page, or vertical on the left-hand side of a page.

[Images:] Any images on a page, including advertisements, animations, logos and decorative images.

[Words:] Text used to present any type of information on a page. For the analysis, all words within the screenshot of the page were counted, including the text from the menu lists.

[Links:] The number of links on each page was counted based on the visibility of the link, including the links that were underlined, distinguished with colours, within a list as a menu or surrounded by different colours (most commonly white).

[Top Left Corner (TLC):] A page is separated in various sections and subjects (Yesilada et al., 2008). This visual distinction is made with the use of colours, tables, lines and spacing. To identify the number of different sections a page is organised into, we create a page chunk rendering, which is the representation of the overall web page layout without any visual elements. The set of variables that are used to identify the chunk rendering of the page are the background colours, headings and subsections, standalone images, and visible lines or borders. When the chunk rendering of the page is revealed (refer to Fig. 1 for an example chunking), the organizational elements need to be identified. To understand how the number of blocks and the overall structure of the page affect users perception, we define the Top-Left-Corner (TLC) variable. TLC is a block’s top left corner. If a box’s left and top sides are not adjacent or have a common side with another box, then its TLC is also counted. We decided to use only the TLC variable for this study. The reason was that after examining the source code of web pages in relation to the chunking classification, we found that TLC could be determined algorithmically in a closer approximation than the rest of the variables. This would later enable us to programmatically determine the number of TLCs for each page which could be used in a prediction model for complexity. More details can be found in Michailidou (2010).

Fig. 1
Download : Download high-res image (286KB)
Download : Download full-size image
Fig. 1. Chunking Example.

3.3. Results
In the statistical analysis presented below, the Score A and Score B represent the means of the scores for each page collected during the first and second time respectively, and the Average Score represents the mean of the Score A and Score B for each page. With this study, we mainly looked at two things:

1.
Score Correlation: Our participants were asked to rate the pages twice, so we checked how consistent they were with their ratings.

2.
Visual Complexity Score (VCS): We investigated the effect of the structural elements of web pages given above (see Section 3.2) on visual complexity, and we focused on providing a model for visual complexity prediction with statistical analysis.

3.3.1. Score correlation
As discussed in Section 3.1, the participants rated each image twice in a counterbalanced sequence. A reliability test between each score was conducted to check for consistency of the participants’ scores, which was achieved by running a bivariate correlation on all the scores.

The complexity rating scores given for each page in the first time was found to be significantly correlated with the rating scores given in the second time (see Table 1), with Pearson correlation coefficients of r(51)  >  0.48 and significant at p = 0.002. Only one page had a smaller correlation value of r(51)  >  0.32 but still significant at p = 0.019. The small effect could be caused due to the user’s possible familiarity with the page. Also, a strong correlation was determined between Score A, Score B and Average Score with r(28)  >  0.97, p  <  0.0001.


Table 1. Visual complexity scores of the web pages, Mean A (Score A): Mean of the first-time scores for each page, Mode A: Most frequent first-time score for each page, Mean B (Score B): Mean of the second-time scores for each page, Mode B: Most frequent second-time score for each page, Average Score: Average of Score A and Score B, Complexity: Score assigned based on (Harper et al., 2009) (S: Simple, M: Medium: C: Complex).

Page ID	Complexity	Mean A	Mode A	Mean B	Mode B	Average Score
20	S	1.47	0	1.75	1	1.61
24	S	2.07	1	2.09	3	2.08
29	S	2.18	2	2.22	1	2.2
27	S	2.35	1	2.85	2	2.6
12	S	3.11	4	3	3	3.05
11	S	3.18	2	3	1	3.09
17	M	3.47	2	3.05	2	3.26
6	S	3.31	1	3.53	2	3.42
2	S	3.07	1	3.78	2	3.43
13	M	3.56	2	4	2	3.78
23	M	3.84	2	4	4	3.92
7	S	4.2	2	3.73	4	3.96
21	S	4.16	2	3.84	2	4
22	M	4.02	4	4.35	4	4.18
26	M	4.42	2	4.51	6	4.46
8	M	4.64	7	4.85	5	4.75
1	C	4.6	5	5.58	8	5.09
10	C	5.36	6	5.45	7	5.41
16	M	5.33	7	5.55	6	5.44
30	C	5.67	7	5.27	6	5.47
5	M	5.69	8	5.55	6	5.62
9	M	5.71	7	5.55	8	5.63
14	M	5.69	4	5.84	6	5.76
4	C	5.78	8	5.84	6	5.81
25	M	6	6	6	8	6
15	C	5.65	6	6.53	7	6.09
28	C	6.15	5	6.27	7	6.21
19	C	6.05	5	6.44	8	6.25
18	C	6.6	8	6.27	7	6.44
3	C	6.93	7	6.78	7	6.85
3.3.2. Visual complexity score (VCS)
Table 1 lists the mean values of the scores given for each page in ascending order of the average of the mean values along with the preassigned level of visual complexity based on Harper et al. (2009). As shown both in the table, the pages pre-assigned as visually simple received lower ratings than the visually complex pages. This can be used as an initial validation of our framework and therefore our assumptions.

In order to understand how the structural elements of a web page can determine the level of visual complexity, each page used for the evaluation was manually analysed by the main evaluator4 by using the variables explained in Section 3.2 (menus, images, words, links and TLC). Table 2 lists the structural elements and their respective number for each page.


Table 2. The number of web page structural elements (TLC: Top Left Corner).

PageID	Menus	Image	Words	TLC	Links
1	4	14	334	28	63
2	1	1	464	11	46
3	3	13	357	22	58
4	2	13	530	29	84
5	1	7	353	20	112
6	1	1	345	8	8
7	0	5	281	4	10
8	2	1	331	10	43
9	1	8	341	17	44
10	3	6	285	17	50
11	2	7	200	8	19
12	0	5	130	11	22
13	1	3	401	16	44
14	2	2	360	20	31
15	4	7	789	19	96
16	2	0	400	11	31
17	1	2	116	10	19
18	1	24	125	21	9
19	4	13	272	17	56
20	1	1	8	4	6
21	1	2	271	4	12
22	5	7	240	15	34
23	2	8	246	17	32
24	3	1	175	7	24
25	2	1	550	17	42
26	6	2	450	14	76
27	1	7	180	6	110
28	5	35	370	19	83
29	2	6	85	5	12
30	2	17	267	17	42
Using the Average Score as a dependent variable and the number of menus, words, TLC and links as predictors in an enter regression method, a significant model emerged (F5,24=12.098, p  <  0.001) with a strong fit of R2adj=0.66. The value of R2 describes how much of the variance in the dependent variable (in our case, Visual Complexity Score or VCS) is accounted for by the regression model from the sample (in our case, web pages) whereas the R2adj describes how much variance in the dependent variable would be accounted for if the model had been derived from the population from which the sample was taken (Field, 2005). In other words, the R2adj gives some idea of how well a model generalises and ideally one would like its value to be the same or very close to the value of R2. Only TLC (t(24)=3.028, p = 0.003), words (t(24)=2.917, p = 004) and images (t(24)=2.234, p = 0.017) were significant.

The second series of regression analysis was then performed by using only the significant variables as predictors, with the dependent variable Average Score5. During our previous empirical studies, it was shown that the number of TLCs in a web page was significantly related to the visual complexity level of the page. Therefore, the number of TLCs was used as the first variable in a stepwise regression analysis. Table 3 shows coefficients of two models where the first model (R2adj= 0.585, SE Est.= 0.938, ANOVA F-ratio= 41.809 p  <  0.0001) uses only the number of TLCs as predictors whereas the second model (R2adj= 0.666, SE Est.= 0.841, ANOVA F-ratio= 20.292 p  <  0.0001, Durbin-Watson= 2.062) uses all the significant variables (TLC, words and images) as predictors.


Table 3. Stepwise Regression - Coefficients.

Average Score
B	SE B	β	t
Model 1				
Constant	2.186	0.401		5.456
TLC	0.166	0.026	0.774	6.466c
Model 2				
Constant	1.743	0.399		4.366
TLC	0.097	0.033	0.452	2.925a
Words	0.053	0.026	0.279	2.067b
Images	0.003	0.001	0.361	2.743b
a. p  <  0.01; b. p  <  0.05; c. p  <  0.001				
Residuals are known as the difference between the values of the outcome predicted by the model and the values of the outcome observed in the sample. These residuals effectively represent the error present in the model, usually caused by outliers (Field, 2005). A way of checking how well the regression model fits is to check the normality of the residuals (the standardized residuals) which can be done by looking at the normal probability plots which are shown in Fig. 2. The straight line in the probability plots represents a normal distribution, and the points show the observed residuals. In a perfectly normally distributed data set, all points will lie on the line.

Fig. 2
Download : Download high-res image (116KB)
Download : Download full-size image
Fig. 2. Average Score – Normal P-P plot – Regression Model Normality Check.

After checking for the best fit of the data and their collinearity, the analysis revealed the following equation based on coefficients given in Table 3:(1)

4. Algorithms and implementation
After we finalise the VCS model with our statistical approach explained above, we implemented the model in a tool called ViCRAM that can be used to compute VCS automatically. As illustrated in Fig. 3, this tool is comprised of three views: (1) Browser View which allows to load and show a web page; (2) Visualisation View which uses different colours to differentiate the parts of the page based on their visual complexity, and (3) Summary Report View which provides the overall VCS of the page and the VCSs of the parts of the page.

Fig. 3
Download : Download high-res image (422KB)
Download : Download full-size image
Fig. 3. The interface of the ViCRAM tool.

Fig. 4 illustrates the work-flow of this tool. The generated views are technically based on two different algorithms called (Alg1) Complexity algorithm (see Section 4.1) and (Alg2) Visualisation algorithm (see Section 4.2). The Complexity algorithm determines the level of visual complexity of a given page, whereas the Visualisation algorithm provides a heat map that highlights the most visually complex areas on the page.

Fig. 4
Download : Download high-res image (211KB)
Download : Download full-size image
Fig. 4. The views of the ViCRAM tool.

When a user wants to compute the visual complexity of a particular web page by using this tool, s/he firstly needs to type its URL in the Browser View to load the page. After the user presses the relevant button in the Visualisation View, the HTML source code of the page is sent to the HTML parser that converts the HTML into a DOM tree structure and style information. A screenshot of the page is also taken at this stage. The Complexity algorithm then analyses the structure and style of the page and provides an output text with information regarding the complexity of the page whereas the Visualisation algorithm uses the methods from the complexity algorithm and analyses the complexity of the page concerning grids and colours to provide visualisation data and output information about the data. The visualisation data are then overlaid on the screenshot of the web page, and the Visualisation View shows the heat map of the analysed page. The information provided by both the Complexity and Visualisation algorithms is reported in the Summary Report View. In the following sections, we provide further details about these two algorithms.

4.1. Complexity algorithm
The Complexity algorithm traverses the DOM tree of a given web page recursively and counts the TLCs, words and images on the page to be used in the computation of the visual complexity score of the page (see Equation 1). Appendix B shows the Complexity algorithm for the ViCRAM tool which consists of two important methods and a set of auxiliary methods. The first method, countElements(node), is a recursive method that performs DOM analysis. As the pseudocode shows (lines 11 - 20) (see Fig. B.1), the method counts the page elements by recursively going through the node using the DOM parser. Some counters are not used in the final equation but are used as part of determining the overall structure of the page used in the TLC count. The second method, countTLC(node), calculates the number of TLCs a page is grouped into. Heuristics were derived in order to apply this classication so the TLC count can be automatically determined. The heuristics, as Fig. B.1 shows in lines 25 - 44, use the DOM parser and the style information of the node to determine if a node will be counted as TLC. One is counted when the node is presented as a block with visible border, has headings, and is a table used for data or for layout. A common characteristic is that a TLC is also presented as a block element or a table. In addition, a node can be identied as a TLC only once.

Once the complexity algorithm was implemented, it was tested with the web pages used in the user study to develop the algorithm (see Section 3). The complexity scores from the user study and the scores generated by the implementation of the algorithm are provided in Table 4. These values are correlated to each other (Spearman correlation rs(26)= 0.559, p = 0.002)6. During our further tests on a set of a hundred web pages from the Alexa Top-100, we found that most pages ranged from 0 - 100, with very few generating complexity higher than 100 which was due to long pages with a lot of text only. Hence, the VCS is divided by 10 times of the calculated score since the 0–10 range was always used as the user ratings scale and was found to be understandable, consistent and acceptable from the participants during the user studies. If the calculated score was more than 100 and consequently the VCS is more than 10, then the tool returns ‘10**’. The “**” is added next to the score along with an explanation that the examined page is very complex and exceeds the higher limit.


Table 4. The visual complexity scores from the user study and the implementation of the complexity algorithm.

Page ID	User Study Score	Algorithm Score
1	5.09	55.59
2	3.43	43.52
3	6.86	62.39
4	5.81	42.11
5	5.62	28.92
6	3.42	NA
7	3.96	67.21
8	4.75	37.43
9	5.63	25.77
10	5.41	27.54
11	3.09	14.75
12	3.06	7.08
13	3.78	32.82
14	5.76	25
15	6.09	56.82
16	5.44	22.62
17	3.26	8.11
18	6.44	NA
19	6.25	27.72
20	1.61	5.78
21	4	15.52
22	4.18	16.56
23	3.92	15.53
24	2.08	11.7
25	6	66.77
26	4.46	NA
27	2.6	32.47
28	6.21	30.42
29	2.2	22.9
30	5.47	105.07
4.2. Visualisation algorithm
The Visualisation algorithm firstly divides a web page into a number of grids (10 rows x 10 columns = 100 grids), secondly counts the TLCs, images and words in each grid, and then calculates the VCS for each grid by using the complexity algorithm (Section 4.1). After that, it assigns a colour to each grid based on the ratio of the VCS of the grid and the overall page. The higher the VCS of the grid is, the closer to red its assigned colour is. Even though the number of grids are static in the current implementation, it can be easily changed in the future.

This algorithm considers the top-left coordinates of the nodes to relate them with the grids. In particular, an image may span in two or more grids, but the algorithm increments the image count of the grid that the image’s top-left coordinate has on the source code. However, this is not the case for text nodes. The word count for a text node is calculated along with the number of grids that the node spans. The average word count is then calculated for each grid that the text node covers.

When the VCS is computed for each grid, the algorithm selects and assigns an appropriate colour for each grid (Red, Orange, Yellow, Yellow Green, Green, Dark Green). These VCSs are also provided in the Summary Report View of the tool along with the number of TLCs, images and words.

4.3. Vicram implementation
The implementation of the ViCRAM tool is based on the Accessibility Designer (aDesigner) tool which is one of the features included in the ACTF project. The aDesigner tool provides a way to evaluate a web page at a glance by developing a visualisation feature that tries to supplement or solve accessibility problems with the current guidelines checkers. It has capabilities to visualise blind users’ usability by using colors and gradations. The visualisation function allows web designers to grasp weak points in their pages, and to recognise how accessible or inaccessible their pages are (Takagi et al., 2004). The aDesigner API was chosen as the basis to implement the ViCRAM tool because aDesigner uses HTML parsers and has visualisation capabilities that match our objectives.

5. Evaluation
To evaluate and validate the VCS predicted by the ViCRAM tool, we conducted another confirmatory user study. In this study, the participants were also asked to rate a number of web pages based on their visual complexity. These ratings were then compared with the complexity scores that the ViCRAM tool provided for the same pages. Our hypothesis was as follows: “Visual Complexity Scores generated automatically by the ViCRAM tool are significantly and positively related to the ratings of users”. If we can support our hypothesis, then we can also conclude that the tool can be used to predict users’ visual complexity perception of a web page.

5.1. Methodology
The methodology of this evaluation was identical to the previous rating study described in Section 3.

5.1.1. Participants
The study was conducted online to allow the participants to access it in their own time and place. The study was advertised through mailing lists and newsgroups, such as CHI-WEB7 and DBWorld8. 104 participants from around the world volunteered to take part in this study (53 male and 51 female). 55 participants were aged up to 25, 24 between 46 and 65 and the rest between 26 and 45. From them, 67 participants had English as their native language. No participants were colour-blind. One dataset from a participant was dropped as they reported that they had a problem loading the pages with their browser.

All the participants reported that they used the Internet daily with 52% using the Internet more than twenty hours per week, nobody less than an hour, 11% for 1–5 hours, 14% for 6–10 hours and the rest between 11 and 20 hours. 97% of them described that they use the web for business/work, 95% for email/chat, 96% for special interests, and 87% for online purchasing. Even though most of the sample used the Internet daily for more than 20 hours a week, all users were familiar with all kinds of browsing (business/chatting/purchasing) which makes the sample more generalisable.

5.1.2. Design and procedure
To control the validation of the tool as much as possible, the same design explained in Section 3.1.2 and also procedure explained in Section 3.1.4 was followed for this evaluation.

5.1.3. Materials
The pages were again selected to be representative of sectors such as public information, business, academic, entertainment, leisure, web services including search engines, and personal home pages (Amitay et al., 2003). The pages were similar to the set of pages used during our previous user study, and part of the Alexa UK top 100 websites to provide a representative sample of the sites people actually browse. Out of the 30 pages selected from the Alexa list, eight of them were the same URL and had the same visual layout (but not content) as the eight pages originally used in the previous user study. The similarity was due to the fact that seven of these eight pages were also on the Alexa list and one of them was selected due to its simple design. To avoid any bias, the screenshots of the web pages were taken from the same monitor, within the same day and all the screenshots had the same size of 1111 x 800 pixels. The source code with any associate files was also saved for each web page during that period.

5.2. Results
Similar to our previous user study, we firstly checked the consistency between the first and second rating scores of the participants for the web pages. After that we investigated whether the complexity scores generated by the ViCRAM tool are significantly and positively related to the ratings of the participants.

5.2.1. Score correlation
A reliability test between each score was conducted to check for consistency of the first and second rating scores of the participants, which was achieved by running a bivariate correlation on all the scores. The complexity rating scores given for each stimuli in the first time were significantly correlated with the rating scores given in the second time, with Spearman’s correlation coefficients of rs(101)  >  0.5 and significant at p  <  0.0001. Two pages had a smaller correlation value of rs(101)  >  0.43 but still significant at p  <  0.0001. The medium effect (0.43) could be caused by the user’s possible familiarity with the page or by the users who changed some of their ratings during the second viewing after they had formed an opinion with respect to visual complexity based on all the thirty images. Also, a strong correlation was determined between the Score A, Score B and Average Score with rs(28)  >  0.983 at p  <  0.0001.

5.2.2. VCS Validation
Table 5 lists the mean values of the rates given for each page by the users (Score A, Score B and Average Score) along with the scores generated by the tool (Alg Score). Besides, Figure 7 shows a scatter plot of these scores. A bivariate correlation test was performed between the score generated by the tool (Alg Score) and the three scores received by the participants (Score A, Score B and Average Score) to examine the level of their relationship. It is important to note that after examining the data, we noticed that a few of the scores did not fit a normal distribution and therefore we decided to examine the non-parametric correlations. Table 6 shows the Spearman’s correlation coefficients between each kind of scores.


Table 5. The Visual Complexity Scores generated by the tool (Alg Score) and by the participants (Score A, Score B and Average Score).

ID	Alg Score	Score A	Score B	Average Score
22	0.83	1.3	1.53	1.41
17	3.03	2.91	2.93	2.92
18	1.19	3.13	3.15	3.14
26	1.45	3.09	3.19	3.14
13	0.79	3.17	3.32	3.25
27	4.34	2.98	3.57	3.27
23	0.83	3.43	3.85	3.64
20	3.13	3.63	3.82	3.72
1	1.45	3.67	4.04	3.86
30	0.9	3.92	4.1	4.01
24	1.88	4.6	4.64	4.62
12	3.81	4.7	4.62	4.66
4	1.77	4.65	4.88	4.77
7	6.67	4.89	4.96	4.93
29	2.47	4.95	5.11	5.03
10	2.15	5.36	5.37	5.36
28	3.32	5.31	5.4	5.36
5	8.84	5.18	5.76	5.47
21	2.51	5.42	5.64	5.53
8	7.65	5.44	5.63	5.54
14	2.42	5.65	5.81	5.73
9	4.11	5.85	5.83	5.84
2	8.4	5.93	6.01	5.97
16	3.18	6.34	6.4	6.37
6	9.55	6.35	6.46	6.4
15	8.42	6.28	6.9	6.59
3	7.16	6.45	6.78	6.62
19	4.45	6.68	6.84	6.76
25	3.07	6.87	6.81	6.84
11	6.01	6.98	7.22	7.1
Fig. 7
Download : Download high-res image (374KB)
Download : Download full-size image
Fig. 7. Scatter Plot of Alg Score, Score A, Score B and Average Score.


Table 6. Spearman’s correlation between the tool and the participants’ visual complexity scores.

Alg Score	Score A	Score B	Average Score
Alg Score		rs(28)=0.644, p  <  0.0001	rs(28)=0.689, p  <  0.0001	rs(28)=0.679, p  <  0.0001
Score A	rs(28)=0.644, p  <  0.0001		rs(28)=0.983, p  <  0.0001	rs(28)=0.994, p  <  0.0001
Score B	rs(28)=0.689, p  <  0.0001	rs(28)=0.983, p  <  0.0001		rs(28)=0.994, p  <  0.0001
Average Score	rs(28)=0.679, p  <  0.0001	rs(28)=0.994, p  <  0.0001	rs(28)=0.994, p  <  0.0001	
The scores generated by the tool were found as significantly and positively correlated with the participants’ scores at the high level of the range rs(28) >  0.64 and p  <  0.0001. It is interesting to note that the Average Score and the score that the automatic tool generated were significantly correlated at rs(28) = 0.68, p  <  0.0001. These results show that the ViCRAM tool could predict the participants’ perception at a high level concerning the visual complexity level of a web page, based on the structural elements and overall layout of the page.

6. Discussion
In this paper, we explained our end-to-end work on complexity prediction. We presented an initial user study to form a model, then an implementation of that model in a tool, and another confirmatory user study to validate the score automatically generated by that tool.

Unlike other approaches that use specific pixel-based features such as colours, typography, etc. (Reinecke, Yeh, Miratrix, Mardiko, Zhao, Liu, Gajos, 2013, Riegler, Holzmann, 2018), our approach uses the source code of web pages which is a very valuable dataset for understanding the structure of web pages and their visual representation. Other approaches are also available which use machine learning algorithms with the source code of web pages to determine their complexities (Wu, Hu, Shi, 2013, Wu, Zuo, Hu, Li, 2016), but these approaches might need computationally extensive training time and a lot of resources. Our approach is relatively simple and it can quickly compute a complexity score for a given web page by counting a specific set of structural elements (TLCs, images and words).

The visual complexity of web pages has been described to affect the difficulty of using web pages but also regarded as a subjective decision by users (Harper et al., 2009). Visually impaired users find many web pages to be complex from an interaction standpoint. Indeed, studies have shown that if a page is too visually complicated visually impaired users will often not even try and interact with the content (Faraday, Sutcliffe, 1998, Hoffos, Sharpless, 1991) or give up after a short time (Lunn, Michailidou, 2007, Lunn, Michailidou, 2008). Therefore, if it is possible to give a visually impaired user some notice of the expected complexity of the interaction required before link traversal, then the time wasted on unproductive audio interaction, scanning, and glancing could be reduced. We hypothesized that by understanding sighted users’ visual perception of web page complexity, we could understand the cognitive effort required for interaction with that page.

The quantitative analysis of the data from our first user study produced a prediction model for generating a complexity score for a given web page that significantly matches users’ perception. This model was then implemented in a tool called ViCRAM. Our hypothesis stated that the visual complexity scores generated automatically by the tool are significantly and positively related to the ratings of users. In other words, the tool can be used to predict users’ visual complexity perception of a web page at a significant and high correlation. The user evaluation supported this hypothesis since it revealed the positive and significant high correlation between the users’ visual complexity ratings of the page and the scores generated by the tool.

It is important to note that the participants were consistent with their answers. We demonstrated this case with the correlation tests in both of the user studies. Asking participants to see and rate each page twice initially raised issues about boredom effect. However, the results show that the ratings of the participants were highly correlated and therefore, reliable.

The participants were also asked to define in their own words what a visually simple and visually complex page was at the end of both initial and confirmatory user studies. The participants described a visually simple page as one with pale colours, clean, can easily find what they are looking for, and has few images and links. They also described a visually simple page as organised, with a clear layout, harmonious and muted colours and a limited amount of different subjects and whose purpose can easily be understood. On the other hand, the participants described a visually complex page as one with a lot of information and categories, difficult to scan or skim text, not uniform in size and shape, has a large number of images, different colours, buttons, animations and generates an overall distraction. These definitions from the participants also support our complexity prediction model.

Our additional analysis also showed that familiarity with the page did not reveal any significant correlation with the users’ ratings or the complexity algorithm scores, but there were some exceptions. For example, the Yahoo page was rated as very complex by the participants and most of the participants were familiar with the page, but the algorithm rated it as one with a medium level of complexity. This could happen because the algorithm does not account for familiarity. Also, the algorithm is based on the structural elements of a web page, and it detects elements that users can not always see as they are hidden in the underlying source code.

In both of our user studies, the time that each participant spent to answer the visual complexity level of a web page was also recorded. No significant correlation was determined between the reaction times and the complexity scores given for the pages. We noticed that there were some outliers in the reaction time data. Sometimes the participants spent even 5–10 minutes on giving an answer. When we removed those outliers and re-run the analysis, we did not see any significance correlation, concluding that the time a participant needed to give a score on the visual complexity level of a web page was not significantly related with the given visual complexity score of the page in our studies. However, further studies can be conducted to better understand and analyse the time effect on complexity.

Predicting user perception of web page complexity can have factors that are not yet considered from both sides of the equation: the user perception and the algorithm. On the one hand, the user perception can be altered from the user’s personality characteristics that are difficult to predict such as the user’s culture and social trends. On the other hand, the algorithm can take other structural elements into account that are not yet evaluated and implemented, such as page type, font sizes, colours and dynamic content, along with improving the existing algorithm. In our algorithm, we also consider all images regardless of their impact on understanding the content. However, images could also be separated as informative/non-informative. Further studies can be conducted to investigate which of these features are better in predicting users’ perception of complexity. More features can be investigated in both from users’ perception and also from algorithmic accuracy.

Furthermore, cognitive and semantic aspects of a stimulus play an important role in visual and scene perception (Henderson, Weeks, Hollingworth, 1999, Rayner, 1998). Eye movements are driven by the properties of the visual world and processes in a person’s mind (Rayner, 1998). Eye tracking and usability evaluation studies try to investigate and understand user behaviour (Freedman, 1984, Jacob, 1991, Jacob, 1995, Lohse, Johnson, 1996, Rayner, 1998, Russo, Leclerc, 1994, Sanders, Simmons, Hoffman, 1979) with increasing interest to web page behaviour (Granka, Joachims, Gay, 2004, Jay, Stevens, Glencross, Chalmers, 2006, McCarthy, Sasse, Riegelsberger, 2003). A general conclusion is that user interaction depends on visual factors and scene semantics (general knowledge about the scene layout). Cognitive overload is a result of the boost of information presented on the web. Understanding how this information and cognitive overload affects user perception and web interaction can lead to solutions that improve the web experience of users. We believe that an initial step towards this goal is to understand web page visual perception and relate a user’s implicit understanding of web page visual complexity with its layout. This work presents the initial findings towards that goal.

In this paper, we focus on visual complexity, but interaction complexity is another aspect which is worthwhile to investigate. A web page can be visually simple, but it does not mean that users will not experience any difficulty while interacting with the page. Some of the possible problems can be the lack of information about input fields, the excessive number of mandatory input fields, the number of clicks needed to complete certain tasks, etc. Some user studies can be conducted to investigate all of the interaction problems and then try to develop a statistical model to predict interaction complexity scores for web pages.

Our work is not without limitations. In our work, we do not consider dynamic elements and their effect on complexity. Further studies can be conducted to better analyse the dynamic content impact on complexity. In our first study, we had 3300 ratings of 55 users on 30 pages, and in our second user study, we had 6240 ratings of 104 users on 30 web pages. Even though we have quite a high number of ratings, further studies can also be conducted with more users on more web pages with more diversity both in terms of technology used and also content presented. In our studies, we selected pages from Alexa top 100 and as we discussed above even though familiarity did not reveal any significant correlation with the users’ ratings or the complexity algorithm scores, but there were some exceptions. Therefore, to eliminate the confounding effect of familiarity (Tinio and Leder, 2009), further studies can be conducted with pages that are much less popular. In our first study, to generate a complexity score for each page, they were manually analysed by the first author. However, to ensure that the findings are consistent and reliable, a different methodology could be employed where there is a second blind rater do this activity and then the outcomes are compared (i.e., inter-rater reliability). Further studies could be conducted to better explore this alternative methodology. In both of these studies, we also tried our best to accommodate users’ qualitative feedback, but of course further studies can also be conducted to systematically capture and analyse the users’ qualitative feedback. To generate a visual complexity score with the ViCRAM tool, one needs to use the ACTF framework, however, the complexity algorithm can also be implemented as part of a web service in the future. In that case, a request can be sent to the web service for a particular page, and the web service can respond to the request by sending the visual complexity score of the page as a JSON file which can be easily processed by both machines and humans. This approach will make the complexity algorithm independent from the ACTF framework, thus allowing researchers and designers to compute a visual complexity score for a particular page without the need for installing the ACTF framework on their computers. Finally, even though we did not test the tool with designers or end users, the tool can be used by designers along with the aDesigner tool to balance web page visual complexity with usability and accessibility as its results were found to be significantly and highly related to the ratings of users. Further studies can also be conducted to observe designers how they use our tool and also the implications of generating a score for visual complexity. Further research is needed to understand how the score is given by our tool aid design (Machado, Romero, Nadal, Santos, Correia, Carballal, 2015, Rosenholtz, Dorai, Freeman, 2011, Rosenholtz, Li, Nakano, 2007). Finally, in our paper, we validate our model and prediction algorithm with a sequence of user studies, however, we could have also compared our complexity prediction accuracy with similar work, such as (Wu et al., 2013). To do this, one needs to ensure that both approaches are compared and validated with the same set of data and parameters. This requires a new study to be conducted to test these approaches with the same evaluation methodology.

6.1. Use cases and implications
As mentioned in the Introduction section, the preliminary findings of this study have been published in Michailidou et al. (2008). Since then, the complexity model has been started to be used in different kinds of studies. In particular, Eraslan et al., (2020b) conducted an eye-tracking study with a set of web pages with different levels of visual complexity to investigate whether high visual complexity on web pages causes non-equivalent web experience for people with autism. The assessment of the visual complexity of web pages has also been required to be used in the validation of web-related algorithms, especially for understanding whether an algorithm performs well on highly complex web pages. For example, Akpinar and Yesilada (2017) evaluated their extended VIPS algorithm for web page segmentation by using a set of web pages with different levels of visual complexity. Eraslan et al., (2020a) also evaluated their web-page segmentation with a set of web pages with varying levels of visual complexity. Another example from Eraslan et al. (2016) investigated whether the STA algorithm – an algorithm designed to identify a trending path of multiple users on a web page – performs well regardless of the visual complexity of web pages. As can be seen from those examples, our work has high potential to be used for different purposes.

7. Conclusions
Visual complexity which can be defined as “the degree of difficulty in providing a verbal description of an image” (Heaps, Handel, 1999, Oliva, Mack, Shrestha, Peeper, 2004), is hard to characterise and systematically assess. In this paper, we presented our end-to-end work that aims to provide a model that can predict the visual complexity of a web page. We mainly focus on predicting the visual complexity of a page as perceived by users where they define visually complex page as one with a lot of information and categories, not uniform in size and shape, has a large number of images, different colours, visual elements, and generates an overall distraction. Predicting complexity of a web page can have many applications ranging from usability testing support to providing better accessibility to visually impaired screen reader users.

Our experimental work explained in this paper allowed us to develop a model to predict visual complexity scores for web pages based on common aspects of their HTML Document Object Model (DOM) and their visual rendering. This model was then implemented into a tool called ViCRAM as part of an open source Eclipse framework. For each web page, a complexity score that determines the page’s level of visual complexity which we refer as Visual Complexity Score (VCS) is provided, and an overlay heatmap that mimics a user’s visual complexity perception by noting the most visually complex areas is generated.

To evaluate and validate the score generated by the tool with our proposed algorithms, a user evaluation study was conducted. This user study supported our hypothesis as the results revealed that the scores automatically generated by our tool are strongly correlated with the ratings of users. Therefore, users can have an initial perception of the visual layout of a web page and designers can use this framework to balance web page visual complexity with usability and accessibility. This is an important contribution to the web accessibility area because by using visual complexity, an identifiable measure, as an implicit marker of cognitive load, web pages can be designed that are easier to interact with, especially for visually impaired users.