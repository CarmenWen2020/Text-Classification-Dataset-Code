Machine learning models hosted in a cloud service are increasingly
popular but risk privacy: clients sending prediction requests to the
service need to disclose potentially sensitive information. In this
paper, we explore the problem of privacy-preserving predictions:
after each prediction, the server learns nothing about clientsâ€™ input
and clients learn nothing about the model.
We present MiniONN, the first approach for transforming an
existing neural network to an oblivious neural network supporting
privacy-preserving predictions with reasonable efficiency. Unlike
prior work, MiniONN requires no change to how models are trained.
To this end, we design oblivious protocols for commonly used operations in neural network prediction models. We show that MiniONN
outperforms existing work in terms of response latency and message sizes. We demonstrate the wide applicability of MiniONN by
transforming several typical neural network models trained from
standard datasets.
CCS CONCEPTS
â€¢ Security and privacy â†’ Privacy-preserving protocols;
KEYWORDS
privacy; machine learning; neural network predictions; secure twoparty computation
1 INTRODUCTION
Machine learning is now used extensively in many application
domains such as pattern recognition [10], medical diagnosis [25]
and credit-risk assessment [3]. Applications of supervised machine
learning methods have a common two-phase paradigm: (1) a training phase in which a model is trained from some training data, and
(2) a prediction phase in which the trained model is used to predict
categories (classification) or continuous values (regression) given
some input data. Recently, a particular machine learning framework,
neural networks (sometimes referred to as deep learning), has gained
much popularity due to its record-breaking performance in many
tasks such as image classification [37], speech recognition [20] and
complex board games [35].
Machine learning as a service (MLaaS) is a new service paradigm
that uses cloud infrastructures to train models and offer online prediction services to clients. While cloud-based prediction services
have clear benefits, they put clientsâ€™ privacy at risk because the
input data that clients submit to the cloud service may contain
sensitive information. A naive solution is to have clients download
the model and run the prediction phase on client-side. However,
this solution has several drawbacks: (1) it becomes more difficult
for service providers to update their models; (2) the trained model
may constitute a competitive advantage and thus requires confidentiality; (3) for security applications (e.g., spam or malware detection
services), an adversary can use the model as an oracle to develop
strategies for evading detection; and (4) if the training data contains sensitive information (such as patient records from a hospital)
revealing the model may compromise privacy of the training data
or even violate regulations like the Health Insurance Portability
and Accountability Act of 1996 (HIPAA).
A natural question to ask is, given a model, whether is it possible
to make it oblivious: it can compute predictions in such a way that
the server learns nothing about clientsâ€™ input, and clients learn
nothing about the model except the prediction results. For general
machine learning models, nearly practical solutions have been proposed [6, 14, 15, 58]. However, privacy-preserving deep learning
prediction models, which we call oblivious neural networks (ONN),
have not been studied adequately. Gilad-Bachrach et al. [28] proposed using a specific activation function (â€œsquareâ€) and pooling operation (mean pooling) during training so that the resulting model
can be made oblivious using their CryptoNets framework. CryptoNets transformations result in reasonable accuracy but incur high
performance overhead. Very recently, Mohassel and Zhang [44]
also proposed new activation functions that can be efficiently computed by cryptographic techniques, and use them in the training
phase of their SecureML framework. What is common to both approaches [28, 44] is that they require changes to the training phase
and thus are not applicable to the problem of making existing neural
models oblivious.
In this paper, we present MiniONN (pronounced minion), a practical ONN transformation technique to convert any given neural
network model (trained with commonly used operations) to an ONN.
Session C3: Machine Learning Privacy CCSâ€™17, October 30-November 3, 2017, Dallas, TX, USA 619
We design oblivious protocols for operations routinely used by neural network designers: linear transformations, popular activation
functions and pooling operations. In particular, we use polynomial
splines to approximate nonlinear functions (e.g., sigmoid and tanh)
with negligible loss in prediction accuracy. None of our protocols
require any changes to the training phase of the model being transformed. We only use lightweight cryptographic primitives such as
secret sharing and garbled circuits in online prediction phase. We
also introduce an offline precomputation phase to perform requestindependent operations using additively homomorphic encryption
together with the SIMD batch processing technique.
Our contributions are summarized as follows:
â€¢ We present MiniONN, the first technique that can transform any common neural network model into an oblivious neural network without any modifications to the
training phase (Section 4).
â€¢ We design oblivious protocols for common operations
in neural network predictions (Section 5). In particular,
we make nonlinear functions (e.g., sigmoid and tanh)
amenable for our ONN transformation with a negligible
loss in accuracy (Section 5.3.2).
â€¢ We build a full implementation of MiniONN and demonstrate its wide applicability by using it to transform neural
network models trained from several standard datasets
(Section 6). In particular, for the same models trained from
the MNIST dataset [38], MiniONN performs significantly
better than previous work [28, 44] (Section 6.1).
â€¢ We analyze how model complexity impacts both prediction accuracy and computation/communication overhead of the transformed ONN. We discuss how a neural
network designer can choose the right tradeoff between prediction accuracy and overhead. (Section 7).
2 BACKGROUND AND PRELIMINARIES
We now introduce the machine learning and cryptographic preliminaries (notation we use is summarized in Table 1).
ğ’® Server
ğ’ Client
X = {x1, ...} Input matrix for each layer
W = {w1, ...} Weight matrix for each layer
B = {b1, ...} Bias matrix for each layer
Y = {y1, ...} Output matrix for each layer
z = {z1, ...} Final predictions
u ğ’®â€™s share of the dot-product triple
v ğ’â€™s share of the dot-product triple
ZN Plaintext space
compar e (x, y) return 1 if x â‰¥ y, return 0 if x < y
E() / D() Additively homomorphic encryption/decryption
pk / sk Public/Private key
xË† E(pk, x )
Hx E(pk, [x1, ...])
âŠ• Addition between two ciphertexts
or a plaintext and a ciphertext
âŠ– Subtraction between two ciphertexts
or a plaintext and a ciphertext
âŠ— Multiplication between
a plaintext and a ciphertext
Table 1: Notation table.
2.1 Neural networks
A neural network consists of a pipeline of layers. Each layer receives
input and processes it to produce an output that serves as input
to the next layer. Conventionally, layers are organized so that the
bottom-most layer receives input data (e.g., an image or a word) and
the top-most layer outputs the final predictions. A typical neural
network1 processes input data in groups of layers, by first applying
linear transformations, followed by the application of a nonlinear
activation function. Sometimes a pooling operation is included to
aggregate groups of inputs.
We will now briefly describe these operations from the perspective of transforming neural networks to ONNs.
2.1.1 Linear transformations. The commonest linear transformations in neural networks are matrix multiplications and additions:
y := W Â· x + b, (1)
where x âˆˆ R
lÃ—1
is the input vector, y âˆˆ R
nÃ—1
is the output, W
âˆˆ R
nÃ—l
is the weight matrix and b âˆˆ R
nÃ—1
is the bias vector.
Convolution is a type of linear transformation, which computes
the dot product of small â€œweight tensorsâ€ (filters) and the neighborhood of an element in the input. The process is repeated, by
sliding each filter by a certain amount in each step. The size of the
neighborhood is called window size. The step size is called stride. In
practice, for efficiency reasons, convolution is converted into matrix multiplication and addition as well [18], similar to equation 1,
except that input and bias vector are matrices: Y := W Â· X + B.
Dropout and dropconnect are types of linear transformations,
where multiplication is done elementwise with zero-one random
masks [30].
Batch normalization is an adaptive normalization method [30]
that shifts outputs y to amenable ranges. During prediction, batch
normalization manifests as a matrix multiplication and addition.
2.1.2 Activation functions. Neural networks use nonlinear transformations of data â€“ activation functions â€“ to model nonlinear relationships between input data and output predictions. We identify
three common categories:
- Piecewise linear activation functions. This category of functions
can be represented as a set of n linear functions within specific
ranges, each of the type fi
(y) = aiy + bi
,y âˆˆ [yi
,yi+1], where
yi and yi+1 are the lower and upper bounds for the range. This
category includes the activation functions:
Identity function (linear): f (y) = [yi]
Rectified Linear Units (ReLU): f (y) = [max(0,yi
)]
Leaky ReLU: f (y) = [max(0,yi
) + a min(0,yi
)]
Maxout (n pieces): f (y) = [max(y1,. . . ,yn )]
- Smooth activation functions. A smooth function has continuous
derivatives up to some desired order over some domain. Some
commonly used smooth activation functions are:
Sigmoid (logistic): f (y) = [
1
1+e
âˆ’yi
]
Hyperbolic tangent (tanh): f (y) = [
e
2yi âˆ’1
e
2yi +1
]
1http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.
html
Session C3: Machine Learning Privacy CCSâ€™17, October 30-November 3, 2017, Dallas, TX, USA 620
Softplus: f (y) = [log(e
yi + 1)]
The sigmoid and tanh functions are closely related [30]:
tanh(x) = 2 Â· siĞ´moid(2x) âˆ’ 1. (2)
They are collectively referred to as sigmoidal functions.
- Softmax. Softmax is defined as:
f (y) = [
e
yi P
j e
yj
]
It is usually applied to the last layer to compute a probability
distribution in categorical classification. However, in prediction
phase, usually it is sufficient to use argmax over the outputs of the
last layer to predict the most likely outcome.
2.1.3 Pooling operations. Neural networks also commonly use
pooling operations that arrange input into several groups and aggregate inputs within each group. Pooling is commonly done by
calculating the average or the maximum value among the inputs
(mean or max pooling). Convolution and pooling operations are only
used if the input data has spatial structure (e.g., images, sounds).
2.1.4 Commonly used neural network operations. As discussed
in Section 2.1.1, all common linear transformations reduce to matrix
multiplications and additions in the prediction phase. Therefore
it is sufficient for an ONN transformation technique to support
making matrix multiplications and additions oblivious.
To get an idea of commonly used activation functions, consider
five top performing neural networks1
in the MNIST [38] and CIFAR10 [36] datasets. Collectively they support the following activation
functions: ReLU [39, 51, 57], leaky ReLU [32, 55], maxout [17, 43]
and tanh [19]. In addition, sigmoidal activation functions are commonly used in language modeling. Finally, as we saw in Section 2.1.3
common pooling operations are mean and max pooling.
We thus argue that for an ONN transformation technique
to be useful in practice, it should support all of the above
commonly used neural network operations. We describe these
in Sections 3 to 5.
Note that although softmax is a popular operation used in the
last layer, it can be left out of an ONN [28] (e.g., the input to the
softmax layer can be returned to the client) because its application
is order-preserving and thus will not change the prediction result.
2.2 Cryptographic preliminaries
2.2.1 Secure two-party computation. Secure two-party computation (2PC) is a type of protocols that allow two parties to jointly
compute a function (f1 (x,y), f2 (x,y)) â† ğ’¡(x,y) without learning
each otherâ€™s input. It offers the same security guarantee achieved
by a trusted third party TTP running ğ’¡: both parties submit their
inputs (i.e., x and y) to TTP, who computes and returns the corresponding output to each party, so that no information has been
leaked except the information that can be inferred from the outputs.
Basically, there are three techniques to achieve 2PC: arithmetic
secret sharing [8], boolean secret sharing [29] and Yaoâ€™s garbled
circuits [59, 60]. Each technique has its pros and cons, and they
can be converted among each other. The ABY framework [21] is a
state-of-the-art 2PC library that implements all three techniques.
2.2.2 Homomorphic encryption. A public key encryption scheme
is additively homomorphic if given two ciphertexts xË†1 := E(pk,x1)
and xË†2 := E(pk,x2), there is a public-key operation âŠ• such that
E(pk,x1 + x2) â† xË†1 âŠ• xË†2. Examples of such schemes are Paillierâ€™s
encryption [48], and exponential ElGamal encryption [24]. This is
simply referred to as homomorphic encryption (HE).
As an inverse of addition, subtraction âŠ– is trivially supported
by additively homomorphic encryption. Furthermore, adding or
multiplying a ciphertext by a constant is efficiently supported:
E(pk,a + x) â† a âŠ• xË† and E(pk,a Â· x1) â† a âŠ— xË†1.
To do both addition and multiplication between two ciphertexts,
fully homomorphic encryption (FHE) or leveled homomorphic encryption (LHE) is needed. However, FHE requires expensive bootstrapping operations and LHE only supports a limited number of
homomorphic operations.
2.2.3 Single instruction multiple data (SIMD). The ciphertext of
a (homomorphic) encryption scheme is usually much larger than
the data being encrypted, and the homomorphic operations on the
ciphertexts take longer time than those on the plaintexts. One way
to alleviate this issue is to encode several messages into a single
plaintext and use the single instruction multiple data (SIMD) [54]
technique to process these encrypted messages in batch without
introducing any extra cost. The LHE library [23] has implemented
SIMD based on the Chinese Reminder Theorem (CRT). In this paper,
we use Hx to denote the encryption of a vector [x1,...,xn] in batch
using the SIMD technique.
The SIMD technique can also be applied to secure two-party
computation to reduce the memory footprint of the circuit and
improve the circuit evaluation time [11]. In traditional garbled
circuits, each wire stores a single input, while in the SIMD version,
an input is split across multiple wires so that each wire corresponds
to multiple inputs. The ABY framework [21] supports this.
3 PROBLEM STATEMENT
We consider the generic setting for cloud-based prediction services,
where a server ğ’® holds a neural network model, and clients ğ’s
submit their input to learn corresponding predictions. The model
is defined as:
z := (WL Â· fLâˆ’1 (...f1 (W1 Â· X + B1)...) + bL ) (3)
The problem we tackle is how to design oblivious neural networks: after each prediction, ğ’® learns nothing about X, and ğ’ learns nothing
about (W1,W2,...,WL ) and (B1,B2,...,bL ) except z. Our security
definition follows the standard ideal-world/real-world paradigm:
the adversaryâ€™s view in real-wold is indistinguishable to that in
ideal-world.
Adversary model. We assume that either ğ’® or ğ’ can be compromised
by an adversary ğ’œ, but not at the same time. We assume ğ’œ to be
semi-honest, i.e., it directs the corrupted party to follow the protocol specification in real-world, and submits the inputs it received
from the environment to TTP in ideal-world. We rely on efficient
implementations of primitives (like 2PC in ABY framework [21])
that are secure against semi-honest adversaries.
A compromised ğ’® tries to learn the values in X, and a compromised ğ’ tries to learn the values in W and B. We do not aim to
protect the sizes of X, W, B, and which f () is being used. However,
ğ’® can protect such information by adding dummy layers. Note that
ğ’s can, in principle, use ğ’®â€™s prediction service as a blackbox oracle
Session C3: Machine Learning Privacy CCSâ€™17, October 30-November 3, 2017, Dallas, TX, USA 621
to extract an equivalent or near-equivalent model (model extraction
attacks [56]), or even infer the training set (model inversion [26]
or membership inference attacks [53]). However, in a client-server
setting, ğ’® can rate limit prediction requests from a given ğ’, thereby
slowing down or bounding this information leakage.
4 MINIONN OVERVIEW
In this section, we explain the basic idea of MiniONN by transforming a toy neural network of the form:
z := Wâ€²
Â· f (W Â· x + b) + b
â€²
(4)
where x =
"
x1
x2
#
, W =
"
w1,1 w1,2
w2,1 w2,2
#
, b =
"
b1
b2
#
, Wâ€² =
"
w
â€²
1,1
w
â€²
1,2
w
â€²
2,1
w
â€²
2,2
#
and b
â€² =
"
b
â€²
1
b
â€²
2
#
.
The core idea of MiniONN is to have ğ’® and ğ’ additively share
each of the input and output values for every layer of a neural
network. That is, at the beginning of every layer, ğ’® and ğ’ will each
hold a â€œshareâ€ such that modulo addition of the shares is equal to
the input to that layer in the non-oblivious version of that neural
network. The output values will be used as inputs for the next layer.
To this end, we have ğ’® and ğ’ first engage in a precomputation
phase (which is independent of ğ’â€™s input x), where they jointly
generate a set of dot-product triplets âŸ¨u,v,w Â· râŸ© for each row of the
weight matrices (W and Wâ€²
in this example). Specifically, for each
row w, ğ’® and ğ’ run a protocol that securely implements the ideal
functionality ğ’¡triplet (in Figure 1) to generate dot-product triplets,
such that:
u1 +v1 (mod N) = w1,1r1 + w1,2r2,
u2 +v2 (mod N) = w2,1r1 + w2,2r2,
u
â€²
1
+v
â€²
1
(mod N) = w
â€²
1,1
r
â€²
1
+ w
â€²
1,2
r
â€²
2
,
u
â€²
2
+v
â€²
2
(mod N) = w
â€²
2,1
r
â€²
1
+ w
â€²
2,2
r
â€²
2
.
Input:
â€¢ ğ’®: a vector w âˆˆ Z
n
N
;
â€¢ ğ’: a random vector r âˆˆ Z
n
N
.
Output:
â€¢ ğ’®: a random number u âˆˆ ZN ;
â€¢ ğ’: v âˆˆ ZN , s.t., u +v (mod N) = w Â· r.
Figure 1: Ideal functionality ğ’¡triplet: generate a dot-product triplet.
When ğ’ wants to ask ğ’® to compute the predictions for a vector x = [x1,x2], for each xi
, ğ’ chooses a triplet generated in the
precomputation phases and uses its ri value to blind xi
.
x
ğ’
1
:= r1, x
ğ’®
1
:= x1 âˆ’ r1 (mod N),
x
ğ’
2
:= r2, x
ğ’®
2
:= x2 âˆ’ r2 (mod N).
ğ’ then sends x
ğ’® to ğ’®, who calculates
y
ğ’®
1
:= w1,1x
ğ’®
1
+ w1,2x
ğ’®
2
+ b1 + u1 (mod N),
y
ğ’®
2
:= w2,1x
ğ’®
1
+ w2,2x
ğ’®
2
+ b2 + u2 (mod N).
Meanwhile, ğ’ sets:
y
ğ’
1
:= v1 (mod N),
y
ğ’
2
:= v2 (mod N).
It is clear that
y
ğ’
1
+ y
ğ’®
1
(mod N) = w1,1x1 + w1,2x2 + b1 and
y
ğ’
2
+ y
ğ’®
2
(mod N) = w2,1x1 + w2,2x2 + b2.
Therefore, at the end of this interaction, ğ’® and ğ’ additively share
the output values y resulting from the linear transformation in
layer 1 without ğ’® learning the input x and neither party learning y.
In Section 5.2 we describe the detailed operations for making linear
transformations oblivious.
For the activation/pooling operation f (), ğ’® and ğ’ run a protocol
that securely implements the ideal functionality in Figure 2, which
implicitly reconstructs each yi
:= y
ğ’
i
+ y
ğ’®
i
(mod N) and returns
x
ğ’®
i
:= f (yi
) âˆ’ x
ğ’
i
to ğ’®, where x
ğ’
i
is ğ’â€™s component of a previously
shared triplet from the precompuation phase, i.e., x
ğ’
1
:= r
â€²
1
and
x
ğ’
2
:= r
â€²
2
. In Sections 5.3 and 5.4, we show how the ideal functionality in Figure 2 can be concretely realized for commonly used
activation functions and pooling operations.
Input:
â€¢ ğ’®: y
ğ’® âˆˆ ZN ;
â€¢ ğ’: y
ğ’ âˆˆ ZN .
Output:
â€¢ ğ’®: a random number x
ğ’® âˆˆ ZN ;
â€¢ ğ’: x
ğ’ âˆˆ ZN s.t., x
ğ’ +x
ğ’® (mod N) = f (y
ğ’® +y
ğ’
(mod N)).
Figure 2: Ideal functionality: oblivious activation/pooling f ().
The transformation of the final layer is the same as the first layer.
Namely, ğ’® calculates:
y
ğ’®
1
:= w
â€²
1,1
x
ğ’®
1
+ w
â€²
1,2
x
ğ’®
2
+ b
â€²
1
+ u
â€²
1
(mod N),
y
ğ’®
2
:= w
â€²
2,1
x
ğ’®
1
+ w
â€²
2,2
x
ğ’®
2
+ b
â€²
2
+ u
â€²
2
(mod N);
and ğ’ sets:
y
ğ’
1
:= v
â€²
1
(mod N),
y
ğ’
2
:= v
â€²
2
(mod N).
At the end, ğ’® returns [y
ğ’®
1
,y
ğ’®
2
] back to ğ’, who outputs the final
predictions:
z1 := y
ğ’
1
+ y
ğ’®
1
,
z2 := y
ğ’
2
+ y
ğ’®
2
.
Note that MiniONN works in ZN , while neural networks require
floating-point calculations. A simple solution is to scale the floatingpoint numbers up to integers by multiplying the same constant to
all values and drop the fractional parts. A similar technique is used
to reduce memory requirements in neural network predictions,
at negligible loss of accuracy [42]. We must make sure that the
absolute value of any (intermediate) results will not exceed âŒŠN/2âŒ‹.
5 MINIONN DESIGN
5.1 Dot-product triplet generation
Recall that we introduce a precomputation phase to generate dotproduct triplets, which are similar to the multiplication triplets used
in secure computations [8]. Multiplication triplets are typically
generated in two ways: using homomorphic encryption (HE-based)
or using oblivious transfer (OT-based). The former is efficient in
terms of communication, whereas the latter is efficient in terms of
computation. Both approaches can be optimized for the dot-product
Session C3: Machine Learning Privacy CCSâ€™17, October 30-November 3, 2017, Dallas, TX, USA 622
generation [44]. In the HE-based approach, dot-products can be
calculated directly on ciphertexts, so that both communication and
decryption time can be reduced.
We further improve the HE-based approach using the SIMD
batch processing technique. The protocol is described in Figure 3.
Using the SIMD technique, ğ’® encrypts the whole vector w into a
single ciphertext of additively homomorphic encryption. ğ’ computesHu â† râŠ—wHâŠ–v, where r and v are random vectors generated by
ğ’. ğ’® decryptsHu and outputs the sum of u. Meanwhile, ğ’ outputs the
sum of v. Even though ğ’® and ğ’ need to generate new dot-product
triplets for each prediction request, ğ’® only needs to transfer wHs
once for all predictions. Furthermore, it can pack multiple ws into
a single ciphertext if needed.
Input:
ğ’®: w âˆˆ Z
n
N
ğ’: r âˆˆ Z
n
N
Output:
ğ’®: a random number u âˆˆ ZN ;
ğ’: v âˆˆ ZN , s.t., u +v (mod N) = w Â· r.
ğ’®: ğ’:
wH â† E(pks ,w) v
$â†âˆ’ Z
n
N wH
Hu â† r âŠ— wH âŠ– v
Hu
u â†
P
(D(sks ,Hu)) v â†
P
(v)
output u output v
Figure 3: Dot-product triplet generation.
Theorem 1. The protocol in Figure 3 securely implements ğ’¡triplet
in the presence of semi-honest adversaries, if E() is semantically secure.
Proof. Our security proof follows the ideal-world/real-world
paradigm: in real-world, parties interact according to the protocol specification, whereas in ideal-world, parties have access to a
trusted party TTP that implements ğ’¡triplet. The executions in both
worlds are coordinated by the environment ğ’ , who chooses the
inputs to the parties and plays the role of a distinguisher between
the real and ideal executions. We aim to show that the adversaryâ€™s
view in real-wold is indistinguishable to that in ideal-world.
Security against a semi-honest server. First, we prove security against
a semi-honest server by constructing an ideal-world simulator Sim
that performs as follows:
(1) receives w from the environment ğ’ ; Sim sends w to TTP and
gets the result u;
(2) starts running ğ’® on input w, and receives wH;
(3) randomly splits u into a vector u
â€²
s.t., u =
P
u
â€²
;
(4) encrypts u
â€² using ğ’®â€™s public key and returns uHâ€²
to ğ’®;
(5) outputs whatever ğ’® outputs.
Next, we show that the view Sim simulates for ğ’® is indistinguishable
from the view of ğ’® interacting in the real execution. ğ’®â€™s view in
the real execution is u = w Â· r âˆ’ v while its view in the ideal
execution is u
â€² = [r
â€²
1
,...,r
â€²
n
]. So we only need to show that any
element wiri âˆ’vi
(mod N) in u is indistinguishable from a random
number r
â€²
i
. This is clearly true since vi
is randomly chosen.
At the end of the simulation, ğ’® outputs u â†
P
u, which is the
same as real execution. Thus, we claim that the output distribution
of ğ’  in real-world is computationally indistinguishable from that
in ideal-world.
Security against a semi-honest client. Next, we prove security against
a semi-honest client by constructing an ideal-world simulator Sim
that works as follows:
(1) receives r from ğ’ , and sends it to TTP;
(2) starts running ğ’ on input r;
(3) constructs wH
â€² â† E(pkâ€²
s
,[0,...,0]) where pkâ€²
s
is randomly
generated by Sim;
(4) gives wH
â€²
to ğ’;
(5) outputs whatever ğ’ outputs.
ğ’â€™s view in real execution is E(pks ,w), which is computationally indistinguishable from its view in ideal execution i.e., E(pkâ€²
s
,[0,...,0])
due to the semantic security of E(). Thus, the output distribution
of ğ’  in real-world is computationally indistinguishable from that
in ideal-world. â–¡
5.2 Oblivious linear transformations
Recall that when ğ’ wants to request ğ’® to compute predictions for
an input X, it blinds each value of X using a random value r from a
dot-product triplet generated earlier: x
ğ’® := x âˆ’ r (mod N). Then,
ğ’ sets X
ğ’ = R, and sends X
ğ’® to ğ’®. The security of the dot-product
generation protocol guarantees that ğ’® knows nothing about the r
values. Consequently, ğ’® cannot get any information about X from
X
ğ’® if all rs are randomly chosen by ğ’ from ZN .
Upon receiving X
ğ’® , ğ’® will input it to the first layer which is typically a linear transformation layer. As we discussed in Section 2.1,
all linear transformations can be turned into matrix multiplications/additions: Y = W Â· X + B. Figure 4 shows the oblivious linear
transformation protocol. For each row of W and each column of
X
ğ’
, ğ’® and ğ’ jointly generate a dot-product triplet: u +v (mod N) =
wÂ·x
ğ’
. Since X
ğ’
is independent of X, they can generate such triplets
in a precomputation phase. Next, ğ’® calculates Y
ğ’® := WÂ·X
ğ’® +B+U,
and meanwhile ğ’ sets Y
ğ’
:= V. Consequently, each element of Y
ğ’®
and Y
ğ’
satisfy:
y
ğ’® + y
ğ’ = w Â· x
ğ’® + b + u +v
= w1 (x1 âˆ’ x
ğ’
1
)+,...,+wl
(xl âˆ’ x
ğ’
l
) + b + u +v
= (w1x1+,...,+wl xl + b) âˆ’ (w1x
ğ’
1 +,...,+wl x
ğ’
l
) + u +v
= y
Due to the fact that âŸ¨U,VâŸ© are securely generated by ğ’¡triplet, the
outputs of this layer (which are the inputs to the next layer) are also
randomly shared between ğ’® and ğ’, i.e., Y
ğ’ = V and Y
ğ’® = Y âˆ’ V
can be used as inputs for the next layer directly.
It is clear that the view of both ğ’® and ğ’ are identical to their
views under the dot-product triplet generation protocol. Therefore,
the oblivious linear transformation protocol is secure if ğ’¡triplet is
securely implemented.
Session C3: Machine Learning Privacy CCSâ€™17, October 30-November 3, 2017, Dallas, TX, USA 623
Input:
ğ’®: W âˆˆ Z
mÃ—l
N
, X
ğ’® âˆˆ Z
lÃ—n
N
, B âˆˆ Z
mÃ—n
N
ğ’: X
ğ’ âˆˆ Z
lÃ—n
N
Output:
ğ’®: A random matrix Y
ğ’®
ğ’: Y
ğ’
s.t., Y
ğ’ + Y
ğ’® = W Â· (X
ğ’ + X
ğ’® ) + B
ğ’®: ğ’:
precomputation
for i = 1 to m
for j = 1 to n
(ui,j
,vi,j
) â† ğ’¡triplet(wi
,x
ğ’
j
)
end
end
Y
ğ’® := W Â· X
ğ’® + B + U Y
ğ’
:= V
output Y
ğ’® output Y
ğ’
Figure 4: Oblivious linear transformation.
A linear transformation layer can also follow an activation layer
or a pooling layer. So, we need to design the oblivious activation/pooling operations in a way that their outputs can be the
inputs to linear transformations: X
ğ’® and X
ğ’
s.t. X
ğ’® + X
ğ’ = X and
X
ğ’ has been used to generate the dot-product triplets for the next
layer. See the following sections.
5.3 Oblivious activation functions
In this section, we introduce the oblivious activation function which
receives y
ğ’
from ğ’ and y
ğ’® from ğ’®, and outputs x
ğ’
to ğ’ and x
ğ’® :=
f (y
ğ’® +y
ğ’
) âˆ’ x
ğ’
to ğ’®, where x
ğ’
is a random number generated by
ğ’. Note that if the next layer is a linear transformation layer, x
ğ’
should be the random value that has been used by ğ’ to generate
a dot-product triplet in the precomputation phase. On the other
hand, if the next layer is a pooling layer, x
ğ’
can be generated on
demand.
5.3.1 Oblivious piecewise linear activation functions. Piecewise
linear activation functions are widely used in image classifications
due to their outstanding performance in training phase as demonstrated by Krizhevsky et al. [37]. We take ReLU as an example to
illustrate how to transform piecewise linear functions into their
oblivious forms. Recall that ReLU is f (y) = max (0,y), where y is
additively shared between ğ’® and ğ’. An oblivious ReLU protocol
will reconstruct y and returnmax (0,y) âˆ’x
ğ’
to ğ’®. This is equivalent
to the ideal functionality ğ’¡ReLU in Figure 5. Actually, we compare
y with N
2
: y >
N
2
implies y is negative (recall that absolute values
of all intermediate results will not exceed âŒŠN/2âŒ‹).
ğ’¡ReLU can be trivially implemented by a 2PC protocol. Specifically, we use a garbled circuit to reconstruct y and calculate b :=
compare (y,0) to determine whether y â‰¥ 0 or not. If y â‰¥ 0, it returns
y, otherwise, it returns 0. This is achieved by multiplying y with
b. The only operations we need for oblivious ReLU are +,âˆ’,Â· and
Input:
â€¢ ğ’®: y
ğ’® âˆˆ ZN ;
â€¢ ğ’: y
ğ’
,r âˆˆ ZN .
Output:
â€¢ ğ’®: x
ğ’® := compare (y,0) Â· y âˆ’ r (mod N) where y = y
ğ’® +
y
ğ’
(mod N);
â€¢ ğ’: x
ğ’
:= r.
Figure 5: The ideal functionality ğ’¡ReLU.
compare, all of which are supported by the 2PC library [21] we used.
So both implementation and security argument are straightforward.
Oblivious leaky ReLU can be constructed in the same way as
oblivious ReLU, except that ğ’® gets:
x
ğ’® := compare (y,0) Â· a Â· y + (1 âˆ’ compare (y,0)) Â· y âˆ’ r (mod N).
5.3.2 Oblivious smooth activation functions. Unlike piecewise
linear functions, it is non-trivial to make smooth functions oblivious. For example, in the sigmoid function f (y) =
1
1+e
âˆ’y , both e
y
and division are expensive to be computed by 2PC protocols [49].
Furthermore, it is difficult to keep track of the floating point value
of e
y
, especially when y is blinded. It is well-known that such
functions can be approximated locally by high-degree polynomials,
but oblivious protocols can only handle low-degree approximation
polynomials efficiently. To this end, we adapt an approximation
method that can be efficiently computed by an oblivious protocol
and incurs negligible accuracy loss.
Approximation of smooth functions. A smooth function f () can be
approximated by a set of piecewise continuous polynomials, i.e.,
splines [22]. The idea is to split f () into several intervals, in each of
which, a polynomial is used to to approximate f (). The polynomials
are chosen such that the overall goodness of fit is maximized. The
approximation method is detailed in the following steps:
(1) Set the approximation range [Î±1,Î±n], select n equally spaced
samples (including Î±1 and Î±n). The resulting sample set is
{Î±1,...,Î±n }
(2) For each Î±i
, calculate Î²i
:= f (Î±i
).
(3) Find m switchover positions (i.e., knots) for polynomials
expressions:
(a) fit an initial approximation Â¯f of order d for the dataset
{Î±i
,Î²i} using polynomial regression (without knots);
(b) select a new knot Î±Ë™i âˆˆ {Î±1,. . . ,Î±n } and fit two new polynomial expressions on each side of the knot (the knot is
chosen such that the overall goodness of fit is maximized);
(c) repeat (b) until the number of knots equals m.
The set of knots is now {Î±Ë™1,...,Î±Ë™m }. Note that Î±Ë™1 = Î±1 and
Î±Ë™m = Î±n.
(4) Fit a smoothing spline ([22], Chapter 5) of the same order using the knots {Î±i} on the dataset {Î±i
,Î²i} and extract the polynomial expression Pi
(Î±) in the each interval [Î±Ë™i
,Î±Ë™i+1],i âˆˆ
{1,m âˆ’ 1}.
2
(5) Set boundary polynomials P0 () (for Î± < Î±Ë™1) and Pm () (for
Î± > Î±Ë™m), which are chosen specifically for f () to closely
2We use the functions in the library scipy.interpolate.UnivariateSpline and
numpy.polyfit [34]
Session C3: Machine Learning Privacy CCSâ€™17, October 30-November 3, 2017, Dallas, TX, USA 624
approximate the behaviour beyond the ranges [Î±1,Î±n]. Thus,
we split f () into m + 1 intervals, and each has a separate
polynomial expression.3
(6) The final approximation is:
Â¯f (Î±) =
ï£±ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´
ï£²
ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´
ï£³
P0 (Î±) if Î± < Î±Ë™1
P1 (Î±) if Î±Ë™1 â‰¤ Î± < Î±Ë™2
. . .
Pmâˆ’1 (Î±) if Î±Ë™mâˆ’1 â‰¤ Î± < Î±Ë™m
Pm (Î±) if Î± â‰¥ Î±Ë™m,
(5)
Note that any univariate monotonic functions can be fitted by
the above procedure.
Oblivious approximated sigmoid. We take sigmoid as an example to
explain how to transform smooth activation functions into their
oblivious forms. We set the polynomial degree d as 1, since linear
functions (as opposed to higher-degree polynomials) are faster and
less memory-consuming to be computed by 2PC. The approximated
sigmoid function is as follows:
Â¯f (y) =
ï£±ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´
ï£²
ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£´
ï£³
0 if y < y1
a1y + b1 if y1 â‰¤ y < y2
. . .
amâˆ’1y + bmâˆ’1 if ymâˆ’1 â‰¤ y < ym
1 if y â‰¥ ym,
(6)
We will show (in Section 6.2) that it approximates sigmoid with
negligible accuracy loss.
The approximated sigmoid function (Equation 6) is in fact a
piecewise linear function. So it can be transformed in the same
way as we explained in Section 5.3.1. The ideal functionality for the
approximated sigmoid ğ’¡sigmoid is shown in Figure 6. Correctness
of this functionality follows the fact that, for yi â‰¤ y < yi+1:
x = ((aiy + bi
) âˆ’ (ai+1y + bi+1)) + ((aiy + bi
) âˆ’ (ai+1y + bi+1))
+... + ((amâˆ’1y + bmâˆ’1) âˆ’ 1) + 1
Input:
â€¢ ğ’®: y
ğ’® âˆˆ ZN ;
â€¢ ğ’: y
ğ’
,r âˆˆ ZN .
Output:
â€¢ ğ’®: x
ğ’® := compare (y1,y) Â· (0 âˆ’ (a1y + b1))
+compare (y2,y) Â· ((a1y + b1) âˆ’ (a2y + b2))
...
+compare (ymâˆ’1,y) Â· ((amâˆ’1y + bmâˆ’1) âˆ’ 1) + 1
âˆ’r (mod N), where y = y
ğ’® + y
ğ’
(mod N);
â€¢ ğ’: x
ğ’
:= r.
Figure 6: The ideal functionality ğ’¡sigmoid.
Even though it is more complex than ğ’¡ReLU, it can still be realized
easily using the basic functionalities provided by 2PC.
In summary, we support all activation functions that are both:
3We apply post-processing to the polynomials to ensure they are within upper and
lower bounds of the function f (), and to ensure that the approximate function Â¯f is
monotonic (if f () is).
(1) monotonic in ranges (âˆ,0) and (0,âˆ);
4
and
(2) either piecewise-polynomial or approximable.
Most commonly used activation functions belong to this class, except softmax that violates the second condition. But softmax can
be replaced by argmax (Section 2.1.2) or left out (Section 2.1).
5.4 Oblivious pooling operations
The pooling layer arranges the inputs into several groups and take
the max or mean of the elements in each group. For mean pooling,
we just have ğ’® and ğ’ calculate the sum of their respective shares and
keep track of the divisor. For max pooling, we use garbled circuits to
realize the ideal functionality ğ’¡max in Figure 7, which reconstructs
each yi and returns the largest one masked by a random number.
The max function can be easily achieved by the compare function.
Inputs:
â€¢ ğ’®: {y
ğ’®
1
,...,y
ğ’®
n
};
â€¢ ğ’: {y
ğ’
1
,...,y
ğ’
n
}, r.
Outputs:
â€¢ ğ’®: x
ğ’® := max (y1,...,yn ) âˆ’ r (mod N) where y1 = y
ğ’®
1
+
y
ğ’
1
(mod N) ... yn = y
ğ’®
n + y
ğ’
n
(mod N);
â€¢ ğ’: x
ğ’
:= r.
Figure 7: The ideal functionality ğ’¡max.
Note that the oblivious maxout activation can be trivially realized
by the ideal functionality ğ’¡max.
5.5 Remarks
5.5.1 Oblivious square function. The square function (i.e., f (y) =
y
2
) is also used as an activation function in [28, 44], because it is
easier to be transformed into an oblivious form. We implement
an oblivious square function by realizing the ideal functionality in
Figure 8 using arithmetic secret sharing.
Input:
â€¢ ğ’®: y
ğ’® âˆˆ ZN ;
â€¢ ğ’: y
ğ’
,r âˆˆ ZN .
Output:
â€¢ ğ’®: x
ğ’® := y
2 âˆ’ r (mod N) where y = y
ğ’® + y
ğ’
(mod N);
â€¢ ğ’: x
ğ’
:= r.
Figure 8: The ideal functionality ğ’¡Square.
5.5.2 Dealing with large numbers. Recall that we must make
sure that the absolute value of any (intermediate) results will not exceed âŒŠN/2âŒ‹. However, the data range grows exponentially with the
number of multiplications, and it grows even faster when the floating point numbers are scaled to integers. Furthermore, the SIMD
4This condition guarantees that our scaling technique (i.e., scale the floating-point
numbers up to integers by multiplying the same constant to all values and drop the
fractional parts) does not change the ranking order and thus does not impact prediction
accuracy. Recall that the model finally outputs a set of probabilities, one for each class.
The class with maximal probability is the prediction.
Session C3: Machine Learning Privacy CCSâ€™17, October 30-November 3, 2017, Dallas, TX, USA 625
technique will shrink the plaintext space so that it cannot encrypt
large numbers. As a result, only a limited number of multiplications
can be supported.
To this end, CryptoNets uses Chinese Remainder Theorem (CRT)
to split large numbers into multiple small parts, work on each part
individually, and combines the results in the end [28]. This method
allows encryptions of exponentially large numbers in linear time
and space, but the overhead grows linearly with the number of
split parts. On the other hand, SecureML has both parties truncate
their individual shares independently [44]. This method may incur
a small error in each intermediate result, which may affect the final
prediction accuracy.
We implement the ideal functionality in Figure 9 using garbled
circuit to securely scale down the data range without affecting
accuracy. It reconstructs y and shift it left by L bits, where L is
a constant known to both ğ’® and ğ’. This is equivalent to x
ğ’® := h
y
2
L
i
âˆ’ r (mod N). They can run this protocol after each layer, or
whenever needed.
Input:
â€¢ ğ’®: y
ğ’® âˆˆ ZN ;
â€¢ ğ’: y
ğ’
,r âˆˆ ZN .
Output:
â€¢ ğ’®: x
ğ’® := leftshift (y,L) âˆ’ r (mod N) where y = y
ğ’® +
y
ğ’
(mod N);
â€¢ ğ’: x
ğ’
:= r.
Figure 9: The ideal functionality ğ’¡trunc.
5.5.3 Security. We have proved the security for the dot-product
triplet generation, and other operations are directly implemented
using 2PC protocols. So the security guarantees of all operations
are straightforward. Furthermore, the output of each operation is
randomly shared between ğ’® and ğ’. As stated in Lemma 2 of [12], a
protocol that ends with secure re-sharing of output is universally
composable. So, after composition, the entire ONN is secure.
6 PERFORMANCE EVALUATION
We implemented MiniONN in C++ using Boost5
for networking. We
used the ABY [21] library for secure two-party computation with
128-bit security parameter and SIMD circuits. We used YASHE [13]
for additively homomorphic encryption, a SIMD version of which is
supported by the SEAL library [23]. The YASHE encryption scheme
maps plaintext messages from the ring ZN [x]/(x
n + 1) to the ring
Zq[x]/(x
n + 1). The ciphertext modulus q determines the security
level. The SEAL library automatically chooses a secure q given the
polynomial degree n (i.e., SIMD batch size). Choice of n is a tradeoff
between parallelism and efficiency of a single encryption. We first
set n =4 096 so that we can encrypt 4 096 elements together in a
reasonable encryption time and ciphertext size. Then we chose the
largest possible plaintext modulus: N =101 285 036 033, which is
large enough for the needed precision since we securely scale down
the value when it becomes large as we discussed in Section 5.5.2.
5http://www.boost.org
To evaluate its performance, we ran the server-side program on
a remote computer (Intel Core i5 CPU with 4 3.30 GHz cores and 16
GB memory) and the client-side program on a local desktop (Intel
Core i5 CPU machine with 4 3.20 GHz cores and 8 GB memory).
We used the chrono library in C++ for time measurement and used
TCPdump for bandwidth measurement. We measured response
latency (including the network delay) and message sizes during
the whole procedure, i.e., from the time ğ’ begins to generate its
request to the time it obtains the final predictions. Each experiment
was repeated 5 times and we calculated the mean and standard
deviation. The standard deviations in all reported results are less
than 3%.
6.1 Comparisons with previous work
The MNIST dataset [38] consists of 70 000 black-white hand-written
digit images (of size 1 Ã— 28 Ã— 28: width and height are 28 pixels) in
10 classes. There are 60 000 training images and 10 000 test images.
Since previous work use MNIST to evaluate their techniques, we
use it to provide a direct comparison with prior work..
Neural network in SecureML [44]. We reproduced the model (Figure 10) presented in SecureML [44]. It uses multi-layer perceptron
(MLP) model with square as the activation function and achieves an
accuracy of 93.1% in the MNIST dataset. We improve the accuracy
of this model to 97.6% by using the Limited-memory BFGS [40] optimization algorithm and batch normalization during training. We
transformed this model with MiniONN and compared the results
with those reported in [28].
(1) Fully Connected: input image 28 Ã— 28, connects the incoming 784 nodes to
the outgoing 128 nodes: R
128Ã—1 â† R
128Ã—784
Â· R
784Ã—1
.
(2) Square Activation: squares the value of each input.
(3) Fully Connected: connects the incoming 128 nodes to the outgoing 128 nodes:
R
128Ã—1 â† R
128Ã—128
Â· R
128Ã—1
.
(4) Square Activation: squares the value of each input.
(5) Fully Connected: fully connects the incoming 128 nodes to the outgoing 10
nodes: R
10Ã—1 â† R
10Ã—128
Â· R
128Ã—1
.
Figure 10: The neural network presented in SecureML [44].
The results (Table 2) show that MiniONN achieves comparable
online performance and significantly better offline performance.
We take the first layer as an example to explain why the SIMD batch
processing technique improves the performance of offline phase.
The first layer connects 784 incoming nodes to 128 outgoing nodes,
which leads to a matrix multiplication: R
128Ã—1 â† R
128Ã—784
Â· R
784Ã—1
.
In SecureML [44], ğ’ encrypts each of the 784 elements separately
and sends them to ğ’®, which leads to 784 encryptions and ciphertext
transfers. ğ’® applies each row of the matrix to the ciphertexts to calculate an encrypted dot-product, which leads to 784Ã—128 = 100 352
homomorphic multiplications. Then ğ’® returns the resulting 128
ciphetexts to ğ’, who decrypts them, which leads to another 128
ciphertext transfers and 128 decryptions. On the other hand, we
duplicate the 784 elements into 128 copies, and encrypt them into
25 ciphertexts, since each ciphertext can pack 4096 elements. ğ’®
encodes the matrix into 25 batches and multiplies them to the ciphertexts, which only leads to 25 homomorphic multiplications.
Table 3 summarizes this comparison.
Session C3: Machine Learning Privacy CCSâ€™17, October 30-November 3, 2017, Dallas, TX, USA 626
Square/MLP/MNIST Latency (s) Message Sizes (MB) Accuracy
(Figure 10) offline online offline online %
by SecureML [44] 4.7 0.18 - - 93.1
by MiniONN 0.9 0.14 3.8 12 97.6
Table 2: Comparison: MiniONN vs. SecureML [44].
SecureML [44] MiniONN
# homomorphic encryptions 784 25
# homomorphic multiplications 100 352 25
# ciphertext transfers 912 50
# homomorphic decryptions 128 25
Table 3: Comparison: MiniONN vs. SecureML [44], dot-product
triplet generations.
Neural network in CryptoNets [28]. We reproduced the model (Figure 11) presented in CryptoNets [28]. It is a CNN model with square
as the activation function as well, and uses mean pooling instead
of max pooling. Due to the convolution operation, it achieves a
higher accuracy of 98.95% in the MNIST dataset. We transformed
this model with MiniONN and compared its performance with the
results reported in CryptoNets [28]. Table 4 shows that MiniONN
achieves 230-fold reduction in latency and 8-fold reduction in message sizes, without degradation in accuracy. CryptoNets uses the
SIMD technique to batch different requests to achieve a throughput
of 51 739 predictions per hour, but these requests must be from the
same client. In scenarios where the same client sends a very large
number of prediction requests and can tolerate response latency
in the order of minutes, CryptoNets can achieve 6-fold throughput
compared to MiniONN. In scenarios where each client sends only
a small number of requests but needs quick responses, MiniONN
decisively outperforms CryptoNets.
(1) Convolution: input image 28 Ã— 28, window size 5 Ã— 5, stride (2, 2), number
of output channels 5. It can be converted to matrix multiplication [18]:
R
5Ã—169 â† R
5Ã—25
Â· R
25Ã—169
.
(2) Square Activation: squares the value of each input.
(3) Pool: combination of mean pooling and linear transformation: R
100Ã—1 â†
R
100Ã—845
Â· R
845Ã—1
.
(4) Square Activation: squares the value of each input.
(5) Fully Connected: fully connects the incoming 100 nodes to the outgoing 10
nodes: R
10Ã—1 â† R
10Ã—100
Â· R
100Ã—1
.
Figure 11: The neural network presented in CryptoNets [28].
Square/CNN/MNIST Latency (s) Message Sizes (MB) Accuracy
(Figure 11) offline online offline online %
by CryptoNets [28] 0 297.5 0 372.2 98.95
by MiniONN 0.88 0.4 3.6 44 98.95
Table 4: Comparison: MiniONN vs. CryptoNets [28].
6.2 Evaluations with realistic models
As we stated in Section 2, a useful ONN transformation technique
must support commonly used neural network operations. Both
CryptoNets and SecureML [44] fall short on this count. In this
section we discuss performance evaluations of realistic models that
are built with popular neural network operations using several
different standard datasets.
Handwriting recognition: MNIST. We trained and implemented another neural network (Figure 12) using the MNIST dataset, but
using ReLU as the activation function. The use of ReLU with a more
complex neural network increases the accuracy of the model in
MNIST to 99.31%, which is close to the state-of-the-art accuracy in
the MNIST dataset (99.79%)6
.
(1) Convolution: input image 28 Ã— 28, window size 5 Ã— 5, stride (1, 1), number
of output channels of 16: R
16Ã—576 â† R
16Ã—25
Â· R
25Ã—576
.
(2) ReLU Activation: calculates ReLU for each input.
(3) Max Pooling: window size 1 Ã— 2 Ã— 2 and outputs R
16Ã—12Ã—12
.
(4) Convolution: window size 5 Ã— 5, stride (1, 1), number of output channels 16:
R
16Ã—64 â† R
16Ã—400
Â· R
400Ã—64
.
(5) ReLU Activation: calculates ReLU for each input.
(6) Max Pooling: window size 1 Ã— 2 Ã— 2 and outputs R
16Ã—4Ã—4
.
(7) Fully Connected: fully connects the incoming 256 nodes to the outgoing 100
nodes: R
100Ã—1 â† R
100Ã—256
Â· R
256Ã—1
.
(8) ReLU Activation: calculates ReLU for each input
(9) Fully Connected: fully connects the incoming 100 nodes to the outgoing 10
nodes: R
10Ã—1 â† R
10Ã—100
Â· R
100Ã—1
.
Figure 12: The neural network trained from the MNIST dataset.
Image classification: CIFAR-10. CIFAR-10 [36] is a standard dataset
consisting of RGB images (of size 3 Ã— 32 Ã— 32, 3 color channels,
width and height are 32) of everyday objects in 10 classes (e.g.,
automobile, bird etc.). The training set has 50 000 images while
the test set has 10 000 images. The neural network is detailed in
Figure 13. It achieves 81.61% prediction accuracy.
(1) Convolution: input image 3 Ã— 32 Ã— 32, window size 3 Ã— 3, stride (1, 1), pad
(1, 1), number of output channels 64: R
64Ã—1024 â† R
64Ã—27
Â· R
27Ã—1024
.
(2) ReLU Activation: calculates ReLU for each input.
(3) Convolution: window size 3 Ã— 3, stride (1, 1), pad (1, 1), number of output
channels 64: R
64Ã—1024 â† R
64Ã—576
Â· R
576Ã—1024
.
(4) ReLU Activation: calculates ReLU for each input.
(5) Mean Pooling: window size 1 Ã— 2 Ã— 2, outputs R
64Ã—16Ã—16
.
(6) Convolution: window size 3 Ã— 3, stride (1, 1), pad (1, 1), number of output
channels 64: R
64Ã—256 â† R
64Ã—576
Â· R
576Ã—256
.
(7) ReLU Activation: calculates ReLU for each input.
(8) Convolution: window size 3 Ã— 3, stride (1, 1), pad (1, 1), number of output
channels 64: R
64Ã—256 â† R
64Ã—576
Â· R
576Ã—256
.
(9) ReLU Activation: calculates ReLU for each input.
(10) Mean Pooling: window size 1 Ã— 2 Ã— 2, outputs R
64Ã—8Ã—8
.
(11) Convolution: window size 3 Ã— 3, stride (1, 1), pad (1, 1), number of output
channels 64: R
64Ã—64 â† R
64Ã—576
Â· R
576Ã—64
.
(12) ReLU Activation: calculates ReLU for each input.
(13) Convolution: window size 1 Ã— 1, stride (1, 1), number of output channels of
64: R
64Ã—64 â† R
64Ã—64
Â· R
64Ã—64
.
(14) ReLU Activation: calculates ReLU for each input.
(15) Convolution: window size 1 Ã— 1, stride (1, 1), number of output channels of
16: R
16Ã—64 â† R
16Ã—64
Â· R
64Ã—64
.
(16) ReLU Activation: calculates ReLU for each input.
(17) Fully Connected Layer: fully connects the incoming 1024 nodes to the outgoing 10 nodes: R
10Ã—1 â† R
10Ã—1024
Â· R
1024Ã—1
.
Figure 13: The neural network trained from the CIFAR-10 dataset.
Language modeling: PTB. Penn Treebank (PTB) is a standard dataset [41]
for language modeling, i.e., predicting likely next words given the
6http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.
html (last accessed May 9, 2017)
Session C3: Machine Learning Privacy CCSâ€™17, October 30-November 3, 2017, Dallas, TX, USA 627
previous words ([45], Chapter 27). We used a preprocessed version
of this dataset7
, which consists of 929 000 training words, 73 000
validation words, and 82 000 test words.
Long Short Term Memory (LSTM) is a neural network architecture that is commonly used for language modeling [33]. Sigmoidal
activation functions are typically used in such networks. We reproduced and transformed a recent LSTM model [61] following the
tutorial8
in Tensorflow [2]. To the extent of our knowledge, this is
the first time language modeling is performed using oblivious models, which paves the way to oblivious neural machine translation.
The model is described in Figure 14.
(1) Fully Connected: input one-hot vector word 10000 Ã— 1, fully connects the
input nodes to the outgoing 200 nodes: R
200Ã—1 â† R
200Ã—10000
Â· R
10000Ã—1
.
(2) LSTM: pad the incoming 200 nodes with another 200 nodes: R
400Ã—1 â†
R
200Ã—1
| R
200Ã—1
, and then process them as follows:
(a) R
800Ã—1 â† R
800Ã—400
Â· R
400Ã—1
(b) R
200Ã—1
| R
200Ã—1
| R
200Ã—1
| R
200Ã—1 â† R
800Ã—1
(c) R
200 â† R
200â—¦ sigmoid(R
200 )+sigmoid(R
200 )â—¦ tanh(R
200 )
(d) R
200 â† sigmoid(R
200 )â—¦ tanh(R
200 )
(3) LSTM: pad the incoming 200 nodes with another 200 nodes: R
400Ã—1 â†
R
200Ã—1
| R
200Ã—1
, and then process them as follows:
(a) R
800Ã—1 â† R
800Ã—400
Â· R
400Ã—1
(b) R
200Ã—1
| R
200Ã—1
| R
200Ã—1
| R
200Ã—1 â† R
800Ã—1
(c) R
200 â† R
200â—¦ sigmoid(R
200 )+sigmoid(R
200 )â—¦ tanh(R
200 )
(d) R
200 â† sigmoid(R
200 )â—¦ tanh(R
200 )
(4) Fully Connected: fully connects the incoming 200 nodes to the outgoing
10000 nodes: R
10000Ã—1 â† R
10000Ã—200
Â· R
200Ã—1
.
Figure 14: The neural network trained from the PTB dataset.
We used the real sigmoid activation functions for training, but replaced them with their corresponding approximations (Section 5.3.2)
for predictions. In our sigmoid approximation, we set the ranges as
[Î±0,Î±n] = [âˆ’30,30] and set the polynomials beyond the ranges as
0 and 1, i.e., Â¯f (y < âˆ’30) := 0 and Â¯f (y > 30) := 1 as in Equation 6.9
Unlike aforementioned image datasets, prediction quality here is
measured by a loss function called cross-entropy loss [45]. Figure 15
shows that the cross-entropy loss achieved by our approximation
method (with more than 12 pieces) is close to the original result
(4.76 vs. 4.74). We also test the new activation function that is proposed in SecureML [44] as an alternative to the sigmoid function.
In this model, it causes the cross-entropy loss to diverge to infinity.
The optimal number of linear pieces differs on the model structure,
e.g., 14 pieces achieved optimal results on the larger models in [61].
Summary of results. Table 5 summarizes the results of the last three
neural networks after being transformed by MiniONN. The performance of the ONNs in MNIST and PTB is reasonable, whereas the
ONN in CIFAR-10 is too expensive. This is due to the fact that the
model in CIFAR-10 (Figure 13) has 7 activation layers, and each layer
receives 2
10 âˆ’ 2
16 neurons. In next section, we will discuss more
about the tradeoffs between prediction accuracy and overhead.
7http://www.fit.vutbr.cz/~imikolov/rnnlm/
8https://www.tensorflow.org/tutorials/recurrent, accessed April 20, 2017. We used the
â€˜smallâ€™ model configuration.
9This is exactly as in the Theano deep learning framework [9], where this approximation is used for numerical stability.
10 11 12 13 14
Number of linear pieces
4.74
5.00
6.00
7.00
8.00
9.00
10.00
11.00
Cross-entropy loss
model without approximation
model with approximated
sigmoid and tanh
random prediction
Figure 15: Cross-entropy loss for models with approximated sigmoid/tanh, evaluate over the full PTB test set.
Latency (s) Message Sizes (MB) Accuracy
offline online offline online %
ReLU/CNN/MNIST
(Figure 12) 3.58 5.74 20.9 636.6 99.31
ReLU/CNN/CIFAR-10
(Figure 13) 472 72 3046 6226 81.61
Sigmoidal/LSTM/PTB
(Figure 14) 13.9 4.39 86.7 474 cross-entropy
loss:4.76
Table 5: Performance of MiniONN transformations of models with
common activation functions and pooling operations.
7 COMPLEXITY, ACCURACY AND
OVERHEAD
In Section 6, we demonstrated that, unlike prior work, MiniONN can
transform existing neural networks into oblivious variants. However, by simplifying the neural network model a designer can trade
off a small sacrifice in prediction accuracy with a large reduction
in the overhead associated with the ONN.
The relationship between model complexity and prediction accuracy is well-known ([30], Chapter 6). In neural networks, model
complexity depends on the network structure: the number of neurons (size of output from each layer), types of operations (e.g., choice
of activation functions) and the number of layers in the network.
While prediction accuracy can increase with model complexity, it
eventually saturates with some level of complexity.
Model complexity vs. prediction overhead. The overhead of linear
transformation is the same as non-private neural networks, since we
introduce a precomputation phase to generate dot-product triples.
Therefore, to investigate the overhead introduced by MiniONN,
we only need to consider the activation functions and pooling operations in a given neural network model. Figure 16 shows the
performance of oblivious ReLU, oblivious square, oblivious sigmoid,
and oblivious max operations (used in both pooling and maxout
activation functions). Both message size and latency grow sublinearly as the number of invocations increases. The experiments are
Session C3: Machine Learning Privacy CCSâ€™17, October 30-November 3, 2017, Dallas, TX, USA 628
2
0 2
4 2
7 2
8 2
10 2
12 2
16
Number of invocations
100
101
102
103
104
105
106
Latency [ms]
ReLU
Square
Sigmoid appr.
with 12 pieces
Max with 2 inputs
Max with 4 inputs
Max with 16 inputs
2
0 2
4 2
7 2
8 2
10 2
12 2
16
Number of invocations
10âˆ’1
100
101
102
103
104
105
Message size [MB]
ReLU
Square
Sigmoid appr.
with 12 pieces
Max with 2 inputs
Max with 4 inputs
Max with 16 inputs
Figure 16: Overhead of oblivious activation functions.
repeated five times for each point. The standard deviation is below
2.5% for all points.
Model complexity vs. prediction accuracy. The largest contribution to
overhead in the online phase are due to activation function usage.
We evaluated the performance of our ReLU/CNN/MNIST network
(Figure 12) by decreasing the number of neurons in linear layers and
the number of channels in convolutional layers, to a fraction Î± of
the original value, according to the changes introduced in Figure 17.
This effectively reduced the number of activation function instances
to the same fraction.
(1) Convolution: input image 28 Ã— 28, window size 5 Ã— 5, stride (1, 1), number
of output channels of âŒŠÎ± Â· 16âŒ‹: R
âŒŠÎ±Â·16âŒ‹Ã—576 â† R
âŒŠÎ±Â·16âŒ‹Ã—25
Â· R
25Ã—576
.
(4) Convolution: window size 5 Ã— 5, stride (1, 1), number of output channels
âŒŠÎ± Â· 16âŒ‹: R
âŒŠÎ±Â·16âŒ‹Ã—(âŒŠÎ±Â·16âŒ‹)
2 â† R
âŒŠÎ±Â·16âŒ‹Ã—400
Â· R
400Ã—(âŒŠÎ±Â·16âŒ‹)
2
.
(7) Fully Connected: fully connects the incoming âŒŠÎ± Â· 16âŒ‹ Â· 16 nodes to the outgoing âŒŠÎ± Â· 100âŒ‹ nodes: R
âŒŠÎ±Â·100âŒ‹Ã—1 â† R
âŒŠÎ±Â·100âŒ‹Ã—âŒŠ(Î±Â·16âŒ‹Â·16)
Â· R
(âŒŠÎ±Â·16âŒ‹Â·16)Ã—1
.
(9) Fully Connected: fully connects the incoming âŒŠÎ± Â· 100âŒ‹ nodes to the outgoing 10 nodes: R
10Ã—1 â† R
10Ã—âŒŠÎ±Â·100âŒ‹
Â· R
âŒŠÎ±Â·100âŒ‹Ã—1
.
Figure 17: Alternative ReLU/CNNs trained from the MNIST dataset.
Figure 18 shows how prediction accuracy varies with Î±. It is
clear that the decline in prediction accuracy is very gradual in the
range 0.25 < Î± < 1 (corresponding 2
11.3
and 2
13.3 ReLU invocations). From Figure 16, we observe that when Î± drops by 75%
from 1 (2
13.3
invocations) to 0.25 (2
11.3
invocations), performance
overhead also drops roughly by 75% (for both latency and message
size) but accuracy drops only by less than a percentage point.
Accuracy vs. overhead. In Table 6, we estimated the overhead for
smaller variants of the ReLU/CNN/MNIST network, w.r.t. to the
base network overhead in Table 5. For example, columns 2 and 3 of
2
10.3 2
10.9 2
11.3 2
11.9 2
12.3 2
12.7 2
12.9 2
13.1 2
13.3
Number of ReLU invocations
95.0
96.0
97.0
98.0
98.5
99.0
99.5
Accuracy [%]
0.125
0.188
0.250
0.375
0.500
0.625
0.750
0.875
1.000
Î±
Figure 18: Model complexity vs. accuracy.
Table 6 show the estimated latencies and message sizes for different
accuracy levels. Thus, if the latency and message size for a particular ONN is perceived as too high, the designer has the option of
choosing a suitable point in the accuracy vs. overhead tradeoff. The
overhead estimates were approximated, but reasonably accurate.
For instance, an actual ReLU/CNN/MNIST model with only 25%
ReLU invocations results in 1.51s latency and 159.2MB message
sizes, both of which are close to the estimate for Î± = 0.25 in Table 6.
8 RELATED WORK
Barni et al. [7] made the first attempt to construct oblivious neural
networks. They simply have ğ’® do linear operations on ğ’â€™s encrypted data and send the results back to ğ’, who decrypts, applies
the non-linear transformations on the plaintexts, and re-encrypts
the results before sending them to ğ’® for next layer processing.
Orlandi et al. [47] noticed that this process leaks significant information about ğ’®â€™s neural network, and proposed a method to
obscure the intermediate results. For example, when ğ’® needs to
know siĞ´n(x) from E(pkc ,x), they have ğ’® sends a âŠ— E(pkc ,x) to ğ’
with a > 0. Obviously, this leaks the sign of x to ğ’. Our work is
targeted for the same setting as these works but provides stricter
security guarantees (no intermediate results will be leaked) and has
significantly better performance.
Gilad-Bachrach et al. [28] proposed CryptoNets based on leveled
homomorphic encryption (LHE). They introduced a simple square
activation function [28]: f (y) = y
2
, because CryptoNets cannot
Î±
Overhead Accuracy (%) Latency (s) Message size (MB)
1.000 5.72âˆ—
636.6âˆ—
99.31
0.875 5.01 557.0 99.27
0.750 4.29 447.5 99.26
0.625 3.58 397.9 99.19
0.500 2.87 317.6 98.96
0.375 2.15 238.7 98.79
0.250 1.44 (1.51âˆ—
) 158.4 (159.2âˆ—
) 98.42
0.188 1.07 119 97.35
0.125 0.72 79.0 95.72
Table 6: Accuracy vs. overhead. âˆ— denotes actual values.
Session C3: Machine Learning Privacy CCSâ€™17, October 30-November 3, 2017, Dallas, TX, USA 629
support commonly used activation functions due to the limitations
of LHE. They also used mean pooling instead of max pooling for
the same reason, even though the latter is more commonly used.
In contrast, MiniONN supports all operations commonly used by
neural network designers, does not require changes to how neural
networks are trained, and has significantly lower overheads at
prediction time. The privacy guarantees to the client are identical.
However, while CryptoNets can hide all information about model
from clients, MiniONN hides the model values (e.g., weight matrices
and bias vectors) while disclosing the number of layers, sizes of
weight matrices and the types of operations used in each layer. We
argue that this is a justifiable tradeoff for two reasons. First, the
performance gain resulting from the tradeoff are truly significant
(e.g., 740-fold improvement in online latency). Second, information
disclosed by MiniONN (like the number of layers and the types of
operations) are exactly those that are described in academic and
white papers. Model values (like weight matrices and bias vectors
in each layer) are usually not disclosed in such literature.
Chabanne et al. [16] also noticed the limited accuracy guarantees
of the square function in CryptoNets. They approximated ReLU
using a low degree polynomial, and added a normalization layer
to make a stable and normally distributed inputs to the activation
layer. However, they require a multiplicative depth of 6 in LHE, and
they did not provide benchmark results in their paper.
Most of the related works focus on the privacy of training phase
(see [4, 5, 31]). For example, Graepel et al. [31] proposed to use training algorithms that can be expressed as low degree polynomials, so
that the training phase can be done over encrypted data. Aslett et
al. [4, 5] presented ways to train both simple models (e.g., Naive
Bayes) as well as more advanced models (e.g., random forests) over
encrypted data. The work on differential privacy can also guarantee
the privacy in training phase (see [1, 27, 52]). By leveraging Intel
SGX combined with several data-oblivious algorithms, Ohrimenko
et al. [46] proposed a way to enable multiple parties to jointly train
a model while guaranteeing the privacy of their individual datasets.
Recently, in SecureML Mohassel and Zhang proposed a twoserver model for privacy-preserving training [44]. Specifically, the
data owners distribute their data among two non-colluding servers
to train various models including neural networks using secure
two-party computation (2PC). While their focus is on training, they
also support privacy-preserving predictions. As such their work
is closest to ours. Independently of us, they too use a precomputation stage to reduce the overhead during the online prediction
phase, support some popular activation functions like ReLU and
use approximations where necessary. MiniONN is different from
their work in several ways. First, by using the SIMD batch processing technique, MiniONN achieves a significant reduction in
the overhead during precomputation without affecting the online
phase (Section 6.1). Second, their approximations require changes
to how models are trained while the distinguishing characteristic of
MiniONN is that it imposes no such requirement. DeepSecure [50]
is another independent work focusing on oblivious neural network
predictions. It leverages Yaoâ€™s garbled circuits and results in 58-fold
performance improvement over CryptoNets.
9 CONCLUSION AND FUTURE WORK
In this paper, we presented MiniONN, which is the first approach
that can transform any common neural network into an oblivious form. Our benchmarks show that MiniONN achieves lower
response latency and message sizes compared to prior work [28, 44].
We intend to design easy-to-use interfaces that allow developers
without any cryptographic background to use MiniONN directly.
We also intend to investigate whether our approach is applicable
to other machine learning models. As a next step, we plan to apply
MiniONN to the neural networks that are being used in production.