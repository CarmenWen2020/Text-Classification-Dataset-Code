Flow correlation is the core technique used in a multitude of
deanonymization attacks on Tor. Despite the importance of ow
correlation attacks on Tor, existing ow correlation techniques are
considered to be ineective and unreliable in linking Tor ows
when applied at a large scale, i.e., they impose high rates of false
positive error rates or require impractically long ow observations
to be able to make reliable correlations. In this paper, we show that,
unfortunately, ow correlation attacks can be conducted on Tor
trac with drastically higher accuracies than before by leveraging
emerging learning mechanisms. We particularly design a system,
called DeepCorr, that outperforms the state-of-the-art by signi-
cant margins in correlating Tor connections. DeepCorr leverages
an advanced deep learning architecture to learn a ow correlation
function tailored to Torâ€™s complex networkâ€”this is in contrast to
previous worksâ€™ use of generic statistical correlation metrics to correlate Tor ows. We show that with moderate learning, DeepCorr
can correlate Tor connections (and therefore break its anonymity)
with accuracies signicantly higher than existing algorithms, and
using substantially shorter lengths of ow observations. For instance, by collecting only about 900 packets of each target Tor ow
(roughly 900KB of Tor data), DeepCorr provides a ow correlation
accuracy of 96% compared to 4% by the state-of-the-art system of
RAPTOR using the same exact setting.
We hope that our work demonstrates the escalating threat of
ow correlation attacks on Tor given recent advances in learning
algorithms, calling for the timely deployment of eective countermeasures by the Tor community.
CCS CONCEPTS
â€¢ Information systems â†’ Trac analysis; â€¢ Security and
privacy â†’ Pseudonymity, anonymity and untraceability;
Privacy-preserving protocols; â€¢ Networks â†’ Network privacy
and anonymity;
KEYWORDS
Trac Analysis; Tor; Flow Correlation Attacks; Anonymous Communications

1 INTRODUCTION
Tor [16] is the most widely used anonymity system with more
than 2 million daily users [74]. It provides anonymity by relaying
clientsâ€™ trac through cascades of relays, known as onion-circuits,
therefore concealing the association between the IP addresses of
the communicating parties. Torâ€™s network comprises around 7,000
public relays, carrying terabytes of trac every day [74]. Tor is
used widely not only by dissidents, journalists, whistleblowers, and
businesses, but also by ordinary citizens to achieve anonymity and
blocking resistance.
To be usable for everyday Internet activities like web browsing,
Tor aims to provide low-latency communications. To make this possible, Tor relays refrain from obfuscating trac features like packet
timings as doing so will slow down the connections.1 Consequently,
Tor is known to be susceptible to ow correlation attacks [14, 51, 68]
in which an adversary tries to link the egress and ingress segments
of a Tor connection by comparing their trac characteristics, in
particular their packet timings and packet sizes.
This paper studies ow correlation attacks on Tor. Flow correlation is the core technique used in a wide spectrum of the
attacks studied against Tor (and similar anonymity systems) [8,
20, 36, 38, 70, 72]. For instance, in the predecessor attack [83] an
adversary who controls/eavesdrops multiple Tor relays attempts
at deanonymizing Tor connections by applying ow correlation
techniques. The Tor project adopted â€œguardâ€ relays to limit such
an adversaryâ€™s chances of placing herself on the two ends of a
target Tor connection. Borisov et al. [8] demonstrated an active
denial-of-service attack that increases an adversaryâ€™s chances of
observing the two ends of a target userâ€™s Tor connections (who then
performs ow correlation). Alternatively, various routing attacks
have been presented on Tor [20, 38, 70, 72] that aim at increasing
an adversaryâ€™s odds of intercepting the ows to be correlated by
manipulating the routing decisions.
Despite the critical role of ow correlation in a multitude of
Tor attacks, ow correlating Tor connections has long been considered to be inecient at scale [37, 55, 66]â€”but not anymore! Even
though Tor relays do not actively manipulate packet timings and
sizes to resist ow correlation, the Tor network naturally perturbs
Tor packets by signicant amounts, rendering ow correlation a
1Note that some Tor bridges (but not the public relays) obfuscate trac characteristics of the Tor ows between themselves and censored clients by using various Tor
pluggable transports [61].
Session 10A: TOR CCSâ€™18, October 15-19, 2018, Toronto, ON, Canada 1962
dicult problem in Tor. Specically, Tor connections experience
large network jitters, signicantly larger than normal Internet connections. Such large perturbations are resulted by congestion on
Tor relays, which is due to the imbalance between Torâ€™s capacity
and the bandwidth demand from the clients. Consequently, existing
ow correlation techniques [34, 45, 53, 72] suer from high rates of
false positives and low accuracies, unless they are applied on very
long ow observations and/or impractically small sets of target
ows. For instance, the state-of-the-art ow correlation of RAPTOR [72] achieves good correlation performance in distinguishing
a small set of only 50 target connections, and even this requires
the collection of 100 MB over 5 minutes of trac for each of the
intercepted ows.
In this work, we take ow correlation attacks on Tor to reality. We develop tools that are able to correlate Tor ows with accuracies signicantly higher than the state-of-the-artâ€”when applied to large anonymity sets and using very short observations
of Tor connections. We argue that existing ow correlation techniques [13, 34, 45, 53, 68, 72] are inecient in correlating Tor trac
as they make use of generic statistical correlation algorithms that
are not able to capture the dynamic, complex nature of noise in Tor.
As opposed to using such general-purpose statistical correlation
algorithms, in this paper we use deep learning to learn a correlation
function that is tailored to Torâ€™s ecosystem. Our ow correlation system, called DeepCorr, then uses the learned correlation function to
cross-correlate live Tor ows. Note that contrary to website ngerprinting attacks [10, 27, 58, 75, 76], DeepCorr does not need to learn
any target destinations or target circuits; instead DeepCorr learns
a correlation function that can be used to link ows on arbitrary
circuits, and to arbitrary destinations. In other words, DeepCorr can
correlate the two ends of a Tor connection even if the connection
destination has not been part of the learning set. Also, DeepCorr
can correlate ows even if they are sent over Tor circuits dierent
than the circuits used during the learning process. This is possible
as DeepCorrâ€™s neural network learns the generic features of noise
in Tor, regardless of the specic circuits and end-hosts used during
the training process.
We demonstrate DeepCorrâ€™s strong performance through large
scale experiments on live Tor network. We browse the top 50,000
Alexa websites over Tor, and evaluate DeepCorrâ€™s true positive and
false positive rates in correlating the ingress and egress segments
of the recorded Tor connections. To the best of our knowledge, our
dataset is the largest dataset of correlated Tor ows, which we have
made available to the public.2 Our experiments show that DeepCorr
can correlate Tor ows with accuracies signicantly superior to existing ow correlation techniques. For instance, compared to the
state-of-the-art ow correlation algorithm of RAPTOR [72], DeepCorr oers a correlation accuracy3 of 96% compared to RAPTORâ€™s
accuracy of 4% (when both collect 900 packets of trac from each
of the intercepted ows)! The following is a highlight of DeepCorrâ€™s
performance:
2https://people.cs.umass.edu/~amir/FlowCorrelation.html
3To be fair, in our comparison with RAPTOR we derive the accuracy metric similar to
RAPTORâ€™s paper [72]: each ow is paired with only one ow out of all evaluated ows.
For the rest of our experiments, each ow can be declared as correlated with arbitrary
number of intercepted ows, which is a more realistic (and more challenging) setting.
â€¢ We train DeepCorr using 25,000 Tor ows generated by
ourselves. Training DeepCorr takes about a day on a single
TITAN X GPU, however we show that an adversary needs
to re-train DeepCorr roughly once a month to preserve its
correlation performance.
â€¢ DeepCorr can be used as a generic correlation function: DeepCorrâ€™s performance is consistent for various test datasets
with dierent sizes and containing ows routed over dierent circuits.
â€¢ DeepCorr outperforms prior ow correlation algorithms
by very large margins. Importantly, DeepCorr enables the
correlation of Tor ows with ow observations much shorter
than what is needed by previous work. For instance, with
only 300 packets, DeepCorr achieves a true positive rate of
0.8 compared to less than 0.05 by prior work (for a xed false
positive rate of 10âˆ’3
).
â€¢ DeepCorrâ€™s performance rapidly improves with longer ow
observations and with larger training sets.
â€¢ DeepCorrâ€™s correlation time is signicantly faster than previous work for the same target accuracy. For instance, each
DeepCorr correlation takes 2ms compared to RAPTORâ€™s
more than 20ms, when both target a 95% accuracy on identical dataset.
We hope that our study raises concerns in the community on the
escalating risks of large-scale trac analysis on Tor communications in light of the emerging deep learning algorithms. A possible
countermeasure to DeepCorr is deploying trac obfuscation techniques, such as those employed by Tor pluggable transports [61], on
all Tor trac. We evaluate the performance of DeepCorr on each of
Torâ€™s currently-deployed pluggable transports, showing that meek
and obfs4-iat0 provide little protection against DeepCorrâ€™s ow
correlation, while obfs4-iat1 provides a better protection against
DeepCorr (note that none of these obfuscation mechanisms are
currently deployed by public Tor relays, and even obfs4-iat1 is
deployed by a small fraction of Tor bridges [55]). This calls for
designing eective trac obfuscation mechanisms to be deployed
by Tor relays that do not impose large bandwidth and performance
overheads on Tor communications.
Finally, note that while we present DeepCorr as a ow correlation attack on Tor, it can be used to correlate ows in other ow
correlation applications as well. To demonstrate this, we also apply
DeepCorr to the problem of stepping stone detection [6, 26, 80]
showing that DeepCorr signicantly outperforms previous stepping
stone detection algorithms in unreliable network settings.
Organization: The rest if this paper is organized as follows. In Section 2, we overview preliminaries of ow correlation and motivate
our work. In Section 3, we introduce our ow correlation system,
called DeepCorr. We describe our experimental setup in Section 4,
and present and discuss our experimental results in Section 5. We
discuss and evaluate possible countermeasures against DeepCorr
in Section 6 and conclude the paper in Section 7.
2 PRELIMINARIES AND MOTIVATION
Flow correlation attacks, also referred to as conrmation attacks,
are used to link network ows in the presence of encryption and
other content obfuscation mechanisms [14, 18, 26, 46, 53, 68, 81, 86].
Session 10A: TOR CCSâ€™18, October 15-19, 2018, Toronto, ON, Canada 1963
In particular, ow correlation techniques can break anonymity in
anonymous communication systems like Tor [16] and mix networks [15, 64, 65] by linking the egress and ingress segments
of the anonymous connections through correlating trac features [4, 14, 51, 63, 68, 78, 79, 87]. Alternatively, ow correlation
techniques can be used to identify cybercriminals who use network
proxies to obfuscate their identities, i.e., stepping stone attackers [69, 84, 86].
2.1 Threat Model
Figure 1 shows the main setting of a ow correlation scenario. The
setting consists of a computer network (e.g., Torâ€™s network) with
M ingress ows and N egress ows. Some of the egress ows are
the obfuscated versions of some of the ingress ows; however, the
relation between such ows can not detected using packet contents due to the use of encryption and similar content obfuscation
techniques like onion encryption. For instance, in the case of Tor,
Fi and Fj are the entry and exit segments of one Tor connection
(see Figure 1), however, such association can not be detected by
inspecting the packet contents of Fi and Fj due to onion encryption.
We call (Fi
, Fj) a pair of associated ows.
The goal of an adversary in this setting is to identify (some or
all of) the associated ow pairs, e.g., (Fi
, Fj), by comparing traf-
c characteristics, e.g., packet timings and sizes, across all of the
ingress and egress ows. Linking associated ow pairs using trac
characteristics is called ow correlation.
A ow correlation adversary can intercept network ows at
various network locations. A Tor adversary, in particular, can intercept Tor ows either by running malicious Tor relays [8, 36, 83] or
by controlling/wiretapping Internet ASes or IXPs [39, 70, 72]. We
further elaborate on this in Section 2.3.
Note that in this paper we study passive ow correlation attacks
only; therefore, active ow correlation techniques, also known
as ow watermarks as introduced in Section 2.5, are out of the
scope of this paper. Also, ow correlation is dierent from website
ngerprinting attacks, as discussed in Section 2.5.
2.2 Existing Flow Correlation Techniques
As mentioned before, ow correlation techniques use trac features, particularly, packet timings, packet sizes, and their variants
(e.g., ow rates, inter-packet delays, etc.), to correlate and link network ows (recall that packet contents can not be used to link ows
in this setting due to content obfuscation, e.g., onion encryption).
For instance, the early work of Paxson and Zhang [86] models
packet arrivals as a series of ON and OFF patterns, which they
use to correlate network ows, and Blum et al. [7] correlate the
aggregate sizes of network packets over time. Existing ow correlation techniques mainly use standard statistical correlation metrics to
correlate the vectors of ow timings and sizes across ows. In the
following, we overview the major types of statistical correlation
metrics used by previous ow correlation algorithms.
Mutual Information The mutual information metric measures
the dependency of two random variables. It, therefore, can be used
to quantify the correlation of ow features across ows, e.g., the
trac features of an egress Tor ow depends on the features of its
corresponding ingress ow. The mutual information technique has
been used by Chothia et al. [13] and Zhu et al. [88] to link ows.
This metric, however, requires a long vector of features (e.g., long
ows) in order to make reliable decisions, as it needs to reconstruct
and compare the empirical distributions of trac features of target
ows.
Pearson Correlation The Pearson Correlation coecient is a
classic statistical metric for linear correlation between random
variables. Unlike the mutual information metric, the Pearson Correlation metric does not need to build the empirical distribution
of the variables it is correlating, and therefore can be applied on
a shorter length of data. The Pearson Correlation metric has been
used by several ow correlation systems [45, 68].
Cosine Similarity The Cosine similarity metric measures the
angular similarity of two random variables. Similar to the Pearson
coecient, it can be directly applied on the sample vectors of two
random variables. This metric has been used by dierent timing
and size correlation systems [34, 53] to link network ows.
Spearman Correlation The Spearman rank correlation metric
measures the statistical dependence between the rankings of two
variables. The metric can be dened as the Pearson correlation
between ranked variables. The recent work of RAPTOR [72] uses
this metric to correlate Tor ows.
2.3 Flow Correlation Attacks on Tor
Flow correlation is the core technique used in a broad range of
attacks studied against Tor (and other anonymity systems). To be
able to perform ow correlation, an adversary needs to observe
(i.e., intercept) some fraction of ows entering and exiting the
Tor network. The adversary can then deanonymize a specic Tor
connection, if she is able to intercept both of the ingress and egress
segments of that Tor connection (by performing a ow correlation
algorithm on those ow segments). Therefore, an adversary can
increase her chances of deanonymizing Tor connections by trying
to intercept a larger fraction of Torâ€™s ingress and egress ows.
There are two main approaches an attacker can take to increase
the fraction of Tor connections she is intercepting. First, by running
a large number of Tor relays and recording the trac features of
the Tor connections they relay. Various studies have shown that
an adversary with access to such malicious relays can increase
her chances of intercepting the both ends of a Tor connection in
dierent ways [3, 8, 28, 49, 83]. For instance, Borisov et al. [8]
demonstrate an active denial-service-attack to increase the chances
of intercepting the ingress and egress segments of a target clientâ€™s
Tor trac. The Tor project has adopted the concept of Tor guard
relays [21] to reduce the chances of performing ow correlation by
an adversary controlling malicious relays, an attack known as the
predecessor attack [83].
Alternatively, an adversary can increase her opportunities of performing ow correlation by controlling/wiretapping autonomous
systems (ASes) or Internet exchange points (IXPs), and recording the trac features of the Tor connections that they transit.
Several studies [22, 52, 72] demonstrate that specic ASes and
IXPs intercept a signicant fraction of Tor trac, therefore are
capable of performing ow correlation on Tor at large scale. Others [20, 38, 39, 70, 72] show that an AS-level adversary can further
increase her chances of ow correlation by performing various
Session 10A: TOR CCSâ€™18, October 15-19, 2018, Toronto, ON, Canada 1964
Figure 1: The main setting of a ow correlation attack on Tor. The adversary intercepts Tor ows either by running malicious
Tor relays or wiretapping Internet ASes and IXPs.
routing manipulations that reroute a larger fraction of Tor connections through her adversarial ASes and IXPs. For instance, Starov
et al. [70] recently show that approximately 40% of Tor circuits are
vulnerable to ow correlation attacks by a single malicious AS, and
Sun et al. [72] show that churn in BGP as well as active manipulation of BGP updates can amplify an adversarial ASâ€™s visibility on
Tor connections. This has lead to various proposals on deploying
AS-aware path selection mechanisms for Tor [2, 20, 54].
2.4 This Paperâ€™s Contributions
While ow correlation is the core of a multitude of attacks on
Tor [3, 8, 20, 22, 28, 38, 39, 49, 52, 54, 70, 72, 72, 83], existing ow
correlation algorithms are assumed to be ineective in linking Tor
connections reliably and at scale [37, 55, 66]. This is due to Torâ€™s
extremely noisy network that applies large perturbations on Tor
ows, therefore rendering trac features across associated ingress
and egress Tor ows hard to get reliably correlated. In particular,
Torâ€™s network applies large network jitters on Tor ows, which is
due to congestion on Tor relays, and many Tor packets are fragmented and repacketized due to unreliable network conditions.
Consequently, existing ow correlation techniques oer poor correlation performancesâ€”unless applied to very large ow observations
as well as unrealistically small sets of target ows.4 For instance,
the state-of-the-art correlation technique of Sun et al. [72] needs to
observe 100MB of trac from each target ow for around 5 minutes to be able to perform reliable ow correlations. Such long ow
observations not only are impractical due to the short-lived nature
of typical Tor connections (e.g., web browsing sessions), but also
impose unbearable storage requirements if applied at large scale
(e.g., a malicious Tor relay will likely intercepte tens of thousands
of concurrent ows). Moreover, existing techniques suer from
4Note that active attacks like [68] are out of our scope, as discussed in Section 2.5, since
such attacks are easily detectable, and therefore can not be deployed by an adversary
at large scale for a long time period without being detected.
high rates of false positive correlations unless applied on an unrealistically small set of suspected ows, e.g., Sun et al. [72] correlate
among a set of only 50 target ows.
Our Approach: We believe that the main reason for the ineectiveness of existing ow correlation techniques is the intensity as
well as the unpredictability of network perturbations in Tor. We
argue that previous ow correlation techniques are inecient in
correlating Tor trac since they make use of general-purpose statistical correlation algorithms that are not able to capture the dynamic,
complex nature of noise in Tor. As opposed to using such generic
statistical correlation metrics, in this paper we use deep learning
to learn a correlation function that is tailored to Torâ€™s ecosystem. We
design a ow correlation system, called DeepCorr, that learns a
ow correlation function for Tor, and uses the learned function to
cross-correlate live Tor connections. Note that contrary to website
ngerprinting attacks [10, 27, 58, 75, 76], DeepCorr does not need
to learn any target destinations or target circuits; instead DeepCorr learns a correlation function that can be used to link ows
on arbitrary circuits, and to arbitrary destinations. In other words,
DeepCorr can correlate the two ends of a Tor connection even if
the connection destination has not been part of the learning set.
Also, DeepCorr can correlate ows even if they are sent over Tor
circuits dierent than the circuits used during the training process.
We demonstrate DeepCorrâ€™s strong correlation performance
through large scale experiments on live Tor network, which we
compare to previous ow correlation techniques. We hope that our
study raises concerns in the community on the increasing risks
of large-scale trac analysis on Tor in light of emerging learning
algorithms. We discuss potential countermeasures, and evaluate
DeepCorrâ€™s performance against existing countermeasures.
2.5 Related Topics Out of Our Scope
Active ow correlation (watermarking) Network ow watermarking is an active variant of the ow correlation techniques introduced above. Similar to passive ow correlation schemes, ow watermarking aims at linking network ows using trac features that
Session 10A: TOR CCSâ€™18, October 15-19, 2018, Toronto, ON, Canada 1965
persist content obfuscation, i.e., packet sizes and timings. By contrast, ow watermarking systems need to manipulate the trac features of the ows they intercept in order to be able to perform ow
correlation. In particular, many ow watermarking systems [29â€“
31, 33, 62, 79, 85] perturb packet timings of the intercepted ows by
slightly delaying network packets to modulate an articial pattern
into the ows, called the watermark. For instance, RAINBOW [33]
manipulates the inter-packet delays of network packets in order
to embed a watermark signal. Several proposals [32, 44, 62, 79, 85],
known as interval-based watermarks, work by delaying packets
into secret time intervals.
While passive ow correlation attacks (studied in this paper) are
information theoretically undetectable, a watermarking adversary
may reveal herself by applying trac perturbations that dier from
that of normal trac. Some active correlation techniques [12, 68]
do not even aim for invisibility, therefore they can be trivially
detected and disabled, making them unsuitable for large scale ow
correlation. Additionally, while passive ow correlation algorithms
can be computed oine, ow watermarks need to be performed by
resourceful adversaries who are able to apply trac manipulations
on live Tor connections. In this paper, we only focus on passive
ow correlation techniques.
Website Fingerprinting Website ngerprinting attacks [10, 24,
25, 27, 40, 47, 57, 58, 75â€“77] use a dierent threat model than ow
correlation techniques. In website ngerprinting, an adversary intercepts a target clientâ€™s ingress Tor trac (e.g., by wiretapping
the link between a Tor client and her guard relay), and compares
the intercepted ingress Tor connection to the trac ngerprints
of a nite (usually small) set of target websites. This is unlike ow
correlation attacks in which the adversary intercepts the two ends of
an anonymous connection, enabling the attacker to deanonymize
arbitrary senders and receivers. Existing website ngerprinting
systems leverage standard machine learning algorithms such as
SVM and kNN to classify and identify target websites, and recent
work [67] has investigated the use of deep learning for website
ngerprinting. In contrary, as overviewed in Section 2.2, prior passive ow correlation techniques use statistical correlation metrics
to link trac characteristics across network ows. We consider
website ngerprinting orthogonal to our work as it is based on
dierent threat model and techniques.
3 INTRODUCING DeepCorr
In this section, we introduce our ow correlation system, called
DeepCorr, which uses deep learning algorithms to learn correlation
functions.
3.1 Features and Their Representation
Similar to existing ow correlation techniques overviewed earlier,
our ow correlation system uses the timings and sizes of network
ows to cross-correlate them. A main advantage [23] of deep learning algorithms over conventional learning techniques is that a deep
learning model can be provided with raw data features as opposed to
engineered trac features (like those used by SVM- and kNN-based
website ngerprinting techniques [10, 24, 25, 27, 47, 57, 58, 75, 76]).
This is because deep learning is able to extract complex, eective
features from the raw input features [23] itself. Therefore, DeepCorr
takes raw ow features as input, and uses them to derive complex
features, which is used by its correlation function.
We represent a bidirectional network ow, i, with the following
array:
Fi = [T
u
i
; S
u
i
;T
d
i
; S
d
i
]
where T is the vector of inter-packet delays (IPD) of the ow i,
S is the vector of iâ€™th packet sizes, and the u and d superscripts
represent â€œupstreamâ€ and â€œdownstreamâ€ sides of the bidirectional
ow i (e.g., T
u
i
is the vector of upstream IPDs of i). Also, note that
we only use the rst ` elements of each of the vectors, e.g., only
the rst ` upstream IPDs. If a vector has fewer than ` elements, we
pad it to ` by appending zeros. We will use the ow representation
Fi during our learning process.
Now suppose that we aim at correlating two ows i and j (say i
was intercepted by a malicious Tor guard relay and j was intercepted
by an accomplice exit relay). We represent this pair of ows with
the following two-dimensional array composed of 8 rows:
Fi,j = [T
u
i
;T
u
j
;T
d
i
;T
d
j
; S
u
i
; S
u
j
; S
d
i
; S
d
j
]
where the lines of the array are taken from the ow representations
Fi and Fj
.
3.2 Network Architecture
We use a Convolutional Neural Network (CNN) [23] to learn a
correlation function for Torâ€™s noisy network. We use a CNN since
network ow features can be modeled as time series, and the CNNs
are known to have good performance on time series [23]. Also,
the CNNs are invariant to the position of the patterns in the data
stream [23], which makes them ideal to look for possibly shifted
trac patterns.5
Figure 2 shows the structure of DeepCorrâ€™s CNN network. The
network takes a ow pair Fi,j as the input (on the left side). DeepCorrâ€™s architecture is composed of two layers of convolution and
three layers of a fully connected neural network. The rst convolution layer has k1 kernels each of size (2,w1), where k1 and w1 are
the hyperparameters, and we use a stride of (2, 1). The intuition
behind using the rst convolution layer is to capture correlation
between the adjacent rows of the input matrix Fi,j
, which are supposed to be correlated for associated Tor ows, e.g., between T
u
i
and T
u
j
.
DeepCorrâ€™s second convolution layer aims at capturing trac
features from the combination of all timing and size features. At
this layer, DeepCorr uses k2 kernels each of size (4,w2), where k2
and w2 are also our hyperparameters, and it uses a stride of (4, 1).
The output of the second convolution layer is attened and fed
to a fully connected network with three layers. DeepCorr uses
max pooling after each layer of convolution to ensure permutation
invariance and to avoid overtting [23]. Finally, the output of the
network is:
pi,j = Î¨(Fi,j)
5Note that our work is the rst to use a learning mechanism for ow correlation. In
our search of eective learning mechanisms for ow correlation, we tried various
algorithms including fully connected neural networks, recurrent neural network (RNN),
and support vector machine (SVM). However, CNN provided the best ow correlation
performance compared to all the other algorithms we investigated, which is intuitively
because CNNs are known to work better for longer data lengths. For instance, we
achieved an accuracy of only 0.4 using fulling-connected neural networks, which is
signicantly lower than our performance with CNNs.
Session 10A: TOR CCSâ€™18, October 15-19, 2018, Toronto, ON, Canada 1966
Figure 2: The network architecture of DeepCorr.
which is used to decide if the two input ows in Fi,j are correlated
or not. To normalize the output of the network, we apply a sigmoid
function [23] that scales the output between zero and one. Therefore,
pi,j shows the probability of the ows i and j being associated
(correlated), e.g., being the entry and exit segments of the same Tor
connection.
DeepCorr declares the ows i and j to be correlated if pi,j > Î·,
where Î· is our detection threshold discussed during the experiments.
The parameters (w1,w2, k1, k2) are the hyperparameters of our
system; we will tune their values through experiments.
3.3 Training
To train our network, we use a large set of ow pairs that we
created over Tor. This includes a large set of associated ow pairs,
and a large set of non-associated ow pairs. An associated ow
pair, Fi,j
, consists of the two segments of a Tor connection (e.g.,
i and j are the ingress and egress segments of a Tor connection).
We label an associated pair with yi,j = 1. On the other hand, each
non-associated ow pair (i.e., a negative sample) consists of two
arbitrary Tor ows that do not belong to the same Tor connection.
We label such non-associated pairs with yi,j = 0. For each captured
Tor entry ow, i, we create NneÐ´ negative samples by forming Fi,j
pairs where j is the exit segment of an arbitrary Tor connection.
NneÐ´ is a hyperparameter whose value will be obtained through
experiments.
Finally, we dene DeepCorrâ€™s loss function using a cross-entropy
function as follows:
L = âˆ’
1
|F |
Ã•
Fi,j âˆˆF
yi,j
log Î¨(Fi,j) + (1 âˆ’ yi,j) log(1 âˆ’ Î¨(Fi,j)) (1)
where F is our training dataset, composed of all associated and
non-associated ow pairs. We used the Adam optimizer [43] to
minimize the loss function in our experiments. The learning rate
of the Adam optimizer is another hyperparameter of our system.
4 EXPERIMENTAL SETUP
In this section, we discuss our data collection and its ethics, the
choice of our hyperparameters, and our evaluation metrics.
4.1 Datasets and Collection
Figure 3 shows our experimental setup for our Tor experiments.
We used several Tor clients that we ran inside separate VMs to
generate and collect Tor trac. We use each of our Tor clients to
browse the top 50,000 Alexa websites over Tor, and captured the
ows entering and exiting the Tor network for these connections
(we use half of the connections for training, and the other half for
testing). Therefore, the entering ows are in Tor cell format, and
the ows exiting Tor are in regular HTTP/HTTPS format. We used
1,000 arbitrary Tor circuits for browsing websites over Tor, i.e., each
circuit was used to browse roughly 50 websites. We used dierent
guard nodes in forming our Tor circuits; we were able to alternate
our guard nodes so by disabling Vanilla Torâ€™s option that enforces
guard relay reuse. We also used a regular Firefox browser, instead
of Torâ€™s browser, to be able to enforce circuit selection. We used
Tor version 0.3.0.9, automated by a Python script.
Note that we did not set up our own Tor relays for the purpose
of the experiments, and we merely used public Tor relays in all of
our experiments. We captured the ingress Tor ows using tcpdump
on our Tor clients. To capture the egress Tor trac (i.e., trac
from exit relays to websites), we made our exit Tor trac tunnel
through our own SOCKS proxy server (as shown in Figure 3), and
we collected the exit Tor trac on our own SOCKS proxy server
using tcpdump. Note that using this data collection proxy may
add additional latency on the collected ows, so the performance
of DeepCorr in practice is better than what we report through
experiments. We also collected 500 websites through Tor pluggable
transport to evaluate them as countermeasures against DeepCorr.
We collected our Tor trac in two steps: rst, we collected trac
over a two weeks period, and then with a three months gap we
collected more Tor trac for a one month period (in order to show
the impact of time on training). We have made our dataset available
publicly. To the best of our knowledge, this is largest dataset of
Session 10A: TOR CCSâ€™18, October 15-19, 2018, Toronto, ON, Canada 1967
Figure 3: Our experimental setup on Tor
correlated Tor ows, and we hope it will be useful to the research
community.
Note that while we only collect web trac, this is not a constraint
of DeepCorr, and it can be used to correlate arbitrary Tor trac.
4.2 Ethics of Data Collection
To make sure we did not overload Torâ€™s network, we ran up to 10
concurrent Tor connections during our data collection. Also, we
alternated the guard nodes used in our circuits to evade overloading
any specic circuits or relays. We did not browse any illegal content
over Tor, and we used an idle time between connections of each of
our clients. As explained above, we collected our ingress and egress
Tor ows on our own Tor clients as well as our own SOCKS proxy
server; therefore, we did not collect any trac of other Tor users.
In our experiments with Tor pluggable transports, we collected
a much smaller set of ows compared to our bare Tor experiments;
we did so because Tor bridges are very scarce and expensive, and
therefore we avoided overloading the bridges.
4.3 Choosing the Hyperparameters
We used Tensorow [1] to implement the neural networks of DeepCorr. We tried various values for dierent hyperparameters of our
system to optimize the ow correlation performance. To optimize
each of the parameters, our network took about a day to converge
(we used a single Nvidia TITAN X GPU).
For the learning rate, we tried {0.001, 0.0001, 0.0005, 0.00005},
and we got the best performance with a learning rate of
0.0001. As for the number of negative samples, NneÐ´, we tried
{9, 49, 99, 199, 299} and 199 gave us the best results. For the window
sizes of the convolution layers, w1 and w2, we tried {5, 10, 20, 30}.
Our best results occurred with w1 = 30 and w2 = 10. We also experimented with {2, 5, 10} for the size of the max pooling, and a max
pooling of 5 gave the best performance. Finally, for the number of
the kernels, k1, k2, we tried {500, 1000, 2000, 3000}, and k1 = 2000
and k2 = 1000 resulted in the best performance. We present the
values of these parameters and other parameters of the system in
Table 1.
4.4 Evaluation Metrics
Similar to previous studies, we use the true positive (TP) and false
positive (FP) error rates as the main metrics for evaluating the
performance of ow correlation techniques. The TP rate measures
the fraction of associated ow pairs that are correctly declared to
Table 1: DeepCorrâ€™s hyperparameters optimized to correlate
Tor trac.
Layer Details
Convolution Layer 1
Kernel num: 2000
Kernel size: (2, 30)
Stride: (2,1)
Activation: Relu
Max Pool 1 Window size: (1,5)
Stride: (1,1)
Convolution Layer 2
Kernel nume: 1000
Kernel size: (2, 10)
Stride: (4,1)
Activation: Relu
Max Pool 2 Window size: (1,5)
Stride: (1,1)
Fully connected 1 Size: 3000, Activation: Relu
Fully connected 2 Size: 800, Activation: Relu
Fully connected 3 Size: 100, Activation: Relu
be correlated by DeepCorr (i.e., a ow pair (i,j) where i and j are the
segments of the same Tor connection, and we have pi,j > Î·). On the
other hand, the FP rate measures the fraction of non-associated ow
pairs that are mistakenly identied as correlated by DeepCorr (e.g.,
when i and j are the segments of two unrelated Tor connections,
yet pi,j > Î·). To evaluate FP, DeepCorr correlates every collected
entry ow to every collected exit ow, therefore, we perform about
(25, 000 âˆ’ 1)
2
false correlations for each of our experiments (we
have 25, 000 Tor connections in our test dataset).
Note that the detection threshold Î· makes a trade o between the
FP and TP rates; therefore we make use of ROC curves to compare
DeepCorr to other algorithms.
Finally, in our comparisons with RAPTOR [72], we additionally
use the accuracy metric (the sum of true positive and true negative
correlations over all correlations), which is used in the RAPTOR
paper. To have a fair comparison, we derive the accuracy metric
similar to RAPTOR: each ow is declared to be associated with
only a single ow out of all evaluated ows, e.g., the ow that
results in the maximum correlation metric, pi,j
. For the rest of our
experiments, each ow can be declared as correlated with arbitrary
number of intercepted ows (i.e., any pairs that pi,j > Î·), which is
a more realistic (and more challenging) setting.
Session 10A: TOR CCSâ€™18, October 15-19, 2018, Toronto, ON, Canada 1968
5 EXPERIMENT RESULTS
In this section we present and discuss our experimental results.
5.1 A First Look at the Performance
As described in the experimental setup section, we browse 50,000
top Alexa websites over Tor and collect their ingress and egress
ow segments. We use half of the collected traces to train DeepCorr
(as described earlier). Then, we use the other half of the collected
ows to test DeepCorr. Therefore, we feed DeepCorr about 25, 000
pairs of associated ow pairs, and 25, 000 Ã— 24, 999 â‰ˆ 6.2 Ã— 108
pairs of non-associated ow pairs for training. We only use the rst
` = 300 packets of each ow (for shorter ows, we pad them to
300 packets by adding zeros). Figure 4 presents the true positive
and false positive error rates of DeepCorr for dierent values of the
threshold Î·. As expected, Î· trades o the TP and FP error rates. The
gure shows a promising performance for DeepCorr in correlating
Tor owsâ€”using only 300 packets of each ow. For instance, for a
FP of 10âˆ’3
, DeepCorr achieves a TP close to 0.8. As shown in the
following, this is drastically better than the performance of previous
work.
On the practicality of false positive error rates Note that a
10âˆ’3 FP may seem too large for a real-world setting in which the
malicious AS/IXP is intercepting several thousands of Tor connections at any time. First, the results presented here are for Tor ows
with only ` = 300 packets to demonstrate DeepCorrâ€™s unique performance on short ows (no previous work has done experiments
with such short lengths of Tor ows with acceptable accuracies). As
shown later, increasing ow length rapidly improves DeepCorrâ€™s
correlation performance, e.g., from Figure 8 a ow length of 450
packets improves FP by close to two orders of magnitude compared
to 300 packets (for a xed TP of 0.8). This is also evident from
Figures 11 and 12. Second, the correlation adversary can deploy a
multi-stage attack to optimize accuracy and trac collection. For
instance, she can apply DeepCorr on the rst 300 packets of *all*
intercepted Tor ows, and then collect more packets for the ow
pairs detected by the rst stage of the attack. She then re-applies
DeepCorr on the longer observations of those ow pairs. Third, the
adversary can perform standard pre-ltering mechanisms to further
reduce FPs, e.g., she can ignore all ow pairs with substantially
dierent start times. In our experiments, all of the ows have the
same starting times.
5.2 DeepCorr Can Correlate Arbitrary Circuits
and Destinations
As discussed earlier, DeepCorr learns a correlation function for
Tor that can be used to correlate Tor ows onâ€”any circuitsâ€”and
toâ€”any destinationsâ€”regardless of the circuits and destinations
used during the training process. To demonstrate this, we compare
DeepCorrâ€™s performance in two experiments, each consisting 2, 000
Tor connections, therefore 2, 000 associated pairs and 2, 000Ã—1, 999
non-associated ow pairs. In the rst experiment, the ows tested
for correlation by DeepCorr use the same circuits and destinations
as the ows used during DeepCorrâ€™s training. In the second experiment, the ows tested for correlation by DeepCorr (1) use circuits
that are totally dierent from the circuits used during training, (2)
are targeted to web destinations dierent from those used during
0.4
0.6
0.8
1.0 TP
0.0 0.2 0.4 0.6 0.8 1.0
Threshold (Î·)
10âˆ’5
10âˆ’4
10âˆ’3
10âˆ’2
Log scale
FP
Figure 4: True positive and false positive error rates of DeepCorr in detecting correlated pairs of ingress and egress Tor
ows for dierent detection thresholds (Î·). Each ow is only
300 packets.
10âˆ’5 10âˆ’4 10âˆ’3 10âˆ’2 10âˆ’1
False Positive
0.0
0.2
0.4
0.6
0.8
1.0
1.2
1.4
True Positive
Trained on the same circuits, destinations and time
Trained on different circuits, destinations and one week gap
Tested data with three months gap
Random Guess
Figure 5: DeepCorrâ€™s performance does not depend on the
circuits and destinations used during the training phase.
training, and (3) are collected one week after the learning ows.
Figure 5 compares DeepCorrâ€™s ROC curve for the two experiments.
As can be seen, DeepCorr performs similarly in both of the experiments, demonstrating that DeepCorrâ€™s learned correlation function
can be used to correlate Tor ows on arbitrary circuits and to arbitrary destinations. The third line on the gure shows the results
when the training set is three months old, showing a degraded
performance, as further discussed in the following.
Session 10A: TOR CCSâ€™18, October 15-19, 2018, Toronto, ON, Canada 1969
0 5 10 15 20 25 30
Day
0.0
0.2
0.4
0.6
0.8
1.0
Correlation value
Correlated
Non-correlated
Figure 6: DeepCorrâ€™s correlation values for associated and
non-associated ows for 30 consecutive days without retraining. The performance only starts to drop after about
three weeks.
5.3 DeepCorr Does Not Need to Re-Train
Frequently
Since the characteristics of Tor trac change over time, any
learning-based algorithm needs to be re-trained occasionally to
preserve its correlation performance. We performed two experiments to evaluate how frequently DeepCorr needs to be retrained.
In our rst experiment, we evaluated our pre-trained model over
Tor ows collected during 30 consecutive days. Figure 6 presents
the output of the correlation function for each of the days for both
associated and non-associated ow pairs. As we can see, the correlation values for non-associated ows do not change substantially,
however, the correlation values for associated ows starts to slightly
degrade after about three weeks. This suggests that an adversary
will need to retrain her DeepCorr only every three weeks, or even
once a month.
As an extreme case, we also evaluated DeepCorrâ€™s performance
using a model that was trained three months earlier. Figure 5 compares the results in three cases: three months gap between training
and test, one week gap between training and test, and no gap. We see
that DeepCorrâ€™s accuracy signicantly degrades with three months
gap between training and testâ€”interestingly, even this signicantly
degraded performance of DeepCorr due to lack of retraining is
superior to all previous techniques compared in Figure 10.
5.4 DeepCorrâ€™s Performance Does Not Degrade
with the Number of Test Flows
We also show that DeepCorrâ€™s correlation performance does not
depend on the number of ows being correlated, i.e., the size of the
test dataset. Figure 7 presents the TP and FP results (for a specic
threshold) on datasets with dierent numbers of ows. As can be
seen, the results are consistent for dierent numbers of ows being
correlated. This suggests that DeepCorrâ€™s correlation performance
will be similar to what derived through our experiments even if
DeepCorr is applied on signicantly larger datasets of intercepted
ows, e.g., on the ows collected by a large malicious IXP.
0.4
0.6
0.8
1.0 TP
0 1000 2000 3000 4000 5000 6000
Number of flows
0.00000
0.00025
0.00050
0.00075
0.00100
FP
Figure 7: DeepCorrâ€™s performance is consistent regardless of
the size of the testing dataset (we use a xed, arbitrary Î·).
5.5 DeepCorrâ€™s Performance Rapidly Improves
with Flow Length
In all of the previous results, we used a ow length of ` = 300
packets. As can be expected, increasing the length of the ows
used for training and testing should improve the performance of
DeepCorr. Figure 8 compares DeepCorrâ€™s performance for dierent
lengths of ows, showing that DeepCorrâ€™s performance improves
signicantly for longer ow observations. For instance, for a target
FP of 10âˆ’3
, DeepCorr achieves T P = 0.62 with ` = 100 packets long
ows, while it achieves T P = 0.95 with ows that contain ` = 450
packets.
Note that the lengths of intercepted ows makes a tradeo between DeepCorrâ€™s performance and the adversaryâ€™s computation
overhead. That is, while a larger ow length improves DeepCorrâ€™s
correlation performance, longer ows impose higher storage and
computation overheads on the trac correlation adversary. A larger
ow length also increase the adversaryâ€™s waiting time in detecting
correlated ows in real-time.
5.6 DeepCorrâ€™s Performance Improves with the
Size of the Training Set
As intuitively expected, DeepCorrâ€™s performance improves when
it uses a larger set of Tor ows during the training phase (i.e.,
DeepCorr learns a better correlation function for Tor with more
training samples). Figure 9 compares DeepCorrâ€™s ROC curve when
trained with dierent numbers of ows (for all of the experiments,
we use a xed number of 1,000 ows for testing). The gure conrms
that increasing the size of the training set improves the performance
of DeepCorr. For instance, for a target FP = 10âˆ’3
, using 1,000
training ows results in T P = 0.56, while using 5,000 ows for
training gives DeepCorr a T P = 0.8. This shows that a resourceful
adversary can improve the accuracy of her ow correlation classier
by collecting a larger number of Tor ows for training. Note that a
Session 10A: TOR CCSâ€™18, October 15-19, 2018, Toronto, ON, Canada 1970
10âˆ’3 10âˆ’2 10âˆ’1
False Positive
0.0
0.2
0.4
0.6
0.8
1.0
True Positive
flow length = 100
flow length = 200
flow length = 300
flow length = 450
Random Guess
Figure 8: DeepCorrâ€™s performance rapidly improves when
using longer ows for training and testing.
10âˆ’3 10âˆ’2 10âˆ’1
False Positive
0.0
0.2
0.4
0.6
0.8
1.0
True Positive
Size of trainig data = 1000
Size of training data = 3000
Size of training data = 5000
Random Guess Figure 9: DeepCorrâ€™s correlation performance improves
with more training data.
larger training set increases the training time, however the learning
process does not need to repeat frequently as evaluated before.
5.7 DeepCorr Signicantly Outperforms the
State-Of-The-Art
In Section 2.2 we overviewed major ow correlation techniques
introduced prior to our work. We perform experiments to compare
DeepCorrâ€™s performance with such prior systems in correlating
Tor ows. Figure 10 compares the ROC curve of DeepCorr to other
systems, in which all of the systems are tested on the exact same
set of Tor ows (each ow is at most 300 packets). As can be seen,
DeepCorr signicantly outperforms the ow correlation algorithms
Table 2: Correlation time comparison with previous techniques
Method One correlation time
RAPTOR 0.8ms
Cosine 0.4ms
Mutual Information 1ms
Pearson 0.4ms
DeepCorr 2ms
used by prior work, as we see a wide gap between the ROC curve of
DeepCorr and other systems. For instance, for a target FP = 10âˆ’3
,
while DeepCorr achieves a TP of 0.8, previous systems provide
TP rates less than 0.05! This huge improvement comes from the
fact that DeepCorr learns a correlation function tailored to Tor
whereas previous systems use generic statistical correlation metrics
(as introduced in Section 2.2) to link Tor connections.
Needless to say, any ow correlation algorithm will improve its
performance by increasing the length of the ows it intercepts for
correlation (equivalently, the trac volume it collects from each
ow); we showed this in Section 5.5 for DeepCorr. To oer reasonable accuracies, previous works have performed their experiments
on ows that contain signicantly more packets (and more data)
than our experiments. For instance, Sun et al. evaluated the state-ofthe-art RAPTOR [72] in a setting with only 50 ows, and each ow
carries 100MB of data over 5 minutes. This is while in our experiments presented so far, each ow has only 300 packets, which is
equivalent to only â‰ˆ 300 KB of Tor trac (in contrast to RAPTORâ€™s
100MB!). To ensure a fair comparison, we evaluate DeepCorr to
RAPTOR in the exact same setup (e.g., 50 ows each 100MB, and
we use the accuracy metric described in Section 4.4). The results
shown in Figure 11 demonstrates DeepCorrâ€™s drastically superior
performance (our results for RAPTOR comply with the numbers
reported by Sun et al. [72]). On the other hand, we show that the
performance gap between DeepCorr and RAPTOR is signicantly
wider for shorter ow observations. To show this, we compare
DeepCorr and RAPTOR based on the volume of trac they intercept from each ow. The results shown in Figure 12 demonstrate
that DeepCorr outperforms signicantly, especially for shorter ow
observations. For instance, RAPTOR achieves a 0.95 accuracy after receiving 100MB from each ow, whereas DeepCorr achieves
an accuracy of 1 with about 3MB of trac. We see that DeepCorr
is particularly powerful on shorter ow observations. We zoomed
in by comparing RAPTOR and DeepCorr for small number of observed packets, which is shown in Figure 13. We see that DeepCorr
achieves an accuracy of â‰ˆ 0.96 with only 900 packets, in contrast
to RAPTORâ€™s 0.04 accuracy.
5.8 DeepCorrâ€™s Computational Complexity
In Table 2, we show the time to perform a single DeepCorr correlation in comparison to that of previous techniques (the correlated
ows are 300 packets long for all the systems). We see that DeepCorr is noticeably slower than previous techniques, e.g., roughly
two times slower than RAPTOR. However, note that since all the
systems use the same length of ows, DeepCorr oers drastically better correlation performance for the same time overhead; for instance,
Session 10A: TOR CCSâ€™18, October 15-19, 2018, Toronto, ON, Canada 1971
10âˆ’5 10âˆ’4 10âˆ’3 10âˆ’2 10âˆ’1 100
False Positive
0.0
0.2
0.4
0.6
0.8
1.0
True Positive
DeepCorr
Mutual Information
RAPTOR
Cosine Correlation
Pearson Correlation
Random Guess
Figure 10: Comparing DeepCorrâ€™s ROC curve with previous systems shows an overwhelming improvement over
the state-of-the-art (all the systems are tested on the same
dataset of ows, and each ow is 300 packets).
âˆ’0.01 0.00 0.01 0.02 0.03 0.04 0.05
False Positive
0.0
0.2
0.4
0.6
0.8
1.0
True Positive
DeepCorr 5 minutes
RAPTOR 5 minutes
Random Guess Figure 11: Comparing DeepCorr to RAPTOR [72] using the
same ow lengths and ow number as the RAPTOR [72] paper.
based on Figure 10, we see that DeepCorr oers a TPâ‰ˆ 0.9 when all
previous systems oer a TP less than 0.2. Therefore, when all the
systems oer similar accuracies (e.g., each using various lengths
of input ows) DeepCorr will be faster than all the systems for
the same accuracy. As an example, each RAPTOR correlation takes
20ms (on much longer ow observations) in order to achieve the
same accuracy as DeepCorr which takes only 2msâ€”i.e., DeepCorr
is 10 times faster for the same accuracy.
Compared to previous correlation techniques, DeepCorr is the
only system that has a training phase. We trained DeepCorr using
a standard Nvidia TITAN X GPU (with 1.5GHz clock speed and
12GB of memory) on about 25,000 pairs of associated ow pairs and
0.3 11.4 22.5 33.5 44.6 55.7 66.8 77.8 88.9 100.0
Estimated flow size (MBytes)
0.0
0.2
0.4
0.6
0.8
1.0
Accuracy
DeepCorr
RAPTOR
Figure 12: Comparing the accuracy of DeepCorr and RAPTOR [72] for various volumes of data intercepted from each
ow. The RAPTOR values are comparable to Figure 6 of the
RAPTOR paper [72].
300 600 900 1200 1500 1800 2100 2400 2700 3000
# Packets
0
20
40
60
80
100
Accuracy (%)
DeepCorr
RAPTOR
Figure 13: Comparing DeepCorr to RAPTOR in correlating
short ows.
25, 000 Ã— 24, 999 â‰ˆ 6.2 Ã— 108 non-associated ow pairs, where each
ow consists of 300 packets. In this setting, DeepCorr is trained
in roughly one day. Recall that as demonstrated in Section 5.3,
DeepCorr does not need to be re-trained frequently, e.g., only once
every three weeks. Also, a resourceful adversary with better GPU
resources than ours will be able to cut down on the training time.
5.9 DeepCorr Works in Non-Tor Applications
as Well
While we presented DeepCorr as a ow correlation attack on Tor, it
can be used to correlate ows in other ow correlation applications
as well. We demonstrate this by applying DeepCorr to the problem
of stepping stone attacks [6, 26, 80]. In this setting, a cybercriminal proxies her trac through a compromised machine (e.g., the
Session 10A: TOR CCSâ€™18, October 15-19, 2018, Toronto, ON, Canada 1972
Figure 14: The network architecture of DeepCorr to detect
stepping stone attacks
Table 3: DeepCorrâ€™s parameters optimized for the stepping
stone attack application.
Layer Details
Convolution Layer 1
Kernel num: 200
Kernel size: (2, 10)
Stride: (1,1)
Activation: Relu
Max Pool 1 Window size: (1,5)
Stride: (1,1)
Fully connected 1 Size: 500, Activation: Relu
Fully connected 2 Size: 100, Activation: Relu
stepping stone) in order to hide her identity. Therefore, a network
administrator can use ow correlation to match up the ingress and
egress segments of the relayed connections, and therefore trace
back to the cybercriminal. Previous work has devised various ow
correlation techniques for this application [17, 33, 53, 59, 81].
For our stepping stone detection experiments, we used the
2016 CAIDA anonymized data traces [11]. Similar to the previous
works [33, 34, 53] we simulated the network jitter using Laplace
distribution, and modeled packet drops by a Bernoulli distribution
with dierent rates. We apply DeepCorr to this problem by learning
DeepCorr in a stepping stone setting. As the noise model is much
simpler in this scenario than Tor, we use a simpler neural network
model for DeepCorr for this application. Also, we only use one
direction of a bidirectional connection to have a fair comparison
with previous systems, which all only use one-sided ows. Figure 14
and Table 3 show our tailored neural network and our choices of
parameters, respectively.
Our evaluations show that DeepCorr provides a performance comparable to â€œOptimalâ€ ow correlation techniques of
Houmansadr et al. [33, 34] when network conditions are stable.
However, when the network conditions becomes noisy, DeepCorr
oers a signicantly stronger performance in detecting stepping
stone attacks. This is shown in Figure 15, where the communication
network has a network jitter with a 0.005s standard deviation, and
the network randomly drops 1% of the packets.
6 COUNTERMEASURES
While previous work has studied dierent countermeasures against
ow correlation and similar trac analysis attacks [2, 9, 19, 35, 41,
42, 50, 56, 61, 82], they remain mostly non-deployed presumably
due to the poor performance of existing ow correlation techniques
at large scale [60, 66]. In the following, we discuss two possible
countermeasures.
0.0000 0.0002 0.0004 0.0006 0.0008 0.0010
False Positive
0.0
0.2
0.4
0.6
0.8
1.0
True Positive
DeepCorr
Cosine
Optimal
Random Guess
Figure 15: DeepCorr outperforms state-of-the-art stepping
stone detectors in noisy networks (1% packet drop rate).
6.1 Obfuscate Trac Patterns
An intuitive countermeasure against ow correlation (and similar
trac analysis attacks like website ngerprinting) is to obfuscate
trac characteristics that are used by such algorithms. Therefore,
various countermeasures have been suggested that modify packet
timings and packet sizes to defeat ow correlation, in particular by
padding or splitting packets in order to modify packet sizes, or by
delaying packets in order to perturb their timing characteristics.
The Tor project, in particular, has deployed various pluggable transports [61] in order to defeat censorship by nation-states who block
all Tor trac. Some of these pluggable transports only obfuscate
packet contents [56], some of them obfuscate the IP address of the
Tor relays [48], and some obfuscate trac patterns [50, 56]. Note
that Torâ€™s pluggable transports are designed merely for the purpose
of censorship resistance, and they obfuscate trac only from a
censored client to her rst Tor relay (i.e., a Tor bridge). Therefore,
Torâ€™s pluggable transports are not deployed by any of Torâ€™s public
relays.
As a possible countermeasure against DeepCorr, we suggest to
deploy trac obfuscation techniques by all Tor relays (including
the guard and middle relays). We evaluated the impact of several
Tor pluggable transports on DeepCorrâ€™s performance. Currently,
the Tor project has three deployed plugs: meek, obfs3, and obs4. We
evaluated DeepCorr on meek and obfs4 (obfs3 is an older version
of obfs4). We also evaluated two modes of obfs4: one with IAT
mode â€œonâ€ [55], which obfuscates trac features, and one with the
IAT mode â€œoâ€, which does not obfuscate trac features. We used
DeepCorr to learn and correlate trac on these plugs. However, due
to ethical reasons, we collected a much smaller set of ows for these
experiments compared to our previous experiments; this is because
Tor bridges are very scarce and expensive, and we therefore avoided
overloading the bridges.6 Consequently, our correlation results are
6Alternatively, we could set up our own Tor bridges for the experiments. We decided
to use real-world bridges to incorporate the impact of actual trac loads in our
experiments.
Session 10A: TOR CCSâ€™18, October 15-19, 2018, Toronto, ON, Canada 1973
Table 4: DeepCorrâ€™s performance if Torâ€™s pluggable transports are deployed by the relays (results are very optimistic
due to our small training set, which is for ethical reasons).
Plug name TP FP
obfs4 with IAT=0 â‰ˆ 0.50 0.0005
meek â‰ˆ 0.45 0.0005
obfs4 with IAT=1 â‰ˆ .10 0.001
very optimistic due to their small training datasets (e.g., a realworld adversary will achieve much higher correlation accuracies
with adequate training). We browsed 500 websites over obfs4 with
and without the IAT mode on, as well as over meek. We trained
DeepCorr on only 400 ows (300 packets each) for each transport
(in contrast to 25,000 ows in our previous experiments), and tested
on another 100 ows. Table 4 summarizes the results. We see that
meek and obfs4 with IAT=0 provide no protection to DeepCorr; note
that a 0.5 TP is comparable to what we get for bare Tor if trained
on only 400 ows (see Figure 9), therefore we expect correlation
results similar to bare Tor with a larger training set. The results are
intuitive: meek merely obfuscates a bridgeâ€™s IP and does not deploy
trac obfuscation (except for adding natural network noise). Also
obfs4 with IAT=0 solely obfuscates packet contents, but not trac
features. On the other hand, we see that DeepCorr has a signicantly
lower performance in the presence of obfs4 with IAT=1 (again,
DeepCorrâ€™s accuracy will be higher for a real-world adversary who
collects more training ows).
Our results suggest that (public) Tor relays should deploy a traf-
c obfuscation mechanism like obfs4 with IAT=1 to resist advanced
ow correlation techniques like DeepCorr. However, this is not a
trivial solution due to the increased cost, increased overhead (bandwidth and CPU), and reduced QoS imposed by such obfuscation
mechanisms. Even the majority [55] of Obfsproxy Tor bridges run
obfs4 without trac obfuscation (IAT=0). Therefore, designing an
obfuscation mechanism tailored to Tor that makes the right balance
between performance, cost, and anonymity remains a challenging
problem for future work.
6.2 Reduce An Adversaryâ€™s Chances of
Performing Flow Correlation
Another countermeasure against ow correlation on Tor is reducing
an adversaryâ€™s chances of intercepting the two ends of many Tor
connections (therefore, reducing her chances of performing ow
correlation). As discussed earlier, recent studies [22, 52, 72] show
that various ASes and IXPs intercept a signicant fraction of Tor
trac, putting them in an ideal position to perform ow correlation
attacks. To counter, several proposals suggest new relay selection
mechanisms for Tor that reduce the interception chances of malicious ASes [2, 5, 41, 54, 71, 73]. None of such alternatives have been
deployed by Tor due to their negative impacts on performance,
costs, and privacy. We argue that designing practical AS-aware
relay selection mechanisms for Tor is a promising avenue to defend
against ow correlation attacks on Tor.
7 CONCLUSIONS
We design a ow correlation system, called DeepCorr, that drastically outperforms the state-of-the-art systems in correlating Tor
connections. DeepCorr leverages an advanced deep learning architecture to learn a ow correlation function tailored to Torâ€™s complex
network (as opposed to previous worksâ€™ use of general-purpose
statistical correlation metrics). We show that with adequate learning, DeepCorr can correlate Tor connections (and therefore break
its anonymity) with accuracies signicantly stronger than existing
algorithms, and using substantially shorter lengths of ow observations. We hope that our work demonstrates the escalating threat
of ow correlation attacks on Tor in rise of advanced learning algorithms, and calls for the deployment of eective countermeasures
by the Tor community.