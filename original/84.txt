Abstract
This paper reports the findings of a study where users () interacted with Malexa, Alexa’s malicious twin. Malexa is an intelligent voice assistant with a simple and seemingly harmless third-party skill that delivers news briefings to users. The twist, however, is that Malexa covertly rewords these briefings to intentionally introduce misperception about the reported events. This covert rewording is referred to as a Malware-Induced Misperception (MIM) attack. It differs from squatting or invocation hijacking attacks in that it is focused on manipulating the “content” delivered through a third-party skill instead of the skill’s “invocation logic.” Malexa, in the study, reworded regulatory briefings to make a government response sound more accidental or lenient than the original news delivered by Alexa. The results show that users who interacted with Malexa perceived that the government was less friendly to working people and more in favor of big businesses. The results also show that Malexa is capable of inducing misperceptions regardless of the user’s political ideology, gender identity, age or frequency of interaction with intelligent voice assistants. We propose a system-level solution for countering Malexa and discuss the implications of using Malexa as a covert “influencer” in people’s living environments.

Previous
Next 
Keywords
Malware-induced misperception (MIM)

Social engineering

Intelligent voice assistants

Amazon Alexa

1. Introduction
People enjoy using intelligent voice assistants like Amazon Alexa or Google Home simply because they are “seamless enough to be irresistible” (Simonite, 2019). But “seamless” and “irresistible” raise concerns about how “secure” and “privacy protecting” a voice assistant is. Studies show that an attacker can successfully issue malicious or hidden voice commands, for example, “to unlock a smart door to a home without the user’s knowledge” (Carlini, Mishra, Vaidya, Zhang, Sherr, Shields, Wagner, Zhou, 2016, Chung, Iorga, Voas, Lee, 2017). Attackers are also able to remotely control an intelligent voice assistant by talking to it through a smart speaker or intercom (Diao, Liu, Zhou, Zhang, 2014, Kumar, Paccagnella, Murley, Hennenfent, Mason, Bates, Bailey, 2018, Vaidya, Zhang, Sherr, Shields, 2015). It is even possible to directly install a malicious application on these devices by exploiting known vulnerabilities (Spring, 2019).

Despite these security flaws, people are happy to have these devices in their homes. They are only reluctant when it comes with possible infringements of their privacy because these devices “always listen” (Lau et al., 2018). Researchers, in this context, have discussed many issues including law enforcement unlawful intrusions (Pfeifle, 2018), behavioral surveillance (Zeng et al., 2017), home abuse (Parkin et al., 2019), or undisclosed third-party information sharing (Ammari, Kaye, Tsai, Bentley, 2019, Zheng, Apthorpe, Chetty, Feamster, 2018). The reluctance is mainly related to the trust (or lack of thereof) in these devices but this is only in context of how and by whom the user’s personal data is handled, not the “trustworthiness” of the data delivered to the user. Hardly anyone paused to ask if and what voice assistants do to preserve the integrity of the content spoken back to the users.

Users choose what content they would like the intelligent voice assistant to deliver to them. An Amazon Alexa user interested in news briefings, for example, downloads and installs a third-party voice application (called a “skill”) from the Alexa Skills Store that pulls daily headlines from a trusted source like The New York Times. The user prompts the voice assistant, “Hey Alexa, tell me the news today.” Alexa reads back several headlines including: “Bernie Sanders Says He Will Keep His Campaign Pace After Minor Heart Attack” (Ember and Martin, 2019). Today, it sounds like Senator Sanders is eager to catch up with the campaign duties after his health issue. Nothing seems suspicious about the above scenario, except that there is a twist. The third-party skill covertly turned Alexa into Malexa. The headline that Malexa, Alexa’s malicious twin, spoke back was not the same one that was pulled from the original source: “Bernie Sanders Says He Will Slow His Campaign Pace After Heart Attack.” Malexa’s goal, in this example, was to induce misperception about Senator Sanders’s heart attack and his future campaigning plans.

Malexa makes small, contextually significant changes to headlines in a dynamic fashion and unbeknownst to the user. Alone these changes could almost be meaningless (in the example headline, Malexa replaced “slow” with “keep” and inserted the word “minor” before “heart attack”). However, Malexa has a misperception agenda, and is designed to slowly, but surely, move a target user toward a specific viewpoint, political position, or general emotional state (Heckman et al., 2015). Malexa can work on a single user, or multiple Malexa units across a state or country could be used in concert. Malexa is a plausible threat because users can be persuaded to download voice applications from sources outside the Alexa Skills Store or even malicious applications that bypassed the store’s checks (an easy task for attackers as indicated in Guo et al., 2020 and Zhang et al., 2019a). We use the Malexa primer to highlight that intelligent voice assistants can misuse the users’ trust by covertly manipulating the content they speak back to them. Ensuring trustworthiness of online content is usually done through visual inspection, for example, spotting social engineering cues in an email. With intelligent voice assistants these cues are not readily available, which motivated us to explore how people perceive seemingly authentic content based on a brief audio inspection (Purington et al., 2017).

Malexa employs a similar concept of covert manipulation to a malicious third-party extension that acts as a man-in-the-middle in exchanging text through a web browser (Sharevski et al., 2020a). This extension, unbeknownst to the user, manipulates how authentic content from a trusted source is presented to them with a goal to induce misperception. We call this a Malware-Induced Misperception (MIM) attack. Studies exploring manipulation of online information point that induced misperceptions represent an effort of a communicating party to “lead an individual towards making false or implausible interpretations of a set of true facts” (Benkler et al., 2018). If this effort is realized through a malware that covertly manipulates the linguistic content communicated through a web browser or an intelligent voice assistant, there is reason to believe that a user potentially “misperceives” a sender’s intent or the context of communication. A malware-induced misperception attack uses an already established conditional trust the target individual has in the source and the authenticity of the online content, instead of impersonating the source or fabricating the content like “fake news” (DiResta et al., 2018).

In this paper we used the MIM attack to covertly turn Alexa into Malexa. Two groups of volunteering participants were brought in controlled laboratory settings to test Malexa’s effect in manipulating users’ perception on news headlines. The control group () received regular news from Alexa while the treatment group () received a reworded version of same news. The objective was to see whether there is a significant difference in how news are perceived between the Alexa group and the Malexa group. To introduce Malexa and what the study found out about “her”, we describe the concept of malware-induced misperception in Section 2. We show each step an MIM attacker can take to build a Malexa in Section 3. Section 4 covers the study design and Section 5 presents the empirical results. Section 6 discusses the implications of using intelligent voice assistants and unverified third-party skills, what Malexa could do in future, and proposes a system-level solution for countering Malexa. Section 7 concludes the paper summarizing Malexa’s potential to influence our perception of the current political climate beyond any trolling campaign.

2. Malware-induced misperception
2.1. Concept
An interesting anecdote prompted us to test the MIM attack with intelligent voice assistants. During the Super Bowl LI, a Google Home ad using the wake-word “OK, Google” reportedly set off many viewers’ own devices (Chung et al., 2017). Burger King quickly used this trick and ran an ad for the Whopper in which an actor playing an employee says that 15 s is not enough time to describe the sandwich and instead asks Google, which cites the definition from Wikipedia. The idea was to set off viewers’ devices to repeat the question and thus extend the ad (Anderson, 2017). However, someone figured it was time for a prank and altered the Wikipedia entry to say that the Whopper contains “cyanide,” is “cancer-causing,” and is the “worst hamburger product” sold by Burger King  (Rodionova, 2019).

The MIM attack, in its basic form, does similar alterations, although different in purpose, targeting, and nature. The purpose of the MIM attack is to socially engineer one’s mental picture or map of reality with the objective to lead an individual towards making false or implausible interpretations of a set of true facts (Benkler et al., 2018). The altered Wikipedia article on the Whopper falls under “fake news” since the prankster’s goal was to disseminate explicitly false information (Benkler et al., 2018). Although there is a distinction between “fake news” and “induced misperceptions” the MIM attack has the potential to deliver ”fake news,” for example, by altering the type of Senator Sanders’s health issue “Bernie Sanders Says He Will Slow His Campaign Pace After Brain Surgery” (replacing “heart attack” with “brain surgery”).

However, unlike the Burger King prank, the MIM attacker doesn’t alter the content at the source, but dynamically alters the authentic content right before it is delivered to a targeted user. The MIM attacker also tries to evade a scenario where the targeted user will immediately scrutinize the delivered content for possible deception or inconsistency (if, for example, the target heard somewhere else that Senator Sanders had a heart attack, an alternation that he had a “brain surgery” will prompt them to immediately scrutinize the delivered content) (Levine, 2014). The Burger King prank targeted all users that happened to watch the said commercial while in proximity of their intelligent voice assistants. Although this is certainly a scale of ultimate interest, the MIM attacker usually targets a smaller user population or a particular individual. The MIM attacker spends time and effort to profile the targets and tailor the attack based on their interests rather than targeting everyone with the objective to harm a particular company. In the Malexa context, for example, the MIM attack could specifically target users based on a particular political candidate they support (Bakshy et al., 2015).

In a real world context, the MIM attack is close to the efforts in which social networks were used to induce misperception through ads and target people of interest (Baldwin, 2018). The infamous example is Cambridge Analytica, a political data firm, which gained access to private information on more than 87 million Facebook users. The firm then offered tools to interested parties that could identify the personalities of voters and influence their political opinion (Granville, 2018). Another similar case of induced misperception was when the UK Labour Party campaign chiefs believed that digital ads requested by party leader, Jeremy Corbyn, were too expensive. Instead, they ran hyper-specific ads through Facebook so that only Corbyn and his team would see them (Baldwin, 2018). The MIM advantage, from the perspective of an attacker, is that the relationship between the target user and an online resource can be manipulated without alerting any of the involved parties. MIM can be employed, for example, to influence a target user to divulge a comment or personal opinion on social media that they otherwise wouldn’t express, fearing social isolation (Sharevski et al., 2020b).

2.2. MIM attack flow
Instead of directly altering the content at its source (e.g. Wikipedia), the MIM attack alters the content before it is presented to the targeted user as shown in Fig. 1. For this man-in-the-middle exploit to take place in a browser or an intelligent voice assistant, the attacker in the first step employs a legitimacy-by-design (seeming legitimate both in visual design and in what the user expects to see from a legitimate application) to persuade the target user to install a third-party extension or a skill (Newman, Vincent). This functionality is preferred because the third-party extension or skill in the second step requires text manipulation permissions from the user (granted in the third step) that the attacker can use in the forth step to dynamically manipulate any text. To execute the attack, the user has to access a web page or invoke a skill in the fifth step so the malware can covertly manipulate the original content in the sixth step.

Fig. 1
Download : Download high-res image (534KB)
Download : Download full-size image
Fig. 1. The MIM attack flow.

An example of a MIM attack is shown in Fig. 2 (top: authentic headline; bottom: reworded headline). Assuming a user have installed the extension and accessed The New York times page, the malware covertly replaced the word “slow” with “keep” and inserted the word “minor” before “heart attack” in a news article covering Senator Sanders’s press conference after he suffered a heart attack. A MIM target user has no reason to question the legitimacy of the article because it comes from a trusted source, The New York Times (the “https” padlock, or the Uniform Resource Locator (URL), and the context check as valid) (Ember and Martin, 2019). The malware manipulation in this example downplays the seriousness of the health issue and the next steps in Senator Sanders’s campaign.

Fig. 2
Download : Download high-res image (386KB)
Download : Download full-size image
Fig. 2. malware extension “off”.

One might say that news agencies, for the same event, will give different headlines in order to appeal to their reader/viewership and induce alternative perceptions. For example, Fox News’s headline about the Senator Sanders’s health issue read “Bernie Sanders says he was ‘more fatigued’ in months leading up to heart attack but ignored symptoms.” Here, the focus is not on the future steps of Senator Sanders’s campaign, but on the past steps that lead to his heart attack in order to present him as an unfit candidate for the office. The difference between Malexa and an alternative editorial media is that Malexa’s goal is to manipulate how one perceives the original’s source agenda (in this case, The New York Times), not to provide an alternative political agenda (Fox News). Malexa, in other words, exploits the credibility of the news source to “nudge” the target individual to interpret the news to the objective of the attacker, not the original (or any other) editor of the headline and the story.

2.3. MIM attack in intelligent voice assistant environment
The MIM attack can be executed into an intelligent voice assistant ecosystem such as the one shown on Fig. 3. Amazon introduced voice assistant skills to allow Alexa to help users with a multitude of tasks. Skills are essentially third-party apps, like browser extensions, offering a variety of services Alexa itself does not provide (Zhang et al., 2019b). To invoke a skill, the user utters a wake-word, a trigger phrase, and the skill’s invocation name. For example, for the spoken sentence “Alexa, tell me the news today,” “Alexa” is the wake-word, “tell me” is the trigger phrase, and “news” is the skill invocation name. In response, Amazon’s cloud relays this request to the third-party server that returns text content as a result e.g. “Bernie Sanders Says He Will Slow His Campaign Pace After Heart Attack.” This response is converted to speech by Alexa, and spoken back to the user through the Alexa-enabled device.

Fig. 3
Download : Download high-res image (161KB)
Download : Download full-size image
Fig. 3. Alexa ecosystem: example.

To publish a skill, the third-party needs to submit the information about their skill including name, invocation name, description and the endpoint where the skill is hosted for a certification process (Amazon, 2019a). This process aims at ensuring that the skill is functional and meets Amazon’s security requirements and policy guidelines. Once a skill is published on the Amazon Skills Store, users can simply activate it by calling its invocation name. Unlike web browser extensions that need to be installed by users explicitly, skills can be automatically discovered (according to the user’s voice command) and transparently launched directly through Alexa.

For a MIM attacker to turn Alexa into Malexa, it needs to map the web browser functionality into a skill that can pass Amazon’s checks. Developing and publishing malicious third-party skills is not a hard process as shown in Zhang et al. (2019b). Although a third-party skill that reads particular news headlines from a predefined source may stand out as a “suspicious” skill, the Alexa Skills Store includes skills that repackage free content in such a fashion. For example, the “World Factbook” skill by Amazon user Laynr, takes content from the Central Intelligence Agency’s (CIA) Factbook, and claims to return feedback to users based on asking Alexa particular questions (Laynr, 2017). The description of the skill does not list very many details, like publishing date (missing) or the publisher (Laynr rather than the actual CIA). Once the skill is downloaded to a device, the target user is not likely to read the full description of the skill, meaning that all cause for distrust no longer exists. The dataset that Laynr used to create the skill is not visible on the Alexa Skills Store, so it is possible that the “World Factbook” skill could alter the CIA Factbook data, for example, to induce misperceptions about a particular country.

As a support for feasible development of third-party skills, Amazon additionally offers “Alexa Skills Blueprints,” which lower the technical barrier to entry by making it possible for someone to create their own Alexa skill without writing any code (Amazon, 2019b). These blueprints are template skills that users without an extensive knowledge and experience in developing applications can customize to perform a variety of different tasks. Blueprints include opportunities for practical skills, like reading a Really Simple Syndication (RSS) feed, or returning static content, like facts or flashcards to a user (Amazon, 2019b). These skills are highly customizable, and can then be distributed on the Alexa Skills Store. Amazon doesn’t require for the skill publisher to disclose the customization details, making it difficult for a user to validate the content delivered by Alexa. These are readily available resources that a MIM attacker can use to invoke the misperception chain.

3. Malexa’s misperception chain
To describe how Alexa can become Malexa, we will use the “misperception chain,” an adaption of the Lockheed Martin’s “cyber kill chain” (Heckman et al., 2015), as shown in Fig. 4. In the first step, purpose definition, the MIM attacker defines the strategic goal as inducing misperception about a topic of interest to a target user which can be achieved either through a malicious browser extension (Sharevski, Treebridge, Jachim, Li, Babin, 2020, Sharevski, Treebridge, Jachim, Li, Babin, Westbrook) or a malicious third-party skill. In the second step, collect intelligence, the MIM attacker determines what the target user will observe (e.g. daily news, regulation reports, etc.), how the target user might interpret those observations (e.g. engage, ignore, scrutinize), how the target user might react (or not) to those observations, and how to monitor the target user’s behavior. Based on this, the MIM attacker designs a cover story in the third step; this is what the MIM attacker wants the target user to perceive and believe. Next, the MIM attacker analyzes the characteristics of the real events and activities that must be hidden or manipulated to support the misperception cover story and plans the attack execution.

Fig. 4
Download : Download high-res image (803KB)
Download : Download full-size image
Fig. 4. Malexa’s misperception chain.

This is when Malexa, based on the “cover story plan” designed in the previous phases, comes to fruition as a malicious third-party skill in the forth step. In real life, the execution of the attack, or the delivery of Malexa, can take various forms assuming the malicious skill is available on the Alexa Skills Store. One option is to use the manipulation of the invocation name or a “skill squatting attack” (Kumar, Paccagnella, Murley, Hennenfent, Mason, Bates, Bailey, 2018, Zhang, Mi, Feng, Wang, Tian, Qian, 2019). Another option is to use social engineering and persuade the target user to download a skill from a direct link or directly from the Alexa Sills Store. In our study, we emulated the attack in controlled lab settings by locally enabling the Malexa skill. In the last phase, the MIM attacker monitors the target user’s reaction (step five) to the “performance” (step six). For the particular implementation of the MIM attack through Malexa, in our case, we conducted a study in controlled settings where participants were exposed to either an Alexa (control) or Malexa “cover story” (treatment) and asked to report their reaction. We did this to explore whether Malexa is a viable tactic in an intelligent voice assistant environment for inducing misperception (contextual, tied to particular news information).

3.1. News manipulation
In our study, we determined that our “targets” will observe, or better said, listen to, news headlines spoken back by an intelligent voice assistant. To start developing the “cover story”, the MIM attacker needs to gain enough of an understanding of their target user to find valence word/phrase pairs that do not raise suspicion for the user as a listener. For this, a MIM attacker can scrape data from a public news source and use machine-learning techniques to find the target pairs for rewording. Because we wanted to test the Malexa effect with actual users in our lab, for the “news” we utilized a selection of Occupational Safety and Hazard Administration (OSHA) news releases published on the US Department of Labour website (US Department of Labor, 2019a). OSHA has been used in an implicit campaign move by Senator Sanders to call for investigation for safety violations in Amazon warehouses (Sanders, 2019).

This choice over regular daily headlines like the Senator Sanders’s health issue was made to eliminate any a priori bias participants might have based on the particular news headline (Shirani-Mehr et al., 2018). Another reason to utilize these releases as “news” is that a participant is not cognizant they exist and is less likely to have any preconceptions about workplace safety regulation. This allowed us to get their first-hand reaction to Malexa’s “cover story”. The OSHA news releases are more strategic because they are highly repetitive, so it is easier for a MIM attacker to predict the wording that may come in future news releases.

The US Department of Labor, responsible for OSHA regulation, kept a full archive with each news release they had published since 2009 (US Department of Labor, 2019a). In order to get a data set that we could use to collect enough text to start predicting terms in future news releases, we decided to scrape all of the full OSHA news releases. In total, we scraped 5573 news releases. To increase the number of samples and to target content presented in individual sentences, each news release was broken into individual sentences, so each sample was a single sentence. In total, there were 58,796 sentences. Due to the repetitive nature of the corpus, we decided to also remove common words unique to this data set, e.g. “OSHA” “occupational,” “safety,” “health,” and “administration.” Next, we used the Term Frequency–Inverse Document Frequency (TF-IDF) statistic to ensure balance of the cluster analysis (Robertson and Jones, 1976).

The TF-IDF statistic is regularly used with the KMeans method for document clustering, including in applications for recommending news articles (Lu, Qin, Cao, Liu, Mengxing, 2014, Singh, Tiwari, Garg, 2011). Therefore, we used KMeans to perform cluster analysis to group sentences that were more closely related to one another. KMeans groups rows into  clusters, based on an assigned number. To choose a value for  we performed a manual qualitative assessment of the groupings based on values for  ranging from 6 to 12. We decided that  worked best for our assessment. This particular choice of TF-IDF for feature extraction with KMeans for document clustering is very common even outside of academia (Foley, Goel, Salnikov). This makes it a likely technique for even an unsophisticated MIM attacker to utilize it.

To establish which clusters had the most repetitive language, we measured each cluster’s “lexical diversity” (Russell, 2013). Lexical diversity shows how rich a language is, and is calculated by dividing the number of unique words in a passage with the total number of words in the passage. If a text has a lexical diversity of one, then that means that each word in the passage only occurs once. We learned that clusters with large amounts of regulatory words have less rich language. Therefore, by targeting regulatory language, rewording fewer valence word/phrases could affect more statements.

Of the ten clusters, four had more regulatory language, so we manually scanned the top 100 words in each of those four clusters to determine which words were worth finding valence pairs for. Next, we identified 136 unique words that seemed particularly susceptible to manipulative rewording, because they were regulatory terms, or seemed easy to change to show the government in a bad light or put the company in a positive light. To find the valence pairs, we selected terms that refer to infractions and replaced them with related words or metaphor phrases from everyday parlance (Sopory, 2017). Metaphor is commonly found in a variety of health risk communication contexts and people process it through engagement with their corresponding conceptual maps of reality (Landau et al., 2018). For example:

•
“faces major retribution” “gets off easy;” “faces major retribution” makes it sound like the government decisively ruled in favor of the worker(s) in a given safety violation case. “Gets off easy” sounds like the government also favored the company in the particular regulatory ruling.

•
“fine” “slap on the wrist;” a “fine” makes it sound like the employer deliberately broke the law. “Slap on the wrist” sounds comparatively more lenient. This rewording is intended to give the impression that the safety violation was more accidental.

•
“punish” “penalize;” “punish” sounds like the company was caused to suffer because of crime or misconduct; “Penalize” suggests that the company was rather put into an unfavorable position but not suffering per se. This rewording is intended to make the government response sound more lenient than it actually is.

The intention with these rewordings is not to significantly change the meaning of a news headline, but to change how the text is perceived (or the perspective of the “cover story”). The goal is to lead a target user towards making false or implausible interpretations of the set of true facts contained in the original OSHA news release. This means that if the target user, after hearing about an interesting OSHA headline decides to read the full news release, their suspicion may not be raised about the inconsistency between the news release and the altered RSS feed.

3.2. Malexa skill development
A MIM attacker interested in creating a custom news briefing third-party skill can use a news skill blueprint and customize the name, category, endpoint, and the frequency for the briefing. To customize the skill’s news delivery logic, the MIM attacker needs to select a valid RSS feed from a regular news source like The New York Times and successfully upload a valid.xml format. The MIM attacker is not required to give any additional context or explanation regarding the delivery logic of the RSS feed: only the name of the feed and category. The RSS feed and the news delivery logic, once implemented, are not visible when enabling or purchasing the skill on the Alexa Skills Store. In our study, we did not publish the Malexa skill but only enabled it locally in the lab. The name of the Malexa skill was simply “news” and we used a predefined selection of OSHA news releases from their official RSS feed (US Department of Labor, 2019b).

The development and code test of Malexa for our study took about six weeks with the work focused on customization of the Alexa news skill blueprint (no code was reused from the our previous work on malicious third-party browser extensions (Sharevski et al., 2020a)). We experienced no difficulties since the blueprints are designed to enable fast skill development and customization even by users without an extensive technical background in application development (the relatively easy process of creating Malexa enables for anybody with minimal technical knowledge to become a MIM attacker and develop various types of Malexas, which has implications for detection and prevention discussed in Section 6). The Malexa code that implements the news delivery logic starts by assigning a title phrase associated with the skill’s invocation name to the legitimate OSHA RSS feed. This enables the user to say “Alexa, tell me the news today,” and hear news from OSHA news releases.

Once invoked, the Malexa code proceeds to extract the news headlines from the OSHA RSS.xml feed. The Malexa code splits each news headline item into individual words and replaces valence words/phrases (e.g. “penalize” with “punish,” or “slap on the wrist” with “fine”). The code contains a string array predefined by an MIM attacker that can contain “valence words” for simplicity, but also phrases, e.g. a metaphor, as we used in our study. A MIM attacker can implement more complex logic where the rewording can take place only in certain parts of news content or only in headlines reporting on a specific person or issue, e.g only safety violations that occurred in Amazon warehouses but not in other companies. After the valence words/phrases are replaced within the news headline, the reworded content will be passed to the original output variable. The output text-to-speech read by Malexa back to the user is the covertly manipulated headline content.

4. Malexa evaluation: research study
Assessing intelligent voice assistants security is a popular line of research. A large part of the work is focused on exploiting the speech processing part of the intelligence, either by replaying voice commands or playing hidden, obfuscated or inaudible commands (Carlini, Mishra, Vaidya, Zhang, Sherr, Shields, Wagner, Zhou, 2016, Diao, Liu, Zhou, Zhang, 2014, Roy, Shen, Hassanieh, Choudhury, 2018, Vaidya, Zhang, Sherr, Shields, 2015); A small, emerging part shifted towards exploring the threats to end users caused by malicious third-party skills (Kumar, Paccagnella, Murley, Hennenfent, Mason, Bates, Bailey, 2018, Zhang, Mi, Feng, Wang, Tian, Qian, 2019). Here, the attacker crafts an invocation name for a malicious skill with a similar pronunciation as that of a legitimate skill (or uses different variations of the target’s invocation utterances) to trick the user into invoking a malicious skill when trying to open the legitimate one (e.g. “Capital One” vs. “Capital Won”).

Our study is also focused on malicious third-party skills, but not in the way they are invoked. Instead, we are interested in covertly manipulating the content the skill delivers back to the user. In a security context, we investigate a man-in-the-middle attack rather than squatting or command hijacking attacks. The goal of this attack that we call MIM is to induce misperception through the content delivered by the voice assistant without making the user suspicious of deception. In other words, we covertly turn Alexa into Malexa and tested whether there is a difference in news perceptions with intelligent voice assistant users who volunteer as participants.

4.1. Study design
Following an Institutional Review Board (IRB) approval for the study, we set up a physical location for recruiting participants in the Greater Chicago Area. The inclusion criteria required participants to be at least 18 years or older, have interacted with an intelligent voice assistant at least a few times in their life (e.g. Alexa, Google Home, Siri, etc.), and were aware of the existence of skills for voice assistants as a way to customize the interactive experience. A convenience sample of 220 participants agreed to be in the study. Each participant was invited to our lab and given an IRB approved non-full disclosure consent statement to review before participating in the study. The study was anonymous and we did not collect any personally identifiable information. We preliminarily asked about the participants’ interest in using customized skills for news delivery and the general consensus was that “they seem convenient to get the news of the day on the fly or when preparing for work.” Participants were initially told they were participating in a usability study that tests a new Alexa skill for news briefings focused on workplace safety. After the participation, we briefed each participant on the true goal of the study (full disclosure) and we asked if they would still agree for us to use their anonymous answer for the analysis (we provided an option to remove it). We also cleared the (M)alexa voice history after each participation.

Participants who agreed to be in the study were randomly assigned to either a control “Alexa” () or a treatment “Malexa” group (). Each participant was asked to prompt the voice assistant with the spoken sentence “Alexa, tell me the news today.” In the control group, Alexa responded: “In OSHA news,

•
Rail Car Services faces major retribution of $130,000 for safety violations leading to disabilities.

•
The government fines New York manufacturer for cover up after employee fractures hand at Long Island plant.

•
The government punishes Florida company for exposing employees to amputations.”

In the treatment group, Malexa dynamically swapped the words “faces major retribution” with “gets off easy,” “fines” with “slap on the wrist,” and “penalizes” with “punishes” in the original OSHA reports, responding: “In OSHA news,

•
Rail Car Services gets off easy with retribution of $130,000 for safety violations leading to disabilities.

•
The government gives a slap on the wrist to New York manufacturer for cover up after employee fractures hand at Long Island plant.

•
The government penalizes Florida company for exposing employees to amputations.”

After they heard the news from either Alexa or Malexa, the participants completed an anonymous questionnaire. The questionnaire asked about their perception of regulatory performance respective of the reported incidents (OECD, 2012) and basic demographic information. We used the collected data to test the following hypotheses:

Hypothesis : Preserving the facts, a covert rewording of the news headlines about workplace safety incidents, done by a malicious third-party skill, won’t affect the intelligent voice assistant user’s perception of the government regulation performance in response to the reported incidents.

Hypothesis : Preserving the facts, a covert rewording of the news headlines about workplace safety incidents, done by a malicious third-party skill, won’t affect the intelligent voice assistant user’s perception that the government response benefited the workers in the reported incidents.

Hypothesis : Preserving the facts, a covert rewording of the news headlines about workplace safety incidents, done by a malicious third-party skill, won’t affect the intelligent voice assistant user’s perception that the government response benefited the companies in the reported incidents.

Because the news headlines are spoken back to the users from a seemingly legitimate news source, and users generally trust the content delivered through the intelligent voice assistants regardless of the frequency of use, we also hypothesize that (Chung et al., 2017),(Lau et al., 2018):

Hypothesis 2: One’s frequency of using intelligent voice assistants does not affect their perception on government regulation performance based on news releases about workplace safety incidents, regardless of the wording of the headline (assuming the facts are preserved).

4.2. Measures
4.2.1. Regulatory performance perception
The Organization for Economic and Co-operation and Development (OECD) recommends perception surveys to gauge citizen satisfaction with the government regulation performance on topics like work safety (OECD, 2012). We adopted this instrument and asked the participants to indicate the perception of governmental regulation performance using a 7-point scale (1 = “extremely inadequate”/“strongly disagree” to 7 = “extremely adequate”/“strongly agree”):

•
Q1: How adequate was the government response in the reported incidents? (Alexa group ; Malexa group );

•
Q2: The workers in the reports personally benefited from the government response.(Alexa group ; Malexa group )

•
Q3: The companies in the reports benefited from the government response. (Alexa group ; Malexa group )

4.2.2. Demographics
Participants consisted of 45% female (N = 99), 49.5% male (N = 109), 5.5% gender non-conforming (N = 12). 66.4% of the participants were between 18 and 22 years old, 15.9% between 23 and 2, 8.2% between 29 and 33, and 9.5% above 32 years old. Participants were asked to provide their political ideology (20.9% “Very Liberal,” 37.3% “Somewhat Liberal,” 29.1% “Neither Liberal Nor Conservative,” 9.1% “Somewhat Conservative,” and 3.6% “Very Conservative”) and how frequently they use intelligent voice assistants (32.7% “Sometimes,” 46.3% “Half the time,” and 21.0% “Most of the time”). In general, we had a gender-balanced, liberal-leaning, and dominantly young sample of participants that were accustomed of using intelligent voice assistants in various forms.

5. Results
5.1. Hypothesis 1
Hypothesis  stated that a covert change of words in news content (preserving the facts) delivered back by an intelligent voice assistant to users on the topic of workplace safety will not affect their perception of the government’s regulation performance (Q1). As shown in Table 1, there is a statistically significant difference in perception between the participants in the Alexa and Malexa groups   (effect size = large) on the adequateness of the government response in the reported OSHA incidents. The participants in the Alexa group perceived the government’s response mostly as “slightly adequate” while the participants in the Malexa group perceived it as “moderately inadequate.” This result rejects Hypothesis  and accepts the alternative hypothesis, that is, Malexa is capable of inducing misperceptions simply by covertly rewording the headlines and without affecting the facts in the news content.


Table 1. The government’s OSHA regulatory performance perception [Q1, Q2, Q3].

Q1	Q2	Q3
Mann-Whitney 	9586.5	8010.5	3789.0
 score	7.596	4.975	4.513
Effect size 	0.512	0.335	0.305
Asmt. Sig. 	0.000*	0.001*	0.001*
Fig. 5 shows the mean and standard deviation for both groups on Q1 responses controlling for the participant’s political ideology. Although we have not achieved a statistically significant difference between the groups of political ideology on the question on the appropriateness of government response, Fig. 5 further confirms the outcome of the Hypothesis . The political ideology does not factor when it comes to inducing misperception, in this case about the appropriateness of governmental OSHA ruling, suggesting that Malexa can target users over the entire political spectrum. Similarly, Malexa can target users regardless of their gender identity and age, as our analysis suggest that participants’ (mis)perceptions are consistent over all the gender and age groups.

Fig. 5
Download : Download high-res image (178KB)
Download : Download full-size image
Fig. 5. Q1 answers M/SD controlling for political ideology - Alexa(blue)/Malexa(red). (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.)

Hypothesis  stated that if the workers were the one that benefited in the reported incidents, there won’t be any difference in the perception between the two groups (Q2). As shown in Table 1, there is a statistically significant difference in perception between the participants in the Alexa and Malexa groups   (effect size = medium) that the workers benefited in the reported OSHA incidents. The participants in the Alexa group “moderately agreed” that the government helped the workers while the participants in the Malexa group appear to “slightly disagree.” This result rejects Hypothesis  and accepts the alternative hypothesis, that is, Malexa is capable of inducing misperceptions that the workers did not benefit from the government response.

Fig. 6 shows the mean and standard deviation for both groups on Q2 responses controlling for the participant’s political ideology. The political ideology, gender identity, and age group also do not factor when it comes to inducing misperception, in this case about the workers as the ones that benefited from the government response in the OSHA ruling. This result further confirms the outcome of the Hypothesis  suggesting that Malexa can target users over the entire political spectrum, all genders, and all age groups and instill a perception that the government is less friendly to working people.

Fig. 6
Download : Download high-res image (196KB)
Download : Download full-size image
Fig. 6. Q2 answers M/SD controlling for political ideology - Alexa(blue)/Malexa(red). (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.)

Hypothesis  stated that if the companies were the one that benefited in the reported incidents, there won’t be any difference in the perception between the two groups (Q2). As shown in Table 1, there is a statistically significant difference in perception between the participants in the Alexa and Malexa groups   (effect size = medium) that the companies benefited in the reported OSHA incidents. The participants in the Alexa group “moderately disagreed” that the government helped the companies while the participants in the Malexa group “slightly agreed.” This result rejects Hypothesis  and accepts the alternative hypothesis, that is, Malexa is capable of inducing misperceptions that the companies indeed benefited from the government response.

Fig. 7 shows the mean and standard deviation for both groups on Q3 responses controlling for the participant’s political ideology. The political ideology, gender identity, and age group also do not factor when it comes to inducing misperception, in this case about the companies as the ones that benefited from the government response in the OSHA ruling. This result further confirms the outcome of the Hypothesis  suggesting that Malexa can target users over the entire political spectrum, all genders, and all age groups and instill a perception that the government is more in favor of big businesses.

Fig. 7
Download : Download high-res image (183KB)
Download : Download full-size image
Fig. 7. Q3 answers M/SD controlling for political ideology - Alexa(blue)/Malexa(red). (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.)

5.2. Hypothesis 2
Hypothesis 2 stated that one’s frequency of using intelligent voice assistants doesn’t affect their perception of news on the government’s regulation performance about workplace safety incidents. We found no statistical significance within both the Alexa and Malexa group for the three categories of frequency of intelligent voice assistant use, accepting the Hypothesis 2. This result indicates that the frequency of interaction with intelligent voice assistant doesn’t affect the perception formation for target individuals, regardless of the wording of the news spoken back to them. We extended the analysis to compare the perceptions between the Alexa and Malexa group when controlling for the frequency of interaction.

As shown in Table 2, there is a statistical significance between the participants’ perceptions for all the frequency groups on Q1. The “somewhat” group of participants in the Alexa scenario perceived the government response “somewhat adequate” while the same subgroup in the Malexa group perceived it as “somewhat inadequate.” The “half the time” group of participants in the Alexa scenario perceived the government response “somewhat adequate” while the same subgroup in the Malexa group perceived it as “moderately inadequate.” The “most of the time” group of participants in the Alexa scenario perceived the government response “somewhat adequate” while the same subgroup in the Malexa group perceived it as “neither adequate nor inadequate.”


Table 2. Q1 perceptions: Alexa/Malexa groups, controlling for the frequency of interaction.

Sometimes	Half time	Most time
Mann-Whitney 	939.5	474	145
 score	5.248	5.602	2.646
Effect size 	0.443	0.554	0.390
Asmt. Sig. 	0.000*	0.000*	0.008*
As shown in Table 3, there is a statistical significance between the participants’ perceptions on Q2 only for the “sometimes” and “half the time” groups. The “somewhat” group of participants in the Alexa scenario “slightly agreed” that the workers benefited from the government response, while they “somewhat disagreed” in the Malexa scenario. The “half the time” group of participants in the Alexa scenario “neither agreed nor disagreed” that the workers benefited from the government response, while they “moderately disagreed” in the Malexa scenario.


Table 3. Q2 perceptions: Alexa/Malexa groups, controlling for the frequency of interaction.

Sometimes	Half time	Most time
Mann-Whitney 	1293.5	762	173.5
 score	3.728	3.396	1.602
Effect size 	0.305	0.339	0.160
Asmt. Sig. 	0.000*	0.001*	0.108
As shown in Table 4, there is a statistical significance between the participants’ perceptions on Q3 only for the “sometimes” and “half the time” groups. Both the “sometimes” and “half the time” participants in the Alexa scenario “neither agreed nor disagreed” that the companies benefited from the government response, while they “somewhat agreed” in the Malexa scenario. In summary, the results show that Malexa can break the ambivalence for people who interact with intelligent voice assistants either sometimes or half the time. These findings indicate that a malicious actor can profile the target users by frequency of interaction. Following the principle of moderation, here too, the most likely targets of the MIM attack are those that supplement their daily news diet with information from intelligent voice assistants, but not entirely rely on them.


Table 4. Q3 perceptions: Alexa/Malexa groups, controlling for the frequency of interaction.

Sometimes	Half time	Most time
Mann-Whitney 	1706	1766	272.5
 score	2.825	3.4	1.056
Effect size 	0.332	0.338	0.161
Asmt. Sig. 	0.005*	0.001*	0.291
6. Discussion
6.1. What Malexa did in our study
This study tested the effects of a malicious third-party skill aiming to covertly manipulate the perception about workplace safety news delivered by an intelligent voice assistant to a targeted user. To do so, the malicious skill covertly launches a MIM attack on the content spoken back to a targeted user, turning Alexa into Malexa. Previous studies exploring malicious third-party skills manipulated the invocation name of the skill with the goal to impersonate an original skill. Although this certainly works in our case and may even be a favorable way of skill invocation, our primary interest was to test whether the victims of the MIM attack in an intelligent voice assistant ecosystem will report different perceptions than users receiving original, non-manipulated content.

The results demonstrate that Malexa is capable of manipulating the perception of the government action in the reported OSHA news. Malexa essentially altered the perspective in these OSHA news releases (or “the cover story” as referred to in the misperception chain) and with that made the participants react as if the government inadequately favored the big business instead of the workers in the safety violations (the opposite reaction was observed for the participants in the Alexa group). We are aware that “adequate” as a response is a subjective valuation respective to the context of the reported incident, one’s trust in government, or a particular regulatory awareness. We see the statistically significant difference in the reported perception between groups as a preliminary evidence of the Malexa’s potential as an “influencer” and not definitive proof that Malexa can do actually “brainwashing” (Aro, 2016).

The potential for “influencing” is further corroborated with the results suggesting that one’s political ideology, gender identity, and age do not factor into one’s perception of OSHA news when talking to Alexa or Malexa. This gives Malexa the opportunity to go beyond the divisive political influencing Cambridge Analytica did during the 2016 election cycle (Baldwin, 2018) and “influence” a much broader set of targeted users to make subjective or implausible interpretations of a set of true facts, instead of dismissing the news headlines as advancing “partisan agendas” (Mummolo, 2016). Malexa also has the advantage in picking up the influencing targets based on how frequently a user interacts with an intelligent voice assistant. The results from our study suggest that the most attractive targets, at least in the context of government regulating news, were the ones using voice assistants moderately, that is, more for getting quick information rather as a main source of information.

6.2. What Malexa can’t do now
As much as we have extolled the “virtues” of Malexa, the MIM attack in an intelligent voice assistant ecosystem has a long way to go. In our study, Malexa was preconfigured to do the replacements for the particular valence word/phrase pairs and affect only the perception on government regulation when it comes to briefings on workplace safety violations. Even if the rewording logic can be updated dynamically, it takes time for a MIM attacker to create “the cover story,” e.g. decide what valence word/phrase pairs to use and when to time the particular rewording. This is not a trivial part of the attack. Neither is the execution, given that the attacker needs to get the targeted user to download the Malexa skill in the first place (or use a squatting/command hijacking attack).

Malexa was also configured to deliver the OSHA news in predefined order, instead of randomly delivering the news to each participant. To our knowledge, we didn’t observe any “question order bias”, but nonetheless we acknowledge this as a limitation to the study. We chose to work with OSHA news releases and measured the perceptions only on three arbitrary selected news headlines. If Malexa worked with, say breaking news headlines or more/less number of headlines, the results might be different than what we observed in our study. This is something we are highly interested in and we are preparing a next round of a Malexa study to comparatively assess its MIM effectiveness based on various types (amount) of stories it voices back to users, e.g. politics, sport, local news, foreign affairs, and entertainment. We are also interested to see how Malexa can affect perception of social media content.

Malexa, in the presented form, might have a problem with scaling and remaining undetected by a wide intelligent voice assistant user base. Users might hear the headlines from Alexa but their primary interface for online information is usually a computer, web browser, or an app. For example, a targeted user can hear what Malexa has to say but read a news article covering the same incident and notice that something is wrong. We took a precaution to make sure that Malexa changes the user/s perception but doesn’t distort the main facts in the news headlines we used in our study. Nonetheless, it is sufficient for a targeted user to simply deem that the third-party skill “acts weird” or that “something phishy” is going on and remove it from their device. Caution is warranted when interpreting the results of our study. We used a convenience sample of intelligent voice assistant users skewed towards a younger, urban, liberal leaning population and future studies might investigate and yield different results based on these demographic factors.

6.3. What Malexa could do in the future
Despite the obvious limitations, the findings of this study provide reasonable ground to discuss how Malexa could be weaponized in the future. We already mentioned the potential of Malexa becoming a covert “influencer,” a cyberoperations vector interesting for actors wishing to interfere with major events like elections (Bradshaw and Howard, 2017). So far, actors resorted to trolling campaigns targeting specific populations on controversial topics discussed via social media (DiResta et al., 2018). However, this requires generating a lot of fabricated content in order to be successful (Paavola et al., 2016). Malexa requires no such thing because it operates on legitimate content. Malexa, in a much more targeted fashion, can determine what a target will hear back (or not, Malexa can be configured to even suppress certain news headlines if they are deemed unfavorable to the objective of the misperception campaign). In our descriptive MIM attack example, Malexa might omit the part of the headline about Senator Sanders’s future presidential campaign plans and only report the part about his heart attack.

In the latest version, Amazon allows Alexa to express emotions with a different tone, for now only “excited” and “disappointed,” and with high, medium, or low volumes (Gao Catherine, 2019). It is possible for the MIM attacker to use these options to deliver news and modify the tone/volume depending on the phrases delivered. Considering that the rewording in our study was intended to make the government response sound more lenient than it actually is, Malexa can deliver these in a low and “dissapointed” voice, for example. This certainly can factor into avoiding any suspicion, having Malexa reinforce the misperceptions by seemingly expressing “her” emotions about them (certainly, this can raise suspicions if the target user doesn’t share the same sentiment with Malexa). In our study, the communication was based on two-step interaction between the user and (M)alexa, that is, participants asked about the news and (M)alexa responded back. In the future, the MIM attack can be implemented for interactions with multiple steps where Malexa chooses the replacement words/phrases based on the real-time analysis of the user input. Studies have have shown that the analysis of the user input should be taken into account in the generation of voice output for intelligent voice assistants (Stier et al., 2020). Based on this, Malexa can contextually adapt the emotions and tones depending on the real-time input from the user during a regular interaction.

Malexa could automatically derive the replacement words/phrases based on the online content a target user usually is interested in (e.g. social media messages, forums, emails, particular news outlets, etc.). We have extended the MIM attack model to use a Markov chain that operates on a candidate text order to suggest a contextually-relevant valence words/phrases as replacements for a targeted online content (Sharevski and Jachim, 2020). Another alternative for an automated valence word/phrase replacement are the methods used to craft adversarial natural language processing samples that maintain the semantic similarities in order to avoid detection (Ren, Deng, He, Che, 2019, Zhang, Sheng, Alhazmi, Li, 2020).

Although we haven’t found any statistically significant relationship between the participants’ political ideology, gender identity, and age, the MIM attacker could nonetheless “profile” the target user in order to further contextualize the misperception. A Linguistic Inquiry and Word Count (LIWC) analysis could be used to determine the target’s emotional, cognitive, and structural components of their online linguistic style (Sharevski, Treebridge, Jachim, Li, Babin, 2020, Tausczik, Pennebaker, 2010). Malexa can use this information to select replacement words that align with this “profile” of the target while in the same time remaining. For example, Malexa can substitute “resentful” with “hate-filled” for users whose online correspondence scores high on negative emotions (assuming the MIM attacker is able to scrape target’s online text) and create a misperception about the intensity of negative emotions used in a given news article.

The best advantage of Malexa is, perhaps, the fact that we openly welcome “her” in our homes (Kiseleva et al., 2016). Most of the discussions on intelligent voice assistants revolve around what they “listen to” but not what they “say.” Facebook trolling posts could be missed, omitted, detected, or ignored because they are visually inspected. It is harder to do so with Malexa because of the voice interface and the interpersonal nature of interaction (Purington et al., 2017). Who will think that Malexa, sitting quietly on the nightstand and ordering our usual groceries from Whole Foods, might be manipulating us behind the scenes  (Laubheimer and Budiu, 2018)?

6.4. How to counter Malexa
Malexa relies on an attacker customizing a blueprint template and registering a seemingly legitimate skill. The malicious skill development is relatively easy and anyone with a minimal technical knowledge and intention to induce misperception can develop a Malexa for any types of news or online content. The first line of defense, therefore, is at the Amazon Skills Store where the certification process should look for any news delivery logic that dynamically rearranges words outside of the content pulled from an external RSS feed or URL. It should potentially require the skill to work with RSS feeds or URLs that allow for content integrity verification by the user on the Alexa phone application. In this direction, a promising solution is the SkillExplorer proposed in Guo et al. (2020) which compares the behaviors of the Amazon skills, especially the privacy information they request. To “explore” malware-induced misperceptions, SkillExplorer could be customized in future to look for the tools that manupulate the content of the RSS feed or URL. One thing to have in mind is the possibility of a MIM attacker trying to sneak Malexa as an “accessibility (a11y) skill,” claiming that the rewording is done to create an assistive natural language skill that for example helps non-native English speakers (Jang et al., 2014). It might be harder to bar a skill from the Alexa Skills Store on these grounds or flag it by the SkillExplorer, therefore, the certification/evaluation process must request all the use cases for these word manipulations upfront to ensure no MIM is hidden in the content delivery logic of the assistive application.

Though this line of defense has also been proposed in other works dealing with malicious skills (Security Research Labs, 2019), it only partially covers the possible countermeasures that could be implemented on the system side. Studies have shown that it is trivial for any policy-violating skills to be easily certified and placed on the Amazon Skills Store (Zhang et al., 2019a). Outside of the Amazon Skills Store, the procedure for invoking skills is inherently insecure, as described in Section 2.3. Designed to cater to the usability of Alexa as a voice assistant, users can simply activate skills by calling their invocation names (automatic discovery). Unlike the process of installing third-party applications where users are explicitly asked about what permissions they allow, Alexa users are not prompted with any verbal prompt or a skill installation prompt on their Alexa smartphone app about what the skill, where the content comes from, and whether the users explicitly allows the transparent invocation of the skill. This could offer an additional level of protection, but still be limited, given that users are notorious in ignoring prompts with applications permissions (Felt et al., 2012) or being unable to distinguish between native (verified) and third-party (unverified) skills Zhang et al., 2019a.

A step further in countering Malexa is to implement possible audio warnings about third-party skills with unverified source of the content (RSS feeds or URLs) and/or unverified publishers similar to web-browser security warnings of visiting a suspicious web page or downloading unverified content (Akhawe, Felt, 2013, Reeder, Felt, Consolvo, Malkin, Thompson, Egelman, 2018). Fig. 8 shows a potential adaptation of the Alexa ecosystem that includes a step where Alexa, instead of transparently invoking a skill from the store, attempts to verify the publisher and the source content. If the verification fails, Alexa informs the user and offers the user to invoke a verified skill instead. In the best case scenario, the user will comply and avoid receiving news from a malicious skill. Users can still say “no” and use the malicious skill, An Alternative step would be for Alexa to provide explanation of the threat of inducing misperceptions and disseminating disinformation and fake news before it delivers the unverified content. Certainly, the proposed adaptation is far from perfect and entails extensive future /usable security research to determine the optimal way of delivering voice-based security warnings, especially with the option for Alexa to express emotions (e.g. “dissapointed” and low volume in the first prompt, “excited” and medium volume in the second prompt to search for an alternative news skill, and “dissapointed” and high in the third prompt explaining the threats of unverified news) (Gao Catherine, 2019). This is a one of the future research steps, and we plan to broadly explore the domain of voice-based security warnings.

Fig. 8
Download : Download high-res image (823KB)
Download : Download full-size image
Fig. 8. Alexa security warnings.

It’s worth noting that the MIM attacker has the alternative to share the skill with other users via direct link, bypassing the Alexa Skills Store. In this case, we recommend that the user verifies the publisher (e.g. “CIA” and not Laynr as with the “CIA Factbook” skill) and more importantly, the delivery logic. If a skill claims it delivers news briefings from say, The New York Times, it is fairly easy for a user to run a quick test and compare the headlines with the online version “verbatim.” However, users should be aware that even legitimate news sources sometimes use different headlines depending on the interface (e.g. a phone app or a web browser) or the edition (Mark, 2016). This is done because interfaces might require different limits on the number of characters in the headline. Or, to increase readership for an initially poor performing headlines by rewording them with clear, powerful words and a conversational tone (interestingly, a MIM attacker can use similar claims to justify that the rewording makes the news “sound better when spoken back”). This may affect the point of using a news skill and can lead to false positives, but carefully looking for deception cues might help users spot and remove a malicious skill (Nicholson et al., 2017).

The ethical implications of Malexa are the same as those related to publishing any vulnerability: the value of publicly sharing a proof-of-concept malicious skill with knowledgeable researchers outweighs the opportunity that potential attackers may benefit from the publication. If this paper introduces a viable attack in the intelligent voice assistant ecosystem due to the simplistic nature of this attack, we believe adversaries will develop and deploy similar attacks, if they have not already. The study itself tests the plausibility of Malexa with a proof-of-concept, locally-executed skill extension (not publicly available on the Alexa Skills Store). In the context of a real-life Malexa, a responsible disclosure would entail contacting Amazon and working with them through the details of the MIM attack.

7. Conclusion
In this work, we introduced Malexa, Alexa’s malicious twin. Malexa’s goal is to covertly manipulate the user’s perception on the content delivered by an intelligent voice assistant. We showed how an attacker, implementing the malware-induced misperception attack, could turn Alexa into Malexa without the user’s knowledge. We then tested Malexa in a study with 220 participants asking for their perception on the government’s regulation performance based on OSHA news briefings. Our findings show that Malexa is able to make the government look much less friendly to the working people and more in favor of big businesses. Malexa showed that “she” is able to manipulate perceptions regardless of one’s political ideology or how frequently one uses a voice assistant. Unlike visually inspected text of Facebook or Twitter posts, text-to-speech voice provides little cues for hearing inspection concerning whether trolling content is used or not. People readily welcome and undoubtedly trust intelligent voice assistants and let them sit in their homes. While people are concerned about what Alexa “listens to,” little do they pay attention to what is actually “spoken back to” them. If Facebook and Twitter are gearing up to combat trolling in the next election cycle, Malexa, as demonstrated in our study, provides a new avenue for delivering trolling content directly into our homes. We hope our results inform the security community about the implications of having an alternative vector for any micro-targeted influence.