We study the problem of dynamic symmetric searchable encryption.
In that setting, it is crucial to minimize the information revealed to
the server as a result of update operations (insertions and deletions).
Two relevant privacy properties have been defined in that context:
forward and backward privacy. The first makes it hard for the server
to link an update operation with previous queries and has been
extensively studied in the literature. The second limits what the
server can learn about entries that were deleted from the database,
from queries that happen after the deletion. Backward privacy was
formally studied only recently (Bost et al., CCS 2017) in a work that
introduced a formal definition with three variable types of leakage
(Type-I to Type-III ordered from most to least secure), as well as the
only existing schemes that satisfy this property. In this work, we
introduce three novel constructions that improve previous results in
multiple ways. The first scheme achieves Type-II backward privacy
and our experimental evaluation shows it has 145 − 253× faster
search computation times than previous constructions with the
same leakage. Surprisingly, it is faster even than schemes with
Type-III leakage which makes it the most efficient implementation
of a forward and backward private scheme so far. The second one
has search time that is asymptotically within a polylogarithmic
multiplicative factor of the theoretical optimal (i.e., the result size
of a search), and it achieves the strongest level of backward privacy
(Type-I). All previous Type-I constructions require time that is
at least linear in the total number of updates for the requested
keywords, even the (arbitrarily many) previously deleted ones. Our
final scheme improves upon the second one by reducing the number
of roundtrips for a search at the cost of extra leakage (Type-III).
KEYWORDS
Searchable Encryption; Forward/Backward Privacy; Cloud Databases

1 INTRODUCTION
Storing an encrypted database at a remote server, while retaining
the ability to access and dynamically maintaining it, is fundamental
for modern computing. Consider for example a client that owns
a database, outsources it to an untrusted server and subsequently
issues search queries of the form “retrieve all documents that contain
keyword w”. Ideally, the server should not only learn nothing about
the content of the documents (which can be achieved by traditional
encryption schemes), but also no additional meta-information, e.g.,
how many times w was searched for, frequency of keywords in the
database, etc. This “perfect” level of privacy is theoretically achievable by strong encryption techniques such as fully-homomorphic
encryption [21], whose large performance overhead however limits
adoption in practice.
Symmetric searchable encryption (SSE), originally proposed by
Song et al. [41], provides a way for accessing this encrypted database efficiently by slightly relaxing the privacy requirements. Concretely, SSE reveals some information to the server during query
execution, known as leakage. This leakage typically includes the
search pattern that reveals which search queries refer to the same
keyword w as well as the access pattern that reveals which files are
returned for a query.
Dynamic symmetric searchable encryption and its leakage.
The first works on SSE focused on static datasets and it was not
until 2009 when SSE schemes that support updates on the database
in a principled manner (also known as dynamic SSE schemes) were
first introduced [13, 27, 28]. Dynamic SSE, however, introduces
additional privacy concerns due to the added functionality. For
example, (a) adding a file f to the database might reveal that f
contains keywords that were searched before, or (b) searching for a
keyword w might reveal which files from the past (that have been
removed from the database now) contained w.
Forward and backward privacy. Schemes that avoid the leakage
associated with case (a) above are called forward private and were
first introduced by Chang and Mitzenmacher [13] and subsequently
Session 5D: Encrypted Search & Computation 2 CCS’18, October 15-19, 2018, Toronto, ON, Canada 1038
Table 1: Comparison of existing forward and backward private dynamic SSE schemes. N is the total number of (document,
keyword) pairs, |W | is the number of distinct keywords, and |D| is total number of documents. For keyword w, aw is the total
number of updates, nw is the number of files currently containing w, and dw is the number of deleted entries for w. RT is the
number of roundtrips for search. BP represents the achieved backward privacy type.O˜ notation hides polylogarithmic factors.
Scheme Computation Communication Client Storage BP Search Update Search Update Search RT
Moneta [7] O˜(aw log N + log3 N) O˜(log2 N) O˜(aw log N + log3 N) O˜(log3 N) 3 O(1) I
Fides [7] O(aw ) O(1) O(aw ) O(1) 2 O(|W |loд|D|) II
Dianadel [7] O(aw ) O(log aw ) O(nw + dw log aw ) O(1) 2 O(|W |loд|D|) III
Janus [7] O(nwdw ) O(1) O(nw ) O(1) 1 O(|W |loд|D|) III
Mitra [Sec. 3] O(aw ) O(1) O(aw ) O(1) 2 O(|W |loд|D|) II
Orion [Sec. 4] O(nw log2 N) O(log2 N) O(nw log2 N) O(log2 N) O(log N) O(1) I
Horus [Sec. 4.3] O(nw logdw log N) O(log2 N) O(nw logdw log N) O(log2 N) O(logdw ) O(|W |loд|D|) III
refined in [5, 19, 20, 30, 42, 43]. Forward privacy has become an
essential property for dynamic SSE schemes, especially in light of
recent file-injection attacks [47] that become particularly effective
when the SSE scheme is not forward-private. Schemes that try to
limit the leakage from case (b) are called backward private and have
been studied far less in the literature. Other than an informal mention in [43] and the “folklore" construction from ORAM techniques,
no formal definition or construction that achieves this property
existed for a long time. Very recently, Bost et al. [7] introduced a
formal definition for backward privacy with three different types
of leakage ordered from most to least secure.
Type-I leakage (Backward Privacy with Insertion Pattern): Type-I
schemes reveal, during a search for w, the number and type of previous updates associated with w, the identifiers of files containing
w currently in the database, and when each such file was inserted.
Type-II leakage (Backward Privacy with Update Pattern): In addition
to the information contained in Type-I leakage, Type-II schemes
also reveal when all updates related to w took place.
Type-III leakage (Weak Backward Privacy): Finally, Type-III schemes
also reveal exactly which deletion update canceled which previous
addition (e.g., the deletion that took place during the tenth operation
canceled the addition from the fifth operation). Note that all three
types satisfy the basic property of hiding the actual identifiers of
files that contained w but were deleted prior to w’s search.
The schemes of Bost et al. [7]. Bost et al. [7] provided four
backward-private constructions that achieve different privacy/efficiency trade-offs. The first one is Fides which is a Type-II construction. At a high level, it uses two deployments of the forward private
SSE scheme of [5] to store update entries of the form (w,id, op)
where w is a keyword, id is a file identifier, and op = add/del. Additions are stored in the first deployment and deletions in the second
one. For searching, the user queries both deployments, retrieves all
entries, removes the deleted ones locally, and requests the files with
the remaining identifiers from the server. In that way, the server
cannot tell which identifiers were deleted.
Dianadel and Janus are Type-III schemes that rely on puncturable cryptographic primitives to achieve better results, by increasing the amount of information leaked. Dianadel uses a puncturable pseudorandom function [4, 8, 29] to achieve better concrete
performance than Fides. Janus uses puncturable encryption [22]
which allows search queries to be executed with a single round of
interaction. Finally, Bost et al, presented Moneta, the only existing
Type-I scheme so far. However, its construction is based on the
recent ORAM scheme of [20] which uses garbled circuits to avoid
interaction. This somewhat limits its potential for adoption in practice, due to the concrete communication overhead, and it serves
mostly as a theoretical result for the feasibility of Type-I schemes.
1.1 Our Results
In this work, we present three SSE schemes with forward and
backward privacy. Our schemes improve the results of [7] in several
ways. A comparison can be seen in Table 1.
Fast Type-II backward privacy. Our first scheme, Mitra (Section 3), offers backward-privacy Type-II. Asymptotically, it achieves
the same performance as Fides, however, due to the use of symmetric encryption our experimental evaluation indicates that it has
145 − 253× better computation time for searches and 86 − 232×
for updates. Surprisingly, Mitra has better overall performance
than Diana del and Janus which only achieve Type-III backward
privacy, which makes Mitra the most efficient existing forward
and backward private SSE. We believe the combination of its low
leakage level, practical performance, and simplicity of design, make
it a great candidate for adoption in practice.
Optimizing the search time. As can be seen in Table 1, all existing
schemes (including Mitra) impose search time of Ω(aw ) where
aw is the total number of updates related to w (clearly nwdw > aw
where nw is the number of documents containing w currently and
dw is the number of previous deletions for w). This in practice
can be very far from the optimal cost which is O(nw ). Inspired by
this, we explore whether backward-private SSE schemes that have
optimal (O(nw )) or quasi-optimal (O(nw · polylog(N))) search time
exist. Crucially, not even Moneta, the construction from [7] that
relies on ORAM achieves this property. Furthermore, even when
examining schemes that are only forward-private, the only known
quasi-optimal construction is from Stefanov et al. [43].
We answer the above question in the affirmative by providing
two schemes, Orion (Section 4) and Horus (Section 4.3), which
have quasi-optimal search time. Orion achieves the strongest level
of backward privacy, Type-I. Asymptotically, it requires O(log N)
rounds of interaction and the search process takes O(nw log2 N)
steps. However, these asymptotics hold even when no deletions
have occurred (i.e., nw = aw ) which motivated us to develop our
Session 5D: Encrypted Search & Computation 2 CCS’18, October 15-19, 2018, Toronto, ON, Canada 1039
last scheme Horus, that improves the efficiency of Orion achieving better search performance and reduced rounds of interaction.
The number of roundtrips for a search of w is only O(logdw ). In
particular, if no (or a constant number of) such deletions have taken
place Horus requires O(1) roundtrips to retrieve the file identifiers.
On the other hand, Horus is only backward private Type-III which,
however, may still be sufficient in many applications.
Section 5 contains our experimental evaluation and a comparison
with the performance of the constructions of [7].
Overview of techniques. Our first scheme uses an approach for
maintaining an encrypted index that has been extensively used
in the literature of dynamic SSE (e.g., [19, 31, 42]). In particular,
Mitra has similarities with the recent construction of Etemad
et al. [19] that is only forward-private. Triplets of the form keyword/document/operation (w,id, op) are stored encrypted in a map
dictionary, using pseudorandomly generated locations, based on
the counter of updates updcnt for w which is maintained locally.
Note that op = add/del, i.e., even deletions are “stored” in this
manner. Every update accesses locations that appear random, as far
as the server can tell. To retrieve the files for w, the client simply
regenerates the pseudorandom positions for the different counter
values 1, . . . ,updcnt for w. In this way, the server does not learn
the identifiers of deleted entries.
Orion again maintains a map data structure where each entry
(w,id) is looked up using as key the pair (w,updcnt ). Every time
a new entry (w,id) is inserted updcnt is incremented. When an
entry (w,id) is deleted, the entry corresponding to the maximum
updcnt value for w is “swapped” to the position corresponding
to updcnt (see Figure 2). In this manner, at all times, the correct
result for a search for w can be retrieved by looking up certain
positions: (w, 1), . . . , (w, nw ). Performing this swapping during updates requires being able to lookup the position (w,updcnt ) while
only knowing (w,id). This is the opposite direction than the one
offered by the map, therefore we need to use a second map that
supports this type of mapping. Orion achieves forward privacy
and backward privacy Type-I, by using two oblivious map (OMAP)
data structures [46] which hide the actual accessed memory locations from the server. Naively implemented, this approach would
require Ω(nw ) rounds of interaction for search, one for retrieving
each entry (w,i), in order to maintain privacy. However, a close
observation of the OMAP instantiation of Wang et al. [46] reveals
that it can support “batch” query execution with the same number
of roundtrips as for the case of a single one, i.e., O(log N).
Horus reduces the obligatory amount of interaction by replacing one of the oblivious maps of Orion (the one that is only used
during searches to retrieve results) with a non-recursive (one-level)
Path-ORAM structure [44]. Normally, this approach would require
O(N) storage at the client in order to maintain the position map
for the ORAM which invalidates outsourcing the database in the
first place. We replace the randomly generated ORAM positions
with ones generated with a pseudorandom function (PRF), thus the
client only needs to store the PRF key. In order to achieve forward
and backward privacy, we need to ensure that the same input is
never consumed by the PRF more than once throughout the execution of updates. This is done by introducing an additional access
counter for each updcnt that measures the number of times the
content of the location corresponding to (w,updcnt ) was edited by
an update (acccnt ). This solves the privacy issues, but it introduces
a new problem: The client needs to know the correct acccnt for each
updcnt = 1, . . . , nw during a search for w. We solve this issue by
having the client perform nw binary searches executed “in parallel”
in order to identify the nw correct acccnt values. We show that
with Horus the maximum value of acccnt is O(dw ) therefore the
number of necessary roundtrips is reduced to O(logdw ). However,
this distribution of previously accessed positions reveals (during
search operations) to the server how many times acccnt was incremented for different updcnt values, which in turn leaks information
about which previous addition was canceled by each deletion.
1.2 Related Work
SSE was first introduced by Song et al. [41] who proposed a lineartime search construction. Curtmola et al. [15] proposed the modern
security definition of SSE that also introduced a sublinear-time
construction. Chase and Kamara [14] introduced the broader notion
of structured encryption to model encryption schemes that allow
for controlled disclosure of some predicate of the data; searchable
encryption is a specific type of structured encryption.
The first schemes to explicitly support efficient updates in the
database were by Kamara et al. [28] and Kamara and Papamanthou [27], with the latter reducing the amount of leakage of the
first one. However, neither of them had forward privacy, which
was first introduced as a notion in [13]. Since then, efficient dynamic SSE schemes with forward privacy have been studied extensively and there are numerous works that proposed improved
constructions [5, 9, 19, 20, 23, 30, 31, 35, 43]. Stefanov et al. [43]
were the first to introduce backward privacy to capture leakage
related to deleted entries without, however, providing a definition
or a construction. The first (and only, to the best of our knowledge) work that focused on backward privacy and defined the
notion as well as offered backward private schemes was the recent
work of Bost et al [7]. Other work on SSE has focused on more
expressive queries [10, 14, 16, 25, 26, 32], multiuser settings [37, 38],
constructions that perform well for data on-disk [2, 11, 17, 33], and
combining SSE with ORAM [20, 35].
2 CRYPTOGRAPHIC BACKGROUND
We introduce here the necessary notations and definitions that will
be used in the paper. We denote by λ ∈ N a security parameter.
PPT stands for probabilistic polynomial-time. By v(λ) we denote
a negligible function in λ. In a two-party protocol P execution
between a client and a server, the notation P(x;y) means that x is
the client’s input and y is the server’s input.
Assume a collection of D documents with identifiersid1, . . . ,idD ,
each of which contains textual keywords from a given alphabet Λ.
We consider the database DB that consists of pairs of file identifiers
and keywords, such that pair (idi
,w) ∈ DB if and only if the file
with identifier idi contains keyword w. Let W denote the set of all
keywords that appear in DB, |W | the number of distinct keywords,
N the number of document/keyword pairs (i.e., N = |DB|), and
DB(w) denote the set of documents that contain keyword w.
Session 5D: Encrypted Search & Computation 2 CCS’18, October 15-19, 2018, Toronto, ON, Canada 1040
Pseudorandom functions. Let Gen(1
λ
) ∈ {0, 1}
λ be a key generation function, and G : {0, 1}
λ × {0, 1}
ℓ → {0, 1}
ℓ
′
be a pseudorandom function (PRF) family. GK (x) denotes G(K, x). G is a
secure PRF family if for all PPT adversaries Adv, | Pr[K ← Gen(1
λ
);
AdvGK (·)(1
λ
) = 1] − Pr[AdvR(·)(1
λ
) = 1]| ≤ v(λ), where R :
{0, 1}
ℓ → {0, 1}
ℓ
′
is a truly random function.
Searchable encryption. A dynamic symmetric searchable encryption scheme (SSE) Σ = (Setup, Search,Update) consists of algorithm
Setup, and protocols Search,Update between a client and a server:
• Setup(λ) is an algorithm that on input the security parameter
outputs (K, σ, EDB) where K is a secret key for the client, σ is the
client’s local state, and EDB is an (empty) encrypted database
that is sent to the server. In the following we sometimes use
the notation Setup(λ, N) to refer to a setup process that takes a
parameter N for the maximum number of entries.
• Search(K, q, σ; EDB) is a protocol for searching the database. In
this paper, we only consider search queries for a single keyword
i.e., q = w ∈ Λ
∗
. The client’s output is DB(w) (empty if w < W ).
The protocol may also modify K, σ and EDB.
• Update(K, op,in, σ; EDB) is a protocol for inserting an entry to
or removing an entry from the database. Operation op can be
add or del, input in consists of a file identifier id and a keyword
w. The protocol may modify K, σ and EDB.
In the above, we followed the definition of [5, 7] with minor
modifications. Given the above API, on input the data collection the
client can run Setup, followed by N calls to Update to “populate”
EDB. Other works (e.g., [19]) follow a different but functionally
equivalent approach that offers a single build operation for this.
Similarly, some existing works [19, 30] have Update take as input
an entire file for addition, or the file identifier for deletion and
the protocol adds/removes all the relevant keywords to/from the
database. Again, this is functionally equivalent as this process can be
decomposed to multiple calls of the above Update protocol. Finally,
we implicitly assume that after receiving the indexes DB(w), the
client performs an additional operation to retrieve the actual files;
we omit this step from when describing our constructions.
Informally, an SSE is correct if the returned result is correct for
every query (for a formal definition, we refer readers to [9]). The
confidentiality of an SSE scheme is parametrized by a leakage function L = (LS tp
, LSr ch
, LU pd t ) which captures the information
that is revealed to an adversarial server throughout the protocol
execution. LS tp corresponds to leakage during setup, LSr ch to
leakage during a search operation, and LU pd t to leakage during
updates. Informally, a secure SSE scheme with leakage L should
reveal nothing about the database DB other than this leakage.
This is formally captured by a standard real/ideal experiment
with two games RealSSE
, IdealSSE presented in Figure 6 in Appendix A, following the definition of [43].
Definition 2.1 ( [44]). An SSE scheme Σ is adaptively-secure
with respect to leakage function L, iff for any PPT adversary Adv
issuing polynomial number of queries q, there exists a stateful
PPT simulator Sim = (SimInit, SimSearch, SimUpdate) such that
| Pr[RealSSE
Adv(λ, q) = 1] − Pr[IdealSSE
Adv,Sim,L
(λ, q) = 1]| ≤ v(λ).
Forward and backward privacy. Forward and backward privacy
are two SSE properties that aim to control what information is
leaked by dynamic schemes in relation to updates that take place.
Informally, a scheme is forward private if it is not possible to relate
an update that takes place to previous operations, at the time during
which it takes place. This is particularly useful in practice, e.g., to
hide whether an addition is about a new keyword or a pre-existing
one (which may have been previously searched for).
Definition 2.2. An L − adaptively − secure SSE scheme that
supports addition/deletion of a single keyword is forward private iff the update leakage function LU pd t can be written as:
LU pd t (op,w,id) = L′U pd t (op,id) where L′
is a stateless function, op is insertion or deletion, and id is a file identifier.
Backward privacy aims to limit the information that the server
can learn when executing a search for a keyword w for which some
entries have previously been deleted. Ideally, the SSE scheme should
reveal nothing to the adversary about these previously deleted
entries, and at least not the file identifiers of the deleted entries [43].
A formal definition was given in [7] for three different types of
backward privacy with varying leakage patterns, from Type-I which
reveals the least information to Type-III which reveals the most.
Before we give the final definition, we provide some additional
functions that will be necessary, following the notation of [7].
Consider a list Q that has one entry for each query executed.
The entry for a search is of the form (u,w) where u is the query
timestamp and w is the searched keyword. That of an update is
(u, op, (w,id)) where op = add/del and id is the modified file. For a
keyword w, let TimeDB(w) be a function that returns the list of all
timestamp/file-identifier pairs of keyword w that have been added
to DB and have not been subsequently deleted.
TimeDB(w) = {(u,id) | (u, add, (w,id)) ∈ Q
and ∀u
′
, (u
′
, del, (w,id)) < Q}
Updates(w) is a function that returns the timestamp of all insertion and deletion operations for w in Q. Formally, Updates(w) =
{u|(u, add, (w,id)) ∈ Q or (u, del, (w,id)) ∈ Q}. Finally, DelHist(w)
is a function that returns the history of deleted entries by giving all
(insertion timestamp, deletion timestamp) pairs to the adversary.
Most importantly, it reveals explicitly which deletion corresponds
to which addition.
DelHist(w) = {(u
add
,u
del) | ∃ id : (u
add
, add, (w,id)) ∈ Q
and (u
del
, del, (w,id)) ∈ Q}
As can be seen, the above functions’ leakage is progressively increasing. We are now ready to formally define backward privacy
with different types of leakage.
Definition 2.3 ([7]). An L-adaptively-secure SSE scheme has
backward privacy:
Type-I (BP-I): iff LU pd t (op,w,id) = L
′
(op), and
LSr ch(w) = L
′′(TimeDB(w), aw ).
Type-II (BP-II): iff LU pd t (op,w,id) = L
′
(op,w), and
LSr ch(w) = L
′′(TimeDB(w), Updates(w)).
Type-III (BP-III): iff LU pd t (op,w,id) = L
′
(op,w), and
LSr ch(w) = L
′′(TimeDB(w), DelHist(w)).
where L
′
and L
′′ are stateless functions.
Note that the above definition assumes schemes leak the documents
Session 5D: Encrypted Search & Computation 2 CCS’18, October 15-19, 2018, Toronto, ON, Canada 1041
that currently contain w in order to account for the leakage from
actually retrieving the files. Namely, TimeDB(w) function explicitly
reveals the indexes of returned documents.
Oblivious Maps. A data structure D is a collection of data supporting certain types of operations such as insertions, deletions, or
lookups. Each type of operation receives corresponding arguments
(e.g., the key of the value to lookup). An oblivious data structure is
a privacy-preserving version of D that aims to hide the type and
content of a sequence of operations performed on the data structure.
Intuitively, for any two possible sequences of k operations, their
resulting access patterns (i.e., the sequence of memory addresses
accessed will executing the operations) must be indistinguishable.
(See [46] for a formal definition).
Our Orion and Horus constructions use as a building block an
oblivious map (OMAP) that is a key/value oblivious data structure,
implemented with an AVL tree as per the instantiation of [46]. To
make the map oblivious, it uses a non-recursive Path-ORAM [44]
structure that stores the set of nodes. (We provide a more detailed
description of Path-ORAM in Appendix B for the non-expert readers.) Each tree node contains the following information node =
((id, data), pos,childrenPos) where data is a value for the map, id
is a key for the map, pos indicates the node’s leaf number in PathORAM, and childrenPos is an array which stores the Path-ORAM
leaf numbers for the node’s children.
OMAP offers three protocols Setup, Find, and Insert for initializing the structure, retrieving the value for a given key, and inserting
a key/value pair. We describe them in detail in Appendix C. At a
high level, Setup initializes a Path-ORAM T and stores an empty
node for the root of the AVL tree at a randomly chosen position
rootID. Subsequent Find, Insert calls, traverse the AVL tree from
the root in order to find/insert a matching node. Each node traversal requires a separate ORAM access. The ORAM position for a
child node is stored at the parent. Finally, all accessed nodes are
re-encrypted and mapped to fresh random positions before being
stored again at T . For insertions, an AVL tree rebalancing process
is executed, again via ORAM read/write accesses.
From the analysis of the AVL tree, each access retrieves O(log N)
nodes for a tree of N nodes, each of which requires retrieving
a log N path from T , thus the overall performance is O(log2 N).
Unlike standard non-recursive Path-ORAM, the client performing
the accesses only needs to remember the leaf position of the root of
the AVL tree (the ORAM stash of sizeO(log2 N) can be downloaded
with every access without affecting the asymptotic behavior) i.e.,
he needs O(1) storage. An example operation of this OMAP is
presented in Figure 1. In order to access node 7, first the root node
6 is retrieved (since the client remembers its ORAM position). Then
by comparison with the children values 4, 9 the client decides to
retrieve the latter, whose position he read from the root node. Then
he repeats this process and picks the left child of node 9. Finally, the
content of the accessed nodes 6, 9, 7 is updated from bottom-up with
new ORAM positions randomly chosen, the nodes are re-encrypted,
permuted and stored in the ORAM.
Since the searched key may be found at any layer, an operation
can terminate early which may reveal information to the server. To
avoid this, [46] pads the number of ORAM accesses with “dummy”
9
6
4 9
2 7
6
7 12 …..
ORAM Tree AVL Tree
4 12
2
ID: …
Key: 2
Pos: …
Children IDs: …
Children Pos: …
Figure 1: Sample AVL Tree and its Path-ORAM [46].
operations that do not alter the tree structure but are indistinguishable from the server’s point of view.
3 MITRA: A SIMPLE FORWARD AND
BACKWARD PRIVATE SCHEME
In this section, we propose Mitra, our first backward and forward
private scheme. Mitra follows a simple approach for storing encrypted records in a manner that leaks nothing to the server during
updates (insertions and deletions), and only the time at which updates took place during searches. The construction uses a key-value
dictionary that stores encrypted values of the form (id, op) where
op is insertion or deletion, and id is the identifier of a specific file
related to this operation. The keys (locations at which values are
stored in the dictionary) are generated via a pseudorandom function in a way that guarantees the client can efficiently generate the
set of all locations related to the specific keyword w for a given
search operation. Compared to [19] which introduced a similar
construction that is only forward private, the main difference of
Mitra that makes it backward private is that, instead of sending
to the server the key that allows him to generate the location and
decrypt the entries for w, we send him the locations directly and
the decryption happens locally at the client.
Setup. The setup algorithm (Algorithm 1) generates a secret key K
on input the security parameter λ. The client initiates two empty
maps (DictW,FileCnt). The first is sent to the server in order to
store encrypted entries whereas FileCnt is stored locally.
Update. In the update procedure (Algorithm 2) the client receives
keyword w, file identifier id, and corresponding operation op =
add/del. For example input (add,w,id) means “add an entry for
keywordw in file id”. The client also has access to the keyK and local
state FileCnt that stores for each distinct keywordw a counter that
denotes how many updates have taken place in relation to w. First,
the client checks whether FileCnt[w] has been initialized or not.
In the latter case he sets the counter value of w to 0. In both cases,
he increments the counter by 1 (lines 1-4). Next, the client runs
the PRF G with key K twice, and computes GK (w, FileCnt[w]||0)
and GK (w, FileCnt[w]||1). The first PRF output is used as the key
addr in which the encrypted value for (id||op) will be stored at the
server, whereas the second PRF output is XORed with the entry
(id||op) and the result becomes the encrypted value val which will
be stored by the server (lines 5-6). The pair (addr,val) is sent to
the server who stores them as DictW[addr] = val (line 7).
Search. Finally, we describe the search process (Algorithm 3). While
searching for all files containing keyword w, the client first looks
up the counter FileCnt[w] which is the total number of updates
Session 5D: Encrypted Search & Computation 2 CCS’18, October 15-19, 2018, Toronto, ON, Canada 1042
Algorithm 1 Mitra Setup(λ)
1: K ← Gen(1
λ
)
2: FileCnt,DictW← empty map
3: σ ← FileCnt
4: EDB ← DictW
5: Send EDB to the server
Algorithm 2 Mitra Update(K, op, (w,id), σ; EDB)
Client:
1: if FileCnt[w] is NULL then
2: FileCnt[w] = 0
3: end if
4: FileCnt[w]++
5: addr = GK (w, FileCnt[w]||0)
6: val = (id||op) ⊕ GK (w, FileCnt[w]||1)
7: Send (addr,val) to the server
Server:
8: Set DictW[addr] = val
Algorithm 3 Mitra Search(K,w, σ; EDB)
Client:
1: TList = { }
2: for i = 1 to FileCnt[w] do
3: Ti = GK (w,i||0)
4: TList = TList ∪ {Ti }
5: end for
6: Send TList to the server
Server:
7: Fw = {}
8: for i = 1 to TList.size do
9: Fw = Fw ∪ DictW[TList[i]]
10: end for
11: Send Fw to the client
Client:
12: Rw = {}
13: for i = 1 to Fw .size do
14: (id||op) = Fw [i] ⊕ GK (w,i||1)
15: Rw = Rw ∪ (id||op)
16: end for
17: Remove ids that have been deleted from Rw
18: return Rw
related to w. He then generates a list TList of all the locations
at which the corresponding entries are stored in DictW at the
server. This is done by evaluating the PRF G on input GK (w,i||0)
for i = 1, . . . , FileCnt[w]. Note that these are the same locations
that were computed during the previous updates for w, since G is
a deterministic function. The list TList of locations is then sent to
the server (lines 1-6). The server retrieves from DictW the values
of all keys from TList and sends them back to the client (lines 7-11).
Upon receiving these encrypted values, the client decrypts them by
computing the PRF outputs GK (w,i||1) for i = 1, . . . , FileCnt[w]
and XORing the i-th of them with the i-th encrypted value.
Security analysis. Our scheme achieves forward privacy and backward privacy Type-II. Forward privacy follows immediately since
the two values(addr,val) that the server observes during an update
are indistinguishable from random, due to the pseudorandomness
of G and the fact that during each update a different input is consumed by the PRF. Indeed, the server does not even learn the type
of operation that is executed (addition/deletion), i.e., the update
leakage is empty. For backward privacy, note that during a search
for w the server will receive a number of PRF evaluations which
he has seen previously during updates. This immediately reveals
when each update operation for w took place. Beyond this, nothing
else is revealed to the server; in particular the server does not learn
which deletion cancels which addition. By the backward privacy
definitions of Section 2, this leakage corresponds to BP-II.
We are now ready to state the following theorem regarding the
security of Mitra (full proof is provided in Appendix D).
Theorem 3.1. AssumingG is a secure PRF, Mitra is an adaptivelysecure SSE scheme with LU pd t (op,w, id) = ⊥ and LSr ch(w) =
(TimeDB(w), Updates(w)).
Efficiency of Mitra. The asymptotic performance of updates is
clearly O(1) for both client and server as they entail a constant
number of operations. The same is true for the communication
size as a single pair of values is transmitted. For search operations,
Mitra requires 2 · FileCnt[w] PRF evaluations and FileCnt[w]
XOR operations from the client, and FileCnt[w] look-ups from the
server. Recall that FileCnt[w] counts the total number of updates
for w which, using our notation from Section 2, is denoted by
aw . Therefore, the overall asymptotic complexity of the search
operation is O(aw ) and the same is true for the communication (as
the size of the communicated lists is aw ). The storage at the server
after N updates have taken place is O(N) since one entry is added
to DictW for each of them. The permanent storage at the client is
O(|W |loд|D|), where |W | is the total number of keywords and |D|
is the total number of documents, since one counter is stored for
each keyword. Finally, Mitra requires a single roundtrip to retrieve
the file identifiers for w. If the actual files need to be retrieved, this
would take one more round of interaction.
In terms of concrete performance, Mitra is extremely fast both
for updates and searches. As we experimentally demonstrate in
Section 5, it significantly outperforms Fides from [7] that achieves
the same level of leakage (BP-II), both in terms of speed and communication bandwidth. In fact it is comparable, and often more
performant than schemes that only achieve BP-III. Note that Mitra
may be especially attractive for the server as no cryptographic operations take place there. This not only improves server performance
(which may be very important in practice, e.g., in the case of a cloud
server where resources are shared across tenants) but also makes
the deployment of Mitra very easy.
Removing Deleted Entries. With Mitra the size of EDB grows
with every update, including deletions. This is a common approach
for building dynamic SSE schemes and has been extensively used
in the literature as an easy method for handling deletions, e.g., [5,
7, 9, 19, 43]. If one wants to avoid this, one technique adopted by
previous schemes is a periodic “clean-up” operation, executed during searches. Mitra can also be modified in a similar way. This
is done by having the client remove deleted entries after a search,
re-encrypting the remaining ones, and sending them back to the
Session 5D: Encrypted Search & Computation 2 CCS’18, October 15-19, 2018, Toronto, ON, Canada 1043
server. Using the encryption of Mitra, which is deterministic, this
would produce the same ciphertexts, violating privacy. To avoid this,
we maintain an extra counter map SrcCnt that is incremented after
every search and is also given as input to the PRF. We call the resulting scheme Mitra∗
. Asymptotically it has the same performance
as Mitra and the same backward privacy type. For completeness,
its construction is described and proven secure in the extended
version of our paper.
4 ORION: BACKWARD AND FORWARD
PRIVATE SSE WITH QUASI-OPTIMAL
SEARCH TIME
We now present Orion and Horus, two SSE schemes that are
backward-private and whose search complexity is only proportional to the number of files nw containing the searched keyword
w, currently in the database. The first one achieves very strong
backward privacy Type-I, i.e., it only reveals the number and type
of updates related to a specific keyword. The second one achieves
backward-privacy Type-III but it has reduced interaction, computation, and communication during search. We first describe a “strawman” solution to highlight the difficulties in achieving schemes with
(quasi-)optimal search time and non-trivial interaction. Consider
the following definition.
Definition 4.1. We say that a dynamic symmetric searchable encryption scheme Σ has optimal (resp. quasi-optimal) search time,
if the asymptotic complexity of Search is O(nw ) (resp. O(nw · polylog(N))). We say that Σ has non-trivial interaction, if it requires
o(nw ) rounds of interaction during Search (else we say Σ has trivial
interaction).
We note here that none of the existing backward and forward
private schemes achieves this property, not even the Moneta construction from [7] which utilizes ORAM to achieve strong backward
privacy, as it has a search time of O˜(aw log N + log3 N). Even if
we restrict our attention to schemes that are only forward private,
the only known construction with quasi-optimal search time is the
work of Stefanov et al. [43] that achieves O(nw log3 N) via the use
of an elaborate multi-layered data structure.
4.1 A Warm-up Solution
Inspired by the dictionary data structure of Mitra, consider a similar scheme where the location of each entry in the dictionary is
now computed based on (w,id) (whereas in Mitra, it was computed from (w, FileCnt[w])). The value of this entry will then be
a “pointer” to the previous and next file identifiers related to w.
The first time a file id for w is inserted, the previous and next file
identifier values are null, and the client stores locally id as the latest
file identifier for w. Then, when another file id′
that contains w is
inserted, the client adds to the dictionary a corresponding entry
with id as the previous identifier and null as the next identifier. He
also replaces id with id′
in his local storage. Finally, he updates the
previous entry for id, setting its next identifier to id′
.
This allows the client to traverse the dictionary like a list. During
search, he needs to remember only the latest file identifier forw. He
accesses the corresponding dictionary entry, retrieves the previous
identifier, and repeats the process until the previous identifier is
null. Deletions can be handled in the standard manner for doublylinked lists. The client first looks up the dictionary position for
(w,id) (assuming he wants to remove the entry for keyword w in
file id), retrieves the previous and next file identifiers id′
,id′′, and
then looks up the dictionary for (w,id′
), (w,id′′). He then sets the
next identifier at the entry for id′
to id′′ and the previous entry for
id′′ to id′
, “eliminating” the in-between entry for id.
Using an oblivious map. As described, the above approach reveals the accessed locations during updates and searches. E.g., if
an entry is added for w, the location of the previous latest entry
for w is revealed, trivially violating forward privacy. Locations accessed during a search can also be related to previously accessed
ones during updates, e.g., leaking in this manner information related to deleted entries, which violates backward privacy. To avoid
this leakage, the entire dictionary data structure can be instantiated with an oblivious map as defined in Section 2. In this manner,
the sequence of locations accessed throughout the protocol is indistinguishable from random ones. Assuming the oblivious map
implementation of [46], the resulting search time is O(nw log2 N).
On the other hand, these accesses need to take place sequentially,
as the next location is only revealed after accessing the previous
one. This requires the assistance of the client, in order to decrypt
each entry and compute the next location to be accessed, therefore
it would take at least nw rounds of interaction (O(nw log N) using
the scheme of [46]), resulting in a scheme with trivial interaction.
Challenge: Reducing interaction. Using the insights gained from
our straw-man solution, oblivious data structures can readily lead
to schemes with quasi-optimal search times. The remaining problem is how to reduce the interaction during search, as it may not
be reasonable to accept in practice a number of roundtrips that
grows with nw . Looking under the hood of the oblivious map construction of [46], it builds a map using an AVL tree. The nodes of
the tree are then stored in a non-recursive (one-level) Path-ORAM
construction (see Appendix B). Each tree node contains not only
its children’s values but also their positions in the Path-ORAM,
thus a map lookup is reduced to O(log N) ORAM accesses, one
for every level of the AVL tree. Garg et al. [20] recently showed
how to make Path-ORAM accesses non-interactive. With their approach, each AVL node would be fetched with a single interaction.
However, they rely on garbled circuits [3] to avoid interaction, and
each access consumes O(log N) garbled circuits that need to be
replaced by freshly encrypted ones by the client before the next
map access. Thus, even with that approach, performing the nw
map lookups necessary for our straw-man scheme would require
nw log N roundtrips, i.e., again resulting in a scheme with trivial
communication.
4.2 Orion Construction
The basic idea behind Orion is to spend a little more time rearranging the entries during a delete operation, in order to facilitate
subsequent searches. Similar to our straw-man solution, we will rely
on oblivious maps to hide the accessed locations. We will use two
of them: OMAPupd that is only accessed during update operations
and OMAPsr c that is accessed during both updates and searches.
High-level overview. The client maintains a counter updtcnt for
each keyword w that shows the number of files containing that
Session 5D: Encrypted Search & Computation 2 CCS’18, October 15-19, 2018, Toronto, ON, Canada 1044
….
1st 2nd UpdtCnt[w]-2 UpdtCnt[w]-1 UpdtCnt[w]
DB(w) IDs F1 F2 F7 F6
LastInd[w]
….
1st 2nd UpdtCnt[w]-2 UpdtCnt[w]-1 UpdtCnt[w]
F1 DB(w) IDs Before F3 F2 F7 F6
Deletion
LastInd[w]
During
Deletion
After
Deletion ….
1st 2nd UpdtCnt[w]-2 UpdtCnt[w]-1
F1 DB(w) IDs F6 F2 F7
UpdtCnt[w]
LastInd[w]
Figure 2: Sample deletion process for Orion.
Algorithm 4 Orion Setup(λ, N)
1: UpdtCnt, LastInd← empty map
2: (T,rootID) ← OMAPsr c .Setup(1
λ
, N)
3: (T
′
,rootID′
) ← OMAPupd .Setup(1
λ
, N)
4: σ ← (rootID,rootID′
,UpdtCnt,LastInd)
5: EDB ← (T,T
′
)
6: Send EDB to the server
keyword currently in the database (initialized to 0, incremented
after insertions, decremented after deletions). For an insertion, the
client stores inOMAPupd a mapping from (w,id) to the corresponding updtcnt and a mapping from (w,updtcnt ) to id in the search
OMAPsr c . That is, the first oblivious map is accessed by file identifier (necessary for deleting specific entries) whereas the second
one is accessed by updtcnt . What allows Orion to handle searches
more efficiently, is the way it handles deletions. For deleting the
entry (w,id), the client first performs a look-up in OMAPupd to
receive the corresponding update counter u. Then, he inserts to
OMAPsr c an entry for (w,u) with value id′
, where id′
is the identifier of the most recently inserted file for keyword w, i.e., the one
retrieved by looking up (w,updcnt ) from OMAPsr c . Simply put, he
swaps the deleted item with the right-most one in Figure 2. Finally,
he looks up (w,updcnt − 1) from OMAPsr c to retrieve the correct
id in preparation for the next update query. What is particularly
useful about this way of handling deletions is that it guarantees
that, any given time, the nw file identifiers corresponding to current
documents containing w can be retrieved by looking up the entries
(w, 1) . . . , (w, nw ).
The final missing piece to reduce the rounds of interaction is
the observation that the oblivious map construction of [46] can
handle “batch” queries without breaking the obliviousness property. A single query takes O(log2 N) time and O(log N) rounds of
interaction. Executing nw queries in batch take O(nw log2 N) time,
which yields quasi-optimal search time, and O(log N) rounds of
interaction, which gives Orion non-trivial communication.
The procedures of Orion are presented in detail in Algorithms 4-
6. Update and search are presented from the view of the client and
blue lines correspond to oblivious map queries. Each such line
corresponds to an interactive protocol, as described in Section 2.
Algorithm 5 Orion Update(K, op, (w,id), σ; EDB)
1: mapKey = (w,id)
2: (rootID′
,updtcnt ) ← OMAPupd .Find(mapKey,rootID′
)
3: if op = add then
4: if updtcnt = NULL or updtcnt = −1 then
5: if UpdtCnt[w] is NULL then
6: UpdtCnt[w] = 0
7: end if
8: UpdtCnt[w]++
9: data = ((w,id), UpdtCnt[w])
10: rootID′ ← OMAPupd .Insert(data,rootID′
)
11: data = ((w, UpdtCnt[w]),id)
12: rootID ← OMAPsr c .Insert(data,rootID)
13: LastInd[w] = id
14: end if
15: else if op = del then
16: if updtcnt > 0 then
17: data = (mapKey, −1)
18: rootID′ ← OMAPupd .Insert(data,rootID′
)
19: UpdtCnt[w]--
20: if UpdtCnt[w] > 0 then ◃ There are entries for w
21: if UpdtCnt[w] + 1 , updtcnt then
22: data = ((w, LastInd[w]),updtcnt )
23: rootID′ ← OMAPupd .Insert(data,rootID′
)
24: data = ((w,updtcnt ), LastInd[w])
25: rootID ← OMAPsr c .Insert(data,rootID)
26: end if
27: key = (w, UpdtCnt[w])
28: (rootID,lastID) ← OMAPsr c .Find(key,rootID)
29: LastInd[w] = lastID
30: else
31: LastInd[w] = 0
32: end if
33: end if
34: end if
35: Execute necessary dummy oblivious map accesses
Algorithm 6 Orion Search(K,w, σ; EDB)
1: R = {}
2: for i = 1 to UpdtCnt[w] do ◃ Execute in batch
3: (rootID,id) ← OMAPsr c .Find((w,i),rootID)
4: R = R ∪ {id}
5: end for
6: return R
Setup. During setup, the client initializes two empty maps UpdtCnt, LastInd. The first stores the last updtcnt value of each keyword (corresponding to the number of files currently in the database
containing the keyword) and the second stores the most recent file
identifier inserted for each keyword. The client also sets up two
oblivious maps. OMAPsr c maintains a mapping (w,updtcnt ) → id,
i.e., on input a keyword and an update counter it returns the corresponding file identifier. OMAPupd stores a mapping of (w,id) →
updtcnt , i.e., on input a keyword and a file identifier it outputs
the update counter of the corresponding entry (negative if the
entry has been previously deleted). The encrypted database EDB
Session 5D: Encrypted Search & Computation 2 CCS’18, October 15-19, 2018, Toronto, ON, Canada 1045
consists of the two oblivious maps, and the local state σ contains
UpdtCnt, LastInd. The secret key K is (implicitly) set to the secret
encryption keys of the oblivious maps.
Update. The client first makes an oblivious access to OMAPupd to
retrieve the update counter for the pair (w,id) corresponding to the
update. We then distinguish the case of addition and deletion. For
addition, the client first sets the new value of UpdtCnt[w] (lines
5-8) and then makes two oblivious accesses: (i) to OMAPupd to
insert a mapping from (w,id) to the UpdtCnt[w], (ii) to OMAPsr c
to insert a mapping from (w, UpdtCnt[w]) to id (lines 9-12). Finally,
he sets LastInd[w] to the newly added id.
For deletion operations, the client first updates the entry for
(w,id) in OMAPupd to indicate the entry has been deleted (lines
17,18). He then decreases UpdtCnt[w] by one to indicate that fewer
files containw now. If there are more files containingw (line 20), the
client has to update the mappings which is done by two oblivious
map accesses. The first (lines 22,23) is to OMAPupd in order to
indicate that the previous latest entry forw is moved to the position
that was vacated after the deletion. The second (lines 24,25) is
matching modification for OMAPsr c . Finally, the client fetches
from OMAPsr c the identifier of the current latest insertion for w
to update the entry LastInd[w] (in preparation for future updates).
In case there were no more entries for w, the client does not make
these accesses and simply sets LastInd[w] = 0.
Eventually, the client performs a number of dummy OMAP
accesses, if necessary, to hide data-dependent paths of the code.
That is, for additions he guarantees to always make two calls to
OMAPupd and one to OMAPsr c whereas for deletions the corresponding numbers are three and two.
Search. Due to the extra effort made during updates, the search
operation is very simple for the client. He first retrieves the number
of files currently containing w, that is, UpdtCnt[w] = nw . Then
he needs to issue nw oblivious accesses to OMAPupd . Importantly,
these accesses can be executed in batch, without having to wait for
the first in order to begin the next one. For example, with reference
to Figure 1, to search for nodes 2, 7 the client could fetch root node
6, compare the children values to deduce he needs both left 4, and
right child 9, retrieve them and repeat this process to get nodes 2, 7.
After this process terminates, the entire accessed subtree needs to
be remapped, re-encrypted, and stored in the ORAM.
To see how this is achieved, recall our description of the oblivious
map from [46] instantiated with an AVL tree T , from Section 2. All
accesses perform a tree traversal beginning from the root and entail
O(log N) node retrievals from T each of which is performed via a
non-recursive Path-ORAM structure. Assuming nw such accesses,
they all have the same first step: retrieving the root node ofT . Then,
the client can decrypt the root and retrieve the ORAM positions of
the two children. In case of multiple queries, it is likely that he needs
both of them. Therefore, for that level of the tree he makes two calls
to ORAM. In general, for the i-th layer of T , the client may need to
perform up to nw ORAM accesses. Since the ORAM is non-recursive,
these nw ORAM accesses can be performed in batch. Following this
process, since T has O(log N) levels, the total number of ORAM
accesses are O(nw log N) and they are performed in batch for each
layer. Thus, the total rounds of interaction is O(log N). After this
step, the client performs the remapping process for all O(nw log N)
retrieved nodes, and updates the ORAM structure, in the same way
as described in Section 2 for the case of a single access. To ensure no
leakage beyond the number nw , we take two additional measures.
For the i-th layer of T the client performs min{nw, 2
i
} accesses,
independently of how many nodes from the next layer he actually
needs (the remaining are dummy queries). Also, this process is
always extended to the maximal possible height of an AVL tree of
N elements (≈ 1.45 log N).
Efficiency of Orion. The asymptotic performance of Orion is
affected by that of the oblivious map of [46]. OMAP accesses take
O(log2 N) steps for a tree of N entries (here N is the total number
of updates executed in the database) and require O(log N) rounds.
For updates, since a constant number of such accesses is made,
the above quantities give us the complexity of Orion. Regarding
searches, recall that they entail nw accesses executed in parallel
and from our above analysis of the algorithm this can be done in
O(nw log2 N) time and withO(log N) interactions. The communication complexity of updates algorithms isO(log2 N) since each oblivious map access requires retrieving O(log N) Path-ORAM paths,
each of length log N. Likewise, the communication complexity of
the search algorithm is O(nw log2 N).
Client storage. For simplicity of presentation, we assumed that the
client stores UpdtCnt and LastInd locally. This implies a local
storage of O(|W | log |D|) since each entry can be at most log |D|
bits. This will typically be a few MB but in case this is unwanted,
we can exploit the already existing oblivious maps to delegate
this storage to the server. For example, we can store in OMAPupd
special mappings (w, 0) → (UpdtCnt[w], LastInd[w]). Whenever
the client needs to access these two maps, he can instead do it via an
oblivious access to OMAPupd . This would increase the number of
roundtrips entailed and communication size, but it does not affect
the asymptotic complexity of the scheme. Also, the oblivious map
requires a stash of O(log2 N) which is normally stored at the client.
Instead, we can outsource it to the server and download it with
every operation. With this two modifications the client storage of
Orion becomes O(1).
Server storage. On the server side, we stress that while the size of
the two trees T,T
′ grows with each update operation (after N of
them), their size becomes O(N), the size of the corresponding PathORAM structures must already be bound to an upper bound of total
updates. E.g., if the client wants the SSE to support up to 1 billion
updates, he initializes each Path-ORAM with a tree structure of 32
levels. This means that the server will have to commit this much
space initially. We do not believe this to be a serious limiting factor
of our scheme due to the relative low cost and high availability of
storage mediums. However, in settings where this may be an issue,
there is an alternative that can be achieved via a simple amortization
trick. The client initializes the Path-ORAM structures with trees of
1 layer. Afterwards, every 2
i updates for i = 1, 2, . . . he retrieves
both oblivious map from the server and sets them up from scratch
using Path-ORAM structures of i + 1 layers, i.e., with capacity 2
i+1
,
and fresh encryption keys. Since setting up an oblivious map with
capacity 2
i+1
takes O(2
i+1
) time and this re-build operation takes
place once every 2
i updates, the amortized cost per update is O(1).
Thus, the asymptotic behavior of Orion is not affected in this way;
the same asymptotics hold but in an amortized manner.
Session 5D: Encrypted Search & Computation 2 CCS’18, October 15-19, 2018, Toronto, ON, Canada 1046
Security analysis. The security analysis of Orion is rather straightforward due to the black-box use of oblivious maps. Throughout the
execution of the protocol, the server only observes a sequence of
Path-ORAM positions each chosen uniformly at random. Due to the
padding with dummy queries discussed above, during updates the
server sees a fixed number of such positions (depending on the type
of update) which implies forward-privacy. During searches, the
number of such locations that the server observes only reveals nw .
In particular, since after every update the accessed ORAM entries
are remapped to freshly chosen random locations, it is impossible
for the server to match the positions accessed during a search with
specific update operations. This implies that Orion has minimal
leakage, i.e., backward-privacy Type-I. We now state the following
(full proof is provided in Appendix E).
Theorem 4.2. Assuming OMAPsr c , OMAPupd are instantiated
with the secure oblivious map of [46], Orion is an adaptively-secure
SSE scheme with LU pd t (op,w, id) = op and LSr ch(w) = TimeDB(w).
4.3 Horus: More Efficient Searches
According to Definition 4.1, Orion has quasi-optimal search time
O(nw log2 N) and non-trivial interaction. One downside, however,
is that the search cost remains the same even when no deletions
take place, i.e., it is actually Θ(nw log2 N). Here, we present Horus,
a modified version of Orion, that achieves better asymptotic and
concrete search time. Searches with Horus takeO(nw log N logdw )
in the worst case. Moreover, if no deletions related to w have taken
place the search time becomes O(nw log N). We describe here Horus at a high-level (detailed description in Appendix F).
The main modification we make to Orion is that we replace
OMAPsr c with simple non-recursive Path-ORAM structure which
is again accessed byw and updcnt . In order to avoid having to store
a position map of size N at the client, we instead generate it using
a secure PRF, using a similar trick to [20]. However, this introduces
a security issue. Since the PRF is a deterministic function, it will
generate the same output when accessing the same (w,updcnt ).
This situation may arise multiple times through the protocol execution. E.g., assume the pairs (w, 5), (w, 11), (w, 4) have been inserted in that chronological order. In this case, (w, 5) corresponds
to updcnt = 1, (w, 11) to updcnt = 2, and (w, 4) to updcnt = 3. To
remove (w, 11) afterwards, the client first retrieves its updcnt = 2
from OMAPupd and then evaluates the PRF on (w, 2) to retrieve
that ORAM path. This last operation violates forward-privacy as it
trivially reveals this is the same ORAM location that was accessed
during the addition process for (w, 11).
Horus avoids this by introducing a counter acccnt associated
with each updcnt mapping to specific previous update location
and it counts how many times this location has been accessed.
In continuation of our previous example, the ORAM position for
the three initial additions (w, 5), (w, 11), (w, 4) for w would be computed as PRFK (w, 1, 1), PRFK (w, 2, 1), PRFK (w, 3, 1) where the first
counter is updcnt and it indicates these are the three first updates
for w, and the second counter is acccnt , indicating each of these
updcnt values is used for the first time. These updcnt, acccnt are
then stored in OMAPupd (whereas in Orion, OMAPupd stored
only updcnt ). When (w, 11) is to be deleted, the client first retrieves
the (updcnt, acccnt ) pair for (w, 11) from OMAPupd . Then, he performs the same “swapping” trick as in Orion, inserting the entry
corresponding to the previous latest update (w, 4) to the OMAP
position that corresponds to the updcnt of the removed entry but
with acccnt incremented by one, i.e., PRFK (w, 2, 2). This ensures
that, throughout the protocol execution, during update operations
the same input is never consumed by the PRF more than once,
which makes the distribution of ORAM positions observed by the
server during updates indistinguishable from random.
However, this introduces an interesting challenge during search
operations: How can the client know the correct acccnt value for all
the nw different values of updcnt counters of keyword w? One idea
would be to retrieve them from OMAPupd , by performing nw accesses in batch, same as before. Clearly, this would require Θ(log N)
roundtrips, which is what we are trying to improve. Instead, the
client engages into nw binary searches executed in batch, where
his goal is to identify the largest acccnt value that has been used
for each updcnt , since these are the locations where the correct file
identifiers for w are stored.
Concretely, to search for the nw file identifiers associated with
w, the client first evaluates PRFK (w, 1, 1), . . . , PRFK (w, nw, 1) and
sends the outputs indicating ORAM positions to the server, in batch.
This step corresponds to “guessing” that the acccnt associated with
each entry is equal to 1. After receiving the responses, he proceeds
for the next binary search step by setting acccnt = maxacc /2, where
maxacc is the maximum observed value of acccnt for keyword
w, as established through the sequence of previous updates. For
each updcnt , if the result is empty, (i.e., this acccnt has not been
previously used) he tries again by moving “left” in the binary search,
else by moving “right”. The process ends when the largest used
acccnt value has been found for all updcnt . The detailed procedure
is somewhat more complicated to account for remapping the entries
after each search, and it is described in Appendix F.
As we prove, the valuemaxacc isO(dw ), i.e., it grows proportionally with the number of deletions forw, thus this process terminates
after O(logdw ) rounds and entails O(nw logdw ) ORAM accesses.
Each such access returns a path of size O(log N) hence the search
time and communication complexity is O(nw logdw log N). In particular, note that if no deletions for w took place, acccnt = 1 for all
entries and the process terminates after a single round. Retrieved entries are then re-mapped using a search counter that is incremented
after every search, to hide future accesses.
On the downside, some of the ORAM positions accessed during
searches were the same ones previously accessed during updates
which introduces some leakage. In particular, simulating this transcript requires explicit knowledge of which addition is negated by
each deletion (as this affects the acccnt of a particular location),
which makes Horus backward-private Type-III.
5 EXPERIMENTAL EVALUATION
We report on the performance of our proposed schemes and compare them with existing ones from [7]. We implemented Mitra,
Orion, and Horus in C++ using the Crypto++ [1] and OpenSSL [36]
libraries for cryptographic operations. Specifically, we used AES128/256 as the PRF. For the schemes of [7], we used the code released
by the authors [6]. This includes Diana, a scheme from [7] that
Session 5D: Encrypted Search & Computation 2 CCS’18, October 15-19, 2018, Toronto, ON, Canada 1047
10−2
10−1
100
101
102
103
104
105
106
107
101 102 103 104 105
Time(milliseconds)
Result Size
Mitra
Dianadel
Fides
(a)
 0
 500
 1000
 1500
 2000
103 104 105 106 107
Time(millisecond)
|DB|
Mitra
Dianadel
Fides
(b)
 0
 0.5
 1
 1.5
 2
103 104 105 106 107
Time(millisecond)
|DB|
Mitra
Dianadel
Fides
(c)
Figure 3: Computation time for: (a) search vs. variable result size for |DB| = 1M, (b) search vs. variable |DB| for result size 1K,
(c) update vs. variable |DB|.
does not support deletions which we modified to implement Dianadel . The repository also includes Janus and the forward private
scheme of [5] which we used as the back-end for building Fides, as
described in [7]. All our implementations are publicly available [12].
For our experiments we used t2.xlarge AWS machines with fourcore Intel Xeon E5-2676 v3 2.4GHz processor, running Ubuntu
14.04 LTS, with 16GB RAM, 100GB SSD (GP2) hard disk, and AESNI enabled. All schemes were executed on a single machine while
storing the database on RAM except for the WAN experiment which
was run on two machines with 21ms roundtrip time (located in
Germany and Ireland). We are interested in measuring executions
time and communication size for search and update operations,
as well as permanent client storage for the different schemes. We
tested for variable database size |DB| = 103–107 using synthetic
records. For each database size, we set |W | to one-hundredth of
the database, and randomly generated the entries. Throughout
the experiments, we considered variable result between 10–105
documents. Unless otherwise specified, after records were inserted
we deleted at random 10% of the entries of the queried keyword to
emulate the impact of deletions on performance. All experiments
were repeated ×10 and the average is reported.
5.1 Performance comparison for Mitra
Our first set of experiments focuses on the performance of Mitra,
and how it compares with Fides and Dianadel from [7]. All three
schemes, were implemented with the “cleanup” process enabled
(that removes deleted entries after a search), as described in section 3
for Mitra and in [7] for the others.
Search. Figure 3 shows the search time when the result size (a)
and the database size (b) change. As is clear from the graphs, the
time growth is strongly linear for all schemes when the result size
grows, whereas it remains visibly unaffected by changes in the
database size. Throughout all the executions, Mitra is 145–253×
faster than Fides, the only scheme with the same leakage type. This
should come as no surprise as Fides relies on one-way trapdoor
permutations (implemented with RSA) to achieve forward and
backward privacy while Mitra relies on symmetric encryption. We
believe the huge improvement in the search time is strong evidence
for the practical performance of Mitra which has concretely very
low overheads, e.g., for a database of 1M records and result size 100,
it takes roughly 0.8ms for computation (most of which is spent at
the client, with the server performing only lookups). To emphasize
the performance of Mitra, observe that Dianadel has worse search
times, despite achieving only Type-III privacy!
Figure 4 shows the communication size when the result size (a)
and the database size (b) change. Again, this grows linearly with
result size and is unaffected by the database size. Comparing Fides
and Mitra, we see that they have very similar communication
sizes (the ratio of the former over the latter is 0.7–1.1). Despite the
fact that with Fides the client only needs to initially send a single
“token” (whereas with Mitra, he sends the entire TList), the search
communication gap becomes smaller due to the server sending the
encrypted indexes and the final cleanup process. In practice, both
have low costs, e.g., Mitra transmits in total approximately 7KB
for a database of 1M records and result size 100. Regarding the
communication size for Dianadel , for the given configurations it
is virtually the same as Mitra; for larger deletion percentages (not
included above), its communication size becomes smaller (down to
3× smaller than that of Mitra).
Update. Figure 3(c) and Figure 4(c) show the update time and
communication size respectively, for variable database sizes. The
obvious conclusions from the figures are that: (i) the update time of
the three schemes is almost unaffected by the size of the database,
(ii) Fides is concretely much slower than the other two, due to
performing an RSA private-key operation (instead of symmetrickey crypto), and (iii) all of them have excellent performance (<
1.5ms for Fides, ≤ 32µs for the rest, and all of them ≤ 56bytes).
Client storage. All three schemes impose |W |loд|D| permanent
storage at the client, in order to maintain the necessary update
counter(s) for each keyword. The difference is that Mitra and
Fides only requires one such counter per keyword, whereas Dianadel requires two, as it stores insertions and deletions separately.
Concretely, local storage for Mitra and Fides was roughly 156KB
for 10K keywords (and twice that for Dianadel).
Comparison with Janus. In the above, we did not compare with
Janus, the non-interactive scheme of [7], for two reasons. Firstly,
it is only Type-III backward private. Secondly, its concrete performance is several times worse than these three schemes. For example,
searching for a keyword with result size 1K in a database of 100K
entries took 51 seconds assuming no previous deletions—this increases by roughly 7× with 10% deleted entries.
Experiments over WAN. In the above experiments, we executed
the code for both the server and the client on the same machine
and we reported computation times for searches and updates. To
Session 5D: Encrypted Search & Computation 2 CCS’18, October 15-19, 2018, Toronto, ON, Canada 1048
102
103
104
105
106
107
108
101 102 103 104 105
Size(Byte)
Result Size
Mitra
Dianadel
Fides
(a)
5×104
6×104
7×104
8×104
9×104
1×105
103 104 105 106 107
Size(Byte)
|DB|
Mitra
Dianadel
Fides
(b)
 10
 20
 30
 40
 50
 60
 70
 80
 90
 100
103 104 105 106 107
Size(Byte)
|DB|
Mitra
Dianadel
Fides
(c)
Figure 4: Communication size for: (a) search vs. variable result size for |DB| = 1M, (b) search vs. variable |DB| for result size
1K, (c) update vs. variable |DB|.
measure the end-to-end time for performing searches in a realworld setting, we ran Mitra and Fides in a WAN setting over two
machines (server and client) with 21ms latency. Figure 5(a) shows
the time breakdown for variable result sizes and |DB| = 1M. Note
that, in Fides the dominant overhead is due to computation whereas
in Mitra due to communication (since T List has to be transferred
to the server). Despite this, in terms of total end-to-end time Mitra
still outperforms Fides! According to our experiments, Mitra is
1.3 − 51× faster than Fides depending on the result size.
As an insight for the practical performance of Mitra, consider
the popular Enron email dataset which contains roughly 0.5M (keyword, email) pairs and 77K keywords. In this case, the performance
of Mitra would be very similar to what we reported for |DB| = 1M
and 10K keywords. Over WAN with 21ms latency and result size 100,
this would take 45ms for communication, 1.3ms for computation,
and the client storage would be 156KB.
5.2 Performance of Orion & Horus
In order to measure the performance of Orion and Horus, we
instantiated the oblivious map of [46] (using Path-ORAM with
AES-256) and used it to implement the two schemes.
The search computation time for both of them is higher than
that of Mitra while better than Fides. For example, for a database
of size 100K and result size of 100, Orion took 38ms and Horus
17ms. For comparison, Fides (which has backward privacy Type-II)
needs 131ms. Let us explain this in more detail. The search time
of Fides grows strictly linearly with the result size, by performing
a corresponding number of public key operations. On the other
hand, the Orion client requests a number of ORAM paths from
the server in order to “parse” the AVL tree. The number of paths
grows with the result size but not necessarily linearly. E.g., assume
the first two paths requested (corresponding to random positions)
only differ at the last bit. In that case, the total number of ORAM
buckets sent from the server would be just log N + 1, instead of
2 log N. In general, the ORAM accesses requested from the client
create a “subtree” of the Path-ORAM and only distinct buckets in
this subtree need to be handled (and re-mapped).
Where Orion and Horus lack in performance is communication
size. The sheer overhead of transmitting all the necessary ORAM
blocks is concretely large. For a database of 1M and result size 100,
Orion transmits 1.7MB and Horus 0.3MB—the same measurement
for Fides is just 5.3KB. At the core of this issue is the “large number
of buckets” problem when using ORAM for SSE. Given the very
small size of the response to an SSE query (excluding actual files,
in case they need to be retrieved) implies a very large overhead
when encrypting them with ORAM due to the retrieval of multiple
buckets per query. A very insightful discussion of the issue can be
found in [34]. Simply put, in certain cases the client would be better
off just encrypting the entire DB index with regular encryption
and downloading it for every query. Still, recent research proposals [24, 39] explore the combination of oblivious structures with
trusted hardware (which in our case would eliminate the need to
transmit over the network), which opens the possibility of testing
our schemes in that setting.
The effect of deletions. Despite the limitation identified above,
one interesting result that we can deduce from our implementation
is the effect of large volumes of deletions on the performance of
different schemes. Recall that, schemes such as Orion and Horus
that have quasi-optimal time should be essentially unaffected by the
number of past deletions for a given result size—indeed that was our
main motivation for exploring this direction. Figures 5(b)(c) show
the required search computation time for all schemes for a database
of size 100K and two cases: (left) “small” result size 100, and (right)
“large” result size 20K. In both cases, we vary the percentage of
previous deletions between 0–50% while the result size remains
fixed. (E.g., for 10% deletions with result size 100, we insert 111
records and delete 11 of them.) As shown in the figures, the overhead
of Fides grows as the deletion percentage increases (the same is
true for Mitra and Dianadel but the effect is not readily visible
due to scale). With Orion and Horus on the other hand, as long as
the result size is fixed the number of previous deletions does not
affect search times, which remain roughly constant.
6 CONCLUSION AND OPEN PROBLEMS
In this work, we introduced three new backward and forward private SSE schemes. Our constructions improve the state-of-the art
ones in several aspects. Mitra is concretely the fastest existing
such scheme, greatly outperforming existing ones, even those with
higher leakage. Our two other constructions, Orion and Horus, are
the first ones to have search time quasi-linear to nw , i.e., the number
of documents that contain keyword w currently in the database. All
previous works that achieve backward-privacy have search time
that is Ω(aw ), that is, at least linear in the total number of updates
(including deletions) related to w; in practice aw can be arbitrarily
Session 5D: Encrypted Search & Computation 2 CCS’18, October 15-19, 2018, Toronto, ON, Canada 1049
100
101
102
103
104
105
106
Mitra Fides Mitra Fides Mitra Fides Mitra Fides
100 1000 10000 100000
Result Size
Time(miliseconds)
Computation
Communication
Total
(a)
 0
 50
 100
 150
 200
 0 10 20 30 40 50
Time(millisecond)
Delete Percentage
Mitra
Dianadel
Fides
Orion
Horus
(b)
0
1×104
2×104
3×104
4×104
 0 10 20 30 40 50
Time(millisecond)
Delete Percentage
Mitra
Dianadel
Fides
Orion
Horus
(c)
Figure 5: (a) Computation, communication, and end-to-end time of WAN with 21ms latency for |DB| =1M and variable result
size. Computation time for search vs.% of deletions for |DB| =100K and result size 100 (b) and 20K (c).
larger than nw . Orion and Horus both achieve this property but
they offer different trade-offs between leakage and search performance. Our work leaves many open problems, such as investigating
whether we can develop a scheme with quasi-linear search time
and non-trivial communication without relying on ORAM (known
to be possible for schemes that are only forward private). Another
direction would be to devise a scheme with optimal search time
(which seems hard for deletion-supporting constructions), or a noninteractive one with quasi-optimal search time. Finally, it would be
interesting to revisit the backward privacy definitions of [7] and
“evaluate” their leakage for real-world applications, e.g., in the light
of possible deletion-specific attacks