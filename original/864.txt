We explore a new security model for secure computation
on large datasets. We assume that two servers have been
employed to compute on private data that was collected
from many users, and, in order to improve the efficiency of
their computation, we establish a new tradeoff with privacy.
Specifically, instead of claiming that the servers learn nothing
about the input values, we claim that what they do learn
from the computation preserves the differential privacy of
the input. Leveraging this relaxation of the security model
allows us to build a protocol that leaks some information in
the form of access patterns to memory, while also providing
a formal bound on what is learned from the leakage.
We then demonstrate that this leakage is useful in a broad
class of computations. We show that computations such as
histograms, PageRank and matrix factorization, which can
be performed in common graph-parallel frameworks such
as MapReduce or Pregel, benefit from our relaxation. We
implement a protocol for securely executing graph-parallel
computations, and evaluate the performance on the three
examples just mentioned above. We demonstrate marked
improvement over prior implementations for these computations.
CCS CONCEPTS
‚Ä¢ Security and privacy ‚Üí Security requirements; Formal security models; Public key (asymmetric) techniques;
KEYWORDS
secure computation, differential privacy, graph parallel computation
1 INTRODUCTION
Privacy and utility in today‚Äôs Internet is a tradeoff, and for
most users, utility is the clear priority. Citizens continue to
contribute greater amounts of private data to an increasing
number of entities in exchange for a wider variety of services.
From a theoretical perspective, we can maintain privacy and
utility if these service providers are willing and able to compute on encrypted data. The theory of secure computation
has been around since the earliest days of modern cryptography, but the practice of secure computation is relatively new,
and still lags behind the advancements in data-mining and
machine learning that have helped to create today‚Äôs tradeoff.
Recently, we have seen some signs that the gap might be
narrowing. The advancements in the field of secure computation have been tremendous in the last decade. The first
implementations computed roughly 30 circuit gates per second, and today they compute as many as 6 million per second
[40]. Scattered examples of live deployments have been referenced repeatedly, but most recently, in one of the more
promising signs of change, Google has started using secure
computation to help advertisers compute the value of their
ads, and they will soon start using it to securely construct
machine learning classifiers from mobile user data [23]. A
separate, more recent line of research also offers promise: the
theory and techniques of differential privacy give service
providers new mechanisms for aggregating user data in a way
that reasonably combines utility and privacy. The guarantee
of these mechanisms is that, whatever can be learned from
the aggregated data, the amount that it reveals about any
single user input is minimal. The Chrome browser uses these
techniques when aggregating crash reports [14], and Apple
claims to be employing them for collecting usage information
from mobile devices. In May, 2017, U.S. Senator Ron Wyden
wrote an open letter to the commission on evidence-based
policymaking, urging that both secure computation and differential privacy be employed by ‚Äúagencies and organizations
that seek to draw public policy related insights from the
private data of Americans [39].‚Äù
The common thread in these applications is large scale
computation, run by big organizations, on data that has been
collected from many individual users. To address this category of problems, we explore new improvements for two-party
secure computation, carried out by two dedicated computational servers, over secret shares of user data. We use a
novel approach: rather than attempting to improve on known
generic constructions, or tailoring a new solution for a particular problem, we instead explore a new trade-off between
efficiency and privacy. Specifically, we propose a model of
secure computation in which some small information is leaked
Session 3B: Differential Privacy 2 CCS‚Äô18, October 15-19, 2018, Toronto, ON, Canada 490
to the computation servers, but this leakage is proven to preserve differential privacy for the users that have contributed
data. More technically, the leakage is a random function of
the input, revealed in the form of access patterns to memory,
and the output of this function does not change ‚Äúby too
much‚Äù when one user‚Äôs input is modified or removed.
The question of what is leaked by memory access patterns
during computation is central to secure computation. Although the circuit model of computation allows us to skirt
the issue, because circuits are data oblivious, when computing
on large data there are better ways of handling the problem, the most well-studied being the use of secure two-party
ORAM [16, 24, 31, 38, 40, 41]. However, when looking at
very large data sets, it is often the case that both circuits and
ORAM are too slow for practical requirements, and there is
strong motivation to look for better approaches. In the area
of encrypted search, cryptographers have frequently proposed
access-pattern leakage as a tradeoff for efficiency [4, 5, 20, 33].
Unfortunately, analyzing and quantifying the leakage caused
by the computation‚Äôs access pattern is quite difficult, as it
depends heavily on the specific computation, the particulars
of the data, and even the auxiliary information of the adversary. Furthermore, recent progress on studying this leakage
has mostly drawn negative conclusions, suggesting that a
lot more is revealed than we might originally have hoped
[3, 11, 19, 21, 27]. Employing the definition of differential
privacy as a way to bound the leakage of our computation
allows us to offer an efficiency / privacy tradeoff that cryptographers have been trying to provide, while quantifying,
in a rigorous and meaningful way, precisely what we have
leaked.
1.1 Graph-Parallel Computations
While the proposed security relaxation is appealing, it is not
immediately clear that it provides a natural way to improve
efficiency. Our main contribution is that we identify a broad
class of highly parallelizable computations that are amenable
to the privacy/ efficiency tradeoff we propose. When computing on plaintext data, frameworks such as MapReduce,
Pregel, GraphLab and PowerGraph have very successfully
enabled developers to leverage large networks of parallelized
CPUs [9, 15, 25, 26]. The latter three mentioned systems are
specifically designed to support computations on data that
resides in a graph, either at the nodes or edges. The computation proceeds by iteratively gathering data from incoming
edges to the nodes, performing some simple computation at
the node, and pushing the data back to the outgoing edges.
This simple iterative procedure captures many important
computational tasks, including histogram, gradient descent
and page-rank, which we focus on in our experimental section, as well as Markov random field parameter learning,
parallelized Gibbs samplers, and name entity resolution, to
name a few more. Recently, Nayak et al. [28], generalizing the
work of Nikolaenko et al. [29], constructed a framework for
securely computing graph-parallel algorithms. They did this
by designing a nicely parallelizable circuit for the gather and
scatter phases, requiring ùëÇ(|ùê∏| + |ùëâ |) log2
(|ùê∏| + |ùëâ |) AND
gates.
1.2 A Connection to Differential Privacy
The memory access pattern induced by this computation is
easily described: during the gather stage, each edge is touched
when fetching the data, and the adjacent node is touched
when copying the data. A similar pattern is revealed during
the scatter phase. (The computation performed during the
apply phase is typically very simple, and can be executed in
a circuit, which is memory oblivious.) Let‚Äôs consider what
might be revealed by this access pattern in some concrete
application. In our framework, each user is represented by
a node in the graph, and provides the data on the edges
adjacent to that node. For example, in a recommendation
system, the graph is bipartite, each node on the left represents
a user, each node on the right represents an item that users
might review, and the edges are labeled with scores indicating
the user‚Äôs review of an item. The access pattern just described
would reveal exactly which items every user reviewed!
Our first observation is that if we use a secure computation
to obliviously shuffle all of the edges in between the gather
and scatter phases, we break the correlation between the
nodes. Now the only thing revealed to the computing parties
is a histogram of how many times each node is accessed ‚Äì i.e. a
count of each node‚Äôs in-degree and out-degree. When building
a recommendation system, this would reveal how many items
each user reviewed, as well as how many times each item was
reviewed. Fortunately, histograms are the canonical problem
for differential privacy. Our second observation is that we can
shuffle in dummy edges to help obscure this information, and,
by sampling the dummy edges from an appropriate distribution (which has to be done within a secure computation), we
can claim that the degrees of each node remain differentially
private.
1.3 Contributions and Related Work
Contributions. We make several new contributions, of both a
theoretical and a practical nature.
Introducing the model. As cryptographers have attempted to
support secure computation on increasingly large datasets,
they have often allowed their protocols to leak some information to the computing parties in the form of access patterns
to memory. This is especially true in the literature on encrypted search. The idea of bounding the leakage in a formal
way, using the definitions from literature on differential privacy, is novel and important. (Concurrent and independent
of our work, He et al. [18] proposed a very similar security
relaxation, which we discuss below.)
Asymptotic improvement. The relaxation we introduce enables us to improve the asymptotic complexity of the target
computations by a factor of (roughly) log ùëõ over the best
known, fully oblivious construction. Using the best known
sorting networks, the construction of Nayak et al. [28] requires ùëÇ((|ùê∏|+|ùëâ |) log(|ùê∏|+|ùëâ |)) AND gates. In comparison,
our construction requires ùëÇ(|ùê∏| + ùõº|ùëâ |) AND gates, where
Session 3B: Differential Privacy 2 CCS‚Äô18, October 15-19, 2018, Toronto, ON, Canada 491
ùõº depends on the privacy parameters, ùúñ and ùõø. For graphs
with |ùê∏| = ùëÇ(ùõº|ùëâ |), our improvement is by a factor of log |ùê∏|.
While we do not have a lower bound in the fully oblivious
model, we find it very exciting that there exist computations
where our relaxation is this meaningful, and we suspect that
improved asymptotic results in the fully oblivious model
are not possible. The details of this improvement appear in
Appendix B.
Concrete improvement. We demonstrate that the asymptotic
improvements lead to tangible gains. We have implemented
our system, and compared the results to the system of Nayak
et al. [28]. We demonstrate up to a 20X factor improvement
in the number of garbled AND gates required in the computation, while preserving differential privacy with strong
parameters: ùúñ = .3 and ùõø = 2
‚àí40. We note that, in practice,
both results are worse than previously described by a factor of
log |ùê∏|. In their implementation, Nayak et al. rely on a practical oblivious sort, using ùëÇ((|ùê∏| + |ùëâ |) log(|ùê∏| + |ùëâ |))2 AND
gates. Our construction using ùëÇ(|ùê∏| + ùõº|ùëâ |) gates requires
performing decryption inside a garbled circuit, which we
avoid in our implementation through the use of a two-party
oblivious shuffle, resulting in ùëÇ((|ùê∏| + ùõº|ùëâ |) log(|ùê∏| + ùõº|ùëâ |))
AND gates. However, we still save a factor of log |ùê∏| for sufficiently dense graphs. The details of this construction appear
in Section 3, and an evaluation of its performance appears in
Section 4.
Securely generating noise We describe a new noise distribution that is amenable to efficient implementation in a garbled
circuit. Dwork et al. previously described an efficient method
for sampling the geometric distribution in a garbled circuit
[12], but they did this for the 1-sided geometric distribution,
which is not immediately useful for providing differential
privacy. Unfortunately, the standard 2-sided geometric distribution needs to be normalized in a way that ruins the
simplicity of the circuit they describe. We construct a slightly
modified distribution that addresses this problem, and prove
that it provides differential privacy.
Related Work. The most relevant work, as already discussed,
is that by Nayak et al. [28], which generalizes the work of
Nikolaenko et al. [29], computing graph parallel computations with full security. (The latter work focused solely on
the problem of sparse matrix factorization.) Papadimitriou
et al. [32] also build a system for the secure computation of
graph-structured data, and ensure differential privacy of the
output. They do not consider differentially private leakage
in the access patterns, and they focus on MPC and the networking challenges that arise in that setting; in particular,
they do not rely on computational servers, but assume that
all data contributors are involved in the computation, and
focus on how to hide the movement of data between users in
a way that preserves privacy of the edge structure. Kellaris
et al. construct protocols for encrypted search, and use differential privacy to bound the leakage from access patterns [22].
This work directly inspired us to consider a more general
approach to modeling leakage in secure computation through
differential privacy. Wagh et al. [35] define and construct
differentially private ORAM in which the server‚Äôs views are
‚Äúsimilar‚Äù on two neighboring access patterns. They consider
the client/server model, and don‚Äôt consider using their construction in a secure computation, but it is very interesting
to note that we could use their result in a generic way to
build a protocol for generic secure computation with differentially private access patterns. While feasibility follows from
their work, the resulting protocol provides no asymptotic
improvement, and would be quite impractical.
Independent of our work, He et al. define a security notion that is similar to our own, and construct new protocols
for private record linkage [18]. Informally, their definition
requires that for any input set ùê∑1, and for two neighbors
ùê∑2, ùê∑‚Ä≤
2
for which ùëì (ùê∑1, ùê∑2) = ùëì (ùê∑1, ùê∑‚Ä≤
2
), the protocol views
should preserve differential privacy. We note that their definition is not in the simulation paradigm, which leads to some
important differences. For one, they only require security
on inputs that map to the same output, and in particular,
cannot apply their definition to randomized functionalities1
.
They also have no correctness requirement (which is captured
in the simulation paradigm by default): this is intentional, as
they explore not just an efficiency / privacy tradeoff, but a
correctness tradeoff as well. Perhaps the biggest difference
between their work and our own is the application space. In
private record linkage protocols, items are typically hashed
into bins, and dummy items are used to hide the load of
each bin. When applying their relaxation, they gain efficiency
over fully secure protocols by cutting down on the number
of dummy items and claiming differential privacy, in place of
statistically hiding the bin load. Since the maximum load on
a bin is log ùëõ
log log ùëõ
with high probability, they can only claim
improvement if they use very few dummy items, and as such
they can only claim fairly weak security. Specifically, in most
of their experiments ùõø = 2
‚àí16. While there may be settings
where this security parameter suffices, it is important to recognize that ùõø denotes the statistical probability that a user‚Äôs
data is completely recovered, and this value is typically set to
2
‚àí40 in MPC research. Finding an application space where
the security relaxation provides significant efficiency improvements while also guaranteeing strong security parameters
was a major challenge, and we view it as one of our main
contributions.
Chan et al. study differential obliviousness in the client /
server model [6]. They also show asymptotic improvement for
several computations, together with lower bounds for fully
secure variants of the same algorithms, demonstrating that
this relaxation allows us to bypass impossibility results. Their
results are purely theoretical, but raise the very interesting
question of whether we can lower-bound the number of AND
gates needed in fully secure graph parallel computation. An
initial version of our work did not include Appendix B, where
we show how to remove the oblivious shuffle, improving the
asymptotic complexity of our solution. While our initial work
1For example, because our protocol outputs random secret shares to
each of the servers, it could not be proven secure under their definition,
but can be proven secure in our simulation paradigm.
Session 3B: Differential Privacy 2 CCS‚Äô18, October 15-19, 2018, Toronto, ON, Canada 492
predates the work of Chan et al., the asymptotic improvement
of Section B was concurrent with their work.
M2R [10] and Ohrimenko et al. [30] consider secure implementation of MapReduce jobs on untrusted cloud servers,
where the adversary has access to the network and storage
back-end, and can observe all encrypted traffic between Map
and Reduce nodes, but cannot corrupt those nodes; they assume secure hardware, such as SGX. They hide flow between
map and reduce operations by shuffling the data produced
by the Mappers in secure hardware before sending it to the
Reducers. They do not use any notion of differential privacy.
Finally, Airavat [34] protects the output of the map-reduce
computation by adding exponentially distributed noise to the
output of computation.
2 NOTATION AND DEFINITIONS
Secret-Shares: We let ‚ü®ùë•‚ü© denote a variable which is XOR
secret-shared between parties. Arrays have a public length
and are accessed via public indices; we use ‚ü®ùë•‚ü©
ùëñ
to specify
element ùëñ within a shared array, and ‚ü®ùë•‚ü©
ùëñ:ùëó
to indicate a
specific portion of the array containing elements ùëñ through ùëó,
inclusive. When we write ‚ü®ùë•‚ü© ‚Üê ùëê, we mean that both users
should fix their shares of ùë• (using some agreed upon manner)
to ensure that ùë• = ùëê. For example, one party might set his
share to be ùëê while the other sets his share to 0.
Multi-Sets: We represent multi-sets over a set ùëâ by a |ùëâ |
dimensional vector of natural numbers: ùê∑ ‚àà N
|ùëâ |
. We refer
to the ùëñth element of this vector by ùê∑(ùëñ). We use |ùê∑| in the
natural way to mean ‚àëÔ∏Ä|ùëâ |
ùëñ=1 ùê∑(ùëñ). We use ùíüùíùùëñ to denote the set
of all multi-sets over ùëâ of size ùëñ, and ùíüùíù =
‚ãÉÔ∏Ä
ùëñ ùíüùíùùëñ
. We define
a metric on these multi-sets in the natural way: |ùê∑1 ‚àí ùê∑2| = ‚àëÔ∏Ä|ùëâ |
ùëñ=1
|ùê∑1(ùëñ) ‚àí ùê∑2(ùëñ)|. We say two multi-sets are neighboring
if they have distance at most 1: |ùê∑1 ‚àí ùê∑2| ‚â§ 1.
Neighboring Graphs: In our main protocol of Section 3, the
input is a data-augmented directed graph, denoted by ùê∫ =
(ùëâ, ùê∏), with user-defined data on each edge. We need to define
a metric on these input graphs, in order to claim security for
graphs of bounded distance.2 For each ùë£ ‚àà ùëâ , we let in-deg(ùë£)
and out-deg(ùë£) denote the in-degree and out-degree of node
ùë£. We define the in-degree profile of a graph ùê∫ as the multiset Din(ùê∫) = {in-deg(ùë£1), . . . , in-deg(ùë£ùëõ)}. Intuitively, this is
a multi-set over the node identifiers from the input graph,
with vertex identifier ùë£ appearing ùëò times if in-deg(ùë£) = ùëò.
We define the full-degree profile of ùê∫ as the pair of multisets: {Din(ùê∫), Dout(ùê∫)}, where
Dout(ùê∫) = {out-deg(ùë£1), . . . , out-deg(ùë£ùëõ)}. We now define
two different metrics on graphs, using these degree profiles.
Later in this section, we provide two different security definitions: we rely on the first distance metric below when
claiming security as defined in Definition 2.5, and rely on
the second metric below when claiming security as defined
in Definition 2.6.
2
In Appendix A, the input to the computation is a multi-set of elements
drawn from some set ùëÜ, rather than a graph, so we use the simple
distance metric described above to define the distance between inputs.
Definition 2.1. We say two graphs ùê∫ and ùê∫
‚Ä≤ have distance at most ùëë if they have in-degree profiles of distance at
most ùëë: |Din(ùê∫) ‚àí Din(ùê∫
‚Ä≤
)| ‚â§ ùëë. We say that ùê∫ and ùê∫
‚Ä≤
are
neighboring if they have distance 1.
Definition 2.2. We say two graphs ùê∫ and ùê∫
‚Ä≤ have fulldegree profiles of distance ùëë if the sum of the distances in
their in-degree profiles and their out-degree profiles is at most
ùëë: |Din(ùê∫) ‚àíDin(ùê∫
‚Ä≤
)|+|Dout(ùê∫) ‚àíDout(ùê∫
‚Ä≤
)| ‚â§ ùëë. We say that
ùê∫ and ùê∫
‚Ä≤ have neighboring full-degree profiles if they have
full-degree profiles of distance 2.
2.1 Differential Privacy
We use the definition that appears in [13].
Definition 2.3. A randomized algorithm ùíß : ùíüùíù ‚Üí ùí≠ùíß,
with an input domain ùíüùíù that is the set of all multi-sets
over some fixed set ùëâ , and output ùí≠ùíß ‚äÇ {0, 1}
*
, is (ùúñ, ùõø)-
differentially private if for all ùëá ‚äÜ ùí≠ùíß and ‚àÄùê∑1, ùê∑2 ‚àà ùíüùíù
such that |ùê∑1 ‚àí ùê∑2| ‚â§ 1:
Pr[ùíß(ùê∑1) ‚àà ùëá] ‚â§ ùëí
ùúñ Pr[ùíß(ùê∑2) ‚àà ùëá] + ùõø
where the probability space is over the coin flips of the
mechanism ùíß.
The above definition describes differential privacy for neighboring multi-sets. Letting ùí¢ denote the set of all graphs, we
define it for neighboring graphs as well:
Definition 2.4. A randomized algorithm ùíß : ùí¢ ‚Üí ùí≠ùíß is
(ùúñ, ùõø)-edge private if for all neighboring graphs, ùê∫1, ùê∫2 ‚àà ùí¢,
we have:
Pr[ùíß(ùê∫1) ‚àà ùëá] ‚â§ ùëí
ùúñ Pr[ùíß(ùê∫2) ‚àà ùëá] + ùõø
2.2 Secure computation with differentially
private access patterns
Input model: We try to keep the definitions general, as we
expect they will find application beyond the space of graphstructured data. However, we use notation that is suggestive
of computation on graphs, in order to keep our notation
consistent with the later sections. We assume that two computation servers have been entrusted to compute on behalf
of a large set of users, ùí±, with |ùí±| = ùëõ, and having sequential
identifiers, 1, . . . , ùëõ. Each user ùëñ contributes data ùë£ùëñ
. They
might each entrust their data to one of the two servers (we
call this the disjoint collection setting), or they might each
secret-share their input with the two-servers (joint collection
setting). In the latter case, we note that both servers learn
the size of each ùë£ùëñ but neither learns the input values; in the
former case, each server learns a subset of the input values,
but learns nothing about the remaining input values (other
than the sum of their sizes).3 Below we will define two variant
security notions that capture these two scenarios.
3We note that the disjoint collection setting corresponds to the ‚Äústandard‚Äù setting for secure computation where each computing party
contributes one set of inputs. Just as in that setting, each of the two
computing parties could pad their inputs to some maximum size, hiding
even the sum of the user input sizes. In fact, we could have them pad
their inputs using a randomized mechanism that preserves differential
privacy, possibly leading to smaller padding sizes, depending on what
Session 3B: Differential Privacy 2 CCS‚Äô18, October 15-19, 2018, Toronto, ON, Canada 493
In all computations that we consider in our constructions,
the input is represented by a graph. In every case, each user is
represented as a node in this graph, and each user input is a
set of weighted, directed edges that originate at their node. In
some applications, the graph is bipartite, with user nodes on
the left, and some distinct set of item nodes on the right: in
this case, all edges go from user nodes to item nodes. In other
applications, there are only user nodes, and every edge is from
one user to another. In the joint collection setting, we can leak
the out-degree of each node, which is the same as the user
input size, but must hide (among other things) the in-degree
of each node. In the disjoint collection setting, the protocol
must hide both the in-degree and out-degree of each node.
We note that in some applications, such as when we perform
gradient descent, the graph is bipartite, and it is publicly
known that the in-degree of every user is 0 (i.e. the movies
don‚Äôt review the viewers). In the joint collection setting, this
knowledge allows for some improvement in efficiency that we
will leverage in Section 4.
Defining secure computation with leakage: For simplicity, we
start with a standard definition of semi-honest security4
, but
make two important changes. The first change is that we
allow certain leakage in the ideal world, in order to reflect
what is learned by the adversary in the real world through
the observed access pattern on memory. The leakage function
is a randomized function of the inputs. The second change
is an additional requirement that this leakage function be
proven to preserve the differential privacy for the users that
contribute data. Our ideal world experiment is as follows.
There are two parties, ùëÉ1 and ùëÉ2, and an adversary ùíÆ that
corrupts one of them. The parties are given input, as described above; we use ùëâ1 and ùëâ2 to denote the inputs of the
computing parties, regardless of whether we are in the joint
collection setting or the disjoint collection setting, and we
let ùëâ = {ùë£1, . . . , ùë£ùëõ} denote the user input. Technically, in
the joint collection setting, ùëâ = ùëâ1 ‚äï ùëâ2, while in the disjoint
collection setting, ùëâ = ùëâ1 ‚à™ ùëâ2. Each computing party submits their input to the ideal functionality, unchanged. The
ideal functionality reconstructs the ùëõ user inputs, ùë£1, . . . , ùë£ùëõ,
either by taking the union of the inputs submitted by the
computation servers in the disjoint collection setting, or by
reconstructing the input set from the provided secret shares
in the joint collection setting. The ideal functionality then
outputs ùëì1(ùë£1, . . . , ùë£ùëõ) to ùëÉ1 and ùëì2(ùë£1, . . . , ùë£ùëõ) to ùëÉ2. These
outputs might be correlated, and, in particular, in our own
use-cases, each party receives a secret share of a single function evaluation: ‚ü®ùëì (ùë£1, . . . , ùë£ùëõ)‚ü©1,‚ü®ùëì (ùë£1, . . . , ùë£ùëõ)‚ü©2. The ideal
functionality also applies some leakage function to the data,
ùíß(ùëâ ), and provides the output of ùíß(ùëâ ), along with ‚àëÔ∏Ä
ùëñ‚ààùí± |ùë£ùëñ
|
the maximum and average input sizes are. We don‚Äôt explore this option
further in this work.
4We stress that our allowance of differentially private leakage brings
gains in the circuit construction, so we could use any generic secure
computation of Boolean circuits, including those that are maliciously
secure, and benefit from the same gains. See more details below.
to ùíÆ.
5 Additionally, depending on the choice of security definition, the ideal functionality might or might not give the
simulator, ‚àÄùëñ ‚àà ùí±, |ùë£ùëñ
|.
Our protocols are described in a hybrid world, in which the
parties are given access to several secure, ideal functionalities. In our implementation, these are replaced using generic
constructions of secure computation (i.e. garbled circuits).
Relying on a classic result of Canetti [2], when proving security, it suffices to treat these as calls to a trusted functionality.
In the definitions that follow, we let ùí¢ denote an appropriate
collection of ideal functionalities.
As is conventionally done in the literature on secure computation, we let hybridùí¢
ùúã,ùíú(ùëß)
(ùëâ1, ùëâ2, ùúÖ) denote a joint distribution over the output of the honest party and, the view of the
adversary ùíú with auxiliary input ùëß ‚àà {0, 1}
*
, when the parties interact in the hybrid protocol ùúã
ùí¢
on inputs ùëâ1 and ùëâ2,
each held by one of the two parties, and computational security parameter ùúÖ. We let idealùí°,ùíÆ(ùëß,ùíß(ùëâ ),‚àÄùëñ‚ààùëâ :|ùë£ùëñ|)
(ùëâ1, ùëâ2, ùúÖ)
denote the joint distribution over the output of the honest party, and the view output by the simulator ùíÆ with
auxiliary input ùëß ‚àà {0, 1}
*
, when the parties interact with
an ideal functionality ùí° on inputs ùëâ1 and ùëâ2, each submitted by one of the two parties, and security parameters
ùúÖ. Letting ùë£ =
‚àëÔ∏Ä
ùëñ‚ààùëâ
|ùë£ùëñ
|), we define the joint distribution
idealùí°,ùíÆ(ùëß,ùíß(ùëâ ),ùë£)
(ùëâ1, ùëâ2, ùúÖ) in a similar way, the only difference being that the simulator is given the sum of the input
sizes and not the value of each input size.
Definition 2.5. Let ùí° be some functionality, and let ùúã be
a two-party protocol for computing ùí°, while making calls to
an ideal functionality ùí¢. ùúã is said to securely compute ùí° in
the ùí¢-hybrid model with ùíß leakage, known input sizes, and
(ùúÖ, ùúñ, ùõø)-security if ùíß is (ùúñ, ùõø)-differentially private, and, for
every PPT, semi-honest, non-uniform adversary ùíú corrupting
a party in the ùí¢-hybrid model, there exists a PPT, nonuniform adversary ùíÆ corrupting the same party in the ideal
model, such that, on any valid inputs ùëâ1 and ùëâ2
{Ô∏Å
hybridùí¢
ùúã,ùíú(ùëß)
(ùëâ1, ùëâ2, ùúÖ)
}Ô∏Å
ùëß‚àà{0,1}*,ùúÖ‚ààN
ùëê
‚â°
{Ô∏Å
ideal(1)
ùí°,ùíÆ(ùëß,ùíß(ùëâ ),‚àÄùëñ‚ààùëâ :|ùë£ùëñ|)
(ùëâ1, ùëâ2, ùúÖ)
}Ô∏Å
ùëß‚àà{0,1}*,ùúÖ‚ààN
(1)
The above definition is the one that we use in our implementations. However, in Section 3 we also describe a modified
protocol that achieves the stronger security definition that
follows, where the adversary does not learn the sizes of individual inputs. This property might be desirable (or maybe
even essential) in the disjoint collection model, where users
have not entrusted one of the two computing parties with
their inputs, or even the sizes of their inputs. On the other
hand, the previous definition is, in some sense, more ‚Äútypical‚Äù
of definitions in cryptography, where we assume that inputs
5
In the joint collection setting, the simulator can infer this value from
the size of the input that was submitted to the ideal functionality. But
it simplifies things to give it to him explicitly.
Session 3B: Differential Privacy 2 CCS‚Äô18, October 15-19, 2018, Toronto, ON, Canada 494
sizes are leaked. It is only in this model where data is outsourced that we can hope to hide each individual input size
among the other inputs.
Definition 2.6. Let ùí° be some functionality, and let ùúã be
a two-party protocol for computing ùí°, while making calls
to an ideal functionality ùí¢. ùúã is said to securely compute ùí°
in the ùí¢-hybrid model with ùíß leakage, and (ùúÖ, ùúñ, ùõø)-security
if ùíß is (ùúñ, ùõø)-differentially private, and, for every PPT, semihonest, non-uniform adversary ùíú corrupting a party in the
ùí¢-hybrid model, there exists a PPT, non-uniform adversary
ùíÆ corrupting the same party in the ideal model, such that,
on any valid inputs ùëâ1 and ùëâ2
{Ô∏Å
hybridùí¢
ùúã,ùíú(ùëß)
(ùëâ1, ùëâ2, ùúÖ)
}Ô∏Å
ùëß‚àà{0,1}*,ùúÖ‚ààN
ùëê
‚â°
{Ô∏Ç
ideal(2)
ùí°,ùíÆ(ùëß,ùíß(ùëâ ),
‚àëÔ∏Ä
ùëñ‚ààùëâ
|ùë£ùëñ|)
(ùëâ1, ùëâ2, ùúÖ)
}Ô∏Ç
ùëß‚àà{0,1}*,ùúÖ‚ààN
(2)
Differentially Private Output: As is typical in secure computation, we are concerned here with how to securely compute
some agreed upon function, rather than what function ought
to be computed. In other words, we view the question of what
the output itself might reveal about the input to be beyond
scope of our work. Our concern is only that the process of
computing the output should not reveal too much. Nevertheless, one could ask that the output of all computations
also be made to preserve differential privacy. Interestingly,
for the specific case of histograms, which we present as an
example in Section A, adding differentially private noise to
the output is substantially more efficient than preserving an
exact count. This is not true for the general protocol, but
the cost of adding noise for these cases has been studied
elsewhere [32], and it would be minor compared to the rest
of the protocol.
Nevertheless, we take a different approach. In all of our
computations, the output of each server is a secret share
of the desired output, and thus it is unconditionally secure.
The question of where to deliver these shares is left to the
user, though we can imagine several useful choices. Perhaps
most obvious, the shares might never be reconstructed, but
rather used later inside another secure computation that
makes decisions driven by the output. Or, as Nikolaenko et
al. suggest [29], when computing gradient descent to provide
users with recommendations, the recommendation vectors can
be sent to the user to store for themselves. Regardless, since
the aim of our work is to study the utility of our relaxation,
this concern is orthogonal, and we mainly leave it alone.
Privacy versus efficiency In the ‚Äústandard‚Äù settings where
differential privacy is employed, additional noise affects the
accuracy of the result. Here, added noise has no impact on the
output, which is always correct, and is protected by the secure
computation. Instead, the tradeoff is with efficiency: using
more noise helps to further hide the true memory accesses
among the fake ones, but requires additional, costly oblivious
computation.
Malicious security and multi-party computation: Extending
these definitions to model malicious adversaries and/or multiparty computation is straightforward, so we omit redundant
detail. Similarly, we stress that by leveraging the security
relaxation defined above, we gain improvement at the circuit
level, so we can easily extend our protocols to either (or both)
of these two settings in a generic way. To make our protocol
from Section 3 secure against a malicious adversary, the only
subtlety to address is that our protocols make iterative use
of multiple secure computations (i.e. the functionality we
realize is reactive), so we would need to authenticate outputs and verify inputs in each of these computations. While
this can be done generically, such authentication comes ‚Äúfor
free‚Äù in many common protocols for secure computation
(e.g. [8, 37]). To extend our protocols to a multiparty setting,
the only subtlety is in constructing a multiparty oblivious
shuffle. With a small number of parties, ùëê, it is very efficient
to implement ùëê iterations of a permutation network, where
in each iteration, a different party chooses the control bits
that determine the permutation. As ùëê grows, it becomes less
clear what the best method is for implementing an oblivious
shuffle. Interestingly, we note that there has been some recent
work on parallelizing multi-party oblivious shuffle [7]. We do
not explore this direction in our work; presenting our protocols in the two-party, semi-honest setting greatly simplifies
the exposition, and suffices to demonstrate the advantages
of our security relaxation. In our performance analysis, we
primarily focus on counting the number of AND gates in
our construction, which makes the analysis more general and
allows for more accurate comparison with prior work (than,
say, comparing the timed performance of systems that use
different frameworks for implementing secure computation).
3 THE OBLIVGRAPH PROTOCOL
In the full version of our paper, we start with a warm-up
protocol for computing histograms. Due to space constraints,
we have moved this to Appendix A. That section helps describe the role that noise plays in our protocol, without the
distraction of the graph structure. The reader might find
it helpful, but could also skip reading it. We note that for
the particular case of histograms, there is a simple way to
provide differentially private output for free. See Appendix
A for details.
In this section, we describe our protocol for graph structured data, and the graph-parallel frameworks that support
highly parallelized computation. There are several frameworks of this type, including MapReduce, Pregel, GraphLab
and others [9, 25, 26]. We describe the framework by Gonzalez et al. [15] called PowerGraph since it combines the
best features from both Pregel and GraphLab. PowerGraph
is a graph-parallel abstraction, consisting of a sparse graph
that encodes computation as vertex-programs that run in
parallel and interact along edges in the graph. While the
implementation of vertex-programs in Pregel and GraphLab
differ in how they collect and disseminate information, they
share a common structure called the GAS model of graph
Session 3B: Differential Privacy 2 CCS‚Äô18, October 15-19, 2018, Toronto, ON, Canada 495
DumGenùëù,ùõº
Computation:
ùëë = 2ùõº|ùëâ |
DummyEdges1:ùëë ‚Üê ‚ä•
for ùëñ = 0 . . . |ùëâ | ‚àí 1
ùëó = 2ùõºùëñ
ùõæùëñ ‚Üê Dùëù,ùõº
ùëò = ùõæùëñ + ùëó
DummyEdgesùëó:ùëò
.ùë£ = ùëñ
Output: ‚ü®DummyEdges‚ü©
Figure 1: The variant of DumGenùëù,ùõº used in our experiments,
which suffices for meeting Definition 2.5. A variant achieving
Definition 2.6 appears in Figure 7 in Appendix A.
computation. The GAS model represents three conceptual
phases of a vertex-program: Gather, Apply, and Scatter. The
computation proceeds in iterations, and in each iteration,
every node gathers (copy) data from their incoming edges, applies some simple computation to the data, and then scatters
(copy) the result to their outgoing edges. Viewing each node
as a CPU (or by assigning multiple nodes to each CPU), the
apply step, which constitutes the bulk of the computational
work, is easily parallelized.
When performing such computations securely, the data is
secret-shared between the computing servers as it moves from
edge to node and back, as well as during the Apply phase. The
Apply phase is performed on these secret shares using any
protocol for secure computation as a black-box. The main
challenge is to hide the movement of the data during the
Gather and Scatter phases, as these memory accesses reveal
substantial information about the user data.
Take matrix factorization as an example: an edge (u, v, Data)
indicates that user u reviewed item v, and the data stored
on the edge indicates the value of the user‚Äôs review. Because
the data is secret shared, the value of the review is never
revealed. During the Gather phase, the right vertex of every
edge is opened, and the data is moved to the corresponding
vertex. After the Apply phase, the left vertex of every edge
is open, and data is pulled back to the edge. If this data
movement were performed in the clear, the memory access
pattern would reveal the edges between nodes, exactly revealing which users reviewed which items. Our first observation
is that, because we touch only the right node of every edge
during the gather, and only the left node of every edge during
the scatter, by adding an oblivious shuffle of the edges between these two phases, we can hide the connection between
neighboring nodes. The leakage of the computation is then
reduced to two histograms: the in-degrees of each node, and,
after the shuffle, the out-degrees of each node!
Histograms are the canonical problem in differential privacy; we preserve privacy by adding noise to these two histograms, in the form of dummy edges. These dummy edges
are marked with a (secret shared) flag, indicating that they
should not influence the computation during the Apply phase,
but they still contain node identifiers, so they contribute to
the number of memory accesses to these nodes during the
ùí°gas
GAS Model Operations
Inputs: Secret share of edges denoted as ‚ü®Edges‚ü©,
each edge is edge : (u, v, uData, vData, isReal). Secret
share of vertices denoted as ‚ü®Vertices‚ü©, each vertex
contains vertex : (x, xData)
Outputs: Updated ‚ü®Vertices‚ü©
Gather(Edges)
for each edge ‚àà Edges
for each vertex ‚àà Vertices
if edge.v == vertex.x
vertex.xData ‚Üê copy(edge.uData)
Applyùëì (Vertices)
for each vertex ‚àà Vertices
vertex ‚Üê ùëì (vertex)
Scatter (Edges)
for each edge ‚àà Edges
for each vertex ‚àà Vertices
if edge.u == vertex.x
edge.uData ‚Üê copy(vertex.xData)
Figure 2: Ideal functionality for a single iteration of the GAS
model operations
Gather and Scatter phases. Details follow below, the formal
protocol specification appears in Figure 3, and the ideal functionality for the PowerGraph framework appears in Figure 2.
We denote the data graph by ùê∫ = (ùëâ, ùê∏). The structure of
each edge is comprised of (u, v, uData, vData, isReal), where
isReal indicates if an edge is ‚Äúreal‚Äù or ‚Äúdummy‚Äù. Each vertex
is represented as (x, xData). The xData field is large enough
to hold edge data from multiple adjacent edges.
Our protocol is in a hybrid model where we assume we
have access to three ideal functionalities: DumGenùëù,ùõº, ùí°Shuffle,
ùí°func. During the initialization phase, the DumGenùëù,ùõº functionality is used to generate secret-shares of the dummy edges.
These are placed alongside the real edges, and are then repeatedly shuffled in with the real edges during the iterative
phases. We describe DumGenùëù,ùõº in detail later in this section.
Both the Gather and Scatter phases begin with calls to
ùí°Shuffle, which takes secret shares of the edge data from each
party, and outputs fresh shares of the randomly permuted
data. In practice we implement this using two sequential,
generic secure computations of the Waksman permutation
network [1], where each party randomly chooses one of the
two permutations. Then, the parties iterate through the shuffled edge set, opening one side of each edge to reveal the
neighboring vertex. Opening these vertices in the clear is
where we leak information, and gain in efficiency. As we mentioned previously, this reveals a noisy histogram of the node
degrees. In doing so, the parties can fetch the appropriate
vertex from memory, without performing expensive oblivious sort operations, as in GraphSC, and without resorting
Session 3B: Differential Privacy 2 CCS‚Äô18, October 15-19, 2018, Toronto, ON, Canada 496
ùúãgas
Secure Graph-Parallel Computation with
Differentially Private Access Patterns
Inputs: Secret share of edges denoted as ‚ü®RealEdges‚ü©,
each edge is edge : (u, v, uData, vData, isReal). Secret
share of vertices denoted as ‚ü®Vertices‚ü©, each vertex
contains vertex : (x, xData). (ùëü stands for number of
real items, and ùëë for number of dummy ones)
Output: ‚ü®Edges‚ü©, ‚ü®Vertices‚ü©
Initialization:
‚ü®DummyEdges‚ü©1:ùëë ‚Üê DumGenùëù,ùõº
‚ü®Edges‚ü©1:ùëü ‚Üê ‚ü®RealEdges‚ü©1:ùëü
‚ü®Edges‚ü©ùëü+1:ùëü+ùëë ‚Üê ‚ü®DummyEdges‚ü©1:ùëë
‚ü®Edges.isReal‚ü©1:ùëü ‚Üê ‚ü®1‚ü©
‚ü®Edges.isReal‚ü©ùëü+1:ùëü+ùëë ‚Üê ‚ü®0‚ü©
Gather(‚ü®Edges‚ü©)
‚ü®Edges‚ü© ‚Üê ùí°Shuffle (‚ü®Edges‚ü©)
for each ‚ü®edge‚ü© ‚àà ‚ü®Edges‚ü©
edge.v ‚Üê Open(‚ü®edge.ùë£‚ü©)
for ‚ü®vertex‚ü© ‚àà ‚ü®Vertices‚ü©
if edge.v == vertex.x
‚ü®vertex.xData‚ü© ‚Üê copy(‚ü®edge.uData‚ü©)
Apply(‚ü®Vertices‚ü©)
for ‚ü®vertex‚ü© ‚àà ‚ü®Vertices‚ü©
‚ü®vertex.xData‚ü© ‚Üê ùí°func (‚ü®vertex.xData‚ü©)
Scatter(‚ü®Edges‚ü©)
‚ü®Edges‚ü© ‚Üê ùí°Shuffle (‚ü®Edges‚ü©)
for each ‚ü®edge‚ü© ‚àà ‚ü®Edges‚ü©
edge.u ‚Üê Open(‚ü®edge.ùë¢‚ü©)
for ‚ü®vertex‚ü© ‚àà ‚ü®Vertices‚ü©
if edge.u == vertex.x
‚ü®edge.uData‚ü© ‚Üê copy(‚ü®vertex.xData‚ü©)
Figure 3: A protocol for two parties to compute a single iteration of the GAS model operation on secret-shared data. This
protocol realizes the ideal functionality described in Figure 2.
to ORAM. After fetching the appropriate node, the secret
shared data is copied to/from the adjacent edge.
During Apply, the parties make a call to an ideal functionality, ùí°func. This functionality takes secret shares of all vertices,
reconstructs the data from the shares, applies the specified
function to the real data at each vertex (while ignoring data
from dummy edges), and returns fresh secret shares of the
aggregated vertex data. In our implementation, we realize
this ideal functionality using garbled circuits. We don‚Äôt focus
on the details here, as they have been described elsewhere
(e.g. [28, 29]).
DumGenùëù,ùõº in detail: The ideal functionality for DumGenùëù,ùõº
appears in Figure 1. The role of DumGenùëù,ùõº is to generate
the dummy elements that create a ‚Äúnoisy‚Äù degree profile, ùíüÃÇÔ∏Ä.
Starting with in-degree profile ùíü = Din(ùê∫), for each ùëñ ‚àà ùëâ ,
ùíüÃÇÔ∏Ä(ùëñ) = ùíü(ùëñ)+ùõæùëñ
, where each ùõæùëñ
is drawn independently from a
shifted geometric distribution, parameterized by a ‚Äústopping‚Äù
probability ùëù, and ‚Äúshift‚Äù of ùõº: we denote the distribution by
Dùëù,ùõº, and define it more precisely below. The shift ensures
that negative values are negligible likely to occur. This is
necessary because the noisy set determines our access pattern
to memory, and we cannot accommodate a negative number
of accesses (or, more accurately, we do not want to omit any
accesses needed for the real data). More specifically, we will
define below a ‚Äúshift function‚Äù ùõº : R √ó R ‚Üí N that maps
every (ùúñ, ùõø) pair to a natural number. (When ùúñ and ùõø are
fixed, we will simply use ùõº to denote ùõº(ùúñ, ùõø).)
The functionality iterates through each vertex identifier
ùëñ ‚àà ùëâ , sampling a random number ùõæùëñ ‚Üê Dùëù,ùõº, and creating ùõæùëñ
edges of the form (‚ä•, ùëñ). The remainder of the array contains
‚Äúblank‚Äù edges, (‚ä•, ‚ä•), which can be tossed away as they are
discovered later in the protocol, after the dummy edges have
all been shuffled 6 DumGenùëù,ùõº returns secret shares of the
dummy edges, ‚ü®DummyEdges‚ü©. The reader should note that
only the right node in each edge is assigned a dummy value,
while the left nodes all remain ‚ä•. This design choice is for
efficiency, and comes at the cost of leaking the exact histogram
defined by the out-degrees of the graph nodes when executing
Open(Edgesi
.u) in the Scatter operation. As an example of
how this impacts privacy, when computing gradient descent
for matrix factorization, this reveals the number of reviews
written by each user, while ensuring that the number of
reviews received by each item remains differentially private.
This hides whether any given user reviewed any specific item,
which suffices for achieving security with known input sizes,
as defined in Definition 2.5. This is the protocol that we use
in our implementation, but we briefly discuss what is needed
to achieve Definition 2.6 in Appendix C.
In some computations, the graph is known to be bipartite,
with all edges starting in the left vertex set and ending in
the right vertex set (again, recommendation systems are a
natural example). In this case, since it is known that all nodes
in the left vertex set have in-degree 0, we do not need to add
dummy edges containing these nodes. This cuts down on the
number of dummies required, and we take advantage of this
when implementing matrix factorization.
Implementing DumGenùëù,ùõº: Intuitively, we sample ùõæùëñ by flipping a biased coin ùëù until it comes up heads. We flip one
more unbiased coin to determine the sign of the noise, and
then add the result to ùõº. We will determine ùëù based on ùúñ and
ùõø. Formally, ùõæùëñ
is sampled as follows:
Pr[ùõæùëñ = ùõº] =
ùëù
2
‚àÄùëò ‚àà N, ùëò , 0 : Pr[ùõæùëñ = ùõº + ùëò] =
1
2
(1 ‚àí
ùëù
2
)ùëù(1 ‚àí ùëù)
|ùëò|‚àí1
.
As just previously described, we view ùëù as the stopping
probability. However, in the first coin flip, we stop with
probability ùëù/2. We note that this is a slight modification to
the normalized 2-sided geometric distribution, which would
typically be written as Pr[ùõæùëñ = ùõº + ùëò] =
1
2‚àíùëù
ùëù(1 ‚àí ùëù)
|ùëò|
. The
advantage of the distribution as it is written above is that it
is very easy to sample in a garbled circuit, so long as ùëù is an
6Revealing these blank edges before shuffling would reveal how many
dummy edges there are of the form (*, ùëñ), which would break privacy.
After all the edges are shuffled, revealing the number of blank edges
only reveals the total number of dummy edges, which is fine.
Session 3B: Differential Privacy 2 CCS‚Äô18, October 15-19, 2018, Toronto, ON, Canada 497
inverse power of 2; normalizing by 1
2‚àíùëù
introduces problems
of finite precision and greatly complicates the sampling circuit.
We note that Dwork et al. [12] suggest using the geometric
distribution with ùëù = 2
‚Ñì
, precisely because it is easy to
sample in a garbled circuit. However, they describe a 1-sided
geometric distribution, which is not immediately useful for
preserving differential privacy, and did not seem to consider
that, after normalizing, the 2-sided distribution cannot be
sampled as cleanly. A security analysis of our mechanism,
including concrete settings of the parameters, appears in
Section 3.1.
We note that with some probability that is dependent on
the choice of ùõº, ‚àÉùëñ ‚àà ùëâ s.t. ùíüÃÇÔ∏Ä(ùëñ) < 0, which leaves us with
a bad representation of a multiset. We therefore modify the
definition of ùí° to output ‚àÖ whenever this occurs, and we
always choose ùõº so that this occurs with probability bound
by ùõø. In our implementation, we set ùõø = 2
‚àí40
.
To securely sample Dùëù,ùõº, each party inputs a random string,
and we let the XOR of these strings define the random tape
for flipping the biased coins. If the first ‚Ñì bits of the random
tape are 1, the first coin is set to heads, and otherwise it
set to tails: this is computed with a single ‚Ñì-input AND
gate. We iterate through the random tape, ‚Ñì bits at a time,
determining the value of each coin, and setting the dummy
elements appropriately. We use one bit from the random tape
to determine the sign of our coin flips, and we add ùõº dummies
to the result. Recall that the output length is fixed, regardless
of this random tape, so after we set the appropriate number
of dummy items based on our coin flips, the remaining output
values are set to ‚ä•.
The cost of this implementation of DumGenùëù,ùõº is ùëÇ(ùëâ ),
though this hides a dependence on ùúñ and ùõø: an exact accounting for various values can be found in Section 4. This cost is
small relative to the cost of the oblivious shuffle, but we did
first consider a much simpler protocol for DumGenùëù,ùõº that
is worth describing. Instead of performing a coin flip inside
a secure computation, by choosing a different distribution,
we can implement DumGenùëù,ùõº without any interaction at all!
To do this, we have each party choose ùëë random values from
{1, . . . , |ùëâ |}, and view them as additive shares (modulo |ùëâ |)
of each dummy item. Note that this distribution is already
one-sided, so we do not need to worry about ùõº, and it already
has fixed length output, so we do not need to worry about
padding the dummy array with ‚ä• values. Intuitively, this
can be viewed as |ùëâ | correlated samples from the binomial
distribution, where the bias of the coin is 1/|ùëâ |. Unfortunately, the binomial distribution performs far worse than the
geometric distribution, and in concrete terms, for the same
values of ùúñ and ùõø, this protocol resulted in 250X more dummy
items. The savings from avoiding the secure computation of
DumGenùëù,ùõº were easily washed away by the cost of shuffling
so many additional items.
3.1 Proof of security
We begin by describing the leakage function ùíß(ùê∫). Intuitively,
we leak a noisy degree profile. As we mentioned previously, we
analyze the simpler DumGenùëù,ùõº algorithm, and prove that the
mechanism provides differential privacy for graphs that have
neighboring in-degree profiles. Then, we proceed afterwards
to show that this leakage function suffices for simulating
the protocol, achieving security in the joint-collection model,
corresponding to Definition 2.5. (Extending the proof to
meet Definition 2.6 is not much harder to do: we would use
the DumGenùëù,ùõº algorithm defined for the disjoint collection
model, and prove that differential privacy holds for graphs
that have neighboring full-degree profiles.)
We remind the reader that we use the following distribution, Dùëù,ùõº for sampling noise:
Pr[ùõæùëñ = ùõº] =
ùëù
2
‚àÄùëò ‚àà N, ùëò , 0 : Pr[ùõæùëñ = ùõº + ùëò] =
1
2
(1 ‚àí
ùëù
2
)ùëù(1 ‚àí ùëù)
|ùëò|‚àí1
.
We define a randomized algorithm, ùí°ùúñ,ùõø : ùê∑ ‚Üí ùíüÃÇÔ∏Ä, whose
input and output are multi-sets over ùëâ : ‚àÄùëñ ‚àà {1, . . . , |ùëâ |},
ùíüÃÇÔ∏Ä(ùëñ) = ùíü(ùëñ) + ùõæùëñ
, where ùõæùëñ ‚Üê Dùëù,ùõº.
Definition 3.1. The leakage function is
ùíß(ùê∫) = (ùí°ùúñ,ùõø (Din(ùê∫)), Doutùê∫) where Din(ùê∫) denotes the indegree profile of graph ùê∫, and Dout(ùê∫) denotes the out-degree
profile.
Theorem 3.2. The randomized algorithm ùíß is (ùúñ, ùõø)-approximate
differentially private, as defined in Definition 2.4.
We note that Dout(ùê∫) can be modeled as auxiliary information about Din(ùê∫), so the proof that ùíß preserves differential
privacy follows from the fact that the algorithm ùí°ùúñ,ùõø is differentially private for graphs with neighboring in-degree profiles.
It is well known that similar noise mechanisms preserve differential privacy, but, for completeness, we prove it below for
our modified distribution, which is much simpler to execute
in a garbled circuit.
Proof. To simplify notation, we use ùí° to denote ùí°ùúñ,ùõø.
Consider any two neighboring graphs, and let ùê∑1, ùê∑2 denote
their neighboring in-degree profiles. Let ùí°ùëÖ denote the range
of ùí°, and let ùíüÃÇÔ∏Ä be a multi-set in ùí°ùëÖ. We say that ùíü ‚àà ÃÇÔ∏Ä Bad if
‚àÉùëñ ‚àà {1, . . . , ùëâ }, ùíüÃÇÔ∏Ä(ùëñ) < 0, and assume for now that ùíüÃÇÔ∏Ä < Bad.
Let ùíüÃÇÔ∏Ä1 = ùí°(ùê∑1), let ùíüÃÇÔ∏Ä2 = ùí°(ùê∑2), and (without loss of
generality) let ùëñ be the value for which ùê∑1(ùëñ) = ùê∑2(ùëñ) + 1. By
the definition of ùí°, for ùëó , ùëñ, Pr[ùíüÃÇÔ∏Ä1(ùëó) = ùíüÃÇÔ∏Ä(ùëó)] = Pr[ùíüÃÇÔ∏Ä2(ùëó) =
ùíüÃÇÔ∏Ä(ùëó)]. Furthermore, for ùëò , ùëó, ùëò , ùëñ, ùëè ‚àà {1, 2}, ùíüÃÇÔ∏Äùëè
(ùëò) and
ùíüÃÇÔ∏Äùëè
(ùëó) are sampled independently. Therefore,
Pr[ùíüÃÇÔ∏Ä1 = ùíüÃÇÔ∏Ä]
Pr[ùíüÃÇÔ∏Ä2 = ùíüÃÇÔ∏Ä]
=
Pr[ùíüÃÇÔ∏Ä1(ùëñ) = ùíüÃÇÔ∏Ä(ùëñ)]
Pr[ùíüÃÇÔ∏Ä2(ùëñ) = ùíüÃÇÔ∏Ä(ùëñ)]
‚â§
1
(1 ‚àí ùëù)
(Note that the case |ùíüÃÇÔ∏Ä(ùëñ)| = |ùíüÃÇÔ∏Ä1(ùëñ)| ‚Äì i.e. where there is
no noise of type ùëñ added to the first dataset ‚Äì Pr[ùíüÃÇÔ∏Ä1=ùíüÃÇÔ∏Ä]
Pr[ùíüÃÇÔ∏Ä2=ùíüÃÇÔ∏Ä]
‚â§
1
1‚àíùëù/2
<
1
1‚àíùëù
.) By choosing 1 ‚àí ùëù = ùëí
‚àíùúñ
, we achieve the
Session 3B: Differential Privacy 2 CCS‚Äô18, October 15-19, 2018, Toronto, ON, Canada 498
desired bound. Then, for any ùëág ‚äÜ ùí°ùëÖ ‚àñ Bad,
Pr[ùí°(ùê∑1) ‚àà ùëág] =
‚àëÔ∏Å
ùê∑‚ààùëág
Pr[ùí°(ùê∑1) = ùê∑]
‚â§
‚àëÔ∏Å
ùê∑‚ààùëág
ùëí
ùúñ Pr[ùí°(ùê∑2) = ùê∑]
= ùëí
ùúñ Pr[ùí°(ùê∑2) ‚àà ùëág]
We now consider the probability that ùí°(ùê∑) ‚àà Bad. Recall,
this is exactly the probability that for some ùëñ ‚àà ùëâ , ùõæùëñ < 0,
which grows as a negligible function in ùõº. We choose ùõº such
that this probability is ùõø. (We will derive the exact function
below, and demonstrate some sample parameters.) Then, for
any ùëá ‚äÜ ùí°ùëÖ, letting ùëág = ùëá ‚àñ Bad,
Pr[ùí°(ùê∑1) ‚àà ùëá] = Pr[ùí°(ùê∑1) ‚àà ùëág] + Pr[ùí°(ùê∑1) ‚àà Bad]
‚â§ ùëí
ùúñ Pr[ùí°(ùê∑2) ‚àà ùëág] + ùõø
‚â§ ùëí
ùúñ Pr[ùí°(ùê∑2) ‚àà ùëá] + ùõø
Setting the parameters Note that the sensitivity of the distance metric defined in Definition 2.1 is 1. Although our
proof here is for neighboring graphs, we can use standard
composition theorems to claim differential privacy for graphs
of distance ùëë, at the cost of scaling ùúñ by a factor of ùëë. We also
note that ùëí
ùúñ = 1/(1 ‚àí ùëù), where ùëù is the stopping probability
defined in our noise distribution.
We set ùõø = 2
‚àí40, and show how to calculate ùõº; this allows
us to give the expected size of ùíüÃÇÔ∏Ä as a function of ùúñ and ùõø. We
first fix some ùëñ ‚àà ùëâ and calculate Pr[ùõæùëñ < 0], and then we
take a union bound over |ùëâ |.
Pr[ùõæùëñ < 0] =
‚àëÔ∏Å‚àû
ùëò=ùõº+1
1
2
(1 ‚àí
ùëù
2
)ùëù(1 ‚àí ùëù)
ùëò‚àí1
=
ùëù
2
(1 ‚àí
ùëù
2
)
‚àëÔ∏Å‚àû
ùëò=0
(1 ‚àí ùëù)
ùõº
(1 ‚àí ùëù)
ùëò
=
ùëù
2
(1 ‚àí
ùëù
2
)(1 ‚àí ùëù)
ùõº 1
1 ‚àí (1 ‚àí ùëù)
=
1
2
(1 ‚àí
ùëù
2
)(1 ‚àí ùëù)
ùõº
After taking a union bound over |ùëâ |, we have Pr[ùí°(ùê∑) ‚àà
Bad] ‚â§ 2
‚àí40 when ùõº >
‚àí40‚àílog(
1
2 ‚àí
ùëù
4
)‚àílog(|ùëâ |)
log(1‚àíùëù)
. Recall that
(1‚àíùëù) = ùëí
‚àíùúñ
. So, as an example, setting ùúñ = .3 and |ùëâ | = 2
12
,
we have ùõº = 118, and E(|ùí°(ùê∑)|) = 118|ùëâ | + |ùê∑|. That is, for
these privacy parameters, we expect to add 118 dummy edges
for each node in the graph.
Theorem 3.3. The protocol ùúãgùëéùë† defined in Figure 3 securely computes ùí°gas with ùíß leakage in the
(ùí°func, ùí°Shuffle, DumGenùëù,ùõº)-hybrid model according to Definition 2.5.
Proof. ( sketch.) Note that we can strengthen our Theorem statement to claim security according to Definition 2.6
by using the variant of DumGenùëù,ùõº found in the right column
of Figure 7. We omit the proof, but, at the end of this section,
we give some intuition for what would change in such a proof.
Recall that the leakage functionality contains
(ùí°(ùíüùíùùëÖ), out-deg(ùëâ )). In particular, then, we assume that
out-deg(ùëâ ) is public knowledge and given to the simulator,
which holds in the joint collection model of Definition 2.5.
Note that |ùëâ | and |ùê∏| are both determined by out-deg(ùëâ ),
and these values will be used by the simulator as well.
We construct a simulator for a semi-honest ùëÉ1. For all
three ideal functionalities, the output is simply an XOR secret
sharing of some computed value. The output of all calls to
these functionalities can be perfectly simulated using random
binary strings of the appropriate length. Let simEdges1 denote
the random string used to simulate the output of ùí°Shuffle
the first time the functionality is called, and let simEdges2
denote the random string used to simulate the output on
the second call. Let simEdges1
.ùë¢ denote the restriction of
simEdges1
to the bits that make up the sharings of Edges.ùë¢,
and let simEdges2
.ùë£ be defined similarly.
There are only two remaining messages to simulate:
Open(edge.u), and Open(edge.v). Recall that there are |ùê∏| +
2ùõº|ùëâ | edges in the Edges array: the original |ùê∏| real edges,
and the 2ùõº|ùëâ | dummy edges generated in DumGenùëù,ùõº. To
simulate the message sent when opening Edges.ùë¢, the simulator uses the values |ùëâ | and out-deg(ùëâ ) to create a bit
string representing a random shuffling of the following array
of size |ùê∏| + 2ùõº|ùëâ |. For each ùë¢ ‚àà ùëâ , the array contains the
identifier of ùë¢ exactly out-deg(ùë¢) times. This accounts for
|ùê∏| =
‚àëÔ∏Ä
ùë¢
out-deg(ùë¢) positions of the array; the remaining
2ùõº|ùëâ | positions are set to ‚ä•, consistent with the left nodes
output by DumGenùëù,ùõº. Letting ùëü denote the resulting bitstring, the simulator sends ùëü ‚äï simEdges1
.ùë¢ to the adversary.
To simulate simEdges2
.ùë£, the simulator creates another
bit-string representing a random shuffling of the following
array, again of size |ùê∏| + 2ùõº|ùëâ |. Letting ùíüÃÇÔ∏Ä = ùí°(ùíüùíùùëÖ) denote
the first element output by the leakage ùíß, the simulator adds
the node identifiers in ùíüÃÇÔ∏Ä to the array. In the remaining |ùê∏| +
2ùõº|ùëâ |‚àí|ùíü| ÃÇÔ∏Ä positions of the array, he adds ‚ä•. Letting ùëü denote
the resulting bit-string, the simulator sends ùëü ‚äï simEdges2
.ùë£
to the adversary.
So far, this results in a perfect simulation of the adversary‚Äôs
view. However, note that the outputs of the two parties should
be correlated. To ensure that the joint distribution over the
adversary‚Äôs view and the honest party‚Äôs output is correct, the
simulator has to submit the adversary‚Äôs input, ‚ü®Vertices‚ü©, to
the trusted party. He receives back a new sharing of Vertices,
and has to ‚Äúplant‚Äù this value in his simulation. Specifically, in
the final iteration of the protocol, when simulating the output
of ùí°func for the last time, the simulator uses ‚ü®Vertices‚ü©, as
received from the trusted party, as the simulated output of
this function call.
Improving asymptotic complexity. The construction in Section 3 requires ùëÇ((|ùê∏| + ùõº|ùëâ |) log(|ùê∏| + ùõº|ùëâ |)) garbled AND
gates. In comparison, the implementation of Nayak et al. [28]
Session 3B: Differential Privacy 2 CCS‚Äô18, October 15-19, 2018, Toronto, ON, Canada 499
uses ùëÇ(|ùê∏| + |ùëâ |) log2
(|ùê∏| + |ùëâ |) garbled gates. As we found
in the previous section, ùõº = ùëÇ(
log ùõø‚àílog |ùëâ |
ùúñ
). When |ùê∏| =
ùëÇ(ùõº|ùëâ |), this amounts to an improvement of ùëÇ(log(|ùê∏|)).
This improvement stems from our ability to replace several
oblivious sorting circuits with oblivious shuffle circuits, which
we are able to do only because of our security relaxation. However, while less practical, Nayak et al. could instead rely on an
asymptotically better algorithm for oblivious sort, reducing
their runtime to ùëÇ((|ùê∏| + |ùëâ |) log(|ùê∏| + |ùëâ |)). We therefore
find it interesting to ask whether our security relaxation admits asymptotic improvement for this class of computations,
in addition to the practical improvements described in the
previous section. Indeed, we show that we can remove the
need for an oblivious shuffle altogether by allowing one party
to shuffle the data locally. As long as the party that knows
the shuffling permutation does not see the access pattern
to ùëâ during the Scatter and Gather phases, the protocol
remains secure. The reason this protocol is less practical than
the protocol of Section 3 is because ùí°func now has to perform
decryption and encryption, which would require large garbled circuits. In Appendix B, we present a construction that
requires ùëÇ(|ùê∏| + ùõº|ùëâ |) garbled AND gates, demonstrating
asymptotic improvement over the best known construction
for this class of computations, whenever |ùê∏| = ùëÇ(ùõº|ùëâ |).
In Appendix C, we briefly discuss two security extensions: how we can achieve Definition 2.6, and how we can rely
on standard composition theorems to achieve node privacy
instead of edge privacy.
Sequential composition. The standard security definition for
secure computation composes sequentially, allowing the servers
to perform repeated computations on the same data without impacting security. With our relaxation, if we later use
the same user data in a new computation, the leakage does
compound. The standard composition theorems from the
literature on differential privacy do apply, and we do not address here how privacy ought to be budgeted across multiple
computations. The reader should note that in our iterative
protocol, there is no additional leakage beyond the first iteration, because we do not regenerate the dummy items: the
leakage in each iteration is the exactly the same noisy degree
profile that was leaked in all prior iterations.
4 IMPLEMENTATION AND EVALUATION
In this section, we describe and evaluate the implementation
of our proposed framework. We implement OblivGraph using
FlexSC, a Java-based garbled circuit framework. We measure
the performance of our framework on a set of benchmark
algorithms in order to evaluate our design. These benchmarks consist of histogram, PageRank and matrix factorization problems which are commonly used for evaluating
highly-parallelizable frameworks. In all scenarios, we assume
that the data is secret-shared across two non-colluding cloud
providers, as motivated in Section 1. For comparison, we compare our results with the closest large-scale secure parallel
graph computation, called GraphSC [28].
4.1 Implementation
Using the OblivGraph framework, the histogram and matrix factorization problems can be represented as directed
bipartite graphs, and PageRank as a directed non-bipartite
graph. When we are computing on bipartite graphs, if we
consider Definition 2.5 where we aim to hide the in-degree of
the nodes (nodes on the left have in-degree 0), the growth
rate of dummy edges is linear in the number of nodes on
the right and it is independent of the real edges or users. If
we consider the stronger Definition 2.6, the growth rate of
dummy edges is linear with max(users, items).
Histogram: In histogram, left vertices represent data elements, right vertices are the counters for each type of data
element, and existence of an edge indicates that the data
element on the left has the type on the right.
Matrix Factorization: In matrix factorization, left vertices
represent the users, right vertices are items (e.g. movies in
movie recommendation systems), an edge indicates that a
user ranked an item, and the weight of the edge represents
the rating value.
PageRank: In PageRank, each vertex corresponds to a
webpage and each edge is a link between two webpages. The
vertex data comprises of two real values, one for the PageRank
of the vertex and the other for the number of its outgoing
edges. Edge data is a real value corresponding to the weighted
contribution of the source vertex to the PageRank of the sink
vertex.
Vertex and Edge representation: In all scenarios, vertices
are identified using 16-bit integers and 1 bit is used to indicate
if the edge is real or dummy. For Histograms, we use an additional 20 bits to represent the counter values. In PageRank,
we represent the PageRank value using a 40-bit fixed-point
representation, with 20-bits for the fractional part. In our
matrix factorization experiments, we factorized the matrix to
user and movie feature vectors; each vector has dimension 10,
and each value is represented as 40-bit fixed-point number,
with 20-bits for the fractional part. We chose these values to
be consistent with GraphSC representation.
System setting: We conduct experiments on both a lab testbed,
and on a real-world scale Amazon AWS deployment. Our lab
testbed comprises 8 virtual machines each with dedicated
(reserved) hardware of 4 CPU cores (2.4 GHz) and 16 GB
RAM. These VMs were deployed on a vSphere Cluster of 3
physical servers and they were interconnected with 1Gbps virtual interfaces. We run our experiments on ùëù ‚àà {1, 2, 4, 8, 16}
pairs of these processors, where in each pair, one processor
works as the garbler, and the other as the evaluator. Each
processor can be implemented by a core in a multi-core VM,
or can be a VM in our compute cluster.
4.2 Evaluation
We use two metrics in evaluating the impact of our security relaxation: circuit complexity (e.g. # of AND gates),
and runtime. Counting AND gates provides a ‚Äúnormalized‚Äù
Session 3B: Differential Privacy 2 CCS‚Äô18, October 15-19, 2018, Toronto, ON, Canada 500
comparison with other frameworks, since circuit size is independent of the hardware configuration and of the chosen
secure computation implementation. However, it is also nice
to have a sense of concrete runtime, so we provide this evaluation as well. Of course, runtime is highly affected by the
choice of hardware, and ours can be improved by using more
processors or dedicated hardware (e.g. AES-NI).
Evaluation setting: For the LAN setup, we use synthesized
data and run all the benchmarks with the similar set of
parameters that have been used in the GraphSC framework.
In our histogram and matrix factorization experiments, we
run the experiments for 2048 users and 128 items. The number
of nodes in our PageRank experiment is set to be 2048.
For real world experiment using AWS, we run matrix factorization using gradient descent on the real-world MovieLens
dataset that contains 1 million ratings provided by 6040 users
to 3883 movies [17] on 2 m4.16xlarge AWS instances on the
Northern Virginia Data-center.
Circuit Complexity: The results presented in Figures 4a, 4b
and 4c are for execution on a single processor, to show the
performance of our design without leveraging the desired
effect of parallelization.
Histogram: Figure 4a demonstrates the number of AND
gates for computing histogram in both the GraphSC and
OblivGraph frameworks. With 2048 data elements and 128
data types, we always do better than GraphSC when ùúñ >= 0.3.
When ùúñ = 0.1, we start outperforming GraphSC when there
are at least 3400 edges.
Matrix Factorization: In Figure 4b, we use the (batch)
gradient decent method for generating the recommendation
model, as in [28, 29]. With 2048 users, 128 items, and ùúñ = 0.3,
we outperform GraphSC once there are at least 15000 edges.
When ùúñ = 0.1, we start outperforming them on 54000 edges.
We always do better than GraphSC when the ùúñ = 1 or higher.
PageRank: Figure 4c provides the result of running PageRank in our framework with 2048 nodes and different values of
ùúñ. With ùúñ = 0.3, we outperform GraphSC when the number
of edges are about 400000, and with ùúñ = 1 we outperform
them on just 130000 edges. In both cases, the graph is quite
sparse, compared to a complete graph of 2 million edges.
Note, though, that our comparison is slightly less favorable
for this computation. Recall, the number of dummy edges
grow with the number of nodes in the graph, and, when hiding
only in-degree in a bipartite graph, this amounts to growing
only with the number of nodes on the right. In contrast, the
runtime of GraphSC grows equivalently with any increase
in users, items, or edges, because their protocol hides any
distinction between these data types. We therefore compare
best with them when there are more users than items. When
looking at a non-bipartite graph, such as PageRank, our
protocol grows with any increase in the size of the singular
set of nodes, just as theirs does. If we increase the number of
items in matrix factorization to 2048, or decrease the number
of nodes in PageRank to 128, the comparison to GraphSC
in the resulting experiments would look similar. We let the
(a) Histogram
(b) Matrix factorization
(c) PageRank
Figure 4: Histogram and Matrix Factorization with 2048 users
and 128 types, PageRank with 2048 webpages, with varying ùúñ
reader extrapolate, and avoid the redundancy of adding such
experiments.
Large scale experiments on Amazon AWS: OblivGraph factorizes the MovieLens recommendation matrix consist of 1
million ratings provided by 6040 users to 3883 movies, in
almost 2 hours while GraphSC does it in 13 hours. We provide results of computing matrix factorization problem for
different values of ùúñ and different numbers of ratings in Table
2. We outperform the best result achieved by GraphSC, using
128 processors and 1M ratings.
Session 3B: Differential Privacy 2 CCS‚Äô18, October 15-19, 2018, Toronto, ON, Canada 501
2
0 2
1 2
2 2
3 2
4 2
5
Processors
102
103
Time(Sec)
Edges
4K
20K
Figure 5: Effect of parallelization on Matrix Factorization computation time
Effect of Parallelization: Figure 5 illustrates that the execution time can be significantly reduced through parallelization.
We achieve nearly a linear speedup in the computation time.
The lines corresponds to two different numbers of edges for
2048 users and 128 movies. Since in our these problems, the
computation is the bottleneck, parallelization can significantly
speed up the computation process. Table 1 shows the effect
of parallelization in our framework as compared to GraphSC
in terms of number of AND gates. As shown in the Table 1,
adding more processors in the GraphSC framework increases
the total number of AND Gates by some small amount. In
contrast, the size of the circuit generated in our framework is
constant in the number of processors: parallelization does not
affect total number of AND gates in the OblivGraph GAS
operations, or in DumGen.
Processors GraphSC [28] OblivGraph
|ùê∏| = 8192 |ùê∏| = 24576 |ùê∏| = 8192 |ùê∏| = 24576
1 4.047ùê∏ + 09 1.035ùê∏ + 10 2.018ùê∏ + 09 4.480ùê∏ + 09
2 4.055ùê∏ + 09 1.039ùê∏ + 10 2.018ùê∏ + 09 4.480ùê∏ + 09
4 4.070ùê∏ + 09 1.046ùê∏ + 10 2.018ùê∏ + 09 4.480ùê∏ + 09
8 4.092ùê∏ + 09 1.057ùê∏ + 10 2.018ùê∏ + 09 4.480ùê∏ + 09
Table 1: Cost of Parallelization on OblivGraph vs. GraphSC
in computing Matrix Factorization
Comparison with a Cleartext Baseline GraphSC [28] compared their execution time with GraphLab [25], a state-ofthe-art framework for running graph-parallel algorithms on
clear text. They ran Matrix Factorization using gradient
descent with input length of 32K in both frameworks and
demonstrated that GraphSC is about 200K - 500K times
slower than GraphLab when run on 2 to 16 processors. Considering our improvements over GraphSC, we estimate our
secure computation to be about 16K-32K times slower than
insecure baseline computation (GraphLab), running the same
experiments.
Oblivious Shuffle: We use an Oblivious Shuffle in our OblivGraph framework, which has a factor of log(ùëõ) less overhead
than the Bitonic sort used in GraphSC. We designed the
Oblivious Shuffle operation based on the Waksman network
[36]. The cost of shuffling is approximately ùêµùëä (ùëõ) using a
Waksman network, where ùëä (ùëõ) = ùëõ log ùëõ ‚àí ùëõ + 1 is the number of oblivious swaps required to permute ùëõ input elements,
2
0 2
1 2
2 2
3 2
4 2
5
Processors
0
1
2
3
4
5
6
# AND Gates
√ó108
16K
DumGen
Shuffle
Gather
Apply
Scatter
Figure 6: Cost of each operation in OblivGraph for Matrix
Factorization
and ùêµ indicates the size of the elements being shuffled. In the
original Waksman switching network, the size of the input, ùëõ,
is assumed to be a power of two. However, in order to have
an Oblivious Shuffle for arbitrary sized input, we must use
an improved version of the Waksman network proposed in
[1] which is called AS-Waksman (Arbitrary-Sized Waksman).
In our current set of experiments, we have only implemented
the original version of the Waksman network and have not
implemented AS-Waksman. We interpolate precisely to determine the size of arbitrary AS-Waksman when using arbitrary
sized input. Details of this appear in the full version of the
paper.
Cost of each operation in OblivGraph framework: In order to
understand how expensive the DumGen and OblivShuffle procedures are, as compared to other GAS model operations, we
show the number of AND gates for each of these procedures
in Figure 6. The figure corresponds to Matrix Factorization
problem, with 2048 users, 128 movies and 20K ratings, with
epsilon 0.5. The cost of a single iteration in the OblivGraph
framework is first dominated by the Apply operation which
computes the gradient descent and second by Oblivious Shuffle. Figure shows the effect of parallelization on decreasing the
circuit size of each operation. See the full version to compare
the cost of DumGen procedure in different protocols.
OblivGraph GraphSC[28]
ùúñ=0.3 ùúñ=0.5 ùúñ=1
# Real Edges 1.2ùëÄ 1.5ùëÄ 1.8ùëÄ 1ùëÄ
Time(hours) 2.2 2.3 2.4 13
Table 2: Runtime of a single iteration of OblivGraph vs.
GraphSC to solve matrix factorization problem in scale, with
real-world dataset, MovieLens with 6040 users ranked 3883
movies
5 CONCLUSION AND OPEN PROBLEMS
We have established a new tradeoff between privacy and
efficiency in secure computation by defining a new security
model in which the adversary is provided some leakage that
is proven to preserve differential privacy. We show that this
Session 3B: Differential Privacy 2 CCS‚Äô18, October 15-19, 2018, Toronto, ON, Canada 502
leakage allows us to construct a more efficient protocol for
a broad class of computations: those that can be computed
in graph-parallel frameworks such as MapReduce. We have
evaluated the impact of our relaxation by comparing the performance of our protocol with the best prior implementation
of secure computation for graph-parallel frameworks.
Our work demonstrates that differentially private leakage is
useful, in that it provides opportunity for more efficient protocols. The protocol we present has broad applicability, but
we leave open the very interesting question of determining,
more precisely, for which class of computations this leakage
might be help. Graph-parallel algorithms have the property
that the access pattern to memory can be easily reduced to
revealing only a histogram of the memory that is accessed,
and histograms are the canonical example in the differential
privacy literature. Looking at other algorithms will likely
introduce very interesting leakage functions that are new
to the differential privacy literature, and security might not
naturally follow from known mechanisms in that space.