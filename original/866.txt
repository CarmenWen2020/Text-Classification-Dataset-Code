Recently, there has been a wealth of effort devoted to the design of
secure protocols for machine learning tasks. Much of this is aimed
at enabling secure prediction from highly-accurate Deep Neural
Networks (DNNs). However, as DNNs are trained on data, a key
question is how such models can be also trained securely. The few
prior works on secure DNN training have focused either on designing custom protocols for existing training algorithms, or on
developing tailored training algorithms and then applying generic
secure protocols. In this work, we investigate the advantages of
designing training algorithms alongside a novel secure protocol,
incorporating optimizations on both fronts. We present QUOTIENT,
a new method for discretized training of DNNs, along with a customized secure two-party protocol for it. QUOTIENT incorporates
key components of state-of-the-art DNN training such as layer normalization and adaptive gradient methods, and improves upon the
state-of-the-art in DNN training in two-party computation. Compared to prior work, we obtain an improvement of 50X in WAN
time and 6% in absolute accuracy.
CCS Concepts
• Security and privacy → Domain-specific security and privacy
architectures; • Computing methodologies → Neural networks.
Keywords
Secure multi-party computation, Privacy-preserving deep learning,
Quantized deep neural networks
1 Introduction
The field of secure computation, and in particular Multi-Party Computation (MPC) techniques such as garbled circuits and lower level
primitives like Oblivious Transfer (OT) have undergone very impressive developments in the last decade. This has been due to
a sequence of engineering and theoretical breakthroughs, among
which OT Extension [26] is of special relevance.
However, classical generic secure computation protocols do not
scale to real-world Machine Learning (ML) applications. To overcome this, recent works have combined different secure computation techniques to design custom protocols for specific ML tasks.
This includes optimization of linear/logistic regressors and neural networks [14, 40, 41, 44], matrix factorization [43], constrained
optimization [29], and k-nearest neighbor classification [51, 52].
For example, Nikolaenko et. al. [44] propose a protocol for secure
distributed ridge regression that combines additive homomorphic
encryption and garbled circuits, while previous works [14, 41] rely
in part on OT for the same functionality.
Practical ML and Secure Computation: Two Ships Passing in
the Night. While there have been massive practical developments
in both cryptography and ML (including the works above), most
recent works for model training [22, 40, 41, 54] and prediction [7,
15, 50] on encrypted data are largely based on optimizations for
either the ML model or the employed cryptographic techniques in
isolation. In this work, we show that there is a benefit in taking a
holistic approach to the problem. Specifically, our goal is to design
an optimization algorithm alongside a secure computation protocol
customized for it.
Secure Distributed Deep Neural Network Training. So far there
has been little work on training Deep Neural Networks (DNNs) on
encrypted data. The only works that we are aware of are ABY3 [40]
and SecureML [41]. Different from this work, ABY3 [40] designs
techniques for encrypted training of DNNs in the 3-party case,
and a majority of honest parties. The work most similar to ours is
SecureML [41]. They propose techniques based on secret-sharing
to implement a stochastic gradient descent procedure for training linear/logistic regressors and DNNs in two-party computation.
While the presented techniques are practical and general, there are
three notable downsides: 1. They require an “offline” phase, that
Session 6B: ML Security II CCS ’19, November 11–15, 2019, London, United Kingdom 1231
while being data-independent, takes up most of the time (more than
80 hours for a 3-layer DNN on the MNIST dataset in the 2-Party
Computation (2PC) setting); 2. Their techniques are not practical
over WAN (more than 4277 hours for a 3-layer DNN on the MNIST
dataset), restricting their protocols to the LAN setting; 3. The accuracy of the obtained models are lower than state-of-the-art deep
learning methods. More recently, secure training using homomorphic encryption has been proposed [22]. While the approach limits
the communication overhead, it’s estimated to require more than
a year to train a 3-layer network on the MNIST dataset, making it
practically unrealizable. Furthermore, the above works omit techniques necessary for modern DNNs training such as normalization
& adaptive gradient methods, instead relying on vanilla SGD with
constant step size. In this paper we argue that significant changes
are needed in the way ML models are trained in order for them to be
suited for practical evaluation in MPC. Crucially, our results show
that securely-trained DNNs do not need to take a big accuracy hit
if secure protocols and ML models are customized jointly.
Our Contributions: In this work we present QUOTIENT, a new
method for secure two-party training and evaluation of DNNs.
Alongside we develop an implementation in secure computation
with semi-honest security, which we call 2PC-QUOTIENT. Our
main insight is that recent work on training deep networks in
fixed-point for embedded devices [57] can be leveraged for secure
training. Specifically, it contains useful primitives such as repeated
quantization to low fixed-point precisions to stabilize optimization.
However, out of the box this work does not lead to an efficient MPC
protocol. To do so, we make the following contributions, both from
the ML and the MPC perspectives:
1. We ternarize the network weights: W ∈ {−1, 0, 1} during the
forward and backward passes. As a result, ternary matrix-vector
multiplication becomes a crucial primitive for training. We then
propose a specialized protocol for ternary matrix-vector multiplication based on Correlated Oblivious Transfer that combines
Boolean-sharing and additive-sharing for efficiency.
2. We further tailor the backward pass in an MPC-aware way, by
replacing operations like quantization and normalization by alternatives with very efficient implementations in secure computation.
We observe empirically in Section 6 that this change has no effect
on accuracy. Alongside these changes, we extend the technique to
residual layers [21], a crucial building block for DNNs.
3. We design a new fixed-point optimization algorithm inspired
by a state-of-the-art floating-point adaptive gradient optimization
procedure [47].
4. We implement and evaluate our proposal in terms of accuracy
and running time on a variety of real-world datasets. We achieve
accuracy nearly matching state-of-the-art floating point accuracy
on 4 out of 5 datasets. Compared to state of the art 2PC secure DNN
training [41], our techniques obtain ∼6% absolute accuracy gains
and >50× speedup over WAN for both training and prediction.
The rest of the paper is organized as follows. In the next section
we introduce notation and necessary background in ML and MPC.
In Section 3 we present our proposed neural network primitives,
and corresponding training procedure. In Section 4 we present a
2PC protocol for it. Finally, we present our experimental evaluation
in Section 5 and conclude with a short discussion.
convolutional
f(Wl
al1)
al1
al
fully-connected residual
f(Wl
al1)
f(Wl+1al
)
Wl+i
al+i1
al1
f(Wl+i
al+i1 + al1)
f(Wl
ijal1
ij )
al
ij
al1
ij
Figure 1: We have protocols for three popular deep neural
network layers:fully-connected, convolutional, and residual.
2 Overview and Problem Description
Here, we introduce concepts in DNN training and inference, fixedpoint encodings, and MPC applied to ML.
2.1 Deep Neural Networks
All DNNs are defined by a core set of operations, called a layer.
These layers are repeatedly applied to an input a
0
to produce a
desired output a
L
, where the number of repetitions (or layers) L
is called the depth of the network. Every layer consists of a linear
operation and a non-linear operation and, depending on the type
of network, each layer takes on different forms. We describe three
popular layer types that make up a large portion of state-of-the-art
DNNs: fully-connected, convolutional, and residual.
Fully-Connected Layers. To define a layer we simply need to define the linear and non-linear operations they use. In fully-connected
layers these operations are as follows. Given an input a
l−1 ∈ R
hl−1
of any fully-connected layer l ∈ {1, . . . , L}, the layer performs
two operations (1) a multiplication: Wl
a
l−1 with a weight matrix
Wl ∈ R
hl ×hl−1
; and (2) a non-linear operation, most commonly the
Rectified Linear Unit: ReLU(x) = max(x, 0). So the full layer computation is a
l = ReLU(Wl
a
l−1
), where a
l
is called the activation
of layer l. Note that ‘fully-connected’ refers to the fact that any
entry of a
l−1
is ‘connected’ to the output a
l via the weight matrix
Wl
, shown schematically in Figure 1 (left). Note that, more generic
representations involve an intermediate step; adding a bias term b
l
to compute (Wl
a
l−1 + b
l
) before performing the non-linear operation (a
l = ReLU(Wl
a
l−1 + b
l
)). In practice, this can be handled by
suitably modifying Wl
and a
l−1 prior to performing operation (1).
Convolutional Layers. Convolutional layers are essentially fullyconnected layers with a very particular connectivity structure.
Whereas the input and output of a fully-connected layer are vectors, the input and output of a convolutional layer are 3rd-order
tensors (i.e., an array of matrices). Specifically, the input a
l−1 ∈
Session 6B: ML Security II CCS ’19, November 11–15, 2019, London, United Kingdom 1232
R
hl−1×wl−1×cl−1 can be thought of as an image with height hl−1
,
width wl−1
, and channels cl−1
(e.g., cl−1 =3 for RGB images).
To map this to an output a
l ∈ R
hl ×wl ×cl
, a convolutional layer
repeatedly looks at small square regions of the input, and moves this
region from left-to-right, from top-to-bottom, until the entire image
has been passed over. Let a
l−1
ij ∈ R
kl−1×kl−1×cl−1 be the kl−1 × kl−1
region starting at entry (i, j). This is then element-wise multiplied
by a set of weights Wl ∈ R
kl ×kl ×cl−1×cl and summed across the
first three dimensions of the tensor. This is followed by a nonlinear operation, also (most often) the ReLU to produce an entry
of the output a
l
ij ∈ R
1×1×1×cl
. Note that a
l−1
ij , a
l
ij , Wl
can all be
vectorized such that a
l
ij = f (Wl
a
l−1
ij ). This operation is shown
in Figure 1 (center). Apart from the dimensionality of the layers,
other hyperparameters of these networks include: the size of square
region, the number of ‘pixels’ that square regions jump in successive
iterations (called the stride), and whether additional pixels with
value 0 are added around the image to adjust the output image size
(called zero-padding).
A common variant of a convolutional layer is a pooling layer. In
these cases, the weights are always fixed to 1 and the non-linear
function is simply f (·) ≜ max{·} (other less popular f (·) include
simple averaging or the Euclidean norm).
Residual Layers. The final layer type we consider are residual
layers [21]. Residual layers take an activation from a prior layer
a
l−1
and add it to a later layerl+i before the non-linear function f (·)
(usually ReLU) is applied: f (Wl+i
a
l+i−1 + a
l−1
). The intermediate
layers are usually convolutional but may be any layer in principle.
Figure 1 (right) shows an example of the residual layer. DNNs
with residual layers were the first to be successfully trained with
more than 50 layers, achieving state-of-the-art accuracy on many
computer vision tasks, and are now building blocks in many DNNs.
Training Deep Neural Networks. The goal of any machine learning classifier is to map an input a
0
to a desired output y (often called
a label). To train DNNs to learn this mapping, we would like to
adjust the DNNs weights {Wl
}
L
l=1
so that the output a
L
after L
layers is: a
L = y. To do so, the most popular method for training
DNN weights is via Stochastic Gradient Descent (SGD). Specifically, given a training dataset of input-output pairs {(a
0
i
,yi)}n
i=1
,
and a loss function ℓ(a
L
,y) that measures the difference between
prediction a
L
and output y (e.g., the squared loss: (a
L − y)
2
). SGD
consists of the following sequence of steps: (1) The randomization
step: sample a random single input a
0
i
, (2) The forward pass: pass a
0
i
through the network to produce prediction a
L
i
, (3) The backward
pass: compute the gradients Gl
and e
l of the loss ℓ(a
L
i
,yi) with
respect to each weight Wl
and layer activation a
l
in the network,
respectively: Gl =
∂ℓ(a
L
i
,yi )
∂Wl
, e
l =
∂ℓ(a
L
i
,yi )
∂a
l
, ∀l ∈ [L], (4) The update step: update each weight by this gradient: Wl = Wl − ηGl
,
where η is a constant called the learning rate. These four steps,
called an iteration are repeated for each input-output pair in the
training set. A pass over the whole training set is called an epoch.
The first step is often generalized to sample a set of inputs, called a
batch, as this exploits the parallel nature of modern GPU hardware
and has benefits in terms of convergence and generalization.
2.1.1 State-Of-The-Art Training: Normalization & Adaptive Step–
Sizes. While the above “vanilla” gradient descent can produce reasonably accurate classifiers, alone they do not produce state-of-theart results. Two critical changes are necessary: (1) normalization;
and (2) adaptive step-sizes. We describe each of these in detail.
Normalization. The first normalization technique introduced for
modern DNNs was batch normalization [25]. It works by normalizing the activations a
l of a given layer l so that across a given batch
of inputs a
l has roughly zero mean and unit standard deviation.
There is now a general consensus in the machine learning community that normalization is a key ingredient to accurate DNNs [5].
Since batch normalization, there have been a number of other successful normalization schemes including weight normalization [49]
and layer normalization [35]. The intuition behind why normalization helps is because it prevents the activations a
l
from growing
too large to destabilize the optimization, especially for DNNs with
many layers that have many nested multiplications. This means
that one can increase the size of the learning rate η (see the update step above for how the learning rate is used in SGD), which
speeds up optimization [5]. Normalization has been shown to yield
speedups of an order of magnitude over non-normalized networks.
Further, without normalization, not only is convergence slower, in
some cases one cannot even reach lower minima [35].
Adaptive Step-Sizes. While normalization allows one to use larger
learning rates η, it is still unclear how to choose the correct learning rate for efficient training: too small and the network takes an
impractical amount of time to converge, too large and the training
diverges. To address this there has been a very significant research
effort into designing optimization procedures that adaptively adjust the learning rate during training [13, 31, 47]. So-called adaptive gradient methods scale the learning rate by the magnitude of
gradients found in previous iterations. This effectively creates a
per-dimension step-size, adjusting it for every entry of the gradient
Gl
for all layers l. Without adaptive gradient methods, finding the
right step-size for efficient convergence is extremely difficult. A
prominent state-of-the-art adaptive gradient method is AMSgrad
[47]. It was developed to address pitfalls with prior adaptive gradient methods. The authors demonstrate that it is able to converge
on particularly difficult optimization problems that prior adaptive
methods cannot. Even on datasets where prior adaptive methods
converge, AMSgrad can cut the time to converge in half.
2.1.2 Training and Inference with Fixed-Point Numbers. Motivated
by the need to deploy DNNs on embedded and mobile devices with
limited memory and power, significant research effort has been
devoted to model quantization and compression. Often the goal
is to rely solely on fixed-point encoding of real numbers. In fact,
Tensorflow offers a lightweight variant to address this goal [19, 27].
These developments are useful for secure prediction. This is because
cryptographic techniques scale with the circuit representation of
the function being evaluated, and so a floating-point encoding and
subsequent operations on that encoding are extremely costly. However, for the task of training, there are few works that perform all
operations in fixed-point [20, 23, 33, 39, 57]. We start by reviewing fixed-point encodings, and the MPC techniques that we will
Session 6B: ML Security II CCS ’19, November 11–15, 2019, London, United Kingdom 1233
consider. Then, in Section 3, we describe how this method can be
modified and improved for secure training.
Notation for Fixed-Point Encodings. We represent fixed-point
numbers as a triple x = (a, ℓ,p), where a ∈ {−2
ℓ−1 − 1, . . . , 2
ℓ−1
}
is the fixed-point integer, ℓ is its range or bit-width, and p is its
precision. The rational number encoded by x is a/2
p
. For simplicity,
we will often make ℓ implicit and write the rational a/2
p
as a(p)
.
As our computations will be over ℓ-bit values in 2’s complement,
overflows/underflows can happen, resulting in large big errors. This
requires that our training procedure is very stable, within a very
controlled range of potential values.
2.2 MPC for Machine Learning
In this section, we introduce MPC techniques, highlighting the
trade-offs that inspire our design of secure training and prediction
protocols. The goal of MPC protocols is to compute a public function
f (·) on private data held by different parties. The computation is
done in a way that reveals the final output of the computation to
intended parties, and nothing else about the private inputs. MPC
protocols work over a finite discrete domain, and thus the function
f (·) must be defined accordingly. Generally, MPC protocols can be
classified depending on (i) a type of structure used to represent f (·)
(generally either Boolean or integer-arithmetic circuits) and (ii) a
scheme to secretly share values between parties, namely Booleansharing, additive-sharing, Shamir-secret-sharing, and more complex
variants (for more information see [11]). The choices (i) and (ii)
define the computational properties of an MPC protocol.
Additive-sharing protocols are very efficient for computations
over large integral domains that do not involve comparisons. This
includes sequences of basic linear algebra operations, such as matrixvector multiplications. Specifically, additions are extremely cheap
as they can be performed locally, while multiplications are more
expensive. On the other hand, computations involving comparisons
require computing a costly bit-decomposition of the values.
In contrast to additive-sharing, protocols based on Booleansharing are well-suited for computations easily represented as
Boolean circuits, such as division/multiplication by powers of two
(via bit shifting), comparisons, and sign(). They are slower at addition and multiplication which require adder and multiplier circuits.
These trade-offs lead to a natural idea recently exploited in several works in the secure computation for ML: one should design
protocols that are customized to full algorithms, such as the training
of linear/logistic regressors and DNNs [14, 41, 44], matrix factorization [43], or k-nearest neighbor classification [51, 52]. Moreover, custom protocols alternate between different secret-sharing
schemes as required by the specific computation being implemented.
Of course, the transformations between secret-sharing schemes
must themselves be implemented by secure protocols1
. This point
is especially relevant to DNN training, as it amounts to a sequence
of linear operations (which are naturally represented as arithmetic
circuits) interleaved with evaluations of non-linear activation functions such as the ReLU (which are naturally represented as Boolean
circuits).
Some MPC frameworks work in the so-called pre-processing
model (see [41]), where computation is split into a data-independent
1This aspect was thoroughly investigated in [11].
offline phase and a data-dependent online phase. Random values
useful for multiplication can be generated offline, and then used
for fast secure multiplication online. In this paper we consider total
time, removing the assumption of an offline phase. This is not a
fundamental limitation, as we have variants of our protocols that
work in the pre-processing model as explained later.
Notation for Secret-Sharing. In this work, we focus on the twoparty computation setting, which excludes solutions that rely on
a majority of honest parties [6, 40]. Inspired by work on functionspecific protocols, in this work we employ both Boolean sharing
and additive sharing. We start by fixing two parties P1 and P2. We
denote the Boolean-share of x ∈ {0, 1} held by P1 as ⟨x⟩1, and ⟨x⟩2
for P2. In Boolean-sharing, the shares satisfy ⟨x⟩1 = x ⊕ ⟨x⟩2, where
⟨x⟩2 is a random bit, and ⊕ signifies the XOR operation. We denote
the additive-share of integer y ∈ Zq held by P1 as [[y]]1, and [[y]]2
for P2. Here [[y]]1 = y − [[y]]2, with random [[y]]2 ∈ Zq. In practice,
q is 2
σ
, for σ ∈ {8, 16, 32, 64, 128}, as these are word lengths offered
in common architectures.
Garbled Circuits. In many of our protocols, we use Yao’s Garbled
Circuits [58] as a subprotocol. In this protocol, the computation is
represented as a Boolean circuit. We will not introduce the protocol
in detail here, and instead, refer the reader to [37] for a detailed
presentation and security analysis. The relevant observation to our
work is the fact that the running time of a Garbled Circuits protocol
is a function of the number of non-XOR gates in a circuit (this is
thanks to the Free-XOR technique [32]). Consequently, designing
efficient, largely XOR circuits is crucial in this setting. In fact, it is
so important that previous works have used digital synthesizers
for this task [10].
Oblivious Transfer. OT is a cryptographic primitive involving
two parties: a Chooser, and a Sender. The Sender holds two messages m0,m1, and the Chooser holds a Boolean value b. After the
execution of OT, the Chooser learns mb
, i.e. the Sender’s message
corresponding to their Boolean value. From the privacy perspective,
an OT protocol is correct if it guarantees that (i) the Chooser learns
nothing about m1−b
, and (ii) the Sender learns nothing about b. As
common in MPC, this is formalized in the simulation framework
(see [17] for details).
OT is a basic primitive in MPC. In fact, any function can be
evaluated securely using only an OT protocol [18] and, moreover
OT is a crucial component of Yao’s Garbled Circuits. A remarkable
advancement in the practicality of OT protocols was the discovery
of OT extension [26]. This protocol allows one to compute a small
number of OTs, say 128, and then bootstrap them to execute many
fast OTs. Since then, optimizations in both the base OTs and the
OT extension procedure [3] have led to implementations that can
perform over ten million OT executions in under one second [55].
In our protocols, we employ a more efficient primitive—Correlated
Oblivious Transfer (COT). COT was introduced in [3] alongside
with an efficient COT Extension protocol that uses roughly half the
communication than the general OT Extension. COT is a particular
case of OT where the Sender does not get to choose its messages,
but instead chooses a function f relating the two messages m0
and m1, as m0 = f (m1). This functionality is enough for important
applications of OT in MPC such as Garbled Circuits and OT-based
Session 6B: ML Security II CCS ’19, November 11–15, 2019, London, United Kingdom 1234
triplet generation (see [11]). Below we define the flavour of COT
that we need in our application, following the notation from [3].
Definition 2.1 (m × COTℓ
). Let f = (fi)i ∈[m]
, be a sequence of
correlation functions, each with signature fi
: {0, 1}
ℓ
7→ {0, 1}
ℓ
held by party P1 (the Sender), and let w ∈ {0, 1}
ℓ
be a sequence
of choice bits held by party P2 (the Chooser). After an execution of
m × COTℓ
(f, w), P1 obtains m random vectors (xi ∈ {0, 1}
ℓ
)
m
i=1
, and
P2 obtains m vectors (
¯
yi ∈ {0, 1}
ℓ
)
m
i=1
such that ∀i ∈ [m] : yi =
(¬wi) · xi + wi
· fi(xi).
2.3 Threat Model
In the two-party model of MPC, the training procedure is outsourced to two servers. This framework has been used in several
previous works [14, 29, 41–44]. It works by first secret-sharing
the training dataset D across the servers. This is depicted in Figure 2 (Left): users secret-share their values (a
0
i
,yi) across two noncolluding servers, which run the training procedure and obtain the
resulting secret-shared model: [[W]]1, [[W]]2. Note that the scenario
where two organizations collaborate to build a model of their respective private data in a privacy-preserving way is a particular
case of this setting: this corresponds to the stage after the users
have shared their data. We present fast MPC protocols to implement
TrainModel(D) from Figure 2 (Left). Thus, all our protocols are
presented as two-party protocols between parties P1 and P2 via
input and output additive-shares. Our protocols are secure in the
semi-honest model for secure computation, as in [41].
Alongside private training, an important related problem is private prediction. This is depicted in Figure 2 (Right). Specifically, a
client C has private data a
0
for which they wish to have a private
prediction y, via the private weights W of server S. In our setting,
a fast protocol for two-party training in MPC immediately yields a
fast protocol for private prediction, as prediction corresponds to
the forward pass in training (i.e., Algorithm 1).
3 Deep Learning for MPC
In this section, we describe new methods to optimize DNNs that
were developed alongside our protocols (in Section 4). Our first
insight is that recent work on training deep networks in fixed-point
[57] can be leveraged for crypto-friendly training. Namely, while
this work was originally intended for embedded devices, it contains
useful primitives such as repeated quantization to low fixed-point
precisions to stabilize optimization. However, out-of-the-box, this
work is unsuited for privacy-preserving protocols. We make the
following modifications: (a) We ternarize the network weights:
W ∈ {−1, 0, 1} during the forward and backward passes. This will
allow matrix multiplication with W to be phrased as repeated 1-outof-2 oblivious transfers (described in Section 4); (b) We construct
an MPC-friendly quantization function for the weight gradients in
the backward pass, replacing a biased coin flip and truncation steps
with a saturation-free quantization, without loss in accuracy; (c)
We replace the backward pass normalization operation, division
by the closest-power-of-two, with the next-power-of-two. While
a seemingly small change, the latter operation has a very efficient
circuit implementation [28] that we leverage in our secure protocols
in the next section. Further, we observe empirically in Section 5
that this change also has no effect on accuracy. Ultimately, these
changes will only speed up the computation of a training iteration in
a secure protocol. If training is based on stochastic gradient descent,
many iterations will still be necessary for convergence. To address
this, we design a new fixed-point adaptive gradient algorithm. It
is inspired by a state-of-the-art floating-point adaptive gradient
method, AMSgrad [47]. This optimization allows us to achieve the
best accuracy to date on all 5 datasets we consider for DNNs trained
in fixed-point (in Figure 8). In this section, we describe the work of
[57], our changes (a-c) mentioned above, and our new fixed-point
adaptive gradient training procedure.
3.1 Deep Models in Fixed-Point
The work by Wu et al. [57] describes an efficient optimization
procedure for DNNs operating entirely on fixed-point encodings of
real numbers. We describe it in detail here.
Quantize gradients, and quantize frequently. The first idea of
WAGE [57] is to introduce functions Qw ,Qa,Qд,Qe that quantize
the weights W, activations a, weight gradients G, activation gradients e to a small, finite set of fixed-point numbers. While previous
work [2, 4, 9, 24, 30, 36, 46, 56] had already introduced the idea
of functions Qw ,Qa to quantize weights and/or activations in the
forward pass, they required the weights W or gradients G, e to
be represented in floating-point in the backward pass, in order to
optimize accurately. In Wu et al. [57], all the quantization functions
take fixed-point numbers with some precision p, i.e., v(p) = v/2
p
and find the nearest number with another precision q, i.e., v(q)
:
v(q) = N(v(p)
,q) =
⌊
v
2
p 2
q
⌉
2
q
(1)
where a
2
p 2
q
is in practice either a division of a multiplication by
2
|p−q | depending on whether p > q. Additionally, Wu et al. [57]
introduce a saturation function S(·), to yield Q(·) as:
v(q) = Q(v(p)
,q) = S(N(v(p)
,q),q). (2)
where S(x,q) = min(max(x, −1 + 2
−q
), 1 − 2
−q
) saturates any x to
be within [−1 + 2
−q
, 1 − 2
−q
].
Weight QuantizationQw . The first function W(pw ) = Qw (W(p)
,pw )
takes as input a p-precision fixed-point weight W(p) and returns
the closest pw -precision weights W(pw ) using the above function:
W(pw ) = Qw (W(p)
,pw ) = Q(W(p)
,pw ) (3)
Activation quantizationQa. The second function a(pa ) = Qa(a(p)
,pa)
is almost identical except it introduces an additional scaling factor
2
−α
as follows:
a(pa ) = Qa(a(p)
,pa) = Q(
a(p)
2
α
,pa). (4)
Where α is a fixed integer determined by the dimensionality of
W, see [57] eq. (7). The intuition is that this is a simple form of
normalization (used in the forward pass), and the authors describe
a technique to set α based on the layer size.
Activation gradient quantizationQe . For the backward pass the
first gradient we must consider is that of the activations, which we
call e. The strategy to quantize e(pe ) = Qe (e(p)
,pe ), is similar to
quantizing the activations. Except here the scaling factor depends
Session 6B: ML Security II CCS ’19, November 11–15, 2019, London, United Kingdom 1235
User i
P1 P2
[[(a
0
,y)]]1 [[(a
0
,y)]]2
W = TrainModel(D)
MPC
[[D]]1 [[D]]2
[[W]]1 [[W]]2
C S
y = predict(W, a
0
)
MPC
Data a
0 Model W
y
Figure 2: (Left) Training in the two-server model of MPC: (i) Each user i shares their labeled data (a
0
i
,yi) across two servers P1
and P2, by giving one share [[(a
0
i
,yi)]]j to each Pj
. (ii) Each Pj compiles their share of a training dataset D, by simply accumulating
all shares received from users. Finally, (iii) P1 and P2 engage in a multi-party computation, in which D is securely reconstructed,
and subsequently used to train a model W, from which each server gets a share. (Right) Private prediction using MPC: A client
C and a server S engage in an multi-party computation protocol for the client to obtain y (the prediction of the server’s model
W for their client’s data a
0
) without the parties disclosing anything about W and a
0
to each other.
Algorithm 1: Fixed-Point Forward Pass (Forward)
Input: Fixed-point weights {Wl
(pw )
}
L
l=1
,
Fixed-point data sample a
0
(pa )
,
Layer-wise normalizers {αl
}
L
l=1
Output: Fixed-point activations {a
l
(pa )
}
L
l=1
1: for l = 1, . . . , L do
2: a
l = f (Wl
(pw )
a
l−1
(pa )
) ▷ pass through layer
3: a
l
(pa )
= Qa(a
l
,pa) ▷ activations precision pa
Algorithm 2: Fixed-Point Backward Pass (Backward)
Input: Fixed-point weights: {Wl
(pw )
}
L
l=1
,
Fixed-point activations (act) {a
l
(pa )
}
L
l=1
,
Fixed-point label y(pa )
,
Activation function f ,
Saturation function S
Output: Fixed-point gradients {Gl
}
L
l=1
1: e
L =
∂ℓ(a
L
(pa )
,y(pa )
)
∂a
L
(pa )
▷ act. gradient
2: for l = L, . . . , 1 do
3: e
l
(pe )
= Qe (e
l
,pe ) ▷ act. gradient precision pe
4: e
l−1 = Wl
(pw )

e
l
(pe )
◦
∂a
l
(pa )
∂f
◦
∂a
l
(pa )
∂S

▷ act. gradient
5: Gl = a
l−1
(pa )
⊤
e
l
(pe )
▷ weight gradient
on the largest value of e:
e(pe ) = Qe (e(p)
,pe ) = Q(
e(p)
2
cpow(max{ |e(p)
| }) ,pe ) (5)
where cpow(x) = 2
⌊log2
(x)⌉ is the closest power of 2 to x. The
intuition here is again that normalization helps, but it cannot be
constant as the gradient magnitude can potentially fluctuate. Thus
normalization needs to be data-specific to stabilize training.
Weight gradient quantization Qд. The second gradient is the
weight gradient G, quantized by G(pд) = Qд(G(p)
,pд), as follows:
G(pд) = Qд(G
n
(p)
,pд) =
sign(Gn
(p)
)
2
pд−1
"
⌊|G
n
(p)
|⌋ + B

|G
n
(p)
| − ⌊|G
n
(p)
|⌋
#
.
(6)
where Gn
(p)
= ηG(p)/2
cpow(max{ |G(p)
| }) is a normalized version of
G(p)
that is also multiplied by the learning rate η, sign(a) returns
the sign of a, and B(p) draws a sample from a Bernoulli distribution
with parameter p. This function normalizes the gradient, applies
a randomized rounding, and scales it down. The idea behind this
function is that additional randomness (combined with the randomness of selecting data points stochastically during training) should
improve generalization to similar, unseen data.
3.2 Cryto-Friendly Deep Models in Fixed-Point
We propose three changes to the quantization functions that do not
affect accuracy, but make them more suitable for MPC.
Weight quantization Qw . We fix pw =1 and set Qw (W(p)
,pw ) =
2Q(W(p)
,pw ) so that our weights are ternary: W ∈ {−1, 0, 1} as
mentioned above.
Activation gradient quantization Qe . We make the following
change to eq. (5) to Qe (e(p)
,pe ) = Q(e(p)/2
npow(max{ |e(p)
| })
,pe ),
where npow(x) = 2
⌈log2
(x)⌉ is the next power of 2 after x. This is
faster than the closest power of 2, cpow(x) as the latter also needs
to compute the previous power of 2 and compare them to x to find
the closest power.
Weight gradient QuantizationQд. We introduce a different quantization function than [57] which is significantly easier to implement using secure computation techniques:
G(pд) = Qд(G(p)
,pд) = N(
G(p)
2
npow(max{ |G(p)
| }) ,pд). (7)
We find that the original quantization function in eq. (6) needlessly
removes information and adds unnecessary overhead to a secure
implementation.
Forward/Backward passes. Algorithm 1 corresponds to prediction in DNNs, and Algorithms 1, 2, 3 describe the training procedure.
Session 6B: ML Security II CCS ’19, November 11–15, 2019, London, United Kingdom 1236
Algorithm 3: Fixed-Point SGD Optimization (∆sgd)
Input: Fixed-point weights {Wl
(pw )
}
L
l=1
,
Fixed-point data D = {(a
0
(pa ),i
,y(pa ),i
}
n
i=1
,
Learning rate η
Output: Updated weights {Wl
(pw )
}
L
l=1
1: for t = 1, . . . ,T do
2: (a
0
(pa )
,y(pa )
) ∼ D
3: {Wl
(pw )
= Qw (Wl
(pw )
,pw )}L
l=1
4: {a
l
(pa )
}
L
l=1
= Forward({Wl
(pw )
}
L
l=1
, a
0
(pa )
)
5: {Gl
}
L
l=1
= Backward({Wl
(pw )
, a
l
(pa )
}
L
l=1
,y(pa )
)
6: {Gl
(pд)
= Qд(Gl
,pд)}L
l=1
▷ weight gradient precision pд
7: {Wl
(pw )
= S(Wl
(pw )
− ηGl
(pд)
,pw )}L
l=1
Apart from the quantization functions note that in the backward
pass (Algorithm 2) we take the gradient of the activation with
respect to the activation function: ∂a
l
(pa )
/∂f and the saturation
function: ∂a
l
(pa )
/∂S. Algorithm 3 describes a stochastic gradient
descent (SGD) algorithm for learning fixed-point weights inspired
by [57]. We keep around two copies of weights in different precisions pw and pw . The weights W(pw ) are ternary and will enable
fast secure forward (Algorithm 1) and backward passes. The other
weights W(pw )
are at a higher precision and are updated with gradient information. We get the ternary weights W(pw ) by quantizing
the weights W(pw )
in line 3. Note that the forward and backward
passes of convolutional networks can be written using the exact
same steps as Algorithms 1 and 2 where weights are reshaped to
take into account weight-sharing. Similarly, for residual networks
the only differences are: (a) line 2 in the forward pass changes; and
(b) line 4 in the backward pass has an added term from prior layers.
3.3 A Fixed-Point Adaptive Gradient Algorithm
One of the state-of-the-art adaptive gradient algorithms is AMSgrad
[47]. However, AMSgrad includes a number of operations that are
possibly unstable in fixed-point: (i) the square-root of a sum of
squares, (ii) division, (iii) moving average. We redesign AMSgrad in
Algorithm 4 to get around these difficulties in the following ways.
First, we replace the square-root of a sum of squares with absolute
value in line 9, a close upper-bound for values v ∈ [−1, 1]. We verify
empirically that this approximation does not degrade performance.
Second, we replace division of Vˆ
(pv )
in line 11 with division
by the next power of two. Third, we continuously quantize the
weighted moving sums of lines 7 and 8 to maintain similar precisions throughout. These changes make it possible to implement
AMSgrad in secure computation efficiently. In the next section we
describe the protocols we design to run Algorithms 1-4 privately.
4 Oblivious Transfer for Secure Learning
In this section, we present custom MPC protocols to implement private versions of the algorithms introduced in the previous section.
Algorithm 4: Fixed-Point AMSgrad Optim. (∆ams)
Input: Fixed-point weights {Wl
(pw )
}
L
l=1
,
Fixed-point data D = {(a
0
(pa ),i
,y(pa ),i
}
n
i=1
, learning rate η
Output: Updated weights {Wl
(pw )
}
L
l=1
1: Initialize: {Ml
, V
l
}
L
l=1
= 0
2: for t = 1, . . . ,T do
3: (a
0
(pa )
,y(pa )
) ∼ D
4: {Wl
(pw )
= 2Q(Wl
(pw )
,pw )}L
l=1
5: {a
l
(pa )
}
L
l=1
= Forward({Wl
(pw )
}
L
l=1
, a
0
(pa )
)
6: {Gl
}
L
l=1
= Backward({Wl
(pw )
, a
l
(pa )
}
L
l=1
,y(pa )
)
7: {Gl =
Gl
2
npow(max{|Gl
|}) }
L
l=1
▷ scale gradients
8: {Ml = N(0.9Ml
,pm) + 0.1Gl
}
L
l=1
▷ weighted mean
9: {V
l = N(0.99V
l
,pv ) + 0.01

Gl

}
L
l=1
10: {Vˆl = max(Vˆl
, V
l
)}L
l=1
11: {Gl =
Ml
2
npow(|Vˆl
|+ϵ)
}
L
l=1
▷ history-based scaling
12: {Gl
(pд)
= N(Gl
,pд)}L
l=1
▷ weight gradient precision pд
13: {Wl
(pw )
= S(Wl
(pw )
− ηGl
(pд)
,pw )}L
l=1
Our protocols rely heavily on OT, and crucially exploit characteristics of our networks such as ternary weights and fixed-point
(8-bit precision) gradients. We present our MPC protocol for neural
network training by describing its components separately. First,
in Section 4.1 we describe our protocol for ternary matrix-vector
multiplication, a crucial primitive for training used both in the forward and backward pass. Next, in Sections 4.2 and 4.3 we describe
the protocols for the forward and backward passes, respectively.
These protocols use our matrix-vector multiplication protocol (and
a slight variant of it), in combination with efficient garbled circuit
implementations for normalization and computation of ReLU.
4.1 Secure Ternary Matrix-Vector Multiplication
A recurrent primitive, both in the forward and backward passes of
the fixed-point neural networks from Section 3 is the product Wa,
for ternary matrix W ∈ {−1, 0, 1}
n×m and fixed-point integer vector a ∈ Z
m
q
. Several previous works on MPC-based ML have looked
specifically at matrix-vector multiplication [41, 51]. As described in
Section 2.2, multiplication is a costly operation in MPC. Our insight
is that if W is ternary, we can replace multiplications by selections,
enabling much faster protocols. More concretely, we can compute
the product z = Wa as shown in Algorithms 5.
Algorithm 5: Ternary-Integer Matrix-Vector Product
Input: Matrix W ∈ {−1, 0, 1}
n×m and vector a ∈ Z
m
q
z = (0)i ∈[n]
for i ∈ [n], j ∈ [m] do
if Wi,j > 0 then zi += aj
if Wi,j < 0 then zi −= aj
return z
Session 6B: ML Security II CCS ’19, November 11–15, 2019, London, United Kingdom 1237
Protocol 6: Boolean-Integer Inner Product
Parties: P1 and P2
Input: Arithmetic shares of integer vector a ∈ Z
m
q
and
Boolean shares of binary vector w ∈ {0, 1}
m
Output: Arithmetic shares of z = w⊤a
1: Each Pi generates random values (zi,j)j ∈[m]
.
2: for j ∈ [m] do
3: Pi sets
mi,0 := ⟨wj⟩i
· [[aj]]i − zi,j
mi,1 := ¬⟨wj⟩i
· [[aj]]i − zi,j
4: P1 and P2 run OT(m1,0,m1,1, ⟨wj⟩2), with P1 as Sender
and P2 as Chooser, for P2 to obtain z
′
1,j
5: P1 and P2 run OT(m2,0,m2,1, ⟨wj⟩1), with P2 as Sender
and P1 as Chooser, for P1 to obtain z
′
2,j
6: Each Pi sets [[z]]i =
Í
j ∈[m]
(zi,j + z
′
i,j
).
A natural choice for implementing the functionality in Algorithm 5 securely are MPC protocols that represent the computation
as a Boolean circuit. In our two-party setting natural choices are
garbled circuits and the GMW protocol [18]. In Boolean circuits,
the If-Then-Else construction corresponds to a multiplexer, and a
comparison with 0 is essentially free: it is given by the sign bit in a
two’s complement binary encoding. However, a circuit implementation of the computation above will require |W| additions and the
same number of subtractions, which need to implemented with
full-adders. Whereas, additions, if computed on additive shares, do
not require interaction and thus are extremely fast.
Our proposed protocol achieves the best of both worlds by
combining Boolean sharing and additive sharing. To this end, we
represent the ternary matrix W by two Boolean matrices W+ ∈
{0, 1}
n×m and W− ∈ {0, 1}
n×m, defined as W+
i,j
= 1 ⇔ Wi,j = 1
and W−
i,j
= 1 ⇔ Wi,j = −1. Now, the product Wa can be rewritten
as W+a − W−a. This reduces our problem from Wa with ternary
W to two computations of Wa with binary W. Note that we can
use the same decomposition to split any matrix with inputs in a
domain of size k into k Boolean matrices, and thus our protocol is
not restricted to the ternary case.
Accordingly, at the core of our protocol is a two-party subprotocol for computing additive shares of Wa, when W is Boolean-shared
among the parties and a is additively shared. In turn, this protocol
relies on a two-party subprotocol for computing additive shares of
the inner product of a Boolean-shared binary vector and an additively shared integer vector. As a first approach to this problem, we
show in Protocol 6 a solution based on oblivious transfer. We state
and prove the correctness of Protocol 6 in Appendix C.
One can think of Protocol 6 as a component-wise multiplication
protocol, as we will use it also for that purpose. The only modification required is in step 6, i.e. the local aggregation of additive shares
of the result of component-wise multiplication. We could directly
use this protocol to implement our desired matrix-vector multiplication functionality, and this leads to very significant improvements
due to the concrete efficiency of OT Extension implementations.
However, we can further optimize this to obtain our final protocol.
Protocol 7: Boolean-Integer Inner Product via m×COTℓ
Parties: P1 and P2
Input: ℓ-bit arithmetic shares of integer vector a ∈ Z
m
q
and
Boolean shares of binary vector w ∈ {0, 1}
m
Output: Arithmetic shares of z = w⊤a
1: Each party Pi constructs a vector of correlation functions
f
i = (fi,j(x))j ∈[m]
,
fi,j(x) = x − [[wj]]i
· [[aj]]i + ¬[[wj]]i
· [[aj]]i
2: The parties run m × COTℓ
(f
1
, [[wj]]2) with P1 acting as
the Sender, and P1 obtains x while P2 obtains y.
3: The parties run m × COTℓ
(f
2
, [[wj]]1) with P2 acting as
the Sender, and P2 obtains x
′ while P1 obtains y
′
.
4: P1 sets [[z]]1 =
Í
j ∈[m]
([[wj]]1 · [[aj]]1 − xj + y
′
j
)
5: P2 sets [[z]]2 =
Í
j ∈[m]
([[wj]]2 · [[aj]]2 − x
′
j + yj)
The use of OT in Protocol 6 has similarities with the GMW protocol, and is inspired by the OT-based method due to Gilboa for
computing multiplication triplets, and discussed in [11]. A similar
idea was used in the protocol for computing the sigmoid function by Mohassel and Zhang [41]. Moreover, concurrently to this
work, Riazi et al. [48] have proposed OT-based protocols for secure
prediction using DNNs. They propose a protocol called Oblivious
Conditional Addition (OCA) that is analogous to Protocol 6. While
their work only addresses secure prediction, our improved protocol
presented in the next section is relevant in their setting as well.
4.1.1 Optimizations: Correlated OT and Packing. In the previous
section, our protocol assumed a standard OT functionality, but actually, we can exploit even more efficient primitives. Our optimization
of Protocol 6, presented in Protocol 7 exploits COT in a way to implement our required inner product functionality. The idea behind
Protocol 7 is simple: note that in Protocol 6 parties choose their
random shares zi,j of intermediate values in the computation, and
they use them to mask OT messages. However, as we only require
the zi,j
’s to be random, one could in principle let the OT choose
them. Note also that the parties can choose their share of the result
as a function of their inputs, which can be implemented in COT.
This is done in lines 4 and 5 of Protocol 7. The next Lemma states
the correctness of Protocol 7. As the protocol consists of just two
executions of m × COTℓ and local additions its security is trivial,
while we present the proof of correctness in Appendix D. Finally,
note that, as in the case of Protocol 6, it is easy to turn Protocol 7
into a protocol for component-wise multiplication.
Lemma 4.1. Let w and a be a Boolean and integer vector, respectively, shared among parties P1, P2. Given an m × COTℓ protocol, the
two-party Protocol 7 is secure against semi-honest adversaries, and
computes an additive share of the inner product w⊤a among P1, P2.
Protocol 8 achieves our goal of computing an arithmetic share
of Wa = W+a − W−a, for an n × m matrix. This is easily achieved
using 2n calls to Protocol 7. This translates into 4n executions of
m × COTℓ
, plus local extremely efficient additions. Note that this
protocol is fully paralellizable, as all the COT executions can be run
in parallel.
Overall, our approach exploits the fact that W is ternary without
having to perform any Boolean additions in secure computations.
Session 6B: ML Security II CCS ’19, November 11–15, 2019, London, United Kingdom 1238
Protocol 8: Ternary-Integer Matrix-Vector Product
Parties: P1 and P2
Input: Arithmetic shares of integer vector a ∈ Z
m
q
and
Boolean shares of binary matrices
W+, W− ∈ {0, 1}
n,m
Output: Arithmetic shares of z = W+a − W−a
1: P1 and P2 compute [[W+a]] using n executions of Protocol 7.
2: P1 and P2 compute [[W−a]] using n executions Protocol 7.
3: Pi sets [[z]]i
:= [[W+a]]i − [[W+a]]i
.
Our experiments in Section 5 show concrete gains over the prior
state-of-the-art.
Communication costs. Using the m × COTℓ Extension protocol
from [3], the parties running Protocol 7 send m(τ + ℓ) bits to each
other to compute inner products of lengthm, where τ is the security
parameter (128 in our implementation). This results in nm(τ + ℓ)
bits being sent/received by each party for the whole matrix-vector
multiplication protocol. In contrast, the OT-based approaches to
matrix-vector multiplication entirely based on arithmetic sharing
from [41] would require at least nmℓ(τ +ℓ) (assuming optimizations
like packing and vectorization).
Packing for matrix-vector multiplication. While the forward
pass of quotient operates over 8-bit vectors (and thus q=8 in Protocol 8), the value of ℓ in implementations of m × COTℓ
is 128, i.e.,
the AES block size. However, we can exploit this to pack 128/8=16
vector multiplications against the same matrix for the same communication and computation. This is very useful in batched gradient
descent, as this results in 16x additional savings in communication and computation. This packing optimization was also used
in [41] for implementing an OT-based offline phase to matrix-vector
multiplications occurring in batched gradient descent.
4.2 Secure Forward Pass
Our protocol for the forward pass (Algorithm 1) is a sequential
composition of Protocol 8, and a garbled circuit protocol with three
components: (i) Evaluation of ReLU, (ii) normalization by a public
data-independent value αl
(line 3 in Algorithm 1), and (iii) quantization function Q(·) in eq. (2) in Section 3.1. The protocol is depicted
in Figure 3. Note that we only show a forward pass for a single
layer, but the protocol trivially composes sequentially with itself,
as input a
ℓ−1
and output a
ℓ
are both secret-shared additively. Security follows directly from the security of the subprotocols, as
their outputs and inputs are always secret-shares. For scalability,
we describe an efficient circuit implementation of (i)-(iii). In our
proposed circuit P1 inputs its share [[z]]1, and a random value chosen in advance that will become its share of the output, denoted
[[a
ℓ
]]1. In the garbled circuit, z is first reconstructed (this requires
|z| parallel additions). For component (i), ReLU, we exploit the fact
that the entries in z are encoded in binary in the circuit using two’s
complement as zi = (bi,k
· · · bi,1), where bi,k = 1 ⇔ zi < 0. Hence
ReLU(zi) = (¬bi,k
) · bi,k
. Note that this is very efficient, as it only
requires to evaluate k|z| NOT and AND gates. The next two steps,
(ii) normalization and (iii) quantization are extremely cheap, as
Party 1 MPC Party 2
⟨Wℓ,+ ⟩1, ⟨Wℓ,− ⟩1,
[[a
ℓ−1
]]1
⟨Wℓ,+ ⟩2, ⟨Wℓ,− ⟩2,
[[a
ℓ−1
]]2
Compute arithmetic shares of
z = Wℓ a
ℓ−1 = Wℓ,+a
ℓ−1 −Wℓ,−a
ℓ−1
(Protocol 8)
[[z]]1
[[a
ℓ
]]1
[[z]]2
r = ReLU(z)
rα = r ◦ 2
−α
a
ℓ = Q(rα , pa )
[[a
ℓ
]]2 = a
ℓ − [[a
ℓ
]]1
Garbled Circuit [[a
ℓ
]]2
Figure 3: Our MPC protocol for private prediction (forward
pass). We compose three protocols to evaluate one layer of
the form f (Wa), with ternary W, and where f = ReLU.
they can be implemented with logical and arithmetic shifts without requiring any secure gate evaluation. Finally, to construct P2’s
output we need to perform a subtraction inside the circuit to compute [[a
ℓ
]]2 = a
ℓ − [[a
ℓ
]]1. Altogether this means that our forward
pass requires execution of Protocol 8 and a garbled circuit protocol
to evaluate a vector addition, a vector subtraction, and a linear
number of additional gates. Moreover, note that this garbled circuit
evaluation can be parallelized across components of the vector z.
4.3 Secure Backward Pass
Figure 4 shows our protocols for the backward pass (Algorithm 2).
Analogous to the forward pass, we depict the pass for one layer,
and observe that all its inputs and outputs are secret-shared. Hence,
it can be easily composed sequentially across layers, and with the
forward pass, and its security follows trivially from the security of
each of the subprotocols.
The protocol works via a sequence of subprotocols. Each of
them produces a result shared among the parties P1, P2 either as
a Boolean-share or an additive share. As in the forward pass, this
protocol leverages Protocol 8, as well as Protocol 7 for componentwise multiplication. Recall that the goal of the backward pass is
to recompute ternary weight matrices {Wℓ
}
L
ℓ=1
by means of a
gradient-based procedure. As {Wℓ
}
L
ℓ=1
is represented in our MPC
protocol by pairs of binary matrices, the protocol to be run for each
layer ℓ takes as inputs the Boolean-shares of such matrices, i.e.,
Wℓ,− and Wℓ,+, from each party. Moreover, the parties contribute
to the protocol arithmetic shares of the input to each layer a
ℓ
computed in the forward pass, as well as the target values y.
The first step of the backward pass is a data-dependent normalization of the activation gradient e
ℓ
, followed by the quantization
step that we described in the forward pass, as shown in Figure 4.
We now describe the design of the Boolean circuit used to compute
these steps.
Session 6B: ML Security II CCS ’19, November 11–15, 2019, London, United Kingdom 1239
Normalization by the infinite norm. Our goal is to design a
(small) circuit that, given e, computes e/2
npow(max{ |e| }). A naive
circuit would compute the absolute value of every entry |e|, compute the maximum value max{|e|}, compute 2
⌈log2
(max{ |e| })⌉, and
finally compute a division. However, computing exponentiation
and logarithm in a garbled circuit would be prohibitively expensive. Such circuits are large, and we have to do this computation in
each layer ℓ, as many times as the number of total iterations. To
overcome this we apply two crucial optimizations: (i) approximate
max{|e|} by bitwise OR of all values |ei
|, and (ii) compute npow
with an efficient folklore procedure for obtaining the number of
leading zeros in a binary string. This requires only b OR gates and
log(b) arithmetic right shits and additions, where b is the bitwidth
of the entries of e (8 in our applications) [28]. Also note that the
division can be computed as an arithmetic right shift. An important
remark is that the denominator is a private value in the circuit,
which means that, although we can use right shift for division, our
circuit needs to first compute all possible right shifts and then select
the result according to the value of npow(max{|e|})). This involves
a subcircuit linear in b, and since in our case b =8 we once again
benefit from having small bitwidth, by trading a small overhead for
costly divisions, logarithms, and exponentiations.
Derivatives of ReLU and saturation. Computing derivatives
of ReLU and saturation S(·) can be done efficiently in a Boolean
representation, as they lie in {0, 1}. Specifically, computation only
involves extracting the sign bit for ReLU, and ANDing a few bits for
saturation. Ultimately we need to compute e ◦ d, for d = ReLU′
(a) ◦
S
′
(a) (line 4 in Algorithm 2). Not that this is a Boolean combination
of integers, so we can alternate between Boolean and arithmetic
shares and compute it with Protocol 7 (for component-wise product).
We can further optimize the procedure by computing ⟨d⟩i
in the
forward pass, since ReLU is already used there. This is commonly
done in ML implementations in the clear.
The remainder of the backward pass involves (i) computing e
l−1
(the rest of line 4 in Algorithm 2), for which we use Protocol 8, and
(ii) an outer product between 8-bit vectors a
l−1⊤
e (line 5). For (ii)
we use a vectorized version of the OT-based multiplication protocol
presented originally in [16], and used in [11, 41].
Overall, this makes our backward pass very efficient, involving
three small garbled circuits (two can be parallelized), and relies
heavily on oblivious transfer computations.
SGD. To implement Algorithm 3 we need to additionally keep
higher precision 8-bit matrices {Wℓ
(pw )
}
L
ℓ=1
as arithmetic shares.
We ternarize these to obtain our weights W, which we implement
in the natural way with a small garbled circuit involving 2 comparisons. The same operations for quantization and normalization of e
can be used for G.
AMSGrad. Almost all of the operations in AMSGrad (Algorithm 4):
quantization, normalization, saturation, and absolute value have
been described as part of the previous protocols. The only addition
is element-wise maximum (line 10), which we do via a comparison
of Boolean shares.
Party 1 MPC Party 2
[[a
i
]]1, [[y]]1, [[Wℓ,+]]1, [[Wℓ,−]]1, [[W¯ℓ ]]1 [[a
i
]]2, [[y]]2, [[Wℓ,+]]2, [[Wℓ,−]]2, [[W¯ℓ ]]2
[[e
ℓ
]]1 = [[a
i
]]1 − [[y]]1 [[e
ℓ
]]2 = [[a
i
]]2 − [[y]]2
r = e
ℓ /2
npow(max{|e
ℓ |})
e = Q(r, pe )
Garbled Circuit
[[e]]1 [[e]]2
d = ReLU′
(a) ◦ S
′
(a)
(Garbled circuit)
⟨d⟩1 ⟨d⟩2 u = e ◦ d
(Prococol 7)
[[u]]1 [[u]]2
e
ℓ−1 = Wℓu = Wℓ,+u − Wℓ,−u
(Protocol 8)
Gl = a
ℓ−1⊤
e
ℓ
(OT-based multiplication)
[[e
ℓ−1
]]1 [[e
ℓ−1
]]2
[[G]]1 [[G]]2
Figure 4: Our protocol for the backward pass, corresponding
to Algorithms 2 from Section 3.
Convolutional & residual layers. Although we have only described a fully connected layer, extending this protocol to convolutional layers is straightforward. The forward and backward passes
of convolutional layers can be written using the same steps as Algorithms 1 and 2 with weight-reshaping. And max-pooling operations
are simply comparisons, efficiently implemented in Boolean shares.
Similarly, for residual networks, we only introduce integer additions in the forward pass (which we can perform on additive shares)
and another computation of d for the backward pass.
5 Experiments
In this section, we present our experimental results for secure DNN
training and prediction.
Experimental Settings. The experiments were executed over two
Microsoft Azure Ds32 v3 machines equipped with 128GB RAM and
Intel Xeon E5-2673 v4 2.3GHz processor, running Ubuntu 16.04.
In LAN experiments, machines were hosted in the same region
(West Europe) with an average latency of 0.3ms and a bandwidth
of 1.82GB/s. For WAN, the machines were hosted in two different
regions (North Europe & East US), with an average latency of 42ms
and a bandwidth of 24.3 MB/sec. The machine specifications were
chosen to be comparable with the ones in [41], hence enabling
direct running time comparisons.
Implementation. We use two distinct code bases. We use the
EMP-toolkit [55] to implement our secure protocols for forward
and backward passes, as described in Section 4. EMP is written in
C++ and offers efficient implementations of OT and COT extension [3]. We extended the semi-honest COT implementation to the
functionality required for Protocol 7, as it is currently limited to
Session 6B: ML Security II CCS ’19, November 11–15, 2019, London, United Kingdom 1240
Network k QUOTIENT (s) GC (s) SecureML (OT) (s) SecureML (LHE) (s)
LAN
103 0.08 0.025 0.028 5.3
104 0.08 0.14 0.16 53
105 0.13 1.41 1.4 512
106 0.60 13.12 14* 5000*
107 6.0 139.80 140* 50000*
WAN
103 1.7 1.9 1.4 6.2
104 1.7 3.7 12.5 62
105 2.6 20 140 641
106 7.3 148 1400* 6400*
107 44 1527 14000* 64000*
Table 1: Comparison of our COT-based component-wise
multiplication of k-dimensional vectors with ternary fixedpoint multiplication using garbled circuits (GC) and SecureML [41] (OT, LHE). One of the vectors hold only ternary
values.
correlation functions of the form f (x) = x ⊕∆ (the ones required by
Yao’s garbled circuits protocol). This code base was used for timing
results. We developed a more versatile insecure Python implementation based on Tensorflow[1] for accuracy experiments. While
this implementation does not use MPC, it mirrors the functionality
implemented using the EMP-toolkit.
Evaluations. For training over QUOTIENT, we use two weight
variables as described in Section 3: (i) ternary (2-bit) weights for the
forward and backward passes, and (ii) 8-bit weights for the SGD
and AMSgrad algorithms. We use 8-bits for the quantized weight
gradients (д), activations (a) and activation gradients (e).
As our protocols are online, to compare with other approaches
employing an offline phase, we take a conservative approach: we
compare their offline computation time with our total computation
time using similar computational resources. We adopt this strategy
because online phases for these approaches are relatively inexpensive and could potentially involve a set-up overhead. Additionally,
our model could easily be divided into offline/online phases, but
we omit this for simplicity.
We employ a naive parallelization strategy: running independent
processes over a mini-batch on different cores. This speeds up the
computation on average by 8-15x over LAN and by about 10-100x
over WAN depending on the number of parallelizable processes.
We leave more involved parallelization strategies to future work.
5.1 Data-independent benchmarking
In this section, we present the running times of the basic building
blocks that will be used for DNN prediction and training.
Component-wise Multiplication. Table 1 compares the running
times of our COT-based approach from Protocol 7 for computing
component-wise product with (i) an implementation of Algorithm 5
in a garbled circuit and (ii) two protocols proposed in SecureML [41]
for the offline phase. Their first protocol corresponds to Gilboa’s
method for oblivious product evaluation (the OT-based variant implemented with a packing optimization). Their second is a sequence
of Paillier encryptions, decryptions, and homomorphic additions
(the LHE variant). For QUOTIENT and GC, one vector is ternary
and the other holds 128-bit values, while for OT and LHE both vectors hold 32-bit values. Although, our protocols for multiplication
Network n QUOTIENT (s) SecureML (OT Vec) (s) SecureML (LHE Vec) (s)
LAN
100 0.08 0.05 1.6
500 0.1 0.28 5.5
1000 0.14 0.46 10
WAN
100 1.7 3.7 2
500 2 19 6.2
1000 2.7 34 11
Table 2: Performance comparison of our matrix-vector multiplication approach with the vectorized approaches of SecureML [41] (OT, LHE). Here we multiply a 128 × n ternary
matrix with an n-dimensional vector.
are independent and suitable for parallelization, here we benchmark without parallelization. QUOTIENT clearly outperforms all
other approaches as soon as we move past the set-up overhead of
the base OTs. Note that most DNN layers involve greater than 104
multiplications, which makes our approach more suitable for those
applications.
Matrix-Vector Product. As discussed in Section 4, our componentwise multiplication directly translates into matrix-vector products
with local additions as described in Protocol 8. However, the protocols from [41] benefit greatly from a vectorization optimization,
and thus a comparison of the matrix-vector multiplication tasks is
important. Table 2 compares the performance of implementation of
Protocol 8, for a ternary 128×n matrix and an n-dimensional vector.
Similar to Table 1, we populate the matrix with ternary values (2-bit
values) and the vector with up to 128-bit values. Our approach is at
least 5x faster than the vectorized LHE protocol from [41] on LAN,
and is roughly 10x faster than the OT protocol from [41] on WAN
for n ≥ 500. In general, the speedup increases as we increase the
number of computations.
Layer Evaluation. Furthermore, we benchmark the basic building
blocks of secure DNN training framework—forward and backward
pass for a variety of different layer sizes. Figure 5 shows the running time of QUOTIENT for the forward pass as we increase the
layer size. We split the total time into time spent on the matrixvector product (Protocol 6) and computation of activation function
(ReLU) using garbled circuits. Figure 6 shows the running time of
the forward and backward pass, over a single batch of size 128.
We report the running time of each of the three required functionalities: quantization & scaling, gradient computation, and error
computation. As we increase the size of the layers, the garbled
circuit for the quantization phase starts to dominate over the COT
based matrix-matrix product required for the gradient computation.
This can primarily be attributed to the quantization and scaling of
the gradient matrix. Our COT-based matrix-vector multiplication
shifts the bottleneck from the multiplication to the garbled circuits
based scaling phase. Finding efficient protocols for that task is an
interesting task for future work. In particular, parallelized garbled
circuits and optimization of the matrix-matrix multiplication in the
gradient computation phase could be explored.
Session 6B: ML Security II CCS ’19, November 11–15, 2019, London, United Kingdom 1241
128 256 512 1024 2048
Size of the Layer (n)
0.0
0.5
1.0
1.5
2.0
2.5
3.0
Time (s)
Matrix-Vector Product
Activation Function
Figure 5: Forward pass time
for single prediction over an
n × n fully connected layer.
128 256 512 1024
Size of the Layer (n)
0
50
100
150
200
250
Time (s)
antization & Scaling
Gradient Computation
Error Computation
Figure 6: Forward and Backward pass time over an n × n
fully-connected layer for 1
batch. Here batch size = 128.
5.2 Experiments on Real-World Data
In this section, we evaluate our proposed QUOTIENT on real-world
datasets. We show that: (i) QUOTIENT incurs only a small accuracy loss with respect to training over floating point, and (ii) it is
more accurate than the state-of-the-art in fixed-point NN training,
namely WAGE [57]. Both (i) and (ii) hold for several state-of-the-art
architectures including fully connected, convolutional, and residual
layers, and for different types of data (for residual layer results see
the Appendix B). For our 2PC protocols, we show that (iii) 2PCQUOTIENT outperforms the existing secure training approach for
DNNs SecureML [41], both in terms of accuracy and running time,
and both in the LAN and WAN settings. We first report the accuracy across a variety of datasets to show (i) and (ii) above. Then we
report running times for 2PC-QUOTIENT training and prediction
to argue (iii).
5.2.1 Datasets and Deep Neural Network Architectures. We evaluate
QUOTIENT on six different datasets. Five of these are privacy
sensitive in nature, as they include health datasets—Thyroid [45],
Breast cancer [8], MotionSense [38], Skin cancer MNIST [53] and a
financial credit dataset—German credit [12]. We also evaluate our
approach on MNIST [34] for the purpose of benchmarking against
prior work.
MNIST contains 60K training and 10K test grayscale (28 × 28) images of 10 different handwritten digits. We adopt the state of the art
floating point convolutional neural network LeNet [34] (32C5-BN32C5-BN-MP2-64C5-BN-64C5-BN-MP2-512FC-BN-DO-10SM)2
into
a fixed-point equivalent of the form 32C5-MP2-64C5-MP2-512FC10MSE for secure training & inference. In addition, we explore
a variety of fully-connected neural networks 2×(128FC)-10MSE,
3×(128FC)-10MSE, 2×(512FC)-10MSE & 3×(512FC)-10MSE. We set
the learning to 1 for both SGD and AMSgrad optimizers.
MotionSense contains smartphone accelerometer and gyroscope
sensor data for four distinct activities namely walking, jogging, upstairs, and downstairs. For each subject, the dataset contains about
30 minutes of continuously recorded data. We use a rolling window,
of size 2.56 seconds each for extracting around 50K samples for training and 11K for testing. As proposed in [38] we use floating point
2BN, DO, and SM are batch-normalization, DropOut, and SoftMax respectively.
Dataset QUOTIENT (%) Floating point (%)
MNIST 99.38 99.48
MotionSense 93.48 95.65
Thyroid 97.03 98.30
Breast cancer 79.21 80.00
German credit 79.50 80.50
Table 3: Accuracy comparison of training over state of the
art floating point neural networks their fixed point equivalents using QUOTIENT.
convolutional neural network 64C3-BN-MP2-DO-64C3-BN-MP2-
DO-32C3-BN-MP2-DO-32C3-BN-MP2-DO-256FC-BN-DO-64FC-BNDO-4SM and it’s fixed-point analogue of the form 64C3-MP2-64C3-
MP2-32C3-MP2-32C3-MP2-256FC-64FC-4MSE. We furthermore explore a fully connected architecture of the form 3×(512FC)-4MSE.
Thyroid contains 3.7K training sample and 3.4K test samples of
21 dimensional patient data. The patients are grouped into three
classes namely normal, hyperfunction and subnormal based on
their thyroid functioning. We use a fully-connected neural network
of the form 2×(100FC)-3SM for this dataset and its analogue fixedpoint network 2×(100FC)-3MSE.
Breast cancer contains 5547 breast cancer histopathology RGB
(50 × 50) images segregated into two classes—invasive ductal carcinoma and non-invasive ductal carcinoma. We use the 90:10 split
for training and testing. We use a convolutional neural network
with 3×(36C3-MP2)-576FC-2SM and a fully-connected network of
the form 3×(512FC) along with their fixed point analogues.
Skin Cancer MNIST contains 8K training and 2K (28 × 28) dermatoscopic RGB images. They have been grouped into seven skin
lesion categories. We use floating point network ResNet-20 [21].
ResNet-20 is made up of batch-normalisation, dropout, SoftMax
layers and employs cross-entropy loss for training in addition to
the residual layers. For the fixed-point version of ResNet-20 [21],
we exclude batch normalisation, average pooling, and SoftMax layers. In addition, we use a fully connected architecture of the form
2×(512FC)-7MSE for secure training.
German credit contains 1k instances of bank account holders. they
have been divided into two credit classes—Good or Bad. Each individual has 20 attributes (7 quantitative and 13 categorical). As a preprocessing step, we normalize the quantitative variables and encode
the categorical variables using one-hot encoding. This amounts to
60 distinct feature for each individual in the dataset. We use the
80:20 split for training and testing. We use a fully-connected neural
network of the form 2×(124FC)-2SM and its fixed point analogue
for this dataset.
5.2.2 Accuracy. We evaluate the accuracy of QUOTIENT on different datasets and architectures. Also, we judge the impact of our
MPC-friendly modifications on accuracy. To do so we consider four
variants of QUOTIENT: (i) Our proposed QUOTIENT, with secure
AMSgrad optimizer (QUOTIENT[AMSgrad=Ours, Norm=Ours]) (ii)
QUOTIENT with the standard AMSgrad optimizer (QUOTIENT[AMSgrad=Std,
Session 6B: ML Security II CCS ’19, November 11–15, 2019, London, United Kingdom 1242
0 10 20 30 40 50
Epoch
0.86
0.88
0.90
0.92
0.94
0.96
0.98
1.00
Test accuracy
MNIST
QUOTIENT[AMSgrad=Ours, Norm=Ours]
QUOTIENT[AMSgrad=Std, Norm=Ours]
SecureML
SecureML convergence
0 10 20 30 40 50
Epoch
0.00
0.20
0.40
0.60
0.80
1.00
MotionSense
QUOTIENT[AMSgrad=Ours, Norm=Ours]
QUOTIENT[AMSgrad=Std, Norm=Ours]
0 10 20 30 40 50
Epoch
0.00
0.20
0.40
0.60
0.80
1.00 Thyroid
QUOTIENT[AMSgrad=Ours, Norm=Ours]
QUOTIENT[AMSgrad=Std, Norm=Ours]
0 10 20 30 40 50
Epoch
0.00
0.20
0.40
0.60
0.80
1.00
Breast cancer
QUOTIENT[AMSgrad=Ours, Norm=Ours]
QUOTIENT[AMSgrad=Std, Norm=Ours]
0 10 20 30 40 50
Epoch
0.00
0.20
0.40
0.60
0.80
1.00
German credit
QUOTIENT (Secure AMSgrad)
QUOTIENT[AMSgrad=Std, Norm=Ours]
Figure 7: Performance comparison of secure AMSgrad and secure SGD for QUOTIENT. The plots compare training curves over
MNIST (using CNN), MotionSense, Thyroid, Breast cancer and German credit datasets.
0 10 20 30 40 50
Epoch
0.86
0.88
0.90
0.92
0.94
0.96
0.98
1.00
Test accuracy
MNIST
QUOTIENT[AMSgrad=Ours, Norm=Ours]
QUOTIENT[AMSgrad=Ours, Norm=WAGE]
QUOTIENT[AMSgrad=Std, Norm=Ours]
Floating point (AMSgrad=Std)
WAGE
0 10 20 30 40 50
Epoch
0.00
0.20
0.40
0.60
0.80
1.00
MotionSense
0 10 20 30 40 50
Epoch
0.00
0.20
0.40
0.60
0.80
1.00 Thyroid
0 10 20 30 40 50
Epoch
0.00
0.20
0.40
0.60
0.80
1.00
Breast cancer
0 10 20 30 40 50
Epoch
0.00
0.20
0.40
0.60
0.80
1.00
German credit Figure 8: Performance comparison of three different variants of QUOTIENT training with floating point and WAGE [57]
training on MNIST, MotionSense, Thyroid, Breast cancer and German credit datasets. QUOTIENT[AMSgrad=Ours, Norm=WAGE]
and QUOTIENT[AMSgrad=Std, Norm=Ours]) differ from (QUOTIENT[AMSgrad=Ours, Norm=Ours] in using next power of 2 vs the
Closet Power of 2 and using standard AMSgrad by changing lines 9 and 11 of secure AMSgrad in Algorithm 4, respectively.
Norm=Ours]) (described in Appendix A) (iii) QUOTIENT with our
proposed AMSgrad with the closest power of 2 (C-Pow2) functionality instead of next power of 2 for quantization (QUOTIENT[AMSgrad=Ours,
Norm=WAGE]) (iv) QUOTIENT with the standard SGD optimizer
(QUOTIENT[SGD=Std, Norm=Ours]).
Comparison with Floating Point Training. As a baseline evaluation of our training using QUOTIENT[AMSgrad=Ours, Norm=Ours],
we compare its performance (upon convergence) with the floating
point training counterparts in Table 3. For MNIST and Breast cancer
datasets QUOTIENT[AMSgrad=Ours, Norm=Ours] training achieves
near state of the art accuracy levels for MNIST and Breast Cancer,
while we differ by at most ∼2% for German credit, MotionSense
and Thyroid datasets.
Secure AMSgrad vs SGD. Figure 7 shows the training curves
for QUOTIENT over all the datasets. In particular, we compare
QUOTIENT[AMSgrad=Ours, Norm=Ours] and QUOTIENT[SGD=Std,
Norm=Ours]. The secure AMSgrad optimizer converges faster than
the secure SGD optimizer especially for convolutional neural networks (used for MNIST, MotionSense, and Breast cancer).
Effects of our Optimizations. In order to evaluate the impact of
our optimizations, we compare training with floating point and
WAGE [57] with the first three variations of QUOTIENT in Figure 8.
For comparison with WAGE, we use the same fixed-point architecture as the ones used with QUOTIENT with an exception of the
choice of optimizer— we employ our proposed secure AMSgrad,
while WAGE uses a SGD optimizer in conjunction with randomized rounding. Moreover, the floating point analogues employ (a)
batch normalization and dropout in each layer, (b) softmax for the
last layer, and (c) cross-entropy loss instead of MSE. We observe
that QUOTIENT[AMSgrad=Ours, Norm=Ours] outperforms all the
other variants and is very close to the floating point training accuracy levels. We believe this is due to our modified AMSgrad, our
normalization scheme, and use of the next power of two (used
in QUOTIENT[AMSgrad=Ours, Norm=Ours]) which may act as an
additional regularization compared with the closest power of two.
5.2.3 End-to-End Running Times. In the previous section, we demonstrated that large networks can match and improve upon the stateof-the-art in fixed-point deep networks. However, often one can
use much simpler networks that are much faster and only sacrifice
little accuracy. In order to balance accuracy and run-time, we design
practical networks for each dataset and evaluate them here.
2PC-QUOTIENT Training. Table 4 shows the running time of
2PC-QUOTIENT for practical networks across all datasets over LAN
and WAN. We report accuracy and timings at 1,5 and 10 epochs. We
note that the training time grows roughly linearly with the number
of epochs. In all cases except the largest MNIST model, 10 epochs
finish in under 12 days. Note that standard large deep models can
easily take this long to train.
Our training protocols port nicely to the WAN settings. On average, our networks are only about 5x slower over WAN than over
LAN. This is is due to the low communication load and round-trip
of our protocols. As a result, we present the first 2PC pragmatic
neural network training solution over WAN.
Session 6B: ML Security II CCS ’19, November 11–15, 2019, London, United Kingdom 1243
LAN
MNIST MotionSense Thyroid Breast cancer German credit
2 × (128FC) 3 × (128FC) 2 × (512FC) 3 × (512FC) 2 × (512FC) 2 × (100FC) 3 × (512FC) 2 × (124FC)
Epoch Time Acc Time Acc Time Acc Time Acc Time Acc Time Acc Time Acc Time Acc
1 8.72h 0.8706 10.05h 0.9023 27.13h 0.9341 38.76h 0.9424 10.07h 0.8048 0.08h 0.2480 14.51h 0.4940 0.03h 0.4100
5 43.60h 0.9438 50.25h 0.9536 135.65h 0.9715 193.80h 0.9745 50.35h 0.8847 0.40h 0.9341 72.55h 0.7360 0.15h 0.725
10 87.20h 0.9504 100.50h 0.9604 271.30h 0.9772 387.60h 0.9812 100.70h 0.8855 0.80h 0.9453 145.10h 0.7600 0.30h 0.7900
WAN
MNIST MotionSense Thyroid Breast cancer German credit
2 × (128FC) 3 × (128FC) 2 × (512FC) 3 × (512FC) 2 × (512FC) 2 × (100FC) 3 × (512FC) 2 × (124FC)
Epoch Time Acc Time Acc Time Acc Time Acc Time Acc Time Acc Time Acc Time Acc
1 50.74h 0.8706 57.90h 0.9023 139.71h 0.9341 190.10h 0.9424 44.43h 0.8048 0.52h 0.2480 74.10h 0.4940 0.15h 0.4100
5 253.7h 0.9438 289.5h 0.9536 698.55h 0.9715 950.5h 0.9745 222.15h 0.8847 2.6h 0.9341 370.5h 0.7360 0.75h 0.725
10 507.4h 0.9504 579h 0.9604 1397.1h 0.9772 1901h 0.9812 444.3h 0.8855 5.2h 0.9453 741h 0.7600 1.5h 0.7900
Table 4: Training time and accuracy values for various datasets and architectures after 1, 5 & 10 training epochs using QUOTIENT over LAN and WAN.
MNIST MotionSense Thyroid Breast cancer German credit
2 × (128FC) 3 × (128FC) 2 × (512FC) 3 × (512FC) Conv 2 × (512FC) Conv 2 × (100FC) 3 × (512FC) Conv 2 × (124FC)
Single Prediction(s) 0.356 0.462 0.690 0.939 192 0.439 134 0.282 3.58 62 0.272
Batched Prediction (s) 2.24 2.88 4.79 6.50 2226 2.91 1455 1.83 44.02 447 1.77
Table 5: Prediction time for various datasets and architectures using QUOTIENT over LAN. Here batch size = 128.
MNIST MotionSense Thyroid Breast cancer German credit
2 × (128FC) 3 × (128FC) 2 × (512FC) 3 × (512FC) 2 × (512FC) 2 × (100FC) 3 × (512FC) 2 × (124FC)
Single Prediction(s) 6.8 8.8 14.4 19.9 9.46 5.99 33.3 5.1
Batched Prediction (s) 8.3 10.9 22.6 29.9 12.08 6.89 69.1 7.3
Table 6: Prediction time for various datasets and architectures using QUOTIENT over WAN. Here batch size = 128.
2PC-QUOTIENT Prediction. In addition to secure training, forward pass of QUOTIENT can be used for secure prediction. Table 5
presents prediction timings for all datasets over LAN. In addition
to fully connected networks, we also report the prediction timings
over convolutional neural networks (we report residual networks
in Appendix B). The timings have been reported for a single point
as well as 128 parallel batched predictions (this corresponds to classifying many data points in one shot). Except for the multi-channel
Breast cancer dataset, all single predictions take less than 1s and
all batched predictions take less than 60s.
Table 6 presents an equivalent of Table 5 under WAN settings.
Here we limit ourselves to only fully connected architectures. While
still practical, per prediction costs over WAN are, on an average,
20x slower than over LAN. This is due to the high initial setup
overhead over WAN. However, batched predictions are only about
4-6x slower over WAN.
Comparison with Previous Work. The only prior work for 2PC
secure training of neural networks that we are aware of is SecureML
[41]. They report the results for only fully connected neural network
training on the MNIST dataset. 2PC-QUOTIENT is able to achieve
its best accuracy levels in less than 16 hours over LAN and less than
90 hours over WAN. This amounts to a speedup of 5x over LAN and
a speedup of 50x over WAN. In particular, QUOTIENT is able to
make 2PC neural network training over WAN practical. Moreover,
our 2PC-QUOTIENT training is able to achieve near state of the art
accuracy of 99.38% on MNIST dataset, amounting to an absolute
improvement of 6% over SecureML’s error rates upon convergence.
In terms of secure prediction, 2PC-QUOTIENT is 13x faster for
single prediction and 7x faster for batched predictions over LAN in
comparison to SecureML. Furthermore, 2PC-QUOTIENT offers a
3x speed-up for a single prediction and 50x for batched predictions
versus SecureML over WAN.
6 Conclusion
In this paper, we introduced QUOTIENT, a new method to train
deep neural network models securely that leverages oblivious transfer (OT). By simultaneously designing new machine learning techniques and new protocols tailored to machine learning training,
QUOTIENT improves upon the state-of-the-art in both accuracy
and speed. Our work is the first we are aware of to enable secure
training of convolutional and residual layers, key building blocks of
modern deep learning. However, training over convolutional networks is still slow, and incurs on a large communication load with
Session 6B: ML Security II CCS ’19, November 11–15, 2019, London, United Kingdom 1244
our methods. Finding dedicated MPC protocols for fast evaluation
of convolutional layers is an interesting venue for further work.