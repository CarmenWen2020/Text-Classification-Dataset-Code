Marginal tables are the workhorse of capturing the correlations
among a set of attributes. We consider the problem of constructing marginal tables given a set of user‚Äôs multi-dimensional data
while satisfying Local Differential Privacy (LDP), a privacy notion
that protects individual user‚Äôs privacy without relying on a trusted
third party. Existing works on this problem perform poorly in the
high-dimensional setting; even worse, some incur very expensive
computational overhead. In this paper, we propose CALM, Consistent Adaptive Local Marginal, that takes advantage of the careful
challenge analysis and performs consistently better than existing
methods. More importantly, CALM can scale well with large data
dimensions and marginal sizes. We conduct extensive experiments
on several real world datasets. Experimental results demonstrate
the effectiveness and efficiency of CALM over existing methods.
1 INTRODUCTION
In recent years, differential privacy [13, 14] has been increasingly
accepted as the de facto standard for data privacy in the research
community [3, 15, 21, 24, 27]. Most early work on DP are in the
centralized setting, where a trusted data curator obtains data from
all individuals, and processes the data in a way that protects privacy
of individual users. For example, the data curator could publish a
private synopsis of the data, enabling analysis on the data, while
hiding individual information.
Recently, techniques for satisfying differential privacy (DP) in
the local setting, which we call LDP, have been studied and deployed. In the local setting for DP, there are many users and one
aggregator. Unlike the centralized setting, the aggregator does not
see the actual private data of each individual. Instead, each user
sends randomized information to the aggregator, who attempts
to infer the data distribution based on that. LDP techniques enable the gathering of statistics while preserving privacy of every
user, without relying on trust in a single trusted third party. LDP
techniques have been deployed by companies like Google [16, 17],
Apple [1], Microsoft [12] and Samsung [9]. Exemplary use cases
include collecting users‚Äô default browser homepage and search engine, in order to understand the unwanted or malicious hijacking of
user settings; or frequently typed emoji‚Äôs and words, to help with
keyboard typing predictions.
Previous works on LDP focus on estimating the frequencies of
frequent values the user possesses [6, 7, 16, 30, 37‚Äì39]. The natural
and more general setting is when each user has multiple attributes,
and the aggregator is interested in the joint distribution of some
of these attributes. That is, the aggregator is interested in marginal tables over some subsets of attributes. Marginal tables are the
workhorse of capturing the correlations among a set of attributes.
Many analysis tasks require the availability of marginal statistics
on multidimensional datasets. For example, finding correlations or
fitting sophisticated prediction models.
Two recent papers [11, 31] considered the problem of publishing marginals under LDP. Kulkarni [11] et al. proposed to apply
the Fourier Transformation method, which was used in publishing
marginals under the centralized DP setting [5]. Ren et al. [31] proposed to apply the Expectation Maximization methods, originally
developed by Fanti et al. [17] to infer marginals. These methods,
however, performs poorly when the number of attributes is more
than a few.
We propose a new method CALM, Consistent Adaptive Local
Marginal, for computing any k-way marginals under the LDP setting. Our approach is inspired by PriView [29], which was designed
for computing arbitrary k-way marginals under centralized DP for
binary datasets (i.e., each attribute is binary). PriView privately
publishes a synopsis of the dataset, which takes the form of m
marginals each of the size ‚Ñì. Using the synopsis, it can reconstruct
any k-way marginal.
Similar to PriView, in CALM a number of marginal tables are
generated. But there are several challenges when changing from
centralized setting to the local setting. We need to integrate FO
Session 2B: Differential Privacy 1 CCS‚Äô18, October 15-19, 2018, Toronto, ON, Canada 212
protocols to construct marginals, and to extend the methods in
PriView to deal with non-binary attributes. Furthermore, since the
privacy parameter œµ affects noises differently in the local setting,
the error analysis, which is essential for guiding the choice of key
algorithmic parameters, changes as well. A common challenge for
many works on differential privacy is that there are often critical
algorithmic parameters, the choice of which greatly impact the
performance of the algorithm. However, such parameters are often
chosen in ad hoc ways, or based on performance on the experimental datasets. Both PriView and CALM have two critical parameters.
In the local setting, an additional source of errors is introduced
and needed to be considered. We carefully analyze how different
errors are affected by different parameters, deriving formulas for
estimating them whenever possible. We then develop an approach
for choosing these parameters in a principled way. Our approach
takes as input one target error threshold, and use an algorithm to
find parameter values, using the formulas for estimating errors.
We have implemented CALM and conducted extensive experimental evaluation to compare CALM with other state of the art
methods. Experimental results show that CALM‚Äôs expected value
of the Sum of Squared Errors is often one to two orders of magnitude lower than the best current method in [11]. In addition, CALM
scales to settings where existing methods do not. To demonstrate
the importance of the marginal information in practice, we also
evaluate the prediction performance of CALM versus other methods by training an SVM model on some fixed marginal. In most
cases, we can see CALM can achieve near optimal results, while
other methods are beaten by the naive method that always output
the majority label.
To summarize, the main contributions of this paper are three
folds:
‚Ä¢ We introduce CALM for the marginal release problem under
local differential privacy, which also work when there are
non-binary attributes.
‚Ä¢ We have conducted careful analysis on errors from three
different sources, and developed an algorithm for choosing
key algorithmic parameters for CALM.
‚Ä¢ The performance of the proposed method is extensively evaluated on real-world datasets and demonstrated to greatly
outperform state-of-the art approaches.
Roadmap. In Section 2, we present background knowledge of
LDP and FO. We then go over the problem definition and existing
solutions in Section 3. We present our proposed method in Section 4.
Experimental results are presented in 5. Finally we discuss related
work in Section 6 and provide concluding remarks in Section 7.
2 BACKGROUND
In the local setting for DP, there are many users and one aggregator.
Each user possesses a value v from domain D, and the aggregator
wants to learn the distribution of values among all users, in a way
that protects the privacy of individual users.
2.1 Differential Privacy in the Local Setting
To protect privacy, each user perturbs the input value v using an
algorithm Œ® and sends Œ®(v) to the aggregator. The formal privacy
requirement is that the algorithm Œ®(¬∑) satisfies the following property:
Definition 1 (œµ Local Differential Privacy). An algorithm
Œ®(¬∑) satisfies œµ-local differential privacy (œµ-LDP), where œµ ‚â• 0, if and
only if for any input v1,v2 ‚àà D, we have
‚àÄT ‚äÜRange(Œ®) : Pr[Œ®(v1) ‚àà T ] ‚â§ e
œµ Pr[Œ®(v2) ‚àà T ],
where Range(Œ®) denotes the set of all possible outputs of Œ®.
Since a user never reveals v to the aggregator and reports only
Œ®(v), the user‚Äôs privacy is still protected even if the aggregator is
malicious.
2.2 Frequency Oracles
A frequency oracle (FO) protocol enables the estimation of the frequency of any value x ‚àà D under LDP, which serves as the building
block of other LDP tasks. It is specified by a pair of algorithms: Œ®
is used by each user to perturb her input value, and Œ¶ is used by
the aggregator.
2.2.1 Generalized Randomized Response (GRR). This FO protocol
generalizes the randomized response technique [40]. Here each user
with private value v ‚àà D sends the true value v with probability
p, and with probability 1 ‚àí p sends a randomly chosen v
‚Ä≤ ‚àà D s.t.
v
‚Ä≤ , v. More formally, the perturbation function is defined as
‚àÄy ‚ààD Pr
Œ®GRR(œµ)
(v) = y

=

p =
e
œµ
e
œµ +d‚àí1
, if y = v
q =
1
e
œµ +d‚àí1
, if y , v
This satisfies œµ-LDP since p
q
= e
œµ
. To estimate the frequency of
v ‚àà D (i.e., the ratio of the users who have v as private value to the
total number of users), one counts how many times v is reported,
and denote the count as C(v), and then computes
Œ¶GRR(œµ)
(v) B
C(v)/n ‚àí q
p ‚àí q
where n is the total number of users. For example, if 20% of users
have value v, the expected number of v in all randomized reports
is 0.2 ‚àó n ‚àó p + 0.8 ‚àó n ‚àó q. If the aggregator sees exactly this number
of reports, the estimated value is
(0.2np + 0.8nq)/n ‚àí q
p ‚àí q
=
0.2p + 0.8q ‚àí q
p ‚àí q
=
0.2p ‚àí 0.2q
p ‚àí q
= 0.2
In [37], it is shown that this is an unbiased estimation of the true
count, and the variance for this estimation is
Var[Œ¶GRR(œµ)
(x)] =
|D| ‚àí 2 + e
œµ
(e
œµ ‚àí 1)
2
¬∑ n
(1)
The accuracy of this protocol deteriorates fast when the domain
size |D| increases. This is reflected in that the variance given in (1)
is linear to |D|.
2.2.2 Optimized Unary Encoding (OUE). The Optimized Unary
Encoding (OUE) [37] avoids having a variance that depends on
|D| by encoding the value into the unary representation. Wlog, let
D = [0..d ‚àí 1]; each value v ‚àà [0..d ‚àí 1] is encoded into a binary
string of length d such that the v-th bit is 1 and all othe bits are 0.
The unary encodings of any two different values differ in exactly
two bits. OUE applies GRR to each bit, but transmits 1‚Äôs and 0‚Äôs
differently. The bit 1 is transmitted as a coin toss, i.e., it is perturbed
Session 2B: Differential Privacy 1 CCS‚Äô18, October 15-19, 2018, Toronto, ON, Canada 213  
to 0 with probability 0.5; this can be viewed as applying GRR with
œµ = 0. Doing this enables us to transmit each of the many (|D| ‚àí 1,
to be precise) 0 bits with the maximum allowed privacy budget œµ, so
that the number of 1‚Äôs resulting from perturbing the 0‚Äôs is as small
as possible. Doing this minimizes the estimation variance when |D|
is large [37].
Given reports y
j
from all users j ‚àà [n], to estimate the frequency
of v, the aggregator counts the number of reports with the bit
corresponding to v set to 1, i.e., C(x) = |{j | y
j
x = 1}|. One then
transforms C(v) to its unbiased estimation
Œ¶OUE(œµ)
(x) B
C(x)/n ‚àí q
1
2
‚àí q
It is proved in [37] that the Œ®OUE(¬∑) satisfies LDP, and estimated
frequency is unbiased and has variance
Var[Œ¶OUE(œµ)
(x)] =
4e
œµ
(e
œµ ‚àí 1)
2
¬∑ n
(2)
2.2.3 Adaptive FO. Comparing (1) with (2), the factor |D| ‚àí 2 + e
œµ
is replaced by 4e
œµ
. This suggests that for smaller |D| (such that
|D| ‚àí 2 < 3e
œµ
), one is better off with GRR; but for large |D|, OUE
is better and has a variance that does not depend on |D|.
For simplicity, we use FO to denote the adaptively chosen protocol, i.e., when domain size is less than 3e
œµ + 2, GRR is used as FO;
otherwise, OUE is used. It has variance
Var[Œ¶FO(œµ)
(x)] = min 
4e
œµ
(e
œµ ‚àí 1)
2
,
|D| ‚àí 2 + e
œµ
(e
œµ ‚àí 1)
2

¬∑
1
n
(3)
3 PROBLEM DEFINITION AND EXISTING
SOLUTIONS
We consider the setting where each user has multiple attributes,
and the aggregator is interested in the joint distribution of some
attributes. Such multi-dimension settings occur frequently in the
situation where LDP is applied. In [11, 31], researchers studied the
problem of constructing marginals in the LDP setting.
3.1 Problem Definition: Centralized Setting
We assume that there are d attributes A = {a1, a2, . . . , ad
}. Each
attribute ai hasci possible values. Wlog, we assume that the values
for ai are [ci] B {0, 1, ¬∑ ¬∑ ¬∑ ,ci ‚àí1}. Each user has one value for each
attribute. Thus user j‚Äôs value is a d-dimensional vector, denoted by
v
j = ‚ü®v
j
1
,v
j
2
, . . . ,v
j
d
‚ü© such that v
j
i
‚àà [ci] for each i. The full domain
for the users‚Äô values is given by D = [c1]√ó [c2]√ó ¬∑ ¬∑ ¬∑ √ó [cd
], in which
√ó denotes cartesian product. The domain D has size |D| =
√éd
i=1
ci
.
Let us first consider the setting of answering marginal queries
in the centralized setting, where the server has all users‚Äô data. For
a population of n users, the full contingency table gives, for each
value v ‚àà D, the fraction of users having the value v. We use F
to denote the full contingency table, and call the fraction for each
value v ‚àà D a cell in the full contingency table.
The full contingency table gives the joint distribution of all attributes in A. However, when the domain size is very large, e.g.,
when there are many attributes, computing the full contingency
table can be prohibitively expensive. Oftentimes, one is interested
in the joint distribution of some subsets of the attributes. Given
a set of attributes A ‚äÜ A, we use VA = {‚ü®v1,v2, . . . ,vd
‚ü© : vi ‚àà
Gender Age
v
1 male teenager
v
2
female teenager
v
3
female adult
v
4
female adult
¬∑ ¬∑ ¬∑ ¬∑ ¬∑ ¬∑ ¬∑ ¬∑ ¬∑
v
n male elderly
(a) Dataset.
v F(v)
‚ü®male, teenager‚ü© 0.20
‚ü®male, adult‚ü© 0.15
‚ü®male, elderly‚ü© 0.20
‚ü®female, teenager‚ü© 0.15
‚ü®female, adult‚ü© 0.20
‚ü®female, elderly‚ü© 0.10
(b) Full contingency table.
v M{gender}
(v)
‚ü®male,‚àó‚ü© 0.55
‚ü®female,‚àó‚ü© 0.45
(c) Marginal table for gender.
v M{age}
(v)
‚ü®‚àó,teenager‚ü© 0.35
‚ü®‚àó,adult ‚ü© 0.35
‚ü®‚àó,elderly‚ü© 0.30
(d) Marginal table for age.
Figure 1: Example of the dataset, the full contingency table,
and the marginal tables.
[ci] if ai ‚àà A, otherwise vi = ‚àó} to denote the set of all possible
values specified by A.
When given a set A of k attributes, the k-way marginal over A,
denoted by MA, gives the fraction of users having each value in VA.
We call the fraction for each value v ‚àà VA a cell of the marginal
table. MA contains fewer cells than the full contingency table F.
Each cell in MA can be computed from summing over the values in
the cells in F that have the same values on the attributes in A.
Figure 1 gives an example where each user has two attributes
gender and age. In the centralized setting, the server has access to
the raw dataset Figure 1(a), from which, it can compute the full contingency table (Figure 1(b)). The two marginal tables (Figure 1(c,d))
can be computed from the contingency table.
3.2 Problem Definition: Local Setting
In the local setting, the aggregator does not have access to the raw
dataset, such as the one shown in Figure 1(a). Instead, each user
possesses one row of the dataset and sends a randomly perturbed
value based on it. Our goal is to have the aggregator to use the
perturbed reports to compute with reasonable accuracy any k-way
marginal. Some methods (such as those proposed in [11]) require
a specification of the maximum k ahead of time. Our proposed
method can support queries of arbitrary k values.
To measure the utility empirically, we use sum of squared error (SSE), i.e., the square of the L2 distance between the true marginal MA and the reconstructed TA. When we query many k-way
marginals, we calculate the SSE for each marginal, and use the
average SSE as the indicator of a method‚Äôs accuracy.
The reconstructed TA can be viewed as a random variable since
random noises are added in the process to satisfy LDP. When a
method is able to produce an unbiased estimation, the expected
value of TA is the true marginal MA, and the expected value of SSE
is the variance of the random variable TA.
Figure 1 gives an example where each user has two attributes
gender and age. The goal is to construct all the marginal tables. Each
user‚Äôs private value corresponds to a row in Figure 1(a). No one has
Session 2B: Differential Privacy 1 CCS‚Äô18, October 15-19, 2018, Toronto, ON, Canada 214
Symbol Description
n The total number of users
v
j Value of user j
d Number of attributes
A The set of all attributes
ai Attribute i
ci Number of possible values for attribute ai
F The full contingency table
A Some set of attributes
MA The marginal table of attribute set A
m Number of marginal tables output by our method
‚Ñì Size of each marginal table in our method
Table 1: List of Notations
the full view of the whole dataset. To construct the marginal tables
Figure 1(c,d), one can let each user report the two values (using an
FO as described earlier), aggregate the users‚Äô reports to construct
the full contingency table (with some noise), and build the marginal
tables. This method is more formally described in the following.
See Table 1 for the list of notations.
3.3 Full Contingency Table Method (FC)
To estimate M, one straightforward approach is to estimate the
full contingency table F first, and then construct M from F. We
call this approach the Full Contingency (FC) table method. In this
method, each user reports her value v ‚àà D using an FO protocol.
The aggregator estimates the frequency of each value in the full
domain. Once having the full contingency table, the aggregator can
compute any k-way marginal.
The main shortcoming of FC is that, since one has to query the
frequency of each value in the full domain of all attributes, the time
complexity and space complexity grows exponentially with the
number of attribute d and can be prohibitively expensive.
Furthermore, even when it is feasible to construct the full contingency table, computing marginals from a noisy full contingency
table can have high variance. For example, suppose we have 32
binary attributes, the domain size is thus 2
32. When constructing a
4-way marginal, each value in the 4-way marginal is the result of
summing up 2
28 noisy entries in the full contingency table. Let Var0
be the variance of estimating each single cell in the full contingency
table, the variance of each cell in the reconstructed marginal is then
2
28 √ó Var0, and the expected SSE is 2
4 √ó 2
28 √ó Var0 = 2
32 √ó Var0.
In general, the variance of computing k-way marginals from the
noisy full contingency table is
VarFC = 2
d
¬∑ Var0 (4)
3.4 All Marginal Method (AM)
To mitigate the exponential dependency on d, one can construct
all the k-way marginals directly. There are two alternatives, one
is to divide the privacy budget œµ into d
k

pieces, and have each
user reports d
k

times, once for each k-way marginal. The second
is to divide the user population into d
k

disjoint groups, and have
users in each group report one k-way marginal. Under the LDP
setting, it is generally better to divide the population than dividing
the privacy budget, because reporting under low privacy budget is
very noisy [26, 37, 38].
Under LDP, estimating fraction frequencies is less accurate with
a smaller group than with a larger group, because the noises have
larger impact when the true counts are small. The variance is inversely proportional to the group size. Thus dividing the population
into d
k

groups will add a d
k

factor to the variance. This factor
results in the following variance.
VarAM = 2
k
¬∑

d
k

¬∑ Var0 (5)
When k is relatively small (and hence d
k

is small), AM performs
better than FC; when k is large, AM could perform worse than FC.
Another limitation of this method is that one has to specify the
value k ahead of time. After the protocol is executed, there is no
way to answer any t-way marginal queries for t > k.
3.5 Fourier Transformation Method (FT)
Fourier Transformation (FT) was used for publishing k-way
marginals in the centralized setting [5]. Kulkarni et al. [11] applied
the technique to the local setting. Effectively, it is an optimization
of the AM method. The motivation underlying FT is that, the calculation of a k-way marginal requires only a few coefficients in the
Fourier domain. Thus, users can submit noisy Fourier coefficients
that are needed to compute the desired k-way marginals, instead
of values in those marginals.
This method results in slightly lower variance than AM. However, in order to reconstruct all k-way marginals, a large number
of coefficients need to be estimated; thus this method would still
perform poorly when k is large. Furthermore, the method is designed to deal with the binary attributes. Therefore, the non-binary
attributes must be pre-processed to binary attributes, resulting in
more dimensions. For example, an attribute with c values has to be
transformed into ‚åàlog2
c‚åâ binary attributes.
The details of FT are presented in Appendix A.1. Here, we briefly
analyze its variance. Specifically, there are √çk
s=0
d
s

coefficients to
be estimated. Estimating TA(v) requires information for a selected
set of 2
k
coefficients, each multiplied by 2
‚àík
. Therefore, this method
has variance
VarFT =
√ï
k
s=0

d
s

¬∑ Var0 (6)
3.6 Expectation Maximization Method (EM)
This method allows each user to upload the value for each attribute
separately with split privacy budget. The aggregator then conducts
Expectation Maximization (EM) algorithm to reconstruct the marginal tables. This approach is first introduced by Fanti et al. [17] for
estimating joint distribution for two attributes, and then generalized
by Ren et al. [31] to handle multiple attributes.
Specifically, denote y
j = ‚ü®y
j
1
,y
j
2
, . . . ,y
j
d
‚ü© as the report from user
j. The algorithm attempts to guess the private value distribution
TA, for any A, by maximizing the probability y
j
are reported from
user j.
Session 2B: Differential Privacy 1 CCS‚Äô18, October 15-19, 2018, Toronto, ON, Can       
The original EM algorithm runs slowly. Therefore, we use the
algorithm proposed in the appendix of [38] to help compute TA.
In most cases, if the initial values are set using the result returned
by this algorithm, the EM algorithm finishes quickly. Specifically,
this algorithm first estimates the value distribution for any single
attribute, and then uses that estimation to estimate distribution for
any pair of attributes, and so on. The method is proven to produce
unbiased estimation.
The detailed protocol of EM is given in Appendix A.2. Overall,
the EM method has the advantage of being able to compute t-way
marginals for any t. But since œµ is split into each attribute, this
method has large variance.
4 CALM: CONSISTENT ADAPTIVE LOCAL
MARGINAL
In this section, we describe our proposed method CALM (Consistent Adaptive Local Marginal) for publishing k-way marginal via
LDP. Our method is inspired by the PriView method for publishing marginal under the centralized DP setting [29], so we describe
PriView first.
4.1 An Overview of PriView
The PriView method was designed for privately computing arbitrary k-way marginals for a dataset with d binary attributes in the
centralized setting. PriView privately publishes a synopsis of the
dataset. Using the synopsis, it can reconstruct any k-way marginal.
The synopsis takes the form of m size-‚Ñì marginals that are called
views. Below we give an overview of the PriView method, using an
example where there are d = 8 attributes {a1, a2, ¬∑ ¬∑ ¬∑ , a8}, and we
aim to answer all 3-way marginals. PriView has the following four
steps. (See [29] for complete specification of PriView.)
Choose the Set of Views. The first step is to choose which
marginals to include in the private synopsis as views. That is, one
needs to choose m sets of attributes. PriView chooses these sets so
that each size-2 (or size-3) marginal is covered by some view. For
example, if aiming to cover all 2-way marginals, then one could
choose the following m = 6 sets of attributes to construct views:
{a1, a2, a3, a4} {a1, a5, a6, a7} {a2, a3, a5, a8}
{a4, a6, a7, a8} {a2, a3, a6, a7} {a1, a4, a5, a8}
Observe that any pair of two attributes are included in at least one
set.
Generate Noisy Views. In this step, for each of the m attribute
sets, PriView constructs a noisy marginal over the attributes in the
set, by adding Laplace noise Lap  m
œµ

to each cell in the marginal
table. This is the only step that needs direct access to the dataset.
After this step, the dataset is no longer accessed.
Consistency Step. Given these noisy marginals/views, some 3-
way marginals can be directly computed. For example, to obtain
the 3-way marginal for {a1, a2, a3}, we can start from the view
for {a1, a2, a3, a4} and marginalizes out a4. However, many 3-way
marginal are not covered by any of the 6 views. For example, if
we want to compute the marginal for {a1, a3, a5}, we have to rely
on partial information provided by the 6 views. We can compute
the marginals for {a1, a3}, {a1, a5}, and {a3, a5}, and then combine
them to construct an estimation for {a1, a3, a5}.
Observe that {a1, a5} can be computed both by using the view for
{a1, a5, a6, a7} and by using the view for {a1, a4, a5, a8}. Since independent noises are added to the two marginals, the two different
ways to compute marginal for {a1, a5} most likely have different results. In addition, the noisy marginals may contain negative values.
PriView performs constrained inference on the noisy marginals
to ensure that the marginals in the synopsis are all non-negative
and mutually consistent. (For self-containment, we included the
description of the consistency step in Appendix A.3.)
Generating k-way Marginals. From the m consistent views, one
can reconstruct any k-way marginals. When given a set of k attributes, if all k attributes are included in one view, then we can
compute the k-way marginal directly. When no view includes all
k attributes, PriView uses Maximum Entropy estimation to compute the k-way marginal. For example, when given the marginals
for {a1, a3}, {a1, a5}, and {a3, a5}, Maximum Entropy estimation
finds among all possibles marginals for {a1, a3, a5} that are consistent with the three known marginals, the one with the maximum
entropy. Note that while the marginal for {a1, a3, a5} have 7 unknowns (the 8 cells must sum up to 1), and each marginal over
{a1, a3}, {a1, a5}, and {a3, a5} gives 3 equations, these equations
are not independent. In this case, the three 2-way marginals together give 6 independent linear constraints on the 7 unknowns,
leaving one degree of freedom.
Discussions. Using the PriView method, one could answer k-way
marginals for arbitrary k values. For a k-way marginal computed
by PriView, there are two sources of errors. Noise Errors are due
to the Laplacian noises added to satisfy DP. Reconstruction Errors
are due to the fact that one has to estimate a k-way marginal from
partial information.
Two important algorithmic parameters affect the magnitude
of these two kinds of errors. They are the number m of
marginals/views in the synopsis, and the size ‚Ñì (i.e., number of
attributes) of these views. With a larger ‚Ñì, the views cover more
combinations of attributes, reducing Reconstruction Errors. However, one would be summing over more noisy entries to compute
any marginal, increasing the Noise Errors. Similarly, a larger m
means more marginals and better coverage of combinations of attributes, which reduces Reconstruction Errors. However, a larger m
also means less privacy budget for each marginal and higher Noise
Errors. Consider the running example with 8 attributes, by using
14 (instead of 6) size-4 marginals, one can ensure that any set of 3
attributes is covered by at least one of the marginals, eliminating
Reconstruction Errors. However, this is done at the cost of adding
noises sampled from Lap 
14
œµ

instead of Lap 
6
œµ

to each cell. Note
that even if any set of 3 attributes is covered, answering 4-way
marginals will still have Reconstruction Errors.
Analysis in [29] shows that the choice of optimal ‚Ñì (size of each
marginal) is independent from parameters such as dataset size n,
privacy parameter œµ, and dimensionality d. In particular, setting
‚Ñì to be around 8 works well. The optimal choice of m (number
of marginals), however, depends on n, œµ,d, and the nature of the
Session 2B: Differential Privacy 1 CCS‚Äô18, October 15-19, 2018, Toronto, ON, Canada 21 
dataset. In [29],m is chosen to fully cover either all 2-way marginals
or all 3-way marginals, using the concept of covering design [18, 19].
4.2 Overview of the CALM Method
‚ë° Construct noisy
marginals
‚ë¢ Ensure
consistency
& non-negativity
‚ë£ Use maximum
entropy
Marginal tables
‚ë† Specify
marginals
ÔøΩÔøΩÔøΩ
ÔøΩÔøΩÔøΩ
ÔøΩÔøΩÔøΩ
Figure 2: Illustration of CALM. The users to the left are partitioned into groups. The aggregator to the right first specifies
the marginals to all the users and aggregate the reports for
each marginal table. Then the aggregator process the data to
publish the final results.
Ideas in the PriView method inspire the CALM method. In the
LDP setting, we cannot compute a noisy marginal by adding Laplace
noise to the true marginal, and we need to use FO protocols to do
so. In PriView, data from all users are used for computing each of
the m views, and the privacy budget is split into m equal portions.
But in the local setting, several previous work pointed out that by
partitioning users into groups, and having each group use the full
privacy budget, the overall error will be smaller [26, 37, 38]. We
adopt this design principle and split the user population into groups,
with reports from users in each group to estimate one marginal.
Figure 2 illustrates how CALM works. The aggregator first
chooses a set of m marginals and the FO protocol to be used (e.g.,
GRR or OUE). The choice of whether to use GRR or OUE is determined by the number of cells in each marginal, because GRR is
more accurate for domains of smaller size and OUE is better for
larger domains. CALM adaptively chooses which of GRR and OUE
to use, based on œµ and the domain size for the marginals.
The aggregator then assigns each user to one of the marginals,
and informs the user which marginal she should report. How this
assignment is done is outside CALM. The aggregator can randomly
partition the population into m groups of approximately the same
size, and assigns users in one group to each marginal. Alternatively,
the aggregator can use public information of the users (such as IP
addresses) to help ensure that each group is representative of the
overall population. It is also possible that the aggregator sends information of all m marginals to each user, having each user randomly
select one and report on that marginal.
Each user projects her private value v onto the marginal she is
reporting and reports the projected value of v via FO. On receiving
users‚Äô reports, the server uses the aggregation algorithm of FO to
obtain the noisy marginal tables. Then the server processes the
data via the consistency and reconstruction steps to obtain the final
results, as in the PriView method.
One main challenge is how to choose the set of m marginals,
and in particular the parameters ‚Ñì (size of each marginal) and m
(number of marginals). The analysis for PriView in [29] is no longer
valid for the local setting. We discuss this in Section 4.3. In addition,
we want to deal with non-binary attributes, which we discuss in
Section 4.4.
4.3 Choosing the Set of Marginals
The most important algorithmic parameters for CALM are the
marginal size ‚Ñì, i.e., the number of attributes in each marginal, and
the marginal number m, i.e., the number of different marginals.
For ease of analysis, we assume all marginals are of equal size and
receive equal number of users to contribute.
Similar to PriView, there are Noise Errors, which are caused
by the addition of noises in the FO protocol, and Reconstruction
Errors, which are caused by the fact that a k-way marginal may not
be covered by any of the chosen marginal, and has to be estimated
using the Maximum Entropy principle.
CALM has one additional source of errors that do not exist in
PriView. CALM splits the user population into groups, and uses the
marginal of one group as an estimation of the marginal of the whole
population. Errors may be caused by the fact that the marginal of
one randomly selected group is not representative of that for the
whole population. We call these Sampling Errors. We analyze these
errors below.
Noise Errors. To understand Noise Errors, we analyze the total
variance of estimating 1-way marginals when they are included in
at least one selected marginal, and how they are affected by the
choice ofm and ‚Ñì. For each ‚Ñì-way marginal table, there are n
m users
reporting it. By Equation (3), the variance for each cell is inversely
proportional to the group size used to estimate it. More specifically,
we have:
Varc = min 
4e
œµ
(e
œµ ‚àí 1)
2
,
L ‚àí 2 + e
œµ
(e
œµ ‚àí 1)
2

¬∑
m
n
Here L is the number of cells in one marginal, and an ‚Ñì-way marginal with binary attributes has L = 2
‚Ñì
cells. Note that when each
attribute has different number of possible values, L is the expected
number of cells in one marginal.
To construct a 1-way marginal from such an ‚Ñì-way marginal,
each cell of the 1-way marginal is the summation of some cells from
the larger (‚Ñì-way) marginal. By linearity of variances, the variance
for any 1-way marginal is Var1 = Varc ¬∑ L.
The above shows that increasing m adds a linear factor to the
variance. However, increasingm also causes a 1-way marginal to be
included more times. When a 1-way marginal is included t times, we
can obtain t estimations of the 1-way marginal, one from each size‚Ñì marginal that includes it. Averaging these t estimations reduces
the variance by a factor of t. More specifically, each size-‚Ñì marginal
includes ‚Ñì attributes. Therefore, in expectation, the information of
each attribute will be contributed from m¬∑‚Ñì
d
‚Ñì-way marginals. The
Session 2B: Differential Privacy 1 CCS‚Äô18, October 15-19, 2018, Toronto, ON, Canada 217
average of these estimates are therefore
NE(n,d, œµ, ‚Ñì) =
Var1
m¬∑‚Ñì
d
= min 
4e
œµ
(e
œµ ‚àí 1)
2
,
L ‚àí 2 + e
œµ
(e
œµ ‚àí 1)
2

¬∑
m
n
¬∑ L ¬∑
d
m ¬∑ ‚Ñì
= min 
4e
œµ
(e
œµ ‚àí 1)
2
,
L ‚àí 2 + e
œµ
(e
œµ ‚àí 1)
2

¬∑
L
‚Ñì
¬∑
d
n
(7)
The key observation here is that the magnitude of Noise Errors
does not depend on m, which is different from PriView. It does
depend on ‚Ñì and œµ, where œµ affects the first term, which is the
variance of the FO protocol. The parameter ‚Ñì affects both the term
L
‚Ñì
and the variance for the FO protocol.
Also note that when we estimate k-way marginals based on the
estimation of marginals of the k attributes, the estimation is affected
by the errors for each of the k attributes, we thus use k ¬∑NE(n,d, œµ, ‚Ñì)
as the Noise Errors when we optimize for a particular k value.
Reconstruction Errors. Reconstruction Errors occur when a kway marginal is not covered by any of the chosen marginal. The
magnitude of Reconstruction Errors depends on to what extent
attributes are correlated. If all attributes are mutually independent,
then Reconstruction Errors do not exist. When attributes are dependent, the general trend is that larger m and larger ‚Ñì will cover
more combination of attributes, reducing reconstruction errors. The
reduction effect of Reconstruction Errors diminishes as m increases.
For example, if all k-ways marginals are already fully covered, Reconstruction Errors are already 0 and cannot be further decreased.
Even if not all k-ways marginals are fully covered, increasing m
beyond some reasonably large number will only cause diminishing
return. Since Reconstruction Errors are dataset dependent, there is
no formula for estimating them.
Sampling Errors. Sampling Errors occur when a marginal in a
group of users deviates from the marginal in the whole population.
The parameter ‚Ñì has no impact on Sampling Errors. However, increasing m would cause each group size n
m to be smaller, raising
Sampling Errors. When computing a marginal from a group of
s = n/m users, each cell in the marginal can be viewed as the sum
of s independent Bernoulli random variables, divided by s. In other
words, each cell is a binomial random variable divided by s. Thus
each cell has variance MA(v)(1‚àíMA(v))
s
, where MA(v) is the fraction
of users with value v in the whole population. The Sampling Errors
for an ‚Ñì-way marginal A are thus
√ï
v ‚ààVA
MA(v)(1 ‚àí MA(v))
s
=
m √ó
√ç
c ‚ààVA MA(v)(1 ‚àí MA(v))
n
Since √ç
v ‚ààVA MA(v) = 1, we have √ç
v ‚ààVA MA(v)(1 ‚àí MA(v)) <
√ç
v ‚ààVA MA(v)¬∑1 = 1. Thus the Sampling Errors are simply bounded
by
SE(n,m) =
m
n
(8)
Choosing m and ‚Ñì. Both m and ‚Ñì affect Reconstruction Errors.
In addition, m affects Sampling Errors, and ‚Ñì affects Noise Errors.
Intuitively, we want to choose m and ‚Ñì to minimize the maximum
of the three kinds of Errors, since the maximum would dominate
the overall errors. However, we do not have a formula to estimate
Reconstruction Errors, which is dataset dependent.
We propose to choose a target error threshold Œ∏, which serves
as a rough estimation of Reconstruction Errors when they are not
zero, and choose m and ‚Ñì as follows:
‚Ä¢ Compute the largest marginal size ‚Ñìu , such that k ¬∑ NE < Œ∏.
‚Ä¢ When ‚Ñìu < k, one chooses ‚Ñìu and the largest m such that
SE < Œ∏.
‚Ä¢ Otherwise, one chooses m and ‚Ñìt ‚àà [k, ‚Ñìu ] such that the
maximum of NE and SE is minimized.
While Œ∏ intends to be a rough estimation of Reconstruction
Errors, it does not need to be chosen based on one particular dataset.
One can run experiments with a public dataset of similar nature
under different parameters, the best level of SSE that can be achieved
is usually a good indicator of the magnitude of Reconstruction
Errors. When a public dataset is unavailable, one can generate
a synthetic dataset under some correlation assumption and run
experiments. In experiments conducted for this paper, we choose
Œ∏ = 0.001, and use it for all datasets and settings.
Algorithm 1 Pseudocode to determine m and ‚Ñì
Require: Dataset parameters n,d, œµ, k, error threshold Œ∏.
Ensure: m and ‚Ñì.
1: procedure Inference(n,d, œµ, Œ∏)
2: Assign mu ‚Üê Œ∏ ¬∑ n, ‚Ñìu ‚Üê 2
3: while k ¬∑ NE(n,d, œµ, ‚Ñìu + 1) ‚â§ Œ∏ do
4: Increment ‚Ñìu ‚Üê ‚Ñìu + 1
5: if ‚Ñìu < k then
6: return min(mu,
 d
‚Ñìu

), ‚Ñìu
7: Assign ‚Ñìb ‚Üê ‚Ñìu
8: while ‚Ñìb > k and CoverDesign(d, k, ‚Ñìb ‚àí 1) ‚â§ mu do
9: Decrement ‚Ñìb ‚Üê ‚Ñìb ‚àí 1
10: if ‚Ñìb == ‚Ñìu then
11: return min(mu,
 d
‚Ñìu

), ‚Ñìu
12: Assign E ‚Üê 1,m ‚Üê mu, ‚Ñì ‚Üê ‚Ñìu
13: for ‚Ñìt
in [‚Ñìb
, ‚Ñìu ] do
14: Assign mt ‚Üê CoverDesign(d, k, ‚Ñìt )
15: if max(SE(n,mt ), k ¬∑ NE(n,d, œµ, ‚Ñìt )) < E then
16: Update E ‚Üê max(SE(n,mt ), k ¬∑ NE(n,d, œµ, ‚Ñìt ))
17: Update m ‚Üê mt
, ‚Ñì ‚Üê ‚Ñìt
18: return m, ‚Ñì
Algorithm 1 gives the pesudocode for determining m and ‚Ñì. The
algorithm uses the formula to calculate Noise Errors NE from (7),
and Sampling Errors SE as in (8). CoverDesign is an external procedure to calculate the number of ‚Ñì-way marginals that can fully
include all k-way marginals. Note that NE is for a single attribute;
one can multiply NE by k to approximate the Noise Errors for the
k-way marginals.
For example, Figure 3 gives the Noise Errors times k (i.e., k ¬∑ NE)
for n = 2
16
,d = 8, and k = 3 when œµ ranges from 0.2 to 2.0. If
we fix Œ∏ = 10‚àí3
, we can read from the figure that when œµ ‚â§ 1.4,
only ‚Ñì = 2 can be used. Because larger ‚Ñì will make NE even larger;
and we choose to allow some RE to exist. When œµ is larger, e.g.,
Session 2B: Differential Privacy 1 CCS‚Äô18, October 15-19, 2018, Toronto, ON, Canada 2  
0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6 1.8 2.0

10‚àí5
10‚àí4
10‚àí3
10‚àí2
10‚àí1
100
Noise Errors
2
3
4
5
6
7
8
Figure 3: Noise Errors times k when n = 2
16
,d = 8, k = 3.
œµ = 2.0, NE is already very small that we can tolerate more NE
to eliminate RE. In this case, both ‚Ñì = 3 and ‚Ñì = 4 will give an
k ¬∑NE < Œ∏, so the goal is to choose an ‚Ñì so that the maximum of k ¬∑NE
and SE is minimized. Specifically, when ‚Ñì = 3, CoverDesign gives
m =

8
3

= 56 to cover all the 3-way marginals, rendering max(NE =
0.00032, SE = 0.00085) = 0.00085; when ‚Ñì = 4, CoverDesign gives
m = 14 (meaning that 14 4-way marginals suffice to cover all 3-
way marginals when d = 8), thus giving max(NE = 0.00076, SE =
0.00021) = 0.00076. Thus ‚Ñì = 4 and m = 14 is used.
4.4 Consistency between Noisy Marginals
When different marginals have some attributes in common, those
attributes are actually estimated multiple times. Utility will increase
if these estimates are utilized together. Specifically, assume a set
of attributes A is shared by s marginals, A1,A2, . . . ,As . That is,
A = A1 ‚à© . . . ‚à© As . Now we can obtain s copies of TA by summing
from cells in each of the TA‚Äôs, i.e., TAi
(v) =
√ç
v‚Ä≤‚ààVAi
,v
‚Ä≤
A
=vA
TAi
(v
‚Ä≤
).
To obtain a better estimation of TA, we use the weighted average
of TAi
for all marginal Ai
. That is,
TA(v) =
√ï
i
wi
¬∑ TAi
(v).
Since each TAi
is unbiased, their average TA(v) is also unbiased.
To determine the distribution of the weights, the intuition is to
put more weights to the more accurate estimations. Specifically,
we minimize the variance of TA(v), i.e., Var[TA(v)] =
√ç
i w
2
i
¬∑
Var
TAi
(v)

=
√ç
i w
2
i
¬∑ Ci
¬∑ Var0, where Ci
is the number of cells
from Ai that contribute to A, i.e., Ci = |{v
‚Ä≤
: v
‚Ä≤ ‚àà VAi
,v
‚Ä≤
A
= vA}|,
and Var0 is the basic variance for estimating a single cell (we assume
each marginal has a similar amount of users, but the analysis can
be easily changed to different number of users). Formally, we have
the following problem:
minimize √ç
i w
2
i
¬∑ Ci
subject to √ç
i wi = 1
According to KKT condition [22, 23], we can derive the solution:
Define L =
√ç
i w
2
i
¬∑ Ci + ¬µ ¬∑ (√ç
i wi ‚àí 1), by taking the partial
derivative of L for each of wi
, we have wi = ‚àí
¬µ
2Ci
. The value of ¬µ
can be solved by the equation √ç
i wi = 1. As a result, ¬µ = ‚àí
2 √ç
i
1
Ci
,
and wi =
1
√ç
Ci
i
1
Ci
. Therefore, the optimal weighted average is
TA(v) =
√ç
i
1
Ci
¬∑ TAi
(v)
√ç
i
1
Ci
Once the accurate TA is obtained, all TAi
‚Äôs can be updated. For
any marginal Ai
, we update all v
‚Ä≤ ‚àà VAi
using the result of v where
v ‚àà VA and v
‚Ä≤
A
= vA. Specifically,
TAi
(v
‚Ä≤
) ‚Üê TAi
(v
‚Ä≤
) +
1
Ci

TA(v) ‚àí TAi
(v)

The remaining reconstruction operations are borrowed from
PriView and described in Appendix A.3. After that, one can obtain
the k-way marginals.
4.5 Discussion
We claim that CALM satisfies œµ-LDP because all the information
from each user to the server goes through an FO with œµ as privacy
budget, and no other information is leaked.
Although CALM is inspired from PriView, there are several differences between the two. Among the differences, many are because
the two methods work under different privacy requirements. That
is, PriView works in the centralized setting of differential privacy,
while CALM works in the local setting. We summarize the differences as follows.
‚Ä¢ In PriView, all the information are accessible to the server.
The server operates on the dataset, adds noise, and then
derive the answers. On the other hand, in CALM, each user
sends noisy information to the server, who aggregates the
reports, and then calculate the answers.
‚Ä¢ CALM can handle non-binary datasets, while PriView is
designed to handle only binary attributes.
‚Ä¢ In PriView, each view is estimated through the information
of all users, with split privacy budget. While in the local
setting, it is known that it is better to partition users into
groups. Therefore, in CALM, each marginal is estimated by
only a group of users.
‚Ä¢ Because of the above, CALM faces Sampling Errors, in addition to Noise Errors and Reconstruction Errors.
‚Ä¢ In PriView, the number of marginals is critical and is dependent on the dataset. On the other hand, CALM is much less
sensitive to the number of views (marginals).
‚Ä¢ In PriView, the optimal view size does not depends on œµ, and
is around 8. However, in CALM, view size affects which FO
protocol to be used and depends on œµ.
5 EVALUATION
We use experiments to empirically evaluate the effectiveness of our
proposed method CALM, and to verify our analysis.
Session 2B: Differential Privacy 1 CCS‚Äô18, October 15-19, 2018, Toronto, ON, Canada 21   
0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6 1.8 2.0

10‚àí3
10‚àí2
10‚àí1
100
Square error
(a) n = 2
16
, d = 8, k = 3
0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6 1.8 2.0

10‚àí3
10‚àí2
10‚àí1
100
Square error
(b) n = 2
16
, d = 16, k = 3
0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6 1.8 2.0

10‚àí3
10‚àí2
10‚àí1
100
Square error
(c) n = 2
16
, d = 32, k = 3
0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6 1.8 2.0

10‚àí3
10‚àí2
10‚àí1
100
Square error
(d) n = 2
18
, d = 16, k = 6
0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6 1.8 2.0

10‚àí3
10‚àí2
10‚àí1
100
Square error
(e) n = 2
18
, d = 32, k = 6
0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6 1.8 2.0

10‚àí3
10‚àí2
10‚àí1
100
Square error
(f) n = 2
18
, d = 32, k = 8
Kosarak
0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6 1.8 2.0

10‚àí3
10‚àí2
10‚àí1
100
Square error
(g) n = 2
16
, d = 8, k = 3
0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6 1.8 2.0

10‚àí3
10‚àí2
10‚àí1
100
Square error
(h) n = 2
16
, d = 16, k = 3
0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6 1.8 2.0

10‚àí3
10‚àí2
10‚àí1
100
Square error
(i) n = 2
16
, d = 32, k = 3
0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6 1.8 2.0

10‚àí3
10‚àí2
10‚àí1
100
Square error
(j) n = 2
18
, d = 16, k = 6
0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6 1.8 2.0

10‚àí3
10‚àí2
10‚àí1
100
Square error
(k) n = 2
18
, d = 32, k = 6
0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6 1.8 2.0

10‚àí3
10‚àí2
10‚àí1
100
Square error
(l) n = 2
18
, d = 32, k = 8
POS
CALM FC AM FT EM Uni
Figure 4: Comparison of different methods on binary datasets. We only plot the methods that are scalable in each setting, Uni
method is a baseline method. Results are shown in log scale.
Session 2B: Differential Privacy 1 CCS‚Äô18, October 15-19, 2018, Toronto, ON, Canada 220
5.1 Experimental Setup
Our experimental setup is largely influenced by that in [11], which
introduced the Fourier Transformation method and ran extensive
comparisons of several methods for this problem.
Environment. All algorithms are implemented in Python 3.5 and
all the experiments are conducted on a PC with Intel Core i7-4790
3.60GHz and 16GB memory.
Datasets. We run experiments on the following four datasets.
‚Ä¢ POS [43]: A dataset containing merchant transactions of half
a million users.
‚Ä¢ Kosarak [2]: A dataset of click streams on a Hungarian website that contains around one million users.
‚Ä¢ Adult [4]: A dataset from the UCI machine learning repository. After removing missing values, the dataset contains
around 50 thousands records. The numerical attributes are
bucketized into categorical attributes.
‚Ä¢ US [32]: A dataset from the Integrated Public Use Microdata
Series (IPUMS). It has around 40k records of the United States
census in 2010.
The first two are transactional datasets where each record contains some items. We treat each item as a binary attribute. Thus
these two datasets are binary. When running experiments with k
binary attributes, we pre-process a dataset to include only the top
d most frequent items. The later two are non-binary datasets, i.e.,
each attribute contains more than two categories.
Evaluation Methodology. To evaluate the performance of different methods, the Sum of Squared Error (SSE) of the marginals
is reported. That is, we compute the ground truth and calculate
the sum of squared difference in each cell. For each dataset and
each method, we choose 50 random k-way marginal queries and
measure their SSE. This procedure is repeated 20 times, with result
mean and standard deviation reported.
Competitors. The FC, AM, and EM methods can be directly applied. For a fair comparison, the FO used in those methods are also
chosen adaptively.
The FT method is unable to deal with the non-binary attributes.
Therefore, we implement the non-binary version of FT by encoding
each non-binary attribute into several binary attributes.
As a baseline comparison, we also plot the SSE of the Uniform
method (Uni in the figures), which always returns a uniform distribution for any marginal tables. Clearly, if the performance of
one method is worse than the Uniform method, the marginal constructed from that method is meaningless.
Experimental Settings. Different methods scale differently with
respect to d, the number of attributes, and k, the size of marginals.
Also, the error depends on n, the size of the dataset. We use three
values of d: 8, 16, and 32. We consider k = 3 for all three settings
of d. We consider k = 6 only for d ‚àà {16, 32}, and k = 8 only for
d = 32. This is because a larger k value makes more sense with a
larger d value.
We consider two dataset sizes n = 2
16 and n = 2
18, which were
used in [11]. Since all methods benefit similarly when n increases,
the comparison results remain valid for other n sizes.
The settings for m and ‚Ñì are given in Table 2 in the appendix.
5.2 SSE on Binary Datasets
Figure 4 illustrates the results for comparing CALM against existing
methods we discussed in Section 3 on two binary datasets Kosarak
and POS.
In all settings, CALM significantly outperforms all existing algorithms, and the advantage of CALM increases for larger d and larger
k values, and for smaller œµ values. For most settings, the difference
between CALM and FT, the closest competitor, is between one and
two orders of magnitude. When œµ is small, e.g., when œµ = 0.2, all
existing algorithms perform close to the Uniform baseline, meaning
they can provide very little information when the privacy budget is
small. Whereas CALM can still provide enough information even
for very small œµ. Furthermore, many methods simply do not scale
to the case of d = 32.
EM performs poorly, in fact it is often worse than the Uniform
baseline. This is because EM requires each user to report information on all d attributes, in order to perform inference. This means
dividing the privacy budget by d, which results in large perturbation. The other methods can split the population into groups,
instead of splitting privacy budget, thus performing better. Also,
when k is larger than 5, the computation time for EM method is
too long to run efficiently (about 20 minutes each query). We thus
do not plot EM for the k = 6, 8 cases.
Among the competitors, FT performs the best. Whend = 8, k = 3,
we can compute the variance for FC, AM and FT using Formulas (4),
(5), and (6). The results are 256 ¬∑ Var0 for FC, 448 ¬∑ Var0 for AM,
and 93 ¬∑ Var0 for FT. From Figures 4a and 4g, we can see that the
experimental results match the analytical comparison.
For d = 16, CALM‚Äôs performance is similar to the case of d =
8. Other methods, however, have significantly larger error. For
example, in Figure 4b, when œµ = 0.2, the squared error of CALM is
0.0055, which is 41 times better than the state-of-the-art method,
i.e., FT with squared error of 0.2266.
The performance of FC does not depends on k, since it constructs
a full contingency table.
When d = 32, most of the existing methods are unable to scale,
especially when k = 8. For the AM method, the number of possible
marginals are
32
8

= 10518300. As a result, the average number
of users that contribute information to each marginal is less than
one when we choose n = 2
16 and 2
18. Similarly, the number of
Fourier coefficients required to reconstruct 8-way marginals are
√ç8
s=1

32
s

= 15033173, resulting less than one user contributes to
each coefficient.
5.3 SSE on Non-binary Datasets
The experimental results for non-binary datasets, i.e., Adult and
US, are shown in Figure 5. To reduce computational complexity, we
pre-process all attributes to contain at most 3 categories.
The experimental results show the superiority of CALM, which
achieves around 1 to 2 orders magnitude of improvement over
existing methods.
By comparing the d = 8 and k = 3 setting in Figure 4 with
Figure 5, we observe that FT performs better than FC and AM in
the binary datasets, whereas performs worse in the non-binary
datasets. The bad performance in the non-binary datasets is due
to the binary encoding process, which dramatically increases the
Session 2B: Differential Privacy 1 CCS‚Äô18, October 15-19, 2018, Toronto, ON, Canada 2  
0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6 1.8 2.0

10‚àí3
10‚àí2
10‚àí1
100
Square error
(a) d = 8, k = 3
0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6 1.8 2.0

10‚àí3
10‚àí2
10‚àí1
100
Square error
(b) d = 15, k = 3
0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6 1.8 2.0

10‚àí3
10‚àí2
10‚àí1
100
Square error
(c) d = 15, k = 6
Adult, n = 2
16
0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6 1.8 2.0

10‚àí3
10‚àí2
10‚àí1
100
Square error
(d) d = 8, k = 3
0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6 1.8 2.0

10‚àí3
10‚àí2
10‚àí1
100
Square error
(e) d = 16, k = 3
0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6 1.8 2.0

10‚àí3
10‚àí2
10‚àí1
100
Square error
(f) d = 16, k = 6
US, n = 2
16
CALM FC AM FT BE Uni
Figure 5: Comparison of different methods in two non-binary datasets. We only plot the methods that are scalable in each
setting, Uni method is a baseline method, BE method is the binary encoding version of CALM. Results are shown in log scale.
number of Fourier coefficients required to reconstruct marginals.
Considering the case where all the 8 attributes have 3 categories,
d and k becomes 16 and 6 after the binary encoding. By variance
analysis, the variance becomes 14893 ¬∑ Var0, which is much larger
than 93 ¬∑ Var0 in the binary datasets.
To demonstrate the impact of handling non-binary attributes in
CALM, we also utilize the idea of binary encoding to implement
CALM, to which the consistent step of PriView can be directly applied. We call the binary encoding version of CALM the BE method.
We observe that the CALM performs better than BE. The reason
is that d and k becomes very large after binary encoding, which
increase the variance. When d = 16, k = 6, the BE takes too much
time in the Maximum Entropy estimation step. Thus, we do not
plot BE in this case.
5.4 Classification Performance
To demonstrate the practical utility of the proposed method, we
train the SVM classifiers using the Adult and US datasets. The goal
of the classifier is to predict whether a user‚Äôs annual income is
above 50k.
To train the model, we pick five attributes (features) and have
each method output the 6-way marginals (five features plus the
annual income label). The features for the Adult dataset are age,
workclass, education, education-num, and occupation, as features;
and the features for the US dataset are WRKRECAL (informed
of work recall), GRADEATT (grade level attending), SCHLTYPE
(public or private school), SCHOOL (school attendance), DIFFPHYS
(ambulatory difficulty). Note that we pick features by their semantic
relationships to the label. After the noisy marginal is obtained, a
synthetic dataset is generated based on this marginal. The synthetic
dataset is then used to train the SVM classifier. There are also two
baselines: NoNoise represents the method without enforcing œµLDP, it is the best case to aim for (using the same set of attributes).
Majority represents the naive method that blindly predict the label
by the majority label. All the methods are evaluated following the
typical process, where 80% of the records are sampled as training
set, and the other 20% are used as the testing set. And we evaluate
its utility by the misclassification rate on the testing set, i.e., the
fraction of records in the testing set that are incorrectly classified.
Figure 6 illustrates the misclassification rate of SVM classifier
trained by different methods. It is shown that in most cases, the
average misclassification rate of CALM is close to NoNoise. When
œµ is small, the classification model trained by FC and AM is not
Session 2B: Differential Privacy 1 CCS‚Äô18, October 15-19, 2018, Toronto, ON, Canada 222
0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6 1.8 2.0

0.2
0.3
0.4
0.5
0.6
Misclassification rate
(a) Binary partition, n = 2
16
0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6 1.8 2.0

0.2
0.3
0.4
0.5
0.6
Misclassification rate
(b) Binary partition, n = 2
18
0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6 1.8 2.0

0.2
0.3
0.4
0.5
0.6
Misclassification rate
(c) Binary partition, n = 2
20
0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6 1.8 2.0

0.2
0.3
0.4
0.5
0.6
Misclassification rate
(d) Non-binary partition, n = 2
16
0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6 1.8 2.0

0.2
0.3
0.4
0.5
0.6
Misclassification rate
(e) Non-binary partition, n = 2
18
0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6 1.8 2.0

0.2
0.3
0.4
0.5
0.6
Misclassification rate
(f) Non-binary partition, n = 2
20
Adult, d = 15, k = 6
0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6 1.8 2.0

0.2
0.3
0.4
0.5
0.6
Misclassification rate
(g) Binary partition, n = 2
16
0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6 1.8 2.0

0.2
0.3
0.4
0.5
0.6
Misclassification rate
(h) Binary partition, n = 2
18
0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6 1.8 2.0

0.2
0.3
0.4
0.5
0.6
Misclassification rate
(i) Binary partition, n = 2
20
0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6 1.8 2.0

0.2
0.3
0.4
0.5
0.6
Misclassification rate
(j) Non-binary partition, n = 2
16
0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6 1.8 2.0

0.2
0.3
0.4
0.5
0.6
Misclassification rate
(k) Non-binary partition, n = 2
18
0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6 1.8 2.0

0.2
0.3
0.4
0.5
0.6
Misclassification rate
(l) Non-binary partition, n = 2
20
US, d = 16, k = 6
CALM FC AM NoNoise Majority FT
Figure 6: Comparison on classification performance. We only plot the methods that are scalable in each setting. NoNoise is the
baseline where no noise is added; Majority is the naive method to always answer the majority label.
Session 2B: Differential Privacy 1 CCS‚Äô18, October 15-19, 2018, Toronto, ON, Canada 223
1 2 3 4 5 6 7 8
View size
31 29 27 25 23 21 19 17 15 13 11
9
7
5
3 1
Number of views
0.0058 0.0022 0.0068 0.0055 0.0056 0.0081 0.0121 0.0150
0.0050 0.0023 0.0075 0.0062 0.0051 0.0095 0.0106 0.0181
0.0052 0.0020 0.0063 0.0071 0.0063 0.0082 0.0107 0.0222
0.0053 0.0020 0.0073 0.0049 0.0062 0.0111 0.0121 0.0150
0.0059 0.0028 0.0059 0.0065 0.0069 0.0097 0.0123 0.0206
0.0057 0.0022 0.0055 0.0060 0.0060 0.0099 0.0121 0.0262
0.0054 0.0024 0.0061 0.0055 0.0048 0.0065 0.0108 0.0176
0.0052 0.0027 0.0073 0.0068 0.0076 0.0067 0.0125 0.0179
0.0054 0.0036 0.0068 0.0069 0.0059 0.0079 0.0143 0.0180
0.0055 0.0029 0.0060 0.0068 0.0091 0.0070 0.0116 0.0203
0.0053 0.0029 0.0070 0.0077 0.0057 0.0090 0.0144 0.0125
0.0054 0.0035 0.0079 0.0071 0.0070 0.0082 0.0153 0.0143
0.0037 0.0054 0.0096 0.0074 0.0120 0.0173 0.0224
0.0043 0.0075 0.0078 0.0117 0.0152 0.0154 0.0164
0.0049 0.0065 0.0086 0.0141 0.0110 0.0160
0.0180
0.005
0.010
0.015
0.020
0.025
(a) ‚Ñì vs m, d = 8, k = 3, œµ = 0.6
1 2 3 4 5 6 7 8
View size
0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6 1.8 2.0

0.0070 0.0038 0.0274 0.0369 0.0319 0.0456 0.0632 0.0770
0.0067 0.0031 0.0121 0.0114 0.0132 0.0143 0.0306 0.0297
0.0067 0.0033 0.0057 0.0087 0.0073 0.0087 0.0166 0.0192
0.0069 0.0029 0.0030 0.0047 0.0043 0.0053 0.0098 0.0116
0.0064 0.0026 0.0029 0.0025 0.0039 0.0044 0.0066 0.0104
0.0065 0.0031 0.0032 0.0021 0.0018 0.0027 0.0034 0.0053
0.0065 0.0029 0.0029 0.0015 0.0016 0.0021 0.0029 0.0037
0.0066 0.0025 0.0008 0.0008 0.0011 0.0015 0.0019 0.0028
0.0069 0.0029 0.0008 0.0007 0.0010 0.0013 0.0018 0.0021
0.0065 0.0025 0.0007 0.0007 0.0007 0.0011 0.0015 0.0021
0.015
0.030
0.045
0.060
0.075
(b) ‚Ñì vs œµ, n = 2
16
, d = 8, k = 3, m = 16
1 2 3 4 5 6 7 8
View size
0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6 1.8 2.0

0.0044 0.0042 0.0595 0.0652 0.0707 0.1158 0.1112 0.1184
0.0048 0.0042 0.0221 0.0252 0.0388 0.0437 0.0568 0.0705
0.0044 0.0037 0.0186 0.0184 0.0256 0.0254 0.0336 0.0402
0.0045 0.0036 0.0037 0.0137 0.0123 0.0177 0.0227 0.0352
0.0044 0.0035 0.0031 0.0065 0.0107 0.0146 0.0161 0.0247
0.0043 0.0036 0.0036 0.0061 0.0051 0.0069 0.0104 0.0197
0.0043 0.0030 0.0029 0.0040 0.0048 0.0049 0.0133 0.0167
0.0041 0.0031 0.0031 0.0030 0.0045 0.0039 0.0071 0.0088
0.0040 0.0033 0.0027 0.0022 0.0037 0.0043 0.0042 0.0061
0.0041 0.0033 0.0027 0.0024 0.0028 0.0033 0.0035 0.0061
0.02
0.04
0.06
0.08
0.10
(c) ‚Ñì vs œµ, n = 2
16
, d = 16, k = 3, m = 16
POS, n = 2
16
1 2 3 4 5 6 7 8
View size
31 29 27 25 23 21 19 17 15 13 11
9
7
5
3 1
Number of views
0.0089 0.0034 0.0053 0.0056 0.0062 0.0080 0.0130 0.0237
0.0086 0.0052 0.0058 0.0052 0.0060 0.0069 0.0109 0.0175
0.0088 0.0042 0.0065 0.0053 0.0042 0.0096 0.0121 0.0164
0.0089 0.0041 0.0061 0.0059 0.0049 0.0076 0.0063 0.0126
0.0086 0.0033 0.0055 0.0061 0.0051 0.0091 0.0147 0.0155
0.0086 0.0039 0.0056 0.0057 0.0065 0.0082 0.0103 0.0127
0.0089 0.0036 0.0063 0.0048 0.0050 0.0084 0.0106 0.0160
0.0085 0.0040 0.0061 0.0051 0.0059 0.0061 0.0142 0.0142
0.0088 0.0054 0.0047 0.0052 0.0059 0.0077 0.0110 0.0123
0.0087 0.0056 0.0051 0.0050 0.0062 0.0068 0.0099 0.0121
0.0088 0.0070 0.0055 0.0071 0.0066 0.0092 0.0098 0.0196
0.0086 0.0071 0.0053 0.0067 0.0089 0.0098 0.0126 0.0152
0.0075 0.0080 0.0081 0.0057 0.0122 0.0120 0.0107
0.0085 0.0071 0.0065 0.0102 0.0120 0.0121 0.0220
0.0045 0.0073 0.0099 0.0162 0.0111 0.0130
0.0146 0.004
0.008
0.012
0.016
0.020
(d) ‚Ñì vs m, d = 8, k = 3, œµ = 0.6
1 2 3 4 5 6 7 8
View size
0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6 1.8 2.0

0.0055 0.0041 0.0274 0.0317 0.0255 0.0430 0.0826 0.1027
0.0053 0.0044 0.0110 0.0122 0.0100 0.0160 0.0310 0.0287
0.0054 0.0037 0.0048 0.0052 0.0050 0.0115 0.0139 0.0153
0.0053 0.0030 0.0030 0.0041 0.0049 0.0089 0.0087 0.0079
0.0054 0.0029 0.0032 0.0037 0.0021 0.0036 0.0034 0.0070
0.0052 0.0029 0.0030 0.0017 0.0020 0.0027 0.0028 0.0042
0.0052 0.0031 0.0027 0.0018 0.0015 0.0022 0.0031 0.0036
0.0052 0.0029 0.0011 0.0009 0.0010 0.0016 0.0017 0.0027
0.0053 0.0031 0.0007 0.0006 0.0007 0.0012 0.0013 0.0023
0.0053 0.0030 0.0005 0.0005 0.0007 0.0009 0.0015 0.0015
0.02
0.04
0.06
0.08
0.10
(e) ‚Ñì vs œµ, d = 8, k = 3, m = 16
1 2 3 4 5 6 7 8
View size
0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6 1.8 2.0

0.0023 0.0027 0.0791 0.0668 0.0728 0.0782 0.1113 0.1184
0.0023 0.0036 0.0227 0.0273 0.0251 0.0436 0.0566 0.0566
0.0021 0.0029 0.0197 0.0130 0.0166 0.0298 0.0300 0.0266
0.0018 0.0026 0.0041 0.0134 0.0115 0.0107 0.0223 0.0301
0.0020 0.0020 0.0025 0.0069 0.0075 0.0087 0.0154 0.0179
0.0018 0.0019 0.0021 0.0039 0.0051 0.0044 0.0099 0.0182
0.0018 0.0019 0.0020 0.0029 0.0049 0.0090 0.0120 0.0121
0.0018 0.0018 0.0019 0.0021 0.0038 0.0038 0.0059 0.0084
0.0019 0.0019 0.0015 0.0018 0.0026 0.0038 0.0050 0.0068
0.0016 0.0016 0.0014 0.0014 0.0019 0.0025 0.0052 0.0054
0.02
0.04
0.06
0.08
0.10
(f) ‚Ñì vs œµ, d = 16, k = 3, m = 16
Kosarak, n = 2
16
Figure 7: Mutual effects of marginal size ‚Ñì, number of marginals m and the privacy budget œµ.
better than Majority (some times even worse than 50%, which is
even worse than random guess, and thus useless).
In the right two columns of Figure 6, we duplicate the datasets 4
times and 16 times to boost the accuracy. It can be seen that more
users will help with accuracy. For example, for the binary case of
the Adult dataset, the accuracy of CALM is almost optimal when
œµ = 1.4 when the dataset is duplicated 4 times; while when the
dataset is duplicated 16 times, the accuracy of CALM is almost
optimal when œµ = 0.8. We also observe that when the dataset
is non-binary (in the even rows of Figure 6), the performance is
slightly worse than if the dataset is binary. This is because in the
non-binary setting, there are more possible values in each attribute,
thus making the result worse.
5.5 Verifying Marginal Parameters
In Figure 7, we use heatmaps to illustrate the impact of marginal
size ‚Ñì, number of marginals m and the privacy budget œµ on the
squared error.
Figure 7a and 7d shows the mutual effect of ‚Ñì and m on POS and
Kosarak, respectively. This is for the setting of d = 8, k = 3, œµ = 0.6.
The two heatmaps show that when ‚Ñì is fixed to a value other than
1 and 8, increasing m will gradually decrease the error, which is in
accordance with the analysis in Section 4.3, as increasing m leads
to covering more marginals, thus reducing Reconstruction Errors.
While increasing m increases Sampling Errors, the level of Sample
Errors even when m = 32 is around 2
‚àí11 = 0.0005. Note that
when ‚Ñì = 1, each marginal includes a single attribute, increasing m
does not reduce Reconstruction Errors. Similarly, when ‚Ñì = 8 all 8
attributes are already covered in any marginal; increasing m thus
doesn‚Äôt change the error.
Figure 7b, 7c, 7e, and 7f show the mutual effect of ‚Ñì and œµ when
m is fixed. We observe that when œµ is small, it is better to choose
smaller ‚Ñì. The reason is that in this case the noise dominates the
error; thus we should choose smaller ‚Ñì to reduce noise. When œµ is
large, lager ‚Ñì is preferred since the effect of Reconstruction Errors
is dominant. The blue numbers show the squared errors under the
optimal setting through analysis, which is approximately approach
to the experiment results.
5.6 Impact of k and the Local Setting
Figure 8 serves two purposes. One is to study the accuracy of CALM
when one chooses parametersm and ‚Ñì that are optimized for k
‚Ä≤
, but
the query is for k-way marginals, where k , k
‚Ä≤
. This is interesting
to know because one may want to support k-way marginals for
different k values. The other is to compare the accuracy of CALM
with the centralized setting of PriView, to understand how much
utility one is giving up for the enhanced privacy of the local setting.
The first row of Figure 8 plots the effect of answering k-way
marginals when optimized for k
‚Ä≤ ‚àà {3, 6, 8} and when using centralized PriView. When k = 3 (the left sub-figure), different settings
of k
‚Ä≤ perform similarly. When k = 6 (the middle sub-figure), optimizing for k
‚Ä≤ = 3 clearly is worse when œµ = 1.2 and 1.4. This
Session 2B: Differential Privacy 1 CCS‚Äô18, October 15-19, 2018, Toronto, ON, Canada 224
0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6 1.8 2.0

10‚àí5
10‚àí4
10‚àí3
10‚àí2
10‚àí1
Square error
k
0 =3 k
0 =6 k
0 =8 priview
(a) n = 2
16
, d = 16, k = 3
0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6 1.8 2.0

10‚àí5
10‚àí4
10‚àí3
10‚àí2
10‚àí1
Square error
k
0 =3 k
0 =6 k
0 =8 priview
(b) n = 2
18
, d = 16, k = 6
0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6 1.8 2.0

10‚àí5
10‚àí4
10‚àí3
10‚àí2
10‚àí1
Square error
k
0 =3 k
0 =6 k
0 =8 priview
(c) n = 2
18
, d = 32, k = 8
Vary œµ
2 4 6 8 10 12 14 16
k
0
10‚àí3
10‚àí2
Square error
 =0.5  =1.0  =1.5  =2.0
(d) n = 2
16
, d = 16, k = 3
2 4 6 8 10 12 14 16
k
0
10‚àí3
10‚àí2
Square error
 =0.5  =1.0  =1.5  =2.0
(e) n = 2
18
, d = 16, k = 6
2 4 6 8 10 12 14 16
k
0
10‚àí3
10‚àí2
Square error
 =0.5  =1.0  =1.5  =2.0
(f) n = 2
18
, d = 32, k = 8
Vary k
‚Ä≤
Figure 8: Kosarak dataset. Using m and ‚Ñì optimized for different k
‚Ä≤
.
is because when optimizing for k
‚Ä≤ = 3, increasing œµ from 1 to 1.2
causes ‚Ñì to increase from 2 to 3, because it is estimated that for
3-way marginals, the noise error when going to ‚Ñì = 3 is sufficiently
low at œµ = 1.2. See Table 2 for the parameters chosen under different configurations. However, when optimizing for k
‚Ä≤ = 6, the
change of ‚Ñì from 2 to 3 occurs when œµ = 1.6. This also happens
when k = 8 (the right sub-figure), where the setting of k
‚Ä≤ = 3
performs bad when œµ = 1.6 and 1.8. Overall, we generally see the
best result when k = k
‚Ä≤
. Also, it appears that if one is unsure about
k, the size of query marginals, one should optimize for a slightly
larger k
‚Ä≤ value.
From first row of Figure 8, we also see that PriView performs
one to two magnitudes better than CALM. This is mainly because
much less noise is needed in the centralized setting. Theoretically,
the amount of noise added in the local setting is Œò

1‚àö
n

, while in
the centralized setting, the amount of noise is Œò

1
n

.
The second row of Figure 8 plots the effect of answering kway marginals while optimizing for a broader range of k
‚Ä≤ values
(from 2 to 16). We consider œµ ‚àà {0.5, 1.0, 1.5, 2.0}. When a setting
results in choosing the same pair of m, ‚Ñì as another configuration,
we reuse the accuracy number in the plot instead of running the
experiments again; thus any difference in a line is due to changes
in m, ‚Ñì. We observe that when œµ is small (i.e., œµ ‚àà {0.5, 1.0}) the
same parameters (m = 65, ‚Ñì = 2, to be precise) are chosen no
matter which k
‚Ä≤ one is optimizing for. Also there is little difference
in accuracy when computing k = 3. When œµ = 1.5 and k = 6,
optimizing for k
‚Ä≤ ‚â• 8 is sub-optimal. In the right sub-figure, we
observe that to compute k = 8-way marginals, optimizing for k
‚Ä≤ ‚â§ 3
results in worse accuracy (especially for œµ = 1.5).
Figure 9 shows SSE for different k-way marginals fixing n = 2
18
and d = 16. For œµ ‚àà {0.5, 1, 1.5, 2}, we plot results for two settings:
m and ‚Ñì optimized for k
‚Ä≤ = 3; and m and ‚Ñì optimized for k
‚Ä≤ = k. We
found that in most cases, the results for the two settings are similar,
because the m and ‚Ñì settings are the same. For œµ = 1.5, when k > 7,
the difference becomes significant. This is mainly because the ‚Ñì
values are different: when k
‚Ä≤ = 3, ‚Ñì = 3 by Algorithm 1; but when
k
‚Ä≤ = k ‚àà {8, 9, 10}, ‚Ñì = 2 by Algorithm 1.
6 RELATED WORK
Differential privacy has been the de facto notion for protecting
privacy. In the centralized settings, many DP algorithms have been
proposed (see [15, 35] for theoretical treatments and [24] in a more
practical perspective). Recently, Uber has deployed a system enforcing DP during SQL queries [21], Google also proposed several
works that combine DP with machine learning, e.g., [28].
Session 2B: Differential Privacy 1 CCS‚Äô18, October 15-19, 2018, Toronto, ON, Canada 225
1 2 3 4 5 6 7 8 9 10
k
10‚àí5
10‚àí4
10‚àí3
10‚àí2
Square error
 =1.0, k
0 =3
 =1.0, k
0 =k
 =2.0, k
0 =3
 =2.0, k
0 =k
1 2 3 4 5 6 7 8 9 10
k
10‚àí5
10‚àí4
10‚àí3
10‚àí2
Square error
 =0.5, k
0 =3
 =0.5, k
0 =k
 =1.5, k
0 =3
 =1.5, k
0 =k
Figure 9: Kosarak dataset, n = 2
18
,d = 16.
In the local setting, we have also seen real world deployment:
Google deployed RAPPOR [16] as an extension within Chrome; Apple [1] uses similar methods to help with predictions of spelling and
other things; Microsoft also deployed an LDP system for telemetry
collection [12].
Of all the problems, one basic mechanism in LDP is to estimate
frequencies of values. For this problem, several mechanisms have
been proposed [6, 7, 16]. Wang et al. compare different mechanisms
using estimation variance [37], and conclude that when the domain size is small, the Generalized Random Response provides best
utility, and Optimal Local Hash (OLH)/Optimal Unary Encoding
(OUE) [37] perform better when the domain is large.
The problem of marginal release is a classic application of histogram estimation, investigated by [11, 31]. These methods have
been examined in Section 3. Note that the problem has also been
investigated in the centralized setting [10, 29, 41], but the techniques cannot be directly used in this setting, because there is no
centralized aggregator that has the overall view of all users‚Äô data,
and the noise in the local setting is significantly larger (dependent
on ‚àö
n instead of sensitivity of the function).
Besides the marginal release problem, there are other problems in
the LDP setting that rely on mechanisms for frequency estimation.
The problem of finding heavy hitters in a very large domain was
exhaustively investigated [6‚Äì8, 17, 20, 25, 34, 38]. The frequent
itemset mining problem is to identify the frequent set of items that
appear simultaneously where each user has a set of items. Qin et al.
proposed LDPMiner [30] that finds the frequent singletons; Wang
et al. [39] improved LDPMiner by proposing SVIM for singleton
mining and SWSM for itemset mining. The problem of empirical
risk minimization also attracts lots of investigation, both from the
theoretically [33, 36, 42] and empirically [26].
7 CONCLUSIONS
In this paper, we propose LDP protocols to construct marginals
for high-dimensional attributes. Instead of directly generating all
marginal tables, we propose to strategically choose sets of attributes,
with which we can reconstruct all k-way marginals. To mitigate
the effect of noise, we adaptively choose randomization algorithm
based on the marginal size. We then consist all marginals to provide
more accurate estimation. Extensive experiments on real-world
datasets are conducted to illustrate the superiority over the current
state of the art.