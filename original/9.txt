Abstract
We explore whether interactive navigational behaviours can be used as a reliable and effective source to measure the progress, achievement, and engagement of a learning process. To do this, we propose a data-driven methodology involving sequential pattern mining and thematic analysis of the low-level navigational interactions. We applied the method on an online learning platform which involved 193 students resulting in six interactive behaviours that are significantly associated with learner achievement including exploration of the first week’s materials and exploration of the forum. The value of including these behaviours in predictive models increased their explainability by 10% and accounted for an overall explainability of 82%. Performance evaluations of the models indicate 91–95% accuracy in identifying low-achieving students. Other relevant findings indicate a strong association between the reduction of the behaviours over time and student achievement. This suggests a relationship between student interface learnability and achievement: achievers become more efficient at using the functionalities of an online learning platform. These findings can provide context to learning progress and theoretical foundations of interventions against unhelpful learning behaviours.

Previous
Next 
Keywords
Interactive behaviour

Interaction pattern

Self-regulated online learning

1. Introduction
While online learning platforms provide students with the convenience to access learning materials at any time and location, concerns relating to the self-regulated nature of these platforms are growing. Tracking engagement, achievement, and abandonment features that occur during the learning process has become a rising research topic in the area of learning analytics. The reasons for this are twofold: first, we can provide context for assessment results and differences in student performance by understanding the differences in their level of engagement (Fredricks et al., 2004). This information is valuable for educators as it can be used to identify students who are struggling at an early stage, with previous research suggesting, using performance predictors, that 50% of students who were close to fail or failed could have been identified before the start of the course (Wham, 2017); second, for those online learning platforms such as connectivist MOOCs (cMOOCs), the absence of an assessment method is one of the challenges of monitoring learning progress. Monitoring these features may provide an assessment-free alternative.

Previous research has associated interactive behaviour with features of learning on Learning Management Systems (LMS) and online search (Motz, Quick, Schroeder, Zook, Gunkel, 2019, Yu, Gadiraju, Holtz, Rokicki, Kemkes, Dietze, 2018), where it was shown that some student interactions are indicators of engagement and can predict knowledge gain. For instance, the number of learning resources visited have been linked to learner achievement (Brooks et al., 2015), the duration of learning sessions can be used to indicate knowledge status (Jansen et al., 2009), properties of search queries such as length or complexity have been linked to learning outcome (White et al., 2009), and the number of resources visited was shown to positively correlate with knowledge gain (Eickhoff et al., 2014).

User interactions can be classified depending on the level of abstraction, which ranges from low-level physical events (e.g. key press) to high-level task related events (e.g. completing an assignment) (Hilbert and Redmiles, 2000). Recent works demand that online learning platforms such as Coursera or Blackboard generate lower level granularity data with rich semantics to allow researchers to extract more interpretable information about what students do in order to create interventions (Fincham, Whitelock-Wainwright, Kovanović, Joksimović, van Staalduinen, Gašević, 2019, Maldonado-Mahauad, Pérez-Sanagustín, Kizilcec, Morales, Munoz-Gama, 2018). Established works use coarse-grained features that combine assessment data and user activity (Ferguson, Clow, 2015, Kizilcec, Piech, Schneider, 2013) suggesting the validity of behavioural data to measure features of learning. Crues et al. (2018) provide a comprehensive overview on these features, which mostly include interactions at the same abstraction level, typically coarse grained events such as the number of posts in the forum. Additionally, it has been suggested that both low-level and high-level interactions should be amalgamated as context may be spread across multiple levels and the composition of these events needs to be taken into consideration to give appropriate interpretations (Hilbert and Redmiles, 2000). In the context of online learning, it has also been suggested considering interaction patterns, as the order of interactions can encode valuable information about learning behaviour (Coleman et al., 2015). Low-level interaction patterns have been described elsewhere as micro-interactions (Breslav et al., 2014), where repeated scrolling, re-visitation, and the number of times a response to an opinion question is changed were helpful in identifying issues such as disengagement, low self-efficacy, and confusion.

Inspired by the above works, we explore these connections in an online learning environment where the lack of formal assessment and the analysis of lower level interactions call for a data-driven approach. Specifically, we investigate whether micro-interactions (i.e. students’ interactive behaviours) can be used to measure learning progress, achievement, and engagement. In the context of this work, we define engagement as the student’s level of participation in course materials and activities on the learning platform (Fredricks et al., 2004), and the number of units completed and whether students received a badge are used to indicate learning progress and achievement respectively. We address the following research questions (RQ):

•
RQ1. Can we identify micro-interactions that are relevant for online learning?

•
RQ2. Which micro-interactions are associated with learning progress and achievement?

•
RQ3. What is the added value of using micro-interactions?

To explore whether interface interactions are associated with learning progress and achievement, higher level task-related behaviours (e.g. watching a video) were generated from UI events (e.g. mouse clicks) by applying data mining techniques and qualitative analysis. Then, we explored the relationship between interactive behaviours and learning progress and achievement. Identifying these relationships provides both additional validation and insights on assessing learning. The contributions of this work are:

•
We developed a data-driven methodology to isolate micro-interactions that are indicators of learning progress and achievement. We then showed the feasibility of our approach by applying this method on an online learning platform.

•
Our approach identified six interactive behaviours which can explain 82% of the student achievement variation and provide a 91–95% accuracy in recognising low-achieving students. The added value of non-engagement metrics added a 10% of explainability of student achievement.

•
In the learning platform that serves as a case study of the methodology, we discovered that interactive behaviours involving exploration of the forum, and exploration of the first week materials before leaving the page are significant predictors of student achievement.

•
By discovering negative correlations between the frequency of interactive behaviours and achievement (i.e. the less frequent, the higher the achievement rate), we were able to associate the learnability of the online learning platform with the learning outcome.

2. Related work
This work builds on the metrics of online learning process (e.g. engagement, achievement), interactive behaviours to predict the features of learning, and sequential pattern mining to identify interactive behaviours.

2.1. Online learning metrics
In the context of self-regulated online learning, the definition and measurement of achievement can be a single (or a combination) of test scores and grades (Brooks, Thompson, Teasley, 2015, Gong, Liu, Zhao, 2018), course/personal learning goals (Farag, 2012, Wilkowski, Deutsch, Russell, 2014), and course completion (Robinson et al., 2016). In a recent study, student achievement was measured as personalised learning objectives, and the rates of receiving a certificate significantly improved for students whose learning objectives include receiving a certificate when compared to the entire course population (Rohloff et al., 2020).

Engagement has been established to be closely related to learning (Kuh, Cruce, Shoup, Kinzie, Gonyea, 2008, Prince, 2004, Zhang, Zhou, Briggs, Nunamaker, 2006) in that the more students engage with activities the better they perform (Kizilcec et al., 2013), while low-achieving students show lower level of engagement (Ding et al., 2018). It has been suggested that low-achieving students receive fewer gamified badges and are less engaged than other students (Ding et al., 2018). It has also been shown that evaluating engagement is key to assessing student retention, learning progress, and test performance (Baek, Shore, 2016, Crues, Bosch, Perry, Angrave, Shaik, Bhat, 2018, Ramesh, Goldwasser, Huang, Daume, Getoor, 2014, Singh, Padmanabhan, de Vreede, de Vreede, Andel, Spector, Benfield, Aslami, 2018). Engagement is often measured by learner activities, and the amount of participation in online learning forums and the interaction with lecture videos are two of the most common measures of engagement. Motivation has shown to be a significant predictor of engagement (de Barba et al., 2016), and it was found that motivation can predict behavioural, emotional, and cognitive engagement (Sun and Rueda, 2012). Previous work suggests that awarding badges has the potential to increase student motivation as it provides clarity of the learning goals and guidance on how to reach them (Abramovich, Schunn, Higashi, 2013, Hamari, 2017).

Approximately only 10% of registered students complete an online course  (Lushnikova et al., 2012). Hence, another frequently explored topic is student dropout/abandonment. Dropouts in online courses refer to the students who discontinue their participation in the course, while abandonment refers to passive participants —students who do not unregister and continue the course without being active participants (Rodriguez, 2012). Many studies have investigated the reasons students drop-out, the most common reasons being lack of intention to complete, personal circumstances, bad course design, limited digital skills, unmet expectations, and negative prior experiences (Gomez Zermeño, Aleman de la Garza, 2016, Onah, Sinclair, Boyatt).

2.2. Modeling student interactions
Interactive behaviours and patterns refer to how the learners interact with an online learning platform and how these behaviours are exhibited over time. These factors have been the focus of previous works which involve assessment and prediction. Web navigation behaviours, for example, have been used to predict the next page students are likely to explore (Pardos et al., 2017), their level of engagement/participation (Ramesh et al., 2014), the likelihood to dropout (Whitehill et al., 2017), and their level of competence (DeBoer, Breslow, 2014, Falakmasir, Gonzalez-Brenes, Gordon, DiCerbo, 2016, Käser, Hallinen, Schwartz, 2017). Similarly, student interactions have been used to evaluate participation behaviours in online collaborative learning (Daradoumis et al., 2006), as well as to predict gaming behaviours in an intelligent tutoring system (Muldner et al., 2011). Further to this, informed by learning design, a recent study analysed interactive patterns to better understand student behaviours such as resource transition and review (Shen et al., 2020), and interaction patterns together with self-regulated learning strategies and self-reported variables were found to be accurate predictors of learner types (Maldonado et al., 2018).

It has also been suggested that learning is associated with interactive behaviours such as browsing patterns (e.g. how students transition through different tasks) (Geigle and Zhai, 2017), click patterns (Park et al., 2017), exploration strategies (including how students interact with videos) (Ali and George, 2015), exploration choices (Käser et al., 2017), and problem-solving strategies (Guerra, Sahebi, Lin, Brusilovsky, 2014, Zhang, Biswas, Dong). Research has shown that students’ grades can be accurately predicted using internal-course interactions combined with student activities and behaviours beyond the learning platform (Pérez-Sanagustín et al., 2019). These works suggest that there are opportunities to relate online interactive behaviours with learning.

2.3. Sequential pattern mining
Sequential pattern mining (henceforth SPM) algorithms identify interaction patterns by extracting the sequential combination of interaction data that generates the most frequent patterns for a given support (i.e. the percentage of sequences that exhibit the pattern). SPMs have traditionally been used in retail where understanding shopping patterns can help increase profit by improving product allocation and display (Aloysius and Binu, 2013).

SPM algorithms generally produce three types of patterns: frequent, closed, and maximal sequential patterns. Frequent patterns are sub-sequences with frequencies exceeding a specified minimum support. Closed sequential patterns are frequent sequential patterns that are not strictly included in other sequential patterns having the same support. Finally, maximal sequential patterns are frequent sequential patterns that are not strictly included in other frequent sequential patterns. For instance, both pattern A {i,j} and pattern B {i,j,k} are frequent with their support counts being 0.3 and 0.2, respectively. We assume that for pattern A, its only frequent super pattern is B. In this case, pattern A is a closed pattern as its only super pattern is less frequent than itself. However, pattern A is not a maximal pattern as its super pattern B is also frequent. The pattern B would be a maximal pattern if it has no super patterns which are frequent. For mining sequential patterns, CM-SPADE generally has the best performance (Fournier Viger et al., 2017), while CM-ClaSP and CloSPan are generally better suited for mining closed sequential patterns, and so is VMSP for maximal sequential pattern mining (Fournier Viger et al., 2014).

SPM is frequently used in the area of educational data mining; to identify sequences of events that can distinguish stronger/weaker groups in an online collaboration environment (Perera et al., 2008), and to extract learning features to classify learner groups (Wang et al., 2004). It has been used in recent e-learning studies to improve online learning platforms, investigate online collaboration, and explore self-regulated learning (Doko, Abazi Bexheti, Hamiti, Prevalla Etemi, 2018, Dunaev, Zaytsev, 2017, Perera, Kay, Koprinska, Yacef, Zaiane, 2008). Previous work has also used SPM to mine student behaviours from length and correctness of steps taken in online programming courses (Hosseini et al., 2017). Compared to other similar purpose data mining techniques such as process mining, SPM is more general as it can be applied to various types of sequences. Both techniques were evaluated qualitatively and quantitatively to predict dropouts in online learning, and it was shown that SPM is better suited to handle the data produced by learning processes for predictive purposes (Deeva et al., 2018).

3. Study: setting, apparatus and data collection
Interaction data was extracted from an online learning platform constructed as a cMOOC (connectivist Massive Open Online Course), a type of MOOC that focuses on participatory learning with an emphasis on collaboration and creation. The platform targeted a specific student group, early career researchers. The courses took place in three waves and ran for four weeks each time: 12 November–16 December 2018, 21 January–17 February 2019, and 17 June–14 July 2019. The learning topic was open science and open research methods. The learning materials were divided into four weeks and three learning modules including 63 micro-learning units. The three modules were:

•
Data and information literacy: how to search, evaluate, and manage digital information.

•
Communication and collaboration: how to use technologies to interact, share, communicate, and collaborate.

•
Content creation: how to develop, integrate, and apply copyright to digital content.

The learning materials included text, videos, illustrations, links to online resources, etc. The platform provided a news page with updates about new modules, a wiki page where students share their reflections collaboratively, and a forum where students discuss the learning materials. Within each page there was a main area containing the learning materials and a left sidebar with links to other pages. For certain learning units the contents also included videos. 193 out of 382 students gave their consent to having their interactions collected. When each wave of the MOOC was finished, an online survey was sent to the students, which had a low response rate (10%). Hence, we should be careful making generalisations from the sample: the majority of students were based in the EU, the gender ratio was balanced and most of them were researchers or PhD students in natural sciences, engineering, and social sciences. We did not associate the demographic data to interaction data.

The platform was instrumented to generate low-level event interactive data, including browser window events such as page loads, and mouse and keyboard interactive data. Some of the events contain additional information, such as mouse coordinates for mouse events. We captured a total number of 281,087 interactive events of the types listed in Table 1 using WevQuery Apaolaza and Vigo (2017). The low-level interface events generated directly from the platform captured student interactions in detail, which can lead to outputs that contain noise such as unintended/unnecessary student actions (Dev and Liu, 2017). A series of pre-processing steps were conducted to combine or transform such events. For example, events which typically follow each other such as mouse-up and mouse-down, were combined and renamed as mouse-press. Key-down were renamed depending on whether they were commands (i.e. return key) or an alphanumeric key and same consecutive events were also merged. To ensure the interpretability of the event sequences, we grouped them by topic, i.e. the homepage, weekly material page, wiki page, forum, news page, and settings. Each recorded event had a corresponding timestamp, URL of the Web page where the event takes place, and the specific DOM element that triggers the event. Hence, a event-node-URL triple represented each event. For example, mouseinorout+main+wiki+multi indicates multiple mouse movements in the main area of the wiki page.


Table 1. Example of captured student interface events.

Type	Events	Description
Mouse	mousedown	Start of mouse click action
mouseup	End of mouse click action
mousemove	Mouse movement
mouseover	Hovering into target
mouseout	Hovering out from target
doubleclick	Double mouse click
mousewheel	Mouse wheel interaction
Selection	select	Selection of page content
cut	Content cut
copy	Content copy
paste	Content paste
Keyboard	keydown	Start of key press action
keyup	End of key press action
keypress	Key press action
Window	load	Page is loaded
resize	Browser window is resized
unload	Window is closed
windowfocus	Browser tab gains focus
windowblur	Browser tab loses focus
scroll	Change of scroll state
Other	change	Input element state change
contextmenu	Opening of context menu
We computed two indicators of learning: the badge status and the number of units completed. The instructors of the course awarded an ‘Open Science Aficionado’ badge to those students that conducted weekly assignments. The specific requirements of the badge were to achieve at least 12 out of 24 attainable points from the following activities: vote on other student’s forum post (1 pt.); comment on other student’s forum post (2 pt); complete weekly assignment (3 pt.). On the weekly assignment of week 1, students had to read an article about the opportunities and challenges of Open Science. On week 2, students had to use social media to share ideas, ask for advice and find research partners to run a research project. Week 3 was about sharing research using workflows involving DOIs, ORCID, open data repositories such as Zenodo and use of Altmetrics. Finally, on week 4, students had to design a research plan using what they had learned in the previous weeks. While the data of all the participants was used for pattern analysis, we excluded 53 students from the predictive modelling analysis as the first wave of the course did not award any badge, leaving 140 students for the analysis involving badges and 193 for progress analysis.

4. Methodology
We propose a three-step methodology to mine interaction data, analyse sequential patterns, and generate interactive behaviours of interest. First, we benchmark sequential pattern mining algorithms and different versions of our dataset to identify the combination yielding the most representative and informative patterns. Once we the set the parameters, we run the algorithms and extract the sequential patterns. Second, we apply thematic analysis to interpret and group the extracted patterns, transforming low-level interactions into higher-level interactive behaviours. Third, the interaction patterns that fall under the emerging themes are used to build user models.

4.1. Benchmarking algorithms and datasets
The success criteria for this stage aim at maximising the number of patterns generated and the sequence length. We benchmarked a number of SPM algorithms including CloSpan, CM-ClaSP, CM-SPADE, CM-SPAM, and VMSP, which were provided by the SPMF Java library (Fournier Viger et al., 2014).

To find representative patterns we maximised the number of patterns generated by a higher number of users. Each of the input sequences was constructed with a single student’s interaction events across all active sessions. To assess how representative a set of patterns are, we explored the value space of the minimum support parameter (i.e. the percentage of students that exhibit a given sequence) in the 0.35–0.8 range as values above 0.5 yielded a small number of patterns and the number of patterns increases dramatically when the support decreases below 0.4. To maximise the amount of information, we maximised the length of the pattern. Longer sequences entail more events, and thus more information. Single-event sequences were therefore excluded.

To explore interaction patterns over time, we split our dataset into sessions. Sessions were split by 5, 30, and 40 min of inactivity in between two consecutive events which is in line with what is suggested in the literature (Jones and Klinkner, 2008). This resulted in three different datasets: gap-5, gap-30, and gap-40 respectively. We compared the medians of sequence size of the three datasets with minimum support of 0.35–0.5. The number of patterns and their lengths were plotted for each dataset to identify intersections that maximised the criteria for representativeness and sequence length.

4.2. Thematic analysis
Thematic analysis is a widely used qualitative analytic method for identifying, analysing, and reporting patterns (themes) within, typically, qualitative data (Braun and Clarke, 2006). While the use of thematic analysis has been used to evaluate online learning platforms (Alturkistani, Majeed, Car, Brindley, Wells, Meinert, 2019, Meinert, Alturkistani, Brindley, Carter, Wells, Car, 2018, Ossiannilsson, Altinay, Altinay, 2015, Papathoma, Blake, Clow, Scanlon), we employ thematic analysis to systematically find themes on the patterns produced by the SPM algorithms in the previous stage and reduce the high number of sequential patterns (and address RQ1), being the first of its kind to adopt this method. We adopted the most common form of thematic analysis, a six-step process described by Braun and Clarke Braun and Clarke (2006): familiarising with the data, coding where we considered the sequential patterns generated as initial codes, and generating the themes. We conducted an inductive approach to determine themes as the data-driven nature of this research. We first followed the semantic approach to transform each code into a sub-theme as a sentence describing the explicit interaction. After the generation of sub-themes, we followed the latent approach with assumptions about the underlying behaviours and determined the final themes. Each theme was created with a combination and transformation of the sub-themes. Examples of this process can be viewed in Table 2 where the codes are results from the sequential pattern mining (i.e. patterns detected in low-level events) and the final themes are the interactive behaviours student exhibit. The interpretation of the behaviours reveals the strategies used by students such as interacting with videos and forums. An independent coder was involved in the thematic analysis. The inter-rater reliability was particularly high, Cohen’s . This may be due to the fact that sub-theme and theme analysis was conducted under a set of agreed rules: for example, load+windowfocus was defined as ‘explore’ rather than ‘load’, ‘focus’, or ‘view’, which reduced ambiguity.


Table 2. Example of theme generation process. The first column gives examples of the initial codes (i.e. sequential patterns). The second column shows the sub-themes generated from initial codes. The third column shows the final themes (i.e. interactive behaviours) generated from the sub-themes.

Codes	Sub-themes	Final themes
 
 
 	 
 	 
 
 
 	 
 
 
 	 
 	 
 
4.3. Modeling student interaction
To investigate RQ2, the behaviours identified through thematic analysis and their corresponding interaction patterns were sought in the original dataset for occurrences of each behaviour within each session for each student. These occurrences were represented in  matrices, where  is the number of students and  the number of sessions exhibited by the student. For instance, the matrix for behaviour  can be represented as follows:
 
where  is the frequency or number of occurrences of behaviour  for  in . Since not all students have the same number of sessions,  on those cells that do not have sessions.

To identify which behaviours were associated with indicators of learning, we computed descriptive statistics of the frequencies including the mean, median, and sum of occurrences. We also considered the sessions where students were ‘inactive’ (i.e. ), as dwell time is indirectly related with achievement through engagement  (Lu, Zhang, Ma, Shao, Liu, Ma, 2019, Yi, Hong, Zhong, Liu, Rajan, 2014). Also, to show the evolution of interactive behaviours, we computed the frequency trend to indicate the general direction in which the number of occurrences is developing. To calculate the trend, we calculated the mean, median, and sum of the occurrences across 3, 5, and 10 surrounding sessions. This is in consideration of the accuracy of calculated trend for a relatively sparse dataset where long periods of inactivity exist. The trend is calculated using two methods: the coefficient of its correlation with the session number variations, and the slope in a polynomial equation with the horizontal line representing the increase of session number (Oliphant, 2006). The trend, a combination of the aforementioned correlation and slope, was categorised in six levels of strength ranging from strong negative trend to strong positive trend. In total, we included 46 features for each behaviour as listed in Table 3. We then conducted correlation and regression analysis between these features, and the number of units completed (indicator of progress) and the badge status (indicator of achievement).


Table 3. Description of the features of interactive behaviours.

Feature	Description
Average	The average of occurrences across all sessions
Sum	The sum of occurrences across all sessions
NumGap	Number of consecutive inactivity sessions
MaxGap	Maximum number of consecutive inactivity sessions
Inactiv	Number of inactivity sessions in total
AvInactiv	Proportion of inactivity sessions in all sessions
Epi_not0	Number of active sessions
Episodes	Number of online sessions in total
Average_not0	Average of occurrences across all active sessions
Median	Median of occurrences across all sessions
T + 3/5/10 + mean/median/sum	Trend calculated as coefficient with the
mean/median/sum of 3/5/10 surrounding sessions
Pl + 3/5/10 + mean/median/sum	Trend calculated as slope with the
mean/median/sum of 3/5/10 surrounding sessions
S + 3/5/10 + mean/median/sum	Trend strength of T3/5/10
Sp + 3/5/10 + mean/median/sum	Trend strength of pl3/5/10
5. Results
Benchmarking

None of the five SPM algorithms was found to be problematic in terms of execution time and memory usage. As far as the number of patterns was concerned, for all of the datasets, CloSpan produced the fewest patterns while CM-SPADE produced the most. The ranges of sequence size under the minimum support of 0.5–0.8 was 2–4 for CloSpan and 2–8 for CM-SPADE, CM-ClaSP, and VMSP. In consideration of the next step of the methodology, it was more efficient to select a maximal sequential pattern mining algorithm to eliminate the possible overlapping sequences. We therefore performed our analysis using the maximal sequential pattern mining algorithm – VMSP on the three datasets.

The dataset with sessions separated by 40 min of inactivity (i.e. gap-40) is the one that generated the longer sequences exhibited by the largest number of students. This was measured as the point at which minimum support and sequence length are both maximised – which was 0.43 for minimum support. As a result, a total number of 110 patterns were generated with a sequence length range of 2–10.

Thematic analysis From the initial 110 patterns, 23 themes emerged from the thematic analysis, as shown in Table 4, including the exploration of the homepage, wiki, and the learning materials of week 1. Video and forum activities, which are known to be associated with learning (Atapattu, Falkner, 2017, Breslow, Pritchard, DeBoer, Stump, Ho, Seaton, 2013, Cormier, Siemens, 2010) were involved in six and five of the 23 interactive behaviours, respectively. Exploration of the left bar menu, which contains the links to different resources, is also a recurrent element suggesting intent to explore pages. We also found that ‘leaving a page’ is present in four of the interactive behaviours, suggesting the action of clicking a link provided in learning materials or the student being distracted from the course (i.e. withdrawing from exploring its contents). Eight interactive behaviours were found to involve interactions on different pages including homepage, materials of week 1, and forum.


Table 4. Interactive behaviours generated from thematic analysis.

Index	Interactive behaviour
1	Visit the wiki page
2	Explore the wiki page and left sidebar
3	Explore the week 1 page main area, left sidebar, and interact with videos
4	Explore the week 1 page main area, left sidebar, and forum main area
5	Explore the week 1 page main area then leave page
6	Explore the week 1 page main area and interacting with videos
7	Explore the week 1 page main area and left sidebar then leave page
8	Explore the week 1 page main area and left sidebar
9	Explore the week 1 main area and forum main area
10	Explore the week 1 page main area
11	Explore the homepage main area, left sidebar, and interact with videos
12	Explore the homepage main area and interact with videos
13	Explore the homepage main area and left sidebar
14	Explore the homepage main area then leave page
15	Explore the homepage main area and week 1 page main area, left sidebar, and interact with videos
16	Explore the homepage main area then week 1 page main area and interact with videos
17	Explore the homepage main area then week 1 page main area and left sidebar
18	Explore the homepage main area and left sidebar then week 1 page main area and interact with videos
19	Explore the homepage main area and left sidebar then week 1 page main area
20	Explore the homepage main area and left sidebar then forum main area
21	Explore the homepage main area
22	Explore the forum main area and left sidebar
23	Explore the forum main area
Modeling student interaction and analysis

For each of the 23 emergent behaviours, we computed the Spearman and Kendall correlation tests between the features in Table 3 and the two indicators of learning we extracted: the badge status and the number of units completed. We found moderate to strong significant correlations  between the features of six of the behaviours and the badge status as shown in Table 5. We found no significant correlations between the behaviours and the number of completed units.


Table 5. Behaviours and corresponding features correlated with achievement, where correlation coefficients  and .

Id	Interactive behaviour	Correlated feature	Spearman’s 	Kendall’s 
Explore the week 1 page main area, left sidebar, and forum main area	NumGap	0.55	0.54
Epi_not0	0.47	0.45
Explore the week 1 page main area then leave page	S10mean	0.48	0.47
T10mean	0.50	0.49
Explore the week 1 main area and forum main area	NumGap	0.48	0.46
Explore the homepage main area and interact with videos	NumGap	0.49	0.46
Explore the forum main area and left sidebar	NumGap	0.57	0.55
Sum	0.51	0.49
Pl10sum	0.49	0.46
Pl5sum	0.49	0.46
Explore the forum main area	NumGap	0.61	0.57
Epi_not0	0.58	0.53
S3sum	0.72	0.71
S5sum	0.64	0.63
From the features in Table 3, Episodes1 (i.e. the number of online sessions in total) correlates positively with whether students received a badge ( ), which is in line with literature that suggests the more active a student is, the more likely a badge will be awarded (Guo, Kim, Rubin, 2014, Jansen, Booth, Smith, 2009, Singh, Padmanabhan, de Vreede, de Vreede, Andel, Spector, Benfield, Aslami, 2018). The highest positive correlation was yielded by the NumGap feature of  ( ), which indicates a relationship between obtaining a badge and the number of continuous sessions where the forum’s main area was not explored. On the other hand, the highest negative correlation was S3sum ( ) for the same behaviour, suggesting a relationship between achievement and a decrease in the exploration of the forum over time.

For the six interactive behaviours that yielded coefficients above 0.45 with badge status in Table 5, we included in a regression analysis the top two strongly correlated positive/negative features of each behaviour as long as they were not variations of the same feature (e.g. S3sum and S5sum for ) to prevent overfitting. These were the dependent variables of the regression analysis, while badge status was the dependent variable. As dependent variable is binary, we used logistic regression analysis. The logistic models with 1–6 predictors were computed following the stepwise backward and forward techniques, where Model (1) has the highest explainability (81.8%, Nagelkerke ) about whether a student received a badge:(1)

The goodness-of-fit test (Hosmer–Lemeshow test) yielded a value of 0.246 and was insignificant (), suggesting that the model fits the data well. Two descriptive measures of goodness-of-fit presented are Cox and Snell  as well as Nagelkerke. These indices are variations of the  concept defined for Ordinary Least Squares regression (OLS) model.

According to Model (1), the log of the odds of receiving a badge is negatively associated with the exploration and withdrawal of the week 1 page main area (), the exploration of the week 1 main area followed by the exploration of the forum (), the exploration of the homepage followed by the interaction with videos (), and the exploration of the forum main area and left sidebar (). Those behaviours that were positively associated with receiving a badge include the exploration of the week 1 page main area, left sidebar, and the forum main area (), and the exploration of the forum main area (). The statistical significance of individual regression coefficients were tested using the Wald chi-square statistic. According to the results, only ’s S10mean () and ’s NumGap () were significant predictors of badge status.

In other words, the more students revisited (S10mean) week 1 materials to then leave the learning platform without interacting with any other section of the MOOC (), the less likely they were to receive a badge. This could be because this behaviour models those who got stuck in week 1, try to catch up several times but make no further advances in the course. We confirm that the number of consecutive sessions in which the forum was not explored (NumGap on ) is a strong predictor of badge status. The odds of students exhibiting  (S10mean) not receiving a badge were 3, whereas the odds for those exhibiting  (NumGap) receiving a badge were 11.

Some of the features included in our previous regression analysis are closely related to engagement, including the number of inactive sessions. We know from previous work that dwell time is related to engagement (Guo, Kim, Rubin, 2014, Lu, Zhang, Ma, Shao, Liu, Ma, 2019, Singh, Padmanabhan, de Vreede, de Vreede, Andel, Spector, Benfield, Aslami, 2018, Yi, Hong, Zhong, Liu, Rajan, 2014). To investigate the added value of including non-engagement metrics in the models and address RQ3, we computed a baseline model with those features in Table 3 that are known indicators of engagement. Similar to the previous analysis, the variable that contributed most was the number of consecutive sessions (NumGap) in which the forum was not explored (). The model with the highest explanatory power (Nagelkerke  = 72.8%) for whether a given student receives a badge is:(2) where only  is a significant predictor of achievement (). We computed  values of the top-20 performing models for both analyses: i.e. the models including all features (AllFeatures) and the models including only engagement features (EngFeatures) —see Table 6. Our results indicate that including non-engagement behaviours in the models adds a 9% of explainability in the best case and a 14% on average for the Nagelkerke .


Table 6. Comparison between models including all the features (AllFeatures) and models including engagement features only (EngFeatures).

Cox & Snell 	Nagelkerke 
AllFeatures	EngFeatures	Diff	AllFeatures	EngFeatures	Diff
Best	0.43	0.38	+0.05	0.82	0.73	+0.09
Average	0.42	0.35	+0.07	0.81	0.67	+0.14
Median	0.42	0.36	+0.06	0.81	0.69	+0.12
We then conducted 10-fold cross validation using a variety of classifiers. As the distribution of students with and without a badge is imbalanced, only 12% (17 out of 140) of the participants received a badge with a 1:7.2 ratio, to avoid losing information in majority examples to undersampling (Haixiang et al., 2016) and overly-optimistic estimates (Santos et al., 2018), we performed oversampling during the cross validation procedure. Synthetic Minority Oversampling Technique (SMOTE) coupled with Tomek Links was performed as the combination of algorithms was suggested to prevent overfitting (Santos et al., 2018). Stratified k-fold cross validation was used to ensure that the proportion of positive to negative examples is kept in the folds. The results for the 10-fold cross validation on the original dataset and with oversampling are shown in Tables 7 and  8, where the accuracy change (difference between the accuracy of the model and the default distribution percentage which is the students without a badge out of all the students), precision, recall, and f1 score are tabulated. We focus on the precision and accuracy as the importance of identifying students with tendencies to fail is greater than identifying students who are likely to succeed. For the original dataset (without oversampling), the accuracy of the models identifying failing students is 91–95%, 3–7% higher than using random selection (i.e. the percentage of students without a badge, 88%). The Bagging classifier yields the largest increase in accuracy across the two sets of results (7%), with precision being 0.90.


Table 7. 10-fold cross validation of the original dataset.

Classifiers	Precision	Recall	F1 Score	Accuracy	Accuracy change
K Nearest Neighbour	0.65	0.60	0.62	0.94	+0.06
Naive Bayes	0.72	0.80	0.73	0.92	+0.04
Decision Tree	0.72	0.75	0.69	0.93	+0.05
Random Forest	0.85	0.75	0.78	0.94	+0.06
Logistic Regression	0.77	0.75	0.73	0.94	+0.06
Support Vector Machine	0.90	0.55	0.67	0.94	+0.06
AdaBoost	0.48	0.50	0.48	0.91	+0.03
Multi-layer Perception	0.57	0.60	0.56	0.93	+0.05
Bagging	0.90	0.60	0.70	0.95	+0.07
Gradient Tree Boosting	0.75	0.60	0.65	0.94	+0.06

Table 8. 10-fold cross validation with oversampling.

Classifiers	Precision	Recall	F1 Score	Accuracy	Accuracy change
K Nearest Neighbour	0.58	0.85	0.66	0.89	+0.01
Naive Bayes	0.57	0.95	0.69	0.89	+0.01
Decision Tree	0.78	0.70	0.69	0.93	+0.05
Random Forest	0.75	0.75	0.71	0.92	+0.04
Logistic Regression	0.73	0.85	0.73	0.91	+0.03
Support Vector Machines	0.63	0.85	0.71	0.91	+0.03
AdaBoost	0.72	0.80	0.73	0.94	+0.06
Multi-layer Perception	0.66	0.85	0.71	0.93	+0.05
Bagging	0.57	0.90	0.65	0.89	+0.01
Gradient Tree Boosting	0.62	0.65	0.59	0.92	+0.04
To investigate the occurrences of the behaviours over time, we analysed the occurrences of the 23 behaviours in each session for the students that eventually received a badge and students who did not. The number of the active students for each session was plotted alongside the average of occurrences for each session among the students who received a badge and the students who did not (data had been smoothed using LOESS Curve Fitting). Based on our observations of the visualisations as shown in Fig. 1, we classified these behaviours into four clusters:

1.
Behaviours of both groups evolve similarly over time (Fig. 1a).

2.
Behaviours were exhibited more by students with the badge (Fig. 1b). Among the six behaviours included in our models,     are in this cluster.

3.
Behaviours of both groups evolve similarly for most sessions, with significant differences only at specific sessions (Fig. 1c). Behaviours  and  are classified in this cluster.

4.
Behaviours were exhibited more by students without the badge for most sessions, with significantly higher peak for badge holders in specific sessions (Fig. 1d).

Fig. 1
Download : Download high-res image (556KB)
Download : Download full-size image
Fig. 1. Graphical representation of the number of the active users for each session (brown line), the average of occurrences for each session among students who received a badge (blue line), the average of occurrences for each session among students who did not receive a badge (purple line). (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.)

6. Discussion
We revisit the research questions formulated at the outset:

RQ1. Can we identify micro-interactions that are relevant for online learning?

We utilised sequential pattern mining and thematic analysis to find implicit regularities that are associated with learning including achievement and unit progress. Our methodology allowed us to investigate micro-interactions in different abstraction levels by grouping fine-grained interaction patterns into higher level behaviours. Our findings suggest that micro-interactions can potentially be used to predict achievement.

RQ2.Which micro-interactions are associated with learning progress and achievement?

We confirmed existing findings on an online learning platform and discovered that interactive behaviours involving exploration of forum, and leaving the online platform after the exploration of the week 1 materials are significant predictors of student achievement. Further to this, our findings provide more clarity on these behaviours: first, the less students abandon week 1 materials, the more likely they are to receive a badge; second, the number of consecutive sessions where the forum’s main area was not explored was strongly related to obtaining a badge. While the first behaviour confirms that withdrawal is directly related to the (lack of) achievement, it is unexpected that the less the forum is explored the more likely students are to achieve a badge (Anderson, Huttenlocher, Kleinberg, Leskovec, 2014, Crues, Bosch, Perry, Angrave, Shaik, Bhat, 2018, Ramesh, Goldwasser, Huang, Daume, Getoor, 2014). This may be due to the forum being perceived as a collaborative tool within the context of a cMOOC, which was used as a resource for students who were struggling. Hence, not required by advanced students.

The interactive behaviours include regularities such as exploration of the forum and interactions with videos. Forum participation and video interactions have been thoroughly investigated in previous research (Atapattu, Falkner, 2017, Kim, Li, Cai, Gajos, Miller, 2014, Romero, López, Luna, Ventura, 2013, Wang et al., 2015), perhaps as they are the most representative of a classic classroom structure. The other frequent elements, for example exploration of homepage, week 1 materials, and leaving week 1 materials are within expectation: exploring the week 1 materials and leaving may be interpreted as distraction from the learning material and abandonment of the course, which resulted in low achievement. This is in line with the visualisations in Fig. 1d illustrating the decrease of interactions over time and previous research indicating that participation tends to drop rapidly within the first week (Evans et al., 2016).

We found that a decrease in exploring the forum is a strong predictor of achievement while the visualisations show that successful students were more active in the forum. The findings may seem contradictory as they indicate that the levels of participation in forums decreased over time for students who received a badge while still remained higher compared to other students throughout the sessions. More forum activities suggests that the student is more likely to receive a badge, which is in line with related studies showing that forum participation positively affects the learning outcome (Anderson, Huttenlocher, Kleinberg, Leskovec, 2014, Crues, Bosch, Perry, Angrave, Shaik, Bhat, 2018, Ramesh, Goldwasser, Huang, Daume, Getoor, 2014). Our findings can be explained in that, in sessions when students were less active in the forum, they were conducting other learning activities in the platform. It may also be the case that as students use the forum to make progress and learn, they depend on the forum less and become more autonomous learners.

We did not find strong relationships with the number of units completed. This may be because obtaining the badge involves a set of diverse activities and it is more informative than student progress. The models can be used to develop, for instance, browser extensions to capture interface interactions in real time and predict student learning trajectory to provide feedback and guidance to students and course leaders.

RQ3. What is the added value of using micro-interactions?

Features of engagement (e.g. the number of inactive sessions) are the strongest predictor of achievement according to our analysis. The explainability of models built with engagement features is as high as 72.8%. By comparing the models from two regression analysis containing different features, we discovered a 10% contribution to the explainability of student achievement from features of behaviours other than engagement, and perhaps learnability. When analysing the performance of the models, results indicate a 3-7% increase in accuracy in identifying students who will not receive a badge versus random selection (i.e. the percentage of students who did not receive a badge) suggesting that micro-interactions implicitly capture learning processes. This can be explained in that the relationship between cognitive engagement and learning outcomes is mediated by user activity (Fincham et al., 2019). Consequently, it can be argued that the user activity in online learning platforms contains behavioural markers that are indicators of cognitive processes and learning outcome.

6.1. The role of user interface learnability
In general, beyond the decrease in the interactions of successful students with the forum, we discovered that students exhibited decreasing trends of participation throughout the online sessions. The decrease can be expected as engagement typically decreases over time on online learning platforms. The correlation results showed that most of the trends were negatively correlated with student achievement, which means that the less students exhibit these behaviours, the more likely they are to receive a badge. As the trend features were calculated using windows of 3/5/10 sessions, the trends can be considered as fluctuations, which was confirmed by the visualisations of occurrences, as shown in Fig. 1a. More fluctuations may mean that students exhibited different interactive behaviours at the same time. As students engage with the platform they became more familiar with the platform, and consequently they became more proficient at navigating and using it. This finding may further support our assumption that interface learnability has an effect on the learning outcome as measured by student achievement. Usability has been investigated in online learning settings to evaluate different platforms and to provide insights on the design of platforms and the learning resources (Tsironis, Katsanos, Xenos, 2016, Xiao, Jiang, Xu, Wang, 2015). Although it has been suggested that there is no widely-agreed definition for learnability, it can be described as ‘the ability to improve performance over entire usage history’ based on a taxonomy provided in Grossman et al. (2009). Learnability as an important and perhaps fundamental component of usability (Abran, Khelifi, Suryn, Seffah, 2003, Nielsen, 1994), has not been thoroughly discussed in online learning platforms.

The plausible association between becoming a more efficient user and student achievement may suggest that the easier a learning platform is to use, the better students learn. It may also suggest that students who are more familiar with the learning platform or skilled navigators are better at learning. This opens up new research avenues into analysing the learnability of platforms and student learning outcome.

6.2. Implications
Methodological implications Previous research has identified the challenges of using sequential pattern mining algorithms in educational studies (Poon et al., 2017): (1) the generation of excessive patterns with limited relevancy and value, and (2) the involvement of domain experts for filtering and labeling purposes. For the first challenge, we proposed a benchmarking process to select the most suitable algorithm, dataset and algorithm parametrisation so that we maximise how informative and representative the results are. The second challenge is relatively common in data mining and analysis scenarios. Through a number of established methodological steps, thematic analysis facilitates the work of domain experts.

Theoretical implications We found that micro-interactions can be behavioural markers of online learning. Whether these micro-interactions are universal is unknown but they are probably not generalisable across online platforms. However, they provide further explainability in terms of interpretability and predictability of what students do.

Implications for practice Despite the current demands on the use of rich interaction data (Maldonado-Mahauad et al., 2018), existing data analysis procedures in online learning do not contemplate low-level events. This may be due to the lack of infrastructure to do so although, actually, there is an availability of these tools (Apaolaza and Vigo, 2017) which facilitate low-level data collection. However, computing patterns and building student models require programming and data science expertise. While they are not widespread and their coverage is limited, tools such as WevQuery-PM (Apaolaza and Vigo, 2019) can lighten this burden.

6.3. Limitations
As each online learning platform is designed for different purposes and audiences, there may be limitations to the extent of generalisability of our conclusions, which is a common issue in online learning (Alonso-Mencía et al., 2020). It is well known that the models defined for MOOCs are typically valid within the scope of the MOOCs under scrutiny: early career researchers on a cMOOC. There are no universal models as students, learning contents, and course designs differ (Motz et al., 2019). Hence, researchers emphasise on methodological approaches to build student models (Lehmann et al., 2012). We contribute to such body of knowledge with this novel approach.

Also, compared to other online learning platforms, the sample size is relatively small due to the fact that the platform was targeting an specialised student group, early career researchers. We counterbalance this limitation with the large amount of data points collected, which give us an in-depth understanding of student behaviour.

While students interactions with the platform were used to predict whether they will receive a badge or not, achieving a badge is also rewarded partly for these same interactions. It could be argued, however, that low-level interactions used in analysis contain many more varieties of higher level interactions (such as viewing the video content and wiki page) than those used to give out the badge. Further to this, the criteria for receiving a badge includes activities and assignments which are not captured in these low-level interactions.

7. Conclusion
We propose a three-step methodology to determine if there are implicit micro-interactions associated with indicators of learning. To suggest its feasibility, we apply the methodology in an online learning platform. From the generated datasets, we extracted student interaction patterns using sequential pattern mining algorithms. To handle the sheer numbers of patterns we applied thematic analysis to group the patterns into themes that represent interactive behaviours. Then we sought the presence of these representative behaviours in the original dataset to build student models. Finally, we conducted statistical analysis between features derived from the models and indicators of learning including achievement and unit progress.

We identified features from six interactive behaviours that strongly correlated with achievement that explain 72% of the student achievement variation, which was 10% higher when non engagement features were included. The models with the interactive behaviours were 3-7% more accurate at identifying students who will not receive a badge than random selection. It would have not been possible to obtain the increases in explainability and accuracy without our data-driven approach. This increase involves deeper insights gained from the relationship between student achievement and learnability of the student interface. In summary, our data-driven approach provides self-regulated learning platforms with a fresh perspective on measuring indicators of learning, and has revealed connections between platform/student learnability and learning outcome.