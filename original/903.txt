Blockchain has found applications to track ownership of digital assets. Yet, several blockchains were shown vulnerable to network attacks. It is thus crucial for companies to adopt secure blockchains before moving them to production. In this paper, we present Red Belly Blockchain (RBBC), the first secure blockchain whose throughput scales to hundreds of geodistributed consensus participants. To this end, we drastically revisited Byzantine Fault Tolerant (BFT) blockchains through three contributions: (i) defining the Set Byzantine Con-sensus problem of agreeing on a superblock of all proposed blocks instead of a single block; (ii) adopting a fair leaderless design to offer censorship-resistance guaranteeing the commit of correctly requested transactions; (iii) introducing sharded verification to limit the number of signature verifications without hampering security. We evaluate RBBC on up to 1000 VMs of 3 different types, spread across 4 continents, and under attacks. Although its performance is affected by attacks, RBBC scales in that its throughput increases to hundreds of consensus nodes and achieves 30k TPS throughput and 3 second latency on 1000 VMs, hence improving by 3× both the latency and the throughput of its closest competitor.
SECTION I.Introduction
Unlike classic replicated state machines (RSM), blockchains [73] aim at offering a peer-to-peer model where many geodistributed participants replicate the system state and where even more requesters can check their balance and issue cryptographically signed transactions. While permissionless blockchains allow any nodes to participate in the consensus protocol and permissioned blockchains allow only a pre-determined set of nodes to participate, new blockchain designs will likely be open permissioned where permissioned nodes offer a Byzantine Fault Tolerant (BFT) consensus service to which permissionless clients have access [15]: Ethereum v2.0 gives permissions in exchange of a proof-of-stake while other blockchains are naturally building upon BFT consensus [50], [37], [63]. The limitation of these blockchains is that they cannot offer high throughput when deployed on hundreds of nodes: verifying all transactions is computationally intensive while agreeing on a block is communication intensive.

In this paper, we propose Red Belly1 Blockchain (RBBC), the first secure blockchain that scales to hundreds of geodistributed consensus nodes. As far as we know, previous blockchains either assume synchrony (a known bound on message delays) or their performance drops when the number of nodes increases. By contrast, RBBC achieves a strong form of scalability where throughput does not drop as the number of consensus nodes increases. Scaling to hundreds of consensus nodes is ideal for a decentralized representative governance where at least one consensus node can run in each of the 195 independent sovereign nations around the world to serve the requests of many more nodes. RBBC is secure in that it prevents double spending [73] by resolving conflicts and not forking—even with asynchrony—and is resilience optimal in that, among the n nodes executing each of its consecutive consensus instances, up to t < n/3 can be Byzantine [60]. The consensus protocol of RBBC is also time optimal [35] and was proved correct for any number of nodes using model checking [9]. As RBBC supports reconfiguration [87], the set of consensus nodes can be changed before being bribed.

To achieve scalability, RBBC offers a new balancing method that totally orders all transactions while assigning them to distinct groups of proposer and verifier nodes. (i) Its leaderless design balances the communication load on multiple proposers, hence avoiding the congestion and slowdown induced by the least responsive node. As opposed to classic Byzantine consensus protocols that rely on a leader to pro-pose transactions, RBBC’s multiple proposers combine distinct sets of transactions into a superblock to solve the new Set Byzantine Consensus problem and commit more transactions per consensus instance. (ii) Its verification sharding balances the computation load across verifiers. As opposed to existing blockchains where all n nodes typically verify every transaction, each of our transaction signatures is verified by between t + 1 and 2t + 1 verifiers.

We conducted the most extensive experiment of a secure blockchain on a thousand virtual machines (VMs) spread over more than 10 countries in 4 continents, under normal conditions and under adversarial attacks. We implemented RBBC over a period of 4 years in 30k lines of code and compared it to the traditional leader-based PBFT [18] with well-known optimizations [10], [50] and the HoneyBadgerBFT [68], and observed that, only RBBC scales to hundreds of geodistributed VMs be they high-end (18 hyperthreaded cores) or low-end VMs (4 vCPUs). The absence of a leader without the need for a common coin yields a 3-fold improvement over the latency and throughput of its closest competitor, HoneyBadgerBFT.

TABLE I Scalable blockchain experiments – the peak throughput of elastico is obtained at 100 nodes with 14 1MB-sized blocks in 700 seconds [66] or at larger scale by producing 16 of them within 800 seconds [93]. Omniledger achieves 3500 TPS when tolerating 25% of adversarial power for 512-byte transactions but goes up to 4 •105 TPS when the adversarial power is 1%. RapidChain peaks at 7384 TPS for 512-byte transactions but needs smaller blocks to achieve a 9-second latency, which leads to 7000 TPS [93]. The throughput of Algorand is 750 MB/h=208 KB/s or 327 MB/h=90 KB/s for a 22-second latency and the impact of attacks on its performance seems negligible [37]. The throughput of Mir is from 0, when a leader stalls after another, to 60,000 TPS, when all n = 100 proposers are correct, with 500-byte payload and no durability, and peaks at 40,000 TPS when n = 4 with 3500-byte payload [83]. The throughput of RBBC peaks at 660,000 TPS for 400-byte transactions with n = 300 (Fig. 3) but varies from 1900 TPS under a 33% coalition attack (Fig. 10) to 30,684 TPS at max scale, where its latency is 3 s (Table IV).
Table I- 
Scalable blockchain experiments – the peak throughput of elastico is obtained at 100 nodes with 14 1MB-sized blocks in 700 seconds [66] or at larger scale by producing 16 of them within 800 seconds [93]. Omniledger achieves 3500 TPS when tolerating 25% of adversarial power for 512-byte transactions but goes up to 4 •105 TPS when the adversarial power is 1%. RapidChain peaks at 7384 TPS for 512-byte transactions but needs smaller blocks to achieve a 9-second latency, which leads to 7000 TPS [93]. The throughput of Algorand is 750 MB/h=208 KB/s or 327 MB/h=90 KB/s for a 22-second latency and the impact of attacks on its performance seems negligible [37]. The throughput of Mir is from 0, when a leader stalls after another, to 60,000 TPS, when all n = 100 proposers are correct, with 500-byte payload and no durability, and peaks at 40,000 TPS when n = 4 with 3500-byte payload [83]. The throughput of RBBC peaks at 660,000 TPS for 400-byte transactions with n = 300 (Fig. 3) but varies from 1900 TPS under a 33% coalition attack (Fig. 10) to 30,684 TPS at max scale, where its latency is 3 s (Table IV).
RBBC also guarantees a level of fairness. RBBC ensures censorship-resistance in that all correctly requested transactions are eventually committed, hence implying blockchain liveness [19] for correctly signed transactions, but offering extra guarantees to requesters. A first consequence of censorship-resistance is to mitigate a series of problems that plague other blockchains, like anomalies [74], unfairness [46], front-running [82], [27] or oligarchy [94], by ordering transactions with their age. A second consequence of this property is partic-ularly appealing at the application level when transactions are well-formed (i.e., non-conflicting and valid): RBBC exchanges O(n3) bits to commit a single transaction, just like lightweight leader-based implementations [48], but without their increased latency.

We start by presenting the background (§II) and our goals and assumptions (§III). Then we present an overview of RBBC (§IV), its design and implementation (§V) and the reason why RBBC is a secure, fair and scalable blockchain (§VI). We evaluate RBBC world-wide on up to 1000 machines and under attacks, and compare it against other blockchains (§VII). We conclude (§VIII) and provide the full proofs (§A) and a disclosure (§B).

SECTION II.Background and Threat Model
Most previous works on Byzantine fault tolerant blockchains expose themselves to a series of threats summarized below.

A. Double spending
The motivation for solving the traditional consensus problem is to guarantee that, all replicas agree on a unique block at a given index [77]. The uniqueness of the block avoids forks that could otherwise allow an attacker to double spend its coins in two branches [80]. This problem is symptomatic of blockchains that offer probabilistic guarantees [73], [91],

[66], [37], [55], [93]: although it helps with scalability as depicted in Table I, the probability that consensus fails grows with the number of blocks that need to be agreed upon [93]. A blockchain based on deterministic consensus ensures that consensus is reached among correct nodes if less than a third of all nodes are Byzantine. The traditional definition, however, unnecessarily limits the scalability of the blockchain [89], [14], [81]: most blockchains decide at most one of the proposed blocks [14], [81]. Indeed, the leader whose proposal is eventually decided needs to propose requests to many nodes, its network interface thus acting as a bottleneck [49], [41]. By contrast, RBBC solves the Set Byzantine Consensus problem by deciding up to Ω(n) proposals (Theorem 2) when n > 3t and no forks are possible even when the communication is asynchronous.

B. Incorrectly signed transactions
To decide a superblock that combines all proposed blocks, one may think of solving a variant of the consensus problem to instead combine proposals, into a decision [8], [76], [22]. For example, Agreement on a Core Set (ACS) [8], Interactive Consistency (IC) [60] and Vector Consensus (VC) [76] all require at least t + 1 (either n − t or t + 1 with n > 3t) proposed values to be decided. In blockchain, however, there may not even be t + 1 compatible proposed blocks: when among 2t + 1 blocks one transaction per block is not correctly signed or transactions of distinct blocks conflict in that they are concurrent and withdraw assets from an account that has insufficient balance. This is why, we introduce the Set Byzantine Consensus problem (§III) that ensures that all correct nodes extract the correctly signed and non-conflicting transactions from the same set of proposals. This allows RBBC to combine proposed valid blocks into a superblock (§V-C) for its performance to increase with the system size.

C. Unfairness
The reason for discarding invalid transactions is to cope with Byzantine requests that may lead to anomalies [74], unfairness [46], front-running [82], [27] or oligarchy [94]: a correct requester could be unable to commit any of its trans-actions due to invalid transactions always being committed first. An influential consortium blockchains called Hyperledger Fabric [5] suffers from this censorship: as acknowledged by its authors a spamming attack could lead its orderer service to order only invalid transactions. Its optimized version, called FastFabric [39], mitigates this issue by being ∼7× faster, however, none of these versions tolerate malicious failures (including denial-of-service attacks). Even its Byzantine-fault tolerant ordering service [81] suffers from this issue as it cannot detect whether a transaction is valid. RBBC favors older requested transactions to achieve censorship-resistance (Theorem 3).

D. Network attacks
Many blockchains assume synchrony where all messages must be delivered in less than a known bounded time [73], [91], [66], [52], [2], [54], [37], [44], [93], [61]. The drawback is the vulnerability to various attacks [45], [74], [75], [32], [33]. Elastico [66], RapidChain [93], OmniLedger [55] achieve scalability by sharding the consensus. This sharding and block-DAGs (e.g., [62]) are not to confuse with our verification sharding as RBBC orders all transactions at once. One obtains the world state of RBBC by accessing the last block, instead of having to traverse the tip of a DAG. A sharded blockchain [90] scales even further than what is presented in Table I, however, some of its transactions are not atomic as their withdrawal is decoupled from their credit. Other blockchains [53], [2], [55], [79] assume synchrony but use a partially synchronous consensus algorithm [31]. Ouroboros reached 247 TPS [51] and assumes synchrony [52] but its Praos version [28], which does not, has no evaluation. RBBC only assumes partial synchrony to tolerate unknown delays.

E. Adversarial schedulers
Recent blockchains avoid completely the synchrony assumptions by solving consensus probabilistically [68], [71], [72], [63]. Stellar [63] requires a probabilistic leader election. The HoneyBadger Byzantine Fault Tolerance (HBBFT) [68] is a blockchain that combines proposals by solving ACS and building upon an asynchronous binary Byzantine consensus algorithm [71] that does not terminate under an adversarial scheduler [85]. As we show in §VII-B5, HBBFT is too costly for our needs because each of its nodes creates n 1 erasure coded messages and n−1 signatures, and verifies Ω(−n2) signatures. BEAT [30] and Dumbo [65], [43] improve over HBBFT when transactions are respectively less than 10 bytes and 250 bytes but build upon the same consensus algorithm [71]. Some consensus alternatives relax this assumption but require more messages, which risks to increase the overhead [72]. RBBC does not assume a fair scheduler.

F. Faulty leaders
To avoid both the cost often associated with randomization (e.g., common coin) and synchrony, various systems [18], [57], [20], [86], [67], [6], [10], [92], [83] assume partial synchrony [31]. Unfortunately, they all rely on some leader and revert to a costly recovery mechanism in case of failure [21], [12], [7]. Tendermint [50] uses a variant of PBFT and cannot scale beyond tens of nodes [14]. SBFT [38] uses threshold signatures to reduce the communication complexity of PBFT. It is evaluated in a wide area network as a key-value store and as a blockchain system that peaks at 172 TPS. ByzCoin [53] relies on PBFT using multicast trees to reduce the number of messages to O(n). Similar to Bitcoin-NG [34], it also relies on a leader. Unfortunately, the leaderless consensus algorithms that could remain uninterrupted despite single points of failure are incomplete [59] or impractical [12] while others [70], [69] cannot be easily extended with verification sharding to implement a blockchain.

G. Single point of slowdown
HotStuff [92] is a replicated state machine (RSM) that tries to reduce the leader load by sending only the digest of each request. Its implementation [48] exchanges asymptotically as many bits as RBBC per committed transaction but requires clients to send their transactions to all correct consensus participants. Mir [83] is a deduplicating total order broad-cast protocol that builds upon our sharded verification [25]. Although not formally stated, it could potentially solve the Set Byzantine Consensus (SBC) problem (Def. 1) as it also leverages multiple proposers. The key difference is that it builds upon the PBFT leader-based consensus algorithm by combining n PBFT proposers (called ‘leaders’ in Mir) and one leader (called ‘primary’ in Mir). In Mir, verification sharding may fail due to a single faulty or slow replica, in which case it reverts to full verification. In RBBC, faulty or slow nodes do not stop verification sharding as verification priorities are assigned to replicas so that faster replicas simply verify on behalf of the faulty or slow replicas. The throughput of HotStuff and Mir drops to 0 when their leader is faulty or slow as was reported separately for HotStuff [88] and Mir [83], [Fig. 10]. As a full-fledged blockchain, RBBC ensures durability (§VI-2) and tolerates failures by always deciding proposals.

H. Human errors
Consensus algorithms are particularly complex, especially when they are monolithic [60], [18], [58]. As blockchains require consensus to handle conflicting transactions from different nodes [42], an erroneous consensus algorithm can lead to dramatic losses. Some algorithms suffer from known errors [1]. Even when formally specified [70], their specification might be too large to be machine-checked and may appear erroneous [84]. Such errors already affected blockchains, for example, when the randomized consensus at the heart of HoneyBadgerBFT was proved non-terminating [85]. An attempt to limit blockchain errors lies on theorem provers [4], [64], however, they check proofs but not algorithms. By contrast, the consensus algorithm of RBBC was formally proved safe and live for any parameters n and t < n/3 using complete model checking [9]. To achieve this, its complex (multivalue) consensus protocol decomposes into reliable broadcast [13] and binary consensus using Ben-Or et al.’s reduction [8]. The reliable broadcast has been verified using model checking [56] while the binary consensus was formally verified with model checking thanks to an additional decomposition [9]. Although these verifications depend on the correctness of the model checker, the compiler, etc., it considerably reduces human errors.

SECTION III.Goals and Assumptions
The goal is to implement a blockchain system whose performance scales with a number of consensus participants that treat (verify cryptographically and totally order) a large amount of transactions sent by requesters all over the world. The communication model is the classic resilience optimal model with partial synchrony [31] and t < n/3 [18].

A. Open permissioned system
We consider an “open” distributed system consisting of nodes that do not need any permission to join, called replicas and requesters (‘requester’ is equivalent to ‘client’ in the distributed computing literature but differs from the notion of ‘client’ of the Ethereum documentation). The replicas receive blocks from other nodes and maintain a copy of the current state of the blockchain whereas the requesters simply act as clients, requesting balances and issuing transactions. We call this an open “permissioned” system because nodes need some permission to play the roles of proposers and verifiers. Each proposer collects a set of requested transactions and proposes it periodically as a batch whereas the verifiers check the transaction signatures, a procedure called verification. This open permissioned model is appealing for assigning permissions based on proof-of-stake in Ethereum v2.0 or for revoking permissions upon misbehaviors in LLB [78].

B. Transaction model
Let T be the set of all possible transactions and let any transaction tx ∈ T be a tuple of 〈a, b, m, σ〉 that represents a transaction with non-forgeable signature σ transferring i amount m from account a to account b (implemented with UTXO as explained in §IV). We use SSL handshake with certificates listed within blocks for secure channels and new blockchain accounts create new key pairs that they use after they receive coins. Let a proposal s ⊂ T be a set of transactions. Let S = T∗be the set of possible proposals, or the set of possible sets of transactions. A transaction 〈a,*, m, σ〉 is provisioned if the balance of a is larger than m; it is valid if it is provisioned and σ is the signature of the owner of a. A set of transactions is valid if all its transactions are valid; it is non-conflicting if no subset of its transactions are cumulatively withdrawing more from any account than its balance. Note that the transactions could be multisigned as their execution is not sharded.

Initially, all nodes have a copy of a special block called the genesis block of the blockchain that indicates the initial balance of some addresses (i.e., accounts) and the identities of the permissioned nodes identified by their public keys. Note that because permissioned nodes are listed in blocks, they do not have to be "pre-selected" as in traditional consortium blockchains [15] but can instead change periodically to avoid bribery attacks [87].

C. Failure model
The failure model is Byzantine in that faulty nodes can fail arbitrarily [77]. We refer to non faulty nodes as correct. More specifically, among the n permissioned nodes there are at most t<n3 faulty nodes. Among these n permissioned nodes, the set V of verifiers contains at least t+1 correct nodes and the set P of proposers contains at least one correct proposer, which is easily ensured with |V| = |P| = n requesters. Note that any number of and replicas that are not part of these permissioned nodes can be faulty. To ensure termination we assume that the communication is partially synchronous [31] in that there exists an unknown global stabilization time after which all messages sent are delivered in less than a fixed amount of time. In order to guarantee that the system is censorship-resistant (Theorem 3) despite Byzantine nodes, we make the following assumptions:

sequential-transaction-requests: a correct requester does not issue invalid transactions or two conflicting transactions;

bounded-requesting-rate: the rate at which transactions enter the mempool (memory pool) of proposers is lower than the rate at which transactions get proposed by proposers.

Note that Assumption (1) only applies to correct nodes in order to guarantee that their transaction will be eventually treated by the system; the system cannot guarantee that transactions issued by Byzantine requesters will be treated (as their trans-actions may be invalid). Assumption (2) precludes Denial-of-Service (DoS) attacks where correct proposers would receive too many requests to keep track of them within their bounded memory. We reduce the likelihood of a DoS attack by showing empirically that RBBC treat a large volume of transactions for a long period (§VII).

D. Goal
Our goal is to implement a censorship-resistance replicated state machine (RSM). By censorship-resistance, we mean that any transaction issued properly by a requester gets committed by the system, hence preventing censorship (§II). By replicated state machine, we refer to a way of totally ordering sets of transactions in the form of a blockchain, starting from the genesis block, so that all transactions are provisioned and linearizable [47]. To this end, we require to solve a variant of the BFT consensus problem to agree on an enumerable valid subset of the union of the proposed values as follows.

Definition 1 (Set Byzantine Consensus): Assuming that each correct node proposes an enumerable set of transactions as a proposal, the Set Byzantine Consensus (SBC) problem is for each of them to decide on a set in such a way that the following properties are satisfied:

SBC-Termination: every correct node eventually decides a set of transactions;

SBC-Agreement: no two correct nodes decide different sets of transactions;

SBC-Validity: a decided set of transactions is a valid non-conflicting subset of the union of the proposed sets;

SBC-Nontriviality: if all nodes are correct and propose an identical valid non-conflicting set of transactions, then this subset is the decided set.

The SBC-Termination and SBC-Agreement properties are common to many Byzantine consensus definition variants, while SBC-Validity is different: it states that transactions proposed by Byzantine proposers could be decided as long as they are valid and non-conflicting. SBC-Validity is inspired by the external validity property [17], [3] that requires a decision to be valid and the idea of deciding at least t + 1 proposed values [60], [8], [29], however, SBC-Validity cannot result from any combination of these properties. For example, the union of strict subsets of all proposed values is a possible SBC decision. SBC-Nontriviality prevents a trivial algorithm that always outputs an empty set from solving the problem.

SECTION IV.Overview of RBBC
This section presents the architecture and the two main novelties of RBBC: its verification leverages the few computational resources when the system is small and its consensus leverages communication to commit more transactions when the system is large and bandwidth becomes limited.

A. Architecture
Figure 1(a) depicts the architecture of RBBC that features a memory pool (or mempool), a Set Byzantine Consensus, a cryptography component, a reconciliation component and a blockchain that stores the superblocks. The set Byzantine consensus includes (i) a verified variant of the reliable broadcast, (ii) a reduction from the multivalue consensus problem to the binary consensus problem, (iii) a binary consensus and (iv) a reconciliation protocol to build a superblock from multiple sets of transactions. From time to time the |P| proposers extract some transactions from their mempool that they propose to the multivalue consensus.

B. Reducing the computation at small scale
Verification is needed to guarantee that the Unspent Transaction Output (UTXO) transactions [73] are correctly signed. As verifications are CPU-intensive and notably affect performance (as we experiment in §VII-A2), the verification is sharded by letting different verifiers verify distinct transactions without reducing security. The expected result is twofold. First, it improves performance as it reduces each verifier’s computational load. Second, it helps scaling by further reducing the per-verifier computational load as the number of verifiers increases. We confirm empirically in §VII-B4 that verification sharding divides the verification load by 3.

As t verifier nodes can be Byzantine, each transaction signature has to be checked by at least t + 1 verifier nodes. If all the t + 1 nodes are unanimous in that the signature check passes, then at least one correct node checked the signature successfully and it follows that the transaction is correctly signed. Given that t nodes may be Byzantine, a transaction may need to be verified by up to 2t+1 times before t+1 equal responses are computed. As depicted in Figure 1(b), we map each transaction to t + 1 primary verifiers that eagerly verify this transaction and t secondary verifiers that lazily verify this transaction if necessary, hence summing up to 2t+1 verifiers.


Fig. 1.
RBBC architecture (Fig. 1(a)), its verification sharding (Fig. 1(b)) and its superblock optimization (Fig. 1(c))

Show All

C. Leveraging bandwidth at larger scales
In a large scale environment, geodistributed proposers are likely to receive different sets of transactions coming from requesters located in their vicinity. Instead of selecting one of these sets as the next block and discarding the others, RBBC combines all the sets of transactions proposed by distinct proposers into a unique superblock to improve the performance (as we quantify in §VII-B5).

In particular, RBBC decides upon multiple proposed sets of transactions. To illustrate why this is key for scalability, consider that each of the n consensus participants have O(1) transactions to propose. As opposed to blockchains based on traditional Byzantine consensus that will decide O(1) transactions, RBBC can decide Ω(n) transactions. As the typical communication complexity of Byzantine consensus is O(n4) bits [18], [23], it results that O(n3) bits are needed per committed transaction in RBBC (cf. Theorem 2), instead of O(n4) and without suffering from a leader bottleneck. Some leader-based approaches limit the communication complexity by relying on threshold encryption [38], [3], [92], our goal is to avoid more cryptography to limit the CPU load.

To illustrate how RBBC achieves this optimization, consider Figure 1(c) that depicts n = 4 permissioned nodes that propose different sets of transactions but that decide a value that is actually a superblock containing the union of all sets of transactions that were proposed. It results from this optimization that the number of transactions decided grows linearly in n as long as each transaction is proposed by a constant number of correct proposers (Theorem 2). Note that some of these transactions may not be executable as they conflict, this is why we need a reconciliation procedure (§V-C).

D. Assigning roles to nodes
We now explain how node roles are assigned for each transaction using a deterministic function. For each consensus instance, we have an ordered set P of permissioned node identifiers where t < |P| ≤ n, indicating the nodes that play the role of primary or secondary proposers for all transactions. Note that this determinism does not imply predictability as one can change proposers [37] directly, and such changes can also be decided deterministically and anonymously by the consensus nodes themselves with a recent voting protocol also based on DBFT [16]. Although 2t < |P| ≤ n is achieve necessary to censorship-resistance a requester never needs to send requests to more than t + 1 proposers.

For a requester to identify the proposers responsible to propose a given transaction tx to the consensus, it executes a deterministic function µ(a) that takes as input the source account a of tx and returns the identifier of a node pi ∈ P, called the primary proposer of transaction tx. To guarantee that a transaction is proposed (despite a faulty primary pro-poser), between t and n − 1 secondary proposers distinct from pi are also selected deterministically. The number of proposers of each transaction tx is at least t+1 to guarantee that tx will be proposed by at least one correct proposer (Theorem 3). The number of proposers can be as large as n, however, fewer proposers lower latency whereas more proposers increase throughput, as we will experiment in §VII-B.

As each transaction must be verified between t + 1 and 2t + 1 times, each proposer pi is also mapped to a set of t + 1 primary_verifierspi and a set of t additional secondary_verifierspi. The primary_verifierspi include pi itself and verify upon reception the signatures of tx, so the basic design makes some nodes both proposers and verifiers. If the verification returns the same t+1 results, then it becomes clear whether the signature of tx is correct. If not, t additional verifications are needed to identify the majority of t + 1 identical responses indicating whether the signature of tx is correct. This is why, the secondary_verifierspi set includes nodes of P that are distinct from the primary_verifierspi and that verify tx in case one or more of the primary verifiers are slow/faulty. One could take more verifiers but would waste CPU (§VII-A2).

SECTION V.Design and Implementation
In this section, we detail the design and implementation of RBBC. Requesters request the balance of an account or send a transaction to t + 1 proposers. All communications are exchanged through SSL-encrypted channels. The following methods are exposed by the permissioned nodes through a JSON RPC to the requesters.


Fig. 2.
The typical message pattern of the consensus protocol between each proposer i and the permissioned nodes that play the role of primary verifiers i (pri. ver. i) and secondary verifiers i (sec. ver. i) for proposer i

Show All

submit(tx) runs at proposer nodes and takes as input a UTXO transaction tx of about 400 bytes from a requester. If tx is provisioned, does not exist already and does not conflict with any transaction of the mempool, then it is placed in the mempool and true is returned. A correct requester calls this method at t + 1 proposers.

balance(a) takes account a as an input and returns its UTXOs. A requester performs this operation by contacting different proposers until t + 1 equal notifications are received. Not only does it allow small devices to securely consult the state without downloading the blockchain, but it also guarantees the integrity of the ledger.

catchup() is a method for lagging or recovering replicas to get updated about the current index of the blockchain.

A. Normal consensus execution
Figure 2 depicts a normal consensus execution when n = |P| = 4 and t = ⌈n/3⌉ − 1 = 1 where a single requester sends one request to t + 1 proposers for simplicity (actually, many requesters typically request proposers in parallel) and each one of the proposers executes the following:

❶ Request. A requester computes µ(a) with the source ac-count a of transaction tx to retrieve the mapped proposers and verifiers of tx and sends the request for transaction tx to the t+1 primary proposers and verifiers. Upon reception, tx is added to the mempool and is verified by the primary verifiers.

❷ Propose. Each proposer selects, from their mempool, trans-actions (i) for which it is the primary proposer and (ii) in decreasing order of their age or the number of blocks appended to the chain since these transactions arrived. This batch is proposed to the consensus by sending it in an INIT message to all other permissioned nodes. (Note that a proposer with an empty mempool starts the consensus with an empty proposal if it receives a non-empty proposal from another node to guarantee sufficiently many participants.)

❸ Echo. Upon reception, permissioned nodes broadcast a digest of each received proposals in ECHO messages.

❹ Ready. Upon reception of n − t equal ECHO messages, verifiers for the received proposals verify them (if not already done in the request step) and send the result in a READY message (§V-B) to all proposers. Upon reception of t + 1 equal READY messages, a node broadcasts the READY message if it has not done so already. (This step is represented by a single message exchange in Figure 2 due to the fast Ready-Decide optimization presented below.)

❺ Decide. Upon reception of 2t + 1 equal READY messages (§V-B) for a particular proposal, the nodes store this proposal in an array indexed by the sender id and input 1 to a corresponding binary consensus instance. This is repeated for other proposals until |P|−t binary consensus instances decide 1, after what a reconciliation (§V-C) combines into a superblock every valid transaction of proposals for which a binary consensus output 1.

a) Good execution complexity and scalability
In good executions, RBBC differs from previous work by committing securely Ω(n) transactions within 4 message delays (–) thanks to the Ready-Decide optimization: In the Ready step, if the verifiers are fast enough to verify before the reception of t+1 equal READY messages, then they broadcast a READY message. As a result, all nodes receive 2t + 1 equal READY messages in a single message delay allowing them to enter the Decide step by inputting 1 to a binary consensus instance directly. At the end, the proposers pick the transactions from the Ω(n) proposals for which the binary consensus decided 1 and reconciliate them into a superblock. The complexity and throughput of RBBC are computed in §V-C and Theorem §2.

B. Verified all-to-all reliable broadcast
For proposers to exchange verified proposals we add a secp256k1 Elliptic Curve Digital Signature Algorithm (ECDSA) verification to the reliable broadcast, which is orig-inally a 3-step one-to-all communication abstraction exchanging INIT, ECHO and READY messages where any message delivered to a correct proposer gets eventually delivered to all correct proposers [13].

Our verified variant of the reliable broadcast adds a verification function verify (lines 6 and 7) before the broadcast of the READY message. A proposer broadcasts an INIT message with a proposal v (line 2). Upon reception, its digest h(v) is broadcast in an ECHO message (line 4); we use the SHA256 digest to save bandwidth as proposals can contain thousands of transactions (§VII-B). Upon reception of n − t equal ECHO messages, the verification of the proposal starts first at the primary_verifiers(v) and later, if necessary, at the secondary ones. After the verification, a list verif of integers indicating the indices of invalid transactions in the proposal is appended to the READY message, which is then broadcast (line 8) with the digest of the corresponding proposal. After receiving the same verif field for h(v) from t + 1 distinct processes (line 9), a node knows which transactions of v are valid. Upon reception of n − t equal READY messages, v is delivered if it contains valid transactions (line 13). The proof that the verified reliable broadcast ensures the properties of reliable broadcast for each valid value (and discards invalid values) relies on the fact that the sets Q of correct proposers and Q′ of verifiers are such that Q∩Q′ ≥ t+1, guaranteeing that the READY message with the same verif will be sent at line 11 (cf. Theorem 4).

C. Agreeing on a superblock
We modify Ben-Or, Kelmer and Rabin’s reduction (lines 14–23) of the multi-value consensus problem to the binary consensus problem [8] to solve the Set Byzantine Consensus (§III) by replacing the reliable broadcast by our verified reliable broadcast (§V-B) and invoking a reconciliation to decide a superblock of non-conflicting provisioned transactions (§III). Symbol → indicates that the array props gets populated in the background by all concurrent reliable broadcasts (line 15). For each of the first proposals delivered at pi by the verified reliable broadcasts (§V-B), pi proposes 1 to a binary consensus instance (line 19). Proposer pi proposes 0 to the remaining binary consensus instances (line 21) after a timer expires and |P | − t consensus return 1. This timer (line 16) increases with the age of the oldest transaction of the mempool to potentially decide it.

All binary consensus instances proceed in parallel (their invocation is non-blocking). The decisions of these binary consensus instances constitute a bitmask that is applied to the set of potentially decidable proposals (line 23). Although the array of verified proposals may differ across correct nodes, the bitmasks of all correct nodes are guaranteed to contain 1s (Lemma 5) and be identical due to the agreement properties of the binary consensus. Note that even though the proposal may not be known yet for some of these indices, it is guaranteed by the reliable broadcast to be eventually delivered at all correct proposers (§V-B). Each correct proposer waits until a proposal has been delivered at each of these indices (line 22), then each correct proposer obtains the same set of proposals after applying the bitmask. The unselected proposals are stored in the mempool for later.

1) Reconciliation
To fill the superblock, we reconciliate the decidable set of proposals (lines 24–29). Correct proposers extract deterministically the transactions by going through all proposals and through each of their transactions one-by-one, to add the non-conflicting provisioned ones to the superblock. With the UTXO model, one can easily ensure all trans-actions are provisioned and non-conflicting by implemeting conflict(tx, ∗) that executes transaction tx and returns false if all the UTXOs tx consumes exist and otherwise returns true. For the sake of fairness (i.e., to not favor any particular proposer), correct proposers traverse the proposals from the index number (k mod n) to index number (k − 1 mod n) where k is the index of the last superblock in the blockchain. This prevents the proposer with the lowest index number from having its proposed transactions added to the superblock with a higher priority than the transactions of other proposers. (We will show in §VII-B that the computation needed for reconciliation is negligible compared to the signature verification.)

2) Binary consensus
To solve the binary consensus deterministically, we chose the binary consensus of DBFT [23] because it is resilience optimal, time optimal and was recently verified with model checking [85], [9]. Each replica refines an estimate value, initially its input value to the consensus, across consecutive rounds until it decides (line 45). It invokes broadcast primitives that deliver some values into a dedicated variable pointed out by → at lines 32, 36 and 39. The bv-broadcast (line 32) is a reliable broadcast for binary values [72]. (We optimize by piggybacking it for r = 1 with the verified-reliable-broadcast at line 15.) One replica per round acts as a coordinator by broadcasting its value c (line 36) that others prioritize (line 38) to help them converge to the same decision. Hence, RBBC is leaderless with multiple coordinators. The binary values are then forwarded in AUX messages (line 39) and each replica waits to receive a sufficiently represented set of these AUX values (lines 40–42). If only one value is sufficiently represented (line 43) and if it corresponds to the parity of the round, then it is decided (line 45). Otherwise, val is set to the parity of the round and another round starts.

3) Complexity
In the worst case, each bin-propose decides within O(t) rounds after the network stabilizes and messages start being delivered in bounded time, which is optimal [35]. Hence, as the (multivalue) propose needs a constant additive factor of message delays, its time complexity is asymptotically optimal. Moreover, propose is resilience optimal as it tolerates any t < n/3 failures [60]. It has the same communication complexity O(n4) as PBFT [18] and DBFT [23] but decides up to n times more transactions than them in good executions: O(n3) bits exchanged per committed transaction batch. Recall that the communication complexity of PBFT is O(n4) bits be-cause there are at most t+1 view-change rounds, the message in each round contains the state received from the previous view-change rounds, which is O(t) bits, and is broadcast by n nodes, leading to (t + 1) • O(t) • (n − 1) • n = O(n4).

SECTION VI.Correctness and Analysis
To explain how RBBC implements a secure, fair and scalable blockchain, we first show that it disallows double-spending by implementing an RSM that stores exclusively valid non-conflicting transactions to reliable storage (Theo-rem 1). Finally, we explain why its throughput scales with the number of nodes (Theorem 2) and show that RBBC offers censorship resistance (Theorem 3). The proofs are deferred to the appendix (§A).

1) Correctness
To avoid forks that could lead to double-spending, RBBC executes consensus instances in a totally ordered sequence and at the end of each instance the decided superblock orders all transactions it contains in the same order at all replicas. Note that to implement a blockchain system, RBBC offers stronger guarantees than a simple RSM, by for example discarding the incorrectly signed transactions eagerly (line 13) and the conflicting transactions lazily (lines 28).

Theorem 1 (Replicated State Machine): In RBBC, all correct replicas observe the same sequence of committed transactions, which are all valid and non-conflicting.

2) Durability
To ensure durability [11], the remaining transactions are stored in a block in an append only log on disk. For recovery, each node keeps a write-ahead log containing the messages it has broadcast during at least the previous two committed multivalued consensus instances, older messages being garbage collected. To ensure transactions are verified and committed quickly, the UTXO table is stored in memory. To minimize the size of the UTXO table, transactions should consume all UTXOs for their account. After a crash, nodes can reconstruct their UTXO table by parsing their blocks from disk. Nodes that need to recover messages from older consensus instances simply collect t+1 equal instances of the decided block.

3) Scalability
We explain the scalability of RBBC by showing that RBBC commits Ω(n) transactions per consensus instance in good executions where BFT algorithms (e.g., PBFT, DBFT, Tendermint, HotStuff) that do not solve SBC (Def. 1) commit O(1) transactions. Note that we could have modified leader-based consensus algorithms (PBFT, Tender-mint, HotStuff) for their leader to batch Ω(n) times more requests, however, this would have added up to the leader load. More dramatically, the leader would have sent Ω(n) bytes to n − 1 nodes, which would have taken a quadratic amount of time in a WAN, where no Ethernet broadcast is available and the message authentication codes (MACs) optimization of PBFT can thus not be used. Instead, RBBC commits Ω(n) transactions as explained below.

Theorem 2 (Scalable throughput): Let m be the number of transactions proposed to the Set Byzantine Consensus by each proposer and let n = |P| be the number of proposers. Assume that proposals are all reliably delivered before the algorithm times out (line 17) but that all ⌈n3⌉−1 Byzantine proposers do not propose any transaction. The Set Byzantine Consensus commits between 2m (as n tends to infinity) and Ω(n) distinct transactions.

4) Deduplication
If a Byzantine client can approximate closely the age parameter and the timing of the system, then more transactions can be proposed twice and their duplicates may persist until the reconciliation phase, after which one of them is discarded (line 28) as they necessarily conflict. One way to reduce the ability of Byzantine to create duplicates is for each node to choose the age parameter for each transaction at random as a small constant. Another is to apply this randomization only for clients that are duplicating transactions. Note that some recent efforts [83] were devoted to eliminate duplicated transactions during the consensus execution by adopting a more conservative approach where each transaction can only be proposed by a single proposer at a time. If the pro-poser of this particular transaction fails, then an epoch change happens and another one is selected. Although the benefit is to eliminate duplicates during the consensus execution, the drawback is to increase linearly this transaction latency as up to t epoch changes may be necessary before proposing it correctly.

Theorem 3 (Censorship-Resistance): Every transaction sent by a correct requester is eventually included in a superblock.

Censorship-resistance differs from other notions of blockchain liveness, validity or resilience by offering guarantees to a correct requester [68], [19], [43] and without requiring its transaction to be sent to all replicas [36], [5]. It is important to note that censorship-resistance does not require all requesters to be correct, it simply ensures that if a requester follows the protocol to submit its transaction, then this transaction is guaranteed to be committed by the system. By contrast, a Byzantine requester not following the request protocol, does not have this guarantee.

SECTION VII.Experimental Evaluation
In this section, we show that RBBC scales up to hundreds of Amazon EC2 VMs running consensus located on different continents and replicating the blockchain state to up to 1000 machines in 14 separate regions. To this end, we compare the performance of (1) RBBC with its verification and its consensus as depicted in §V, HBBFT which corresponds to the original code of the HoneyBadgerBFT protocol as made available by its authors [68] and (3) CONS1 that corresponds to a variant of RBBC with the classic 3-step leader-based BFT algorithm of PBFT taken from [26] with BFT-SMaRt optimizations [18], [50], [10].

We ran four types of experiments with parameters from Table II: (i) with a varying fault tolerance and verification in a geodistributed environment (§VII-A1); (ii) with all three blockchains on low-end machines (§VII-B); (iii) with up to 1000 replicas all updating their copy of all account balances (§VII-C); and finally (iv) with Byzantine failures (§VII-D).

Machine specifications We ran the blockchains on all the 14 Amazon datacenters that we had at our disposal at the time of the experiment, North Virginia, Ohio, North California, Oregon, Canada, Ireland, Frankfurt, London, Tokyo, Seoul, Singapore, Sydney, Mumbai, São Paulo. Each pair of datacenters is separated by a specific delay and bandwidth listed in §VII-C. We tested three VM types: (1) high-end c4.8xlarge instances with an Intel Xeon E5-2666 v3 processor of 18 hyperthreaded cores, 60 GiB RAM and 10 Gbps network performance when run in the same datacenter where storage is backed by Amazon’s Elastic Block Store (EBS) with 4 Gbps dedicated throughput; (2) mid-end c4.4xlarge instances with an Intel Xeon E5-2666 v3 processors with 16 vCPUs and 30 GiB RAM with “high” network performance (as defined by Amazon), 2 Gbps EBS dedicated throughput; (3) low-end c4.xlarge instances with an Intel Xeon E5-2666 v3 processor of 4 vCPUs, 7.5 GiB RAM and “moderate” network performance, and 750 Mbps EBS dedicated throughput. To limit the bottleneck effect on the leader of PBFT, we always place the leader in the most central (w.r.t. latency) region, Oregon. When not specified, proposals contain 10,000 transactions and t is set to ⌈n3⌉−1.

Leader-based (CONS1) and randomized BFT (HBBFT) CONS1 is the classic 3-step leader-based Byzantine consensus implementation similar to PBFT [18], the Tendermint consensus [50], and including the concurrency optimizations of BFT-SMaRt [10]. To reduce network consumption CONS1 is implemented using digests in messages that follow the initial broadcast. Both CONS1 and HBBFT variants use a classic verification, as in traditional blockchain systems [73], [91], that takes place at every proposer upon delivery of the decided superblock from consensus. HBBFT uses a common coin [71] and reliable broadcast with erasure codes.

A. Peak scalability and leaderless fault tolerance
In a leaderless case, the first messages from n − t replicas determine the performance, so a lower t can yield lower performance. We stress test RBBC in a single datacenter with up to 300 high-end VMs and a fixed fault tolerance to measure how fast it could go in a consortium setting. To this end, we fix t to the largest possible fault tolerance with n = 20 nodes and increase the number of nodes from 20 to 300 permissioned nodes in steps of 20. The results, shown in Figure 3, indicate that the throughput scales to hundred of nodes with a practical latency: the throughput scales up to n = 260 nodes to reach 660,000 TPS while the latency remains lower than 4 seconds. At n = 280 throughput drops slightly. We discuss below the impact of varying t on performance.

TABLE II Parameters used in the experiments


Fig. 3.
The performance (latency and throughput) of RBBC in a single datacenter

Show All


Fig. 4.
Impact of fault tolerance on RBBC and verification on 3 blockchains with n = 140 geodistributed machines

Show All

1) Impact of fault tolerance without a leader
Next we evaluate the performance when running 10 high-end VMs in each of the 14 regions for a total of 140 machines. We varied t from the minimum to its maximum value (46<1403) with sharded verification as depicted in Figure 4 (left).

The peak throughput of 151,000 TPS is achieved with the fault-tolerance parameter t = 12. When t ≤ 6, performance is limited by the (t − 1)th slowest node as the consensus waits for a higher number of n − t proposers. The peak throughput occurs while waiting for n − t = 128 nodes, probably because it avoids waiting for any node of the slowest region (Table III), São Paulo. When t ≥ 24, performance tends to be limited and drops further as the fault tolerance t keeps increasing. We conjecture (and show below) that the t + 1 necessary cryptographic verifications induce a higher computational load as t increases. As mentioned in section II, alternative leaderless Byzantine consensus algorithms lack details [59] or have exponential complexity, which would be impractical [12].

2) Impact of verification sharding
To verify the conjecture that more verifications slow performance down, we ran additional experiments and measured the performance with different numbers of verifications per transaction. As depicted in Figure 4 (right), we compared all three blockchains with all nodes verifying all transactions (all) and without any verification (no verification). The performance of all blockchains is higher without verification than with full verification. RBBC is the most affected, dropping from 219,000 TPS to 33,000 TPS while HBBFT and CONS1 throughputs drop less but from a lower peak. As we will show, there are factors other than verification, like the use of a leader and erasure codes §VII-B3, that have a larger impact on these algorithms, yet this confirms that our previous conjecture was correct.

B. Scaling throughput up to hundreds of low-end machines
We now experiment on up to 240 low-end VMs evenly spread on 5 datacenters in Europe (Ireland and Frankfurt) and the United States (Oregon, Northern California, and Ohio). Following up on our previous verification observations, we precisely measured the CPU usage with go pprof on microbenchmarks and confirmed that the workload could be CPU-bound. In particular, we measured that dedicating the 4 vCPUs of these low-end instances led to verify about 7800 serialized transactions per second with 97% of CPU time spent verifying signatures and 3% spent deserializing and updating the UTXO table.

1) RBBC vs. a leader-based BFT blockchain
Figure 5 shows the throughput and latency of RBBC with t + 1 proposers and CONS1 with different sizes of proposals. As CONS1 is limited to a single proposer (its leader) while RBBC supports multiple proposers, we tested whether CONS1 performance would be better with batching more transactions per proposal than RBBC. With proposal size of 1000, RBBC throughput increases from 3000 TPS to 9000 TPS because of the additional resources and proposals of the growing number of nodes. It flattens out around 10,000 TPS while latency increases from 2 to 8 seconds. By contrast, CONS1 throughput decreases as the number of nodes increases, despite larger proposals.

Fig. 5. - 
The performance of CONS1 with batching and RBBC with t + 1 proposer nodes; the number following the algorithm name represents the number of transactions in the proposals; solid lines represent throughput, dashed lines represent latency
Fig. 5.
The performance of CONS1 with batching and RBBC with t + 1 proposer nodes; the number following the algorithm name represents the number of transactions in the proposals; solid lines represent throughput, dashed lines represent latency

Show All


Fig. 6.
Comparing throughput and latency of CONS1 and RBBC with t + 1 proposer nodes on 100 geodistributed nodes; each point represents the number of transactions in the proposals, either 10, 100, 1000, 2500, 5000 or 10000

Show All

2) Latency vs. throughput at 100 nodes
To better understand the difference in performance of RBBC compared to the leader-based approach, we depicted on Figure 6 the evolution of the latency as a function of throughput at a reasonable number of consensus nodes, n = 100 and different proposal sizes of 1 to 5000. We clearly see that the throughput of CONS1 reaches a limit of about 1100 TPS while RBBC approaches 14,000 TPS, which indicates a 12-fold speedup of RBBC over CONS1. CONS1 has a better minimum latency of 270 ms compared to 640 ms for RBBC for proposals of size 1 but CONS1 latency explodes rapidly. (HBBFT does not appear due to lower performance.)


Fig. 7.
The performance of HBBFT and RBBC with n proposer nodes. The number following the algorithm name represents the number of transactions in the proposals; solid lines represent throughput, dashed lines represent latency

Show All


Fig. 8.
The number of times a transaction is verified in RBBC with proposal size of 100 transactions, with either t + 1 or n proposer nodes; the dashed lines t + 1 and 2t + 1 represent the minimum and maximum number of possible verifications

Show All

3) RBBC vs. a randomized BFT blockchain
Figure 7 depicts the performance of RBBC and HBBFT with n proposers, with proposal sizes of 100 and 1000 transactions. With a larger portion of proposers the throughput and latency of RBBC increases faster. With n proposers, the throughput peaks at 11,124 TPS and latency reaches 25,100 ms with 240 nodes, while with t + 1 proposers the throughput was lower and the latency was much lower. HBBFT performance degrades as the number of nodes increases: latency increases and throughput decreases (we omit latencies beyond n = 100 as they reach minutes). This is because each node broadcasts n 1 erasure coded messages with as many distinct signatures−to distinct nodes that must be echoed, yielding Ω(n2) verifications per node.

4) Reducing verification count helps scaling
To better measure the performance gain of verification sharding, we recorded the average number of times a transaction was verified for t+1 and n proposer nodes. The results are shown in Figure 8 where we observe that with t + 1 proposers the number of verifications stays close to the optimal, while with n proposers the number of verifications remains around the middle of t + 1 and 2t + 1. This is likely due to the increased load on the system causing verifications to occur in different orders at different nodes. This tends to confirm that verification sharding is important for scalability.

Fig. 9. - 
Throughput and latency comparison of the blockchain solutions with n = 140 and t = 46, and proposal sizes of 1, 10, 100, 1000 and 10000 transactions
Fig. 9.
Throughput and latency comparison of the blockchain solutions with n = 140 and t = 46, and proposal sizes of 1, 10, 100, 1000 and 10000 transactions

Show All

5) Comparing the blockchains
Figure 9 explored the effect of deciding the unions of proposals when running the blockchain. CONS1 has the lowest latency because in all executions the leader acts correctly, allowing it to terminate in only 3 message delays, where RBBC requires 4 message delays. Due to its inherent concurrency, RBBC offers the best latency/throughput tradeoff: at 1000 ms latency, RBBC offers 12,100 TPS whereas at 1750 ms latency, CONS1 offers only 5800 TPS. Note that RBBC does not feature the classic costly view change [21], [12], [7], [92] and its latency remains similar despite faults (cf. §VII-D). Finally, HBBFT has the worst performance because its consensus [71] is randomized and it uses erasure codes: each node spends over 200 ms to compute 1000 transactions for each of the 140 proposals of this experiment. Again this confirms the important CPU load induced by the signature verifications.

Fig. 10. - 
Comparing throughput and latency of RBBC and HBBFT, with normal and Byzantine behaviors on 100 geodistributed nodes; all n nodes are making proposals of 100 transactions
Fig. 10.
Comparing throughput and latency of RBBC and HBBFT, with normal and Byzantine behaviors on 100 geodistributed nodes; all n nodes are making proposals of 100 transactions

Show All

C. Evaluation with 1000 VMs
Before spawning 1000 VMs to confirm RBBC scalability, we measured the variation of latencies and bandwidth between our 14 Amazon EC2 datacenters (cf. Table III). The minimum latency is 11 ms between London and Ireland, whereas the maximum latency is 332 ms observed between Sydney and São Paulo. Bandwidth between Ohio and Singapore is measured at approximately 64.9 Mbits/s (with variance between 6.5 Mbits/s and 20.4 Mbits/s).

To avoid wasting bandwidth, we segregated the roles: all 1000 VMs act as servers, keeping a local copy of the balances of all accounts. On these replicas, 10 clients per 840 low-end machines (60 VMs in each of 14 datacenters) send transactions and 160 high-end machines (40 machines in each of the Ire-land, London, Ohio and Oregon datacenters) decide upon each superblock. Each of the 8,400 clients start with 100 UTXOs of size 64 bytes each (for a state database of size 51.27 MiB) and each proposal contains up to 1000 transactions. The resulting performance is depicted in Table IV. Interestingly, the throughput is only around 30,000 TPS but this is not due to the low capacity of RBBC but due to the difficulty of generating the workload: the replicas are located in 14 different datacenters and have to wait for owning a UTXO before they can request a transaction that consumes it. The asynchronous write latency measures the time a proposer acknowledges a transaction reception. Importantly, the transaction commit time (latency) remains about 3 seconds despite the large traffic.

D. Experiments under Byzantine attacks
We evaluate RBBC under 2 Byzantine attacks:

Byz1 The payload of the reliable broadcast messages is altered so that no proposal is delivered for reliable broadcast instances led by faulty proposers. To this end, the binary payloads of the binary consensus messages are flipped. The goal of this behavior is to reduce throughput and increase latency.

Byz2 The Byzantine proposers form a coalition in order to maximize the bandwidth cost of the reliable broadcast using the digests described in §V-B. As a result, for any reliable broadcast initiated by a Byzantine proposer, t+1 correct proposers will deliver the full message while the remaining t will only deliver the digest of the message, meaning they will have to request the full message from t + 1 different proposers from whom they receive ECHO messages.

TABLE III Heatmap of the bandwidth (MBPS) in the top right triangle and latency (ms) in the bottom left triangle between 14 AWS regions

TABLE IV Performance of RBBC with 1000 replicas spread in 14 datacenters
Table IV- 
Performance of RBBC with 1000 replicas spread in 14 datacenters
Fig. 11. - 
Comparing bandwidth usage and latency of RBBFT and HBBFT with normal and Byzantine behaviors on 100 geodistributed nodes
Fig. 11.
Comparing bandwidth usage and latency of RBBFT and HBBFT with normal and Byzantine behaviors on 100 geodistributed nodes

Show All

Experiments are run with 100 low-end machines using the same 5 datacenters from US and Europe and with n proposers (as in §VII-B). Figure 10 shows the impact of Byz1 on performance with n proposers and proposal sizes of 100. RBBC throughput drops from 5700 TPS to 1900 TPS due to having t less proposals being accepted (the proposals sent by Byzantine proposers are invalid) and to the increase in latency. The latency increases due to the extra rounds needed to be executed by the binary consensus to terminate with 0. The throughput of HBBFT drops from 350 to 256 TPS due to fewer proposals but the latency decreases because with less proposals erasure codes require less computation.

Byz2 is designed against the verified reliable broadcast of §V-B, to delay the delivery of the message to t of the correct proposers, and increasing the bandwidth used. HBBFT avoids this problem by using erasure codes, but has a higher bandwidth usage in the correct case. Figure 11 shows its impact on bandwidth usage and latency for RBBC and HBBFT with n proposers and proposal sizes of 100. The bandwidth usage of RBBC increases from 538 MB to 2622 MB per multivalued consensus instance compared to HBBFT, which uses 3600 MB in all cases. Furthermore, the latency of RBBC increases from 920 ms to 2300 ms.

Regarding corruptions, as it is impossible to solve consensus when t ≥ n/3 [60], one must either replace the permissioned nodes [87] before n/3 collude, or implement an eventually consistent alternative of the service [78].

SECTION VIII.Conclusion
Blockchains tend to adopt an open permissioned model where a subset of the nodes with some permissions (e.g., PoS) can decide upon the next block. RBBC is the first of these that does not need synchrony to scale to hundreds of geo-distributed permissioned nodes. To this end, it solves the Set Byzantine Consensus problem, adopts a leaderless design that offers censorship-resistance and introduces sharded verification. World-wide experiments demonstrate that it triples the performance of its closest competitor.