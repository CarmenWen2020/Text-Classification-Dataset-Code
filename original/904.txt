We present AI 2 , the first sound and scalable analyzer for deep neural networks. Based on overapproximation, AI 2 can automatically prove safety properties (e.g., robustness) of realistic neural networks (e.g., convolutional neural networks). The key insight behind AI 2 is to phrase reasoning about safety and robustness of neural networks in terms of classic abstract interpretation, enabling us to leverage decades of advances in that area. Concretely, we introduce abstract transformers that capture the behavior of fully connected and convolutional neural network layers with rectified linear unit activations (ReLU), as well as max pooling layers. This allows us to handle real-world neural networks, which are often built out of those types of layers. We present a complete implementation of AI 2 together with an extensive evaluation on 20 neural networks. Our results demonstrate that: (i) AI 2 is precise enough to prove useful specifications (e.g., robustness), (ii) AI 2 can be used to certify the effectiveness of state-of-the-art defenses for neural networks, (iii) AI 2 is significantly faster than existing analyzers based on symbolic analysis, which often take hours to verify simple fully connected networks, and (iv) AI 2 can handle deep convolutional networks, which are beyond the reach of existing methods.
Recent years have shown a wide adoption of deep neural networks in safety-critical applications, including self-driving cars [2], malware detection [44], and aircraft collision avoidance detection [21]. This adoption can be attributed to the near-human accuracy obtained by these models [21], [23].

Despite their success, a fundamental challenge remains: to ensure that machine learning systems, and deep neural networks in particular, behave as intended. This challenge has become critical in light of recent research [40] showing that even highly accurate neural networks are vulnerable to adversarial examples. Adversarial examples are typically obtained by slightly perturbing an input that is correctly classified by the network, such that the network misclassifies the perturbed input. Various kinds of perturbations have been shown to successfully generate adversarial examples (e.g., [3], [12], [14], [15], [17], [18], [29], [30], [32], [38], [41]). Fig. 1 illustrates two attacks, FGSM and brightening, against a digit classifier. For each attack, Fig. 1 shows an input in the Original column, the perturbed input in the Perturbed column, and the pixels that were changed in the Diff column. Brightened pixels are marked in yellow and darkened pixels are marked in purple. The FGSM [12] attack perturbs an image by adding to it a particular noise vector multiplied by a small number ϵ (in Fig. 1, ϵ=0.3). The brightening attack (e.g., [32]) perturbs an image by changing all pixels above the threshold 1−δ to the brightest possible value (in Fig. 1, δ=0.085).

Adversarial examples can be especially problematic when safety-critical systems rely on neural networks. For instance, it has been shown that attacks can be executed physically (e.g., [9], [24]) and against neural networks accessible only as a black box (e.g., [12], [40], [43]). To mitigate these issues, recent research has focused on reasoning about neural network robustness, and in particular on local robustness. Local robustness (or robustness, for short) requires that all samples in the neighborhood of a given input are classified with the same label [31]. Many works have focused on designing defenses that increase robustness by using modified procedures for training the network (e.g., [12], [15], [27], [31], [42]). Others have developed approaches that can show non-robustness by underapproximating neural network behaviors [1] or methods that decide robustness of small fully connected feedforward networks [21]. However, no existing sound analyzer handles convolutional networks, one of the most popular architectures.

Key Challenge: Scalability and Precision
The main challenge facing sound analysis of neural networks is scaling to large classifiers while maintaining a precision that suffices to prove useful properties. The analyzer must consider all possible outputs of the network over a prohibitively large set of inputs, processed by a vast number of intermediate neurons. For instance, consider the image of the digit 8 in Fig. 1 and suppose we would like to prove that no matter how we brighten the value of pixels with intensity above 1-0.085, the network will still classify the image as 8 (in this example we have 84 such pixels, shown in yellow). Assuming 64- bit floating point numbers are used to express pixel intensity, we obtain more than 101154 possible perturbed images. Thus, proving the property by running a network exhaustively on all possible input images and checking if all of them are classified as 8 is infeasible. To avoid this state space explosion, current methods (e.g., [18], [21], [34]) symbolically encode the network as a logical formula and then check robustness properties with a constraint solver. However, such solutions do not scale to larger (e.g., convolutional) networks, which usually involve many intermediate computations.


Fig. 2:

A high-level illustration of how ai2 checks that all perturbed inputs are classified the same way. Ai2 first creates an abstract element A1 capturing all perturbed images. (Here, we use a 2-bounded set of zonotopes.) It then propagates A1 through the abstract transformer of each layer, obtaining new shapes. Finally, it verifies that all points in A4 correspond to outputs with the same classification.

Show All

Key Concept: Abstract Interpretation for AI
The key insight of our work is to address the above challenge by leveraging the classic framework of abstract interpretation (e.g., [6], [7]), a theory which dictates how to obtain sound, computable, and precise finite approximations of potentially infinite sets of behaviors. Concretely, we leverage numerical abstract domains - a particularly good match, as AI systems tend to heavily manipulate numerical quantities. By showing how to apply abstract interpretation to reason about AI safety, we enable one to leverage decades of research and any future advancements in that area (e.g., in numerical domains [39]). With abstract interpretation, a neural network computation is overapproxi-mated using an abstract domain. An abstract domain consists of logical formulas that capture certain shapes (e.g., zonotopes, a restricted form of polyhedra). For example, in Fig. 2, the green zonotope A1 overapproximates the set of blue points (each point represents an image). Of course, sometimes, due to abstraction, a shape may also contain points that will not occur in any concrete execution (e.g., the red points in A2).

The AI2 Analyzer
Based on this insight, we developed a system called AI2 (Abstract Interpretation for Artificial Intelligence. AI2 is the first scalable analyzer that handles common network layer types, including fully connected and convolutional layers with rectified linear unit activations (ReLU) and max pooling layers.

To illustrate the operation of AI2, consider the example in1.

Fig. 2, where we have a neural network, an image of the digit 8 and a set of perturbations: brightening with parameter 0.085. Our goal is to prove that the neural network classifies all perturbed images as 8. AI2 takes the image of the digit 8 and the perturbation type and creates an abstract element A1 that captures all perturbed images. In particular, we can capture the entire set of brightening perturbations exactly with a single zonotope. However, in general, this step may result in an abstract element that contains additional inputs (that is, red points). In the second step, A1 is automatically propagated through the layers of the network. Since layers work on concrete values and not abstract elements, this propagation requires us to define abstract layers (marked with #) that compute the effects of the layers on abstract elements. The abstract layers are commonly called the abstract transformers of the layers. Defining sound and precise, yet scalable abstract transformers is key to the success of an analysis based on abstract interpretation. We define abstract transformers for all three laver types shown in Fig. 2.

At the end of the analysis, the abstract output A4 is an overapproximation of all possible concrete outputs. This enables AI2 to verify safety properties such as robustness (e.g., are all images classified as 8?) directly on A4. In fact, with a single abstract run, AI2 was able to prove that a convolutional neural network classifies all of the considered perturbed images as 8.

We evaluated AI2 on important tasks such as verifying robustness and comparing neural networks defenses. For example, for the perturbed image of the digit 0 in Fig. 1, we showed that while a non-defended neural network classified the FGSM perturbation with ϵ=0.3 as 9, this attack is provably eliminated when using a neural network trained with the defense of [27]. In fact, AI2 proved that the FGSM attack is unable to generate adversarial examples from this image for any ϵ between 0 and 0.3.

Main Contributions
Our main contributions are:

sound and scalable method for analysis of deep neural networks based on abstract interpretation (Section IV).

AI2, an end-to-end analyzer, extensively evaluated on feed-forward and convolutional networks (computing with 53 000 neurons), far exceeding capabilities of current systems (Section VI).

An application of AI2 to evaluate provable robustness of neural network defenses (Section VII).

SECTION II.

Representing Neural Networks as Conditional Affine Transformations
In this section, we provide background on feedforward and convolutional neural networks and show how to transform them into a representation amenable to abstract interpretation. This representation helps us simplify the construction and description of our analyzer, which we discuss in later sections. We use the following notation: for a vector x¯¯¯∈Rn,xi denotes its ith entry, and for a matrix W∈Rn×m,Wi denotes its ith row and Wi,j denotes the entry in its ith row and jth column.


Fig. 3:

Definition of CAT functions.

Show All

CAT Functions
We express the neural network as a composition of conditional affine transformations (CAT), which are affine transformations guarded by logical constraints. The class of CAT functions, shown in Fig. 3, consists of functions f:Rm→Rn for m,n∈N and is defined recursively. Any affine transformation f(x¯¯¯)=W⋅x¯¯¯+b¯¯ is a CAT function, for a matrix W and a vector b. Given sequences of conditions E1,…,Ek and CAT functions f1,…,fk, we write:

f(x¯¯¯)=caseE1:f1(x¯¯¯),…,caseEk:fk(x¯¯¯).

View Source
This is also a CAT function, which returnsx fi(x¯¯¯) for the first Ei satisfied by x¯¯¯. The conditions are conjunctions of constraints of the form xi≥xj,xi≥0 and xi<0. Finally, any composition of CAT functions is a CAT function. We often write f′′∘f′ to denote the CAT function f(x¯¯¯)=f′′(f′(x¯¯¯)).

Layers
Neural networks are often organized as a sequence of layers, such that the output of one layer is the input of the next layer. Layers consist of neurons, performing the same function but with different parameters. The output of a layer is formed by stacking the outputs of the neurons into a vector or three-dimensional array. We will define the functionality in terms of entire layers instead of in terms of individual neurons.

Reshaping of Inputs
Layers often take three-dimensional inputs (e.g., colored images). Such inputs are transformed into vectors by reshaping. A three-dimensional array x¯¯¯∈Rm×n×r can be reshaped to x¯¯¯U∈Rm⋅n⋅r in a canonical way, first by depth, then by column, finally by row. That is, given x¯¯¯:

x¯¯¯v=(x1,1,1…x1,1,rx1,2,1…x1,2,r…xm,n,1…xm,n,r)T.

View SourceRight-click on figure for MathML and additional features.
Activation Function
Typically, layers in a neural network perform a linear transformation followed by a non-linear activation function. We focus on the commonly used rectified linear unit (ReLU) activation function, which for x∈ R. is defined as ReLU (x)=max(O,x), and for a vector x¯¯¯∈Rm as ReLU (x¯¯¯)=(ReLU(x1), …, ReLU(xm)).

ReLu to CAT
We can express the ReLU activation function as ReLU=ReLUn∘o ReLU 1 where ReLUi processes the ith entry of the input x¯¯¯ and is given by:

ReLUi(x¯¯¯)=case(xi≥0):x¯¯¯,case(xi<0):Ii←0⋅x¯¯¯.

View SourceRight-click on figure for MathML and additional features.
Ii←0 is the identity matrix with the ith row replaced by zeros.

Fully Connected (FC) Layer
An FC layer takes a vector of size m (the m outputs of the previous layer), and passes it to n neurons, each computing a function based on the neuron's weights and bias, one weight for each component of the input. Formally, an FC layer with n neurons is a function FC W,b¯¯:Rm→−Rn parameterized by a weight matrix W∈Rn×m and a bias b¯¯∈Rn. For x¯¯¯∈Rm, we have:

FCW,b¯(x¯¯¯)=ReLU(W⋅x¯¯¯+b¯¯).

View Source
Fig. 4a shows an FC layer computation for x¯¯¯=(2,3,1).

Convolutional Layer
A convolutional layer is defined by a series of t filters Fp,q=(FI′,q1,…, Fp,qt), parameterized by the same p and q, where p≤m and q≤n. A filter Fp,gi is a function parameterized by a three-dimensional array of weights W∈Rp×q×r and a bias b∈R. A filter takes a three-dimensional array and returns a two-dimensional array:

Fp,qi:Rm×n×r→R(m−p+1)×(n−q+1).

View Source
The entries of the output y¯¯¯ for a given input x¯¯¯ are given by:

yi,j=ReLU(∑i′=1p∑j′=1q∑k′=1Wi′,j′,k′.x(i+i′−1),(j+j′−1),k′+b).

View Source
Intuitively, this matrix is computed by sliding the filter along the height and width of the input three-dimensional array, each time reading a slice of size p×q×r, computing its dot product with W (resulting in a real number), adding b, and applying ReLU. The function ConvF, corresponding to a convolutional layer with t filters, has the following type:

ConvF:Rm×n×r→R(m−p+1)×(n−q+1)×t.

View SourceRight-click on figure for MathML and additional features.
As expected, the function ConvF returns a three-dimensional array of depth t, which stacks the outputs produced by each filter. Fig. 4b illustrates a computation of a convolutional layer with a single filter. For example:

y1,1,1=ReLU((1⋅0+0⋅4+(−1)⋅(−1)+2⋅0)+1)=2.

View SourceRight-click on figure for MathML and additional features.
Here, the input is a three-dimensional array in R4×4×1. As the input depth is 1, the depth of the filter's weights is also 1. The output depth is 1 because the layer has one filter.

Convolutional Layer to CAT
For a convolutional layer ConvF, we define a matrix WF whose entries are those of the weight matrices for each filter (replicated to simulate sliding), and a bias b¯¯F consisting of copies of the filters' biases. We then treat the convolutional layer ConvF like the equivalent fCWF,b¯F. We provide formal definitions of WF and b¯¯F in Appendix A. Here, we provide an intuitive illustration of the translation on the example in Fig. 4b. Consider the first entry y1,1=2 of y¯¯¯ in Fig. 4b:

y1,1=ReLU(W1,1⋅x1,1+W1,2⋅x1,2+W2,1⋅x2,1+W2,2⋅x2,2+b).

View Source
When x¯¯¯ is reshaped to a vector x¯¯¯v, the four entries x1,1,x1,2,x2,1 and x2,2 will be found in xv1,xv2,xv5 and xv6, respectively. Similarly, when y¯¯¯ is reshaped to y¯¯¯u, the entry y1,1 will be found in yu1. Thus, to obtain yv1=y1,1, we define the first row in WF such that its 1st,2nd,5th, and 6th entries are W1,1,W1,2,W2,1 and W2,2. The other entries are zeros.


Fig. 4:

One example computation for each of the three layer types supported by ai2.

Show All

We also define the first entry of the bias to be b. For similar reasons, to obtain yv2=y1,2, we define the second row in WF such that its 2nd,3rd,6th, and 7th entries are W1,1,W1,2,W2,1 and W2,2 (also b2=b). By following this transformation, we obtain the matrix WF∈R9×R16 and the bias b¯¯F∈R9:

To aid understanding, we show the entries from W that appear in the resulting matrix WF in bold.

Max Pooling: (MP) Layer
An MP laver takes a three-dimensional array x¯¯¯∈Rm×n×r and reduces the height m of x¯¯¯ by a factor of p and the width n of \overline{x}x¯¯¯ by a factor of qq (for pp and qq dividing mm and n). Depth is kept as-is. Neurons take as input disjoint subrectangles of \overline{x}x¯¯¯ of size p\times qp×q and return the maximal value in their subrectangle. Formally, the MP layer is a function \mathrm{MaxPoo}1_{p,q}:\mathbb{R}^{m\times n\times r}\rightarrow \mathbb{R}^{\frac{m}{p}\times\frac{n}{q}\times r}MaxPoo1p,q:Rm×n×r→Rmp×nq×r that for an input \overline{x}x¯¯¯ returns the three-dimensional array \overline{y}y¯¯¯ given by:\begin{align*} y_{i,j,k}=\max(\{x_{i^{\prime},j^{\prime},k}\vert p\cdot(i-1) < i^{\prime}\leq p\cdot i\\ \qquad \qquad q\cdot(j-1) < j^{\prime}\leq q\cdot j\}). \end{align*}

yi,j,k=max({xi′,j′,k|p⋅(i−1)<i′≤p⋅iq⋅(j−1)<j′≤q⋅j}).

View Source
Fig. 4c illustrates the max pooling computation for p=2,q=2p=2,q=2 and r=1r=1 For example, here we have:\begin{equation*} y_{1,1,1}=\max(\{x_{1,1,1},\ x_{1,2,1},\ x_{2,1,1},\ x_{2,2,1}\})=2. \end{equation*}

y1,1,1=max({x1,1,1, x1,2,1, x2,1,1, x2,2,1})=2.

View Source
Max Pooling to CAT
Let \mathrm{MaxPoo}1_{p,q}^{\prime}:\mathbb{R}^{m\cdot n\cdot r}\rightarrow \mathbb{R}^{\frac{m}{p}\frac{n}{\mathrm{q}}r}MaxPoo1′p,q:Rm⋅n⋅r→Rmpnqr be the function that is obtained from Max \mathrm{Poo}1_{p,\mathrm{q}}Poo1p,q by reshaping its input and output: \mathrm{MaxPoo}1_{p,q}^{\prime}(\overline{x}^{v})=\mathrm{MaxPoo}1_{p,q}(\overline{x})^{v}MaxPoo1′p,q(x¯¯¯v)=MaxPoo1p,q(x¯¯¯)v. To represent max pooling as a CAT function, we define a series of CAT functions whose composition is \mathrm{MaxPoo}1_{p,q}^{\prime}MaxPoo1′p,q: \begin{equation*} \mathrm{MaxPoo}1_{p,q}^{\prime}=f_{\frac{m}{p}\cdot\frac{n}{q}r^{\mathrm{O}}}\ldots\circ f_{1}\circ f^{\mathrm{MP}}. \end{equation*}

MaxPoo1′p,q=fmp⋅nqrO…∘f1∘fMP.

View SourceRight-click on figure for MathML and additional features.
Fig. 5: - The operation of the transformed max pooling layer.
Fig. 5:

The operation of the transformed max pooling layer.

Show All

The first function is f^{\mathrm{MP}}(\overline{x}^{v})=W^{\mathrm{MP}}\cdot\overline x^{v}fMP(x¯¯¯v)=WMP⋅x¯¯¯v, which reorders its input vector \overline{x}^vx¯¯¯v to a vector \overline{x}^{\mathrm{MP}}x¯¯¯MP in which the values of each max pooling subrectangle of \overline{x}x¯¯¯ are adjacent. The remaining functions execute standard max pooling. Concretely, the function f_{i}\in\{f_{1},\ \ldots,\ f_{\frac{m}{p}\cdot\frac{n}{q}r}\}fi∈{f1, …, fmp⋅nqr} executes max pooling on the ith subrectangle by selecting the maximal value and removing the other values. We provide formal definitions of the CAT functions f^{\mathrm{MP}}fMP and f_{i}fi in Appendix A. Here, we illustrate them on the example from Fig. 4c, where r=1r=1.

The CAT computation for this example is shown in Fig. 5. The computation begins from the input vector \overline{x}^vx¯¯¯v, which is the reshaping of \overline{x}x¯¯¯ from Fig. 4c. The values of the first 2\times 22×2 subrectangle in \overline{x}x¯¯¯ (namely, 0, 1, 2 and −4) are separated in \overline{x}^vx¯¯¯v by values from another subrectangle (3 and −2). To make them contiguous, we reorder \overline{x}^{v}x¯¯¯v using a permutation matrix W^{\mathrm{MP}}WMP, yielding \overline{x}^{\mathrm{MP}}x¯¯¯MP. In our example, W^{\mathrm{MP}}WMP is:

One entry in each row of W^{\mathrm{MP}}WMP is 1, all other entries are zeros. If row ii has entr jj set to 1, then the j^{\mathrm{th}}jth value of \overline{x}^vx¯¯¯v is moved to the i\mathrm{th}ith entry of \overline{x}^{\mathrm{MP}}x¯¯¯MP. For example, we placed a one in the fifth column of the third row of W^{\mathrm{MP}}WMP to move the value x_{5}^{v}xv5 to entry 3 of the output vector.

Next, for each i\in\{1,\ \ldots,\ \frac{m}{p}\cdot\frac{n}{q}\}i∈{1, …, mp⋅nq}, the function f_{i}fi takes as input a vector whose values at the indices between ii and i+p\cdot q-1i+p⋅q−1 are those of the i\mathrm{th}ith subrectangle of \overline{x}x¯¯¯ in Fig. 4c. It then replaces those p\cdot qp⋅q values by their maximum:\begin{equation*} f_{i}(\overline{x})=(x_{1},\ \ldots,\ x_{i-1},\ x_{k},\ x_{i+p\cdot q},\ \ldots,\ x_{m\cdot n-(p\cdot q-1)(i-1)}), \end{equation*}

fi(x¯¯¯)=(x1, …, xi−1, xk, xi+p⋅q, …, xm⋅n−(p⋅q−1)(i−1)),

View Source where the index k\in\{i,\ \ldots,\ i+p\cdot q-1\}k∈{i, …, i+p⋅q−1} is such that x_{k}xk is maximal. For kk given, f_{i}fi can be written as a CAT function: f_{i}(\overline{x})=W^{(i,k)}\cdot\overline{x}fi(x¯¯¯)=W(i,k)⋅x¯¯¯, where the rows of the matrix W^{(i,k)}\in \mathbb{R}^{(m\cdot n-(p\cdot q-1)i)\times(m\cdot n-(p\cdot q-1)\cdot(i-1))}W(i,k)∈R(m⋅n−(p⋅q−1)i)×(m⋅n−(p⋅q−1)⋅(i−1)) are given by the following sequence of standard basis vectors:\begin{equation*} \overline{e}_{1}, \ldots,\overline{e}_{i-1}, \overline{e}_{k},\overline{e}_{i+p\cdot q}, \ldots,\overline{e}_{m\cdot n-(p\cdot q-1)\cdot(i-1)}. \end{equation*}
e¯¯¯1,…,e¯¯¯i−1,e¯¯¯k,e¯¯¯i+p⋅q,…,e¯¯¯m⋅n−(p⋅q−1)⋅(i−1).

View SourceRight-click on figure for MathML and additional features.
For example, in Fig. 5, f_{1}(\overline{x}^{\mathrm{MP}})=W^{(1,3)}\cdot\overline{x}^{\mathrm{MP}}f1(x¯¯¯MP)=W(1,3)⋅x¯¯¯MP deletes 0,1 and −4. Then it moves the value 2 to the first component, and the values at indices 5, …, 16 to components 2, …, 13. Overall, W^{(1,3)}W(1,3) is given by:

As, in general, kk is not known in advance, we need to write f_{i}fi as a CAT function with a different case for each possible index kk of the maximal value in \overline{x}x¯¯¯. For example, in Fig. 5:\begin{align*} f_{1}(\overline{x})= \begin{matrix} \mathbf{case} (x_{1}\geq x_{2})\wedge(x_{1}\geq x_{3})\wedge(x_{1}\geq x_{4}):W^{(1,1)}\cdot\overline{x},\\ \mathbf{case} (x_{2}\geq x_{1})\wedge(x_{2}\geq x_{3})\wedge(x_{2}\geq x_{4}):W^{(1,2)}\cdot\overline{x},\\ \mathbf{case}(x_{3}\geq x_{1})\wedge(x_{3}\geq x_{2})\wedge(x_{3}\geq x_{4}):W^{(1,3)}\cdot\overline{x},\\ \mathbf{case} (x_{4}\geq x_{1})\wedge(x_{4}\geq x_{2})\wedge(x_{4}\geq x_{3}):W^{(1,4)}\cdot\overline{x}. \end{matrix} \end{align*}

f1(x¯¯¯)=case(x1≥x2)∧(x1≥x3)∧(x1≥x4):W(1,1)⋅x¯¯¯,case(x2≥x1)∧(x2≥x3)∧(x2≥x4):W(1,2)⋅x¯¯¯,case(x3≥x1)∧(x3≥x2)∧(x3≥x4):W(1,3)⋅x¯¯¯,case(x4≥x1)∧(x4≥x2)∧(x4≥x3):W(1,4)⋅x¯¯¯.

View Source
In our example, the vector \overline{x}^{\mathrm{MP}}x¯¯¯MP in Fig. 5 satisfies the third condition, and therefore f_{1}(\overline{x}^{\mathrm{MP}})=W^{(1,3)}\cdot\overline{x}^{\mathrm{MP}}f1(x¯¯¯MP)=W(1,3)⋅x¯¯¯MP. Taking into account all four subrectangles, we obtain:\begin{equation*} \mathrm{MaxPoo}1_{2,2}^{\prime}=f_{4}\circ f_{3}\circ f_{2}\circ f_{1}\circ f^{\mathrm{MP}}. \end{equation*}

MaxPoo1′2,2=f4∘f3∘f2∘f1∘fMP.

View SourceRight-click on figure for MathML and additional features.
In summary, each function f_{i}fi replaces p\cdot{q}p⋅q components of their input by the maximum value among them, suitably moving other values. For \overline{x}^vx¯¯¯v in Fig. 5:\begin{equation*} MaxPool_{2,2}^{\prime}(\overline{x}^{v})=W^{(4,7)}\cdot W^{(3,6)}\cdot W^{(2,2)}\cdot W^{(1,3)}\cdot W^{\mathrm{MP}}\cdot\overline{x}^{v}. \end{equation*}

MaxPool′2,2(x¯¯¯v)=W(4,7)⋅W(3,6)⋅W(2,2)⋅W(1,3)⋅WMP⋅x¯¯¯v.

View SourceRight-click on figure for MathML and additional features.
Network Architectures
Two popular architectures of neural networks are fully connected feedforward (FNN) and convolutional (CNN). An FNN is a sequence of fully connected layers, while a CNN [19] consists of all previously described layer types: convolutional, max pooling, and fully connected.


Fig. 6:

(a) Abstracting four points with a polyhedron (gray), zonotope (green), and box (blue). (b) The points and abstractions resulting from the affine transformer.

Show All

SECTION III.

Background: Abstract Interpretation
We now provide a short introduction to Abstract Interpretation (AI). AI enables one to prove program properties on a set of inputs without actually running the program. Formally, given a function f:\mathbb{R}^{m}\rightarrow \mathbb{R}^{n}f:Rm→Rn, a set of inputs X\subseteq \mathbb{R}^{m}X⊆Rm, and a property C\subseteq \mathbb{R}^{n}C⊆Rn, the goal is to determine whether the property holds, that is, whether \forall\overline{x}\in X.\ f(\overline{x})\in C∀x¯¯¯∈X. f(x¯¯¯)∈C.

Fig. 6 shows a CAT function f:\mathbb{R}^{2}\rightarrow \mathbb{R}^{2}f:R2→R2 that is defined as f(\overline{x})=\begin{pmatrix} 2 & -1\\ 0 & 1 \end{pmatrix}\cdot \overline{x}f(x¯¯¯)=(20−11)⋅x¯¯¯ and four input points for the function f, given as X=\{(0,1),\ (1,1),\ (1,3),\ (2,2)\}. Let the property be C=\{(y_{1},\ y_{2})\in \mathbb{R}^{2}\vert y_{1}\geq-2\}, which holds in this example. To reason about all inputs simultaneously, we lift the definition of f to be over a set of inputs X rather than a single input:\begin{equation*} T_{f}:\mathcal{P}(\mathbb{R}^{m})\rightarrow \mathcal{P}(\mathbb{R}^{n}),\qquad T_{f}(X)=\{f(\overline{x})\vert \overline{x}\in X\}. \end{equation*}View Source

The function T_{f} is called the concrete transformer of f. With T_{f}, our goal is to determine whether T_{f}(X)\subseteq C for a given input set X. Because the set X can be very large (or infinite), we cannot enumerate all points in X to compute T_{f}(X). Instead, AI overapproximates sets with abstract elements (drawn from some abstract domain \mathcal{A}) and then defines a function, called an abstract transformer of f. which works with these abstract elements and overapproximates the effect of T_{f}. Then, the property C can be checked on the resulting abstract element returned by the abstract transformer. Naturally, because AI employs overapproximation, it is sound, but may be imprecise (i.e., may fail to prove the property when it holds). Next, we explain the ingredients of AI in more detail.

Abstract Domains
Abstract domains consist of shapes expressible as a set of logical constraints. A few popular numerical abstract domains are: Box (i.e., Interval), Zonotope, and Polyhedra. These domains provide different precision versus scalability trade-offs (e.g., Box's abstract transformers are significantly faster than Polyhedra's abstract transformers, but polyhedra are significantly more precise than boxes). The Box domain consists of boxes, captured by a set of constraints of the form a\leq x_{i}\leq b, for a, b\in \mathbb{R}\cup\{-\infty,\ +\infty\} and a\leq b. A box B contains all points which satisfy all constraints is B. In our example, X can be abstracted by the following box:\begin{equation*} B=\{0\leq x_{1}\leq 2,1\leq x_{2}\leq 3\}. \end{equation*}View SourceRight-click on figure for MathML and additional features.

Note that \mathrm{B} is not very precise since it includes 9 integer points (along with other points), whereas X has only 4 points.

The Zonotope domain [10] consists of zonotopes. A zono-tope is a center-symmetric convex closed polyhedron Z\subseteq \mathbb{R}^{n} that can be represented as an affine function:\begin{equation*} z:[a_{1},\ b_{1}]\times[a_{2},\ b_{2}]\times\cdots\times[a_{m},\ b_{m}]\rightarrow \mathbb{R}^{n}. \end{equation*}View SourceRight-click on figure for MathML and additional features.

In other words, z has the form z(\overline{\epsilon})=M\cdot\overline{\epsilon}+\overline{b} where \overline\epsilon is a vector of error terms satisfying interval constraints \overline{\epsilon}_{i}\in[a_{i},\ b_{i}] for 1\leq i\leq m. The bias vector \overline{b} captures the center of the zonotope, while the matrix M captures the boundaries of the zonotope around \overline{b}. A zonotope z represents all vectors in the image of z (i.e., z[[a_{11}]\times\cdots\times[a_{m},\ b_{m}]]). In our example, X can be abstracted by the zonotope z:[-1,1]^{3}\rightarrow \mathbb{R}^{2}: \begin{equation*} z(\epsilon_{1},\ \epsilon_{2},\ \epsilon_{3})=(1+0.5\cdot\epsilon_{1}+0.5\cdot\epsilon_{2},2+0.5\cdot\epsilon_{1}+0.5\cdot\epsilon_{3}). \end{equation*}View SourceRight-click on figure for MathML and additional features.

Zonotope is a more precise domain than Box: for our example, z includes only 7 integer points.

The Polyhedra [8] domain consists of convex closed polyhedra, where a polyhedron is captured by a set of linear constraints of the form A\cdot\overline{x}\leq\overline{b}, for some matrix a and a vector \overline{b}. A polyhedron C contains all points which satisfy the constraints in C. In our example, X can be abstracted by the following polyhedron:\begin{equation*} C=\{x_{2}\leq 2\cdot x_{1}+1,\ x_{2}\leq 4-x_{1},\ x_{2}\geq 1,\ x_{2}\geq x_{1}\}. \end{equation*}View SourceRight-click on figure for MathML and additional features.

Polyhedra is a more precise domain than Zonotope: for our example, C includes only 5 integer points.

To conclude, abstract elements (from an abstract domain) represent large, potentially infinite sets. There are various abstract domains, providing different levels of precision and scalability.

Abstract Transformers
To compute the effect of a function on an abstract element, AI uses the concept of an abstract transformer. Given the (lifted) concrete transformer T_{f}:\mathcal{P}(\mathbb{R}^{m})\rightarrow \mathcal{P}(\mathbb{R}^{n}) of a function f:\mathbb{R}^{m}\rightarrow \mathbb{R}^{n}, an abstract transformer of T_{f} is a function over abstract domains, denoted by T_{f}^{\#}:\mathcal{A}^{m}\rightarrow \mathcal{A}^{n}. The superscripts denote the number of components of the represented vectors. For example, elements in \mathcal{A}^{m} represent sets of vectors of dimension m. This also determines which variables can appear in the constraints associated with an abstract element. For example, elements in \mathcal{A}^{m} constrain the values of the variables x_{1}, \ldots, x_{m}.

Abstract transformers have to be sound. To define soundness, we introduce two functions: the abstraction function \alpha and the concretization function \gamma. An abstraction function \alpha^{m}:\mathcal{P}(\mathbb{R}^{m})\rightarrow \mathcal{A}^{m} maps a set of vectors to an abstract element in \mathcal{A}^{m} that overapproximates it. For example, in the Box domain:\begin{equation*} \alpha^{2}(\{(0,1), (1,1), (1,3), (2,2)\})=\{0\leq x_{1}\leq 2, 1\leq x_{2}\leq 3\}. \end{equation*}View SourceRight-click on figure for MathML and additional features.

A concretization function \gamma^{m}:A^{m}\rightarrow \mathcal{P}(\mathbb{R}^{m}) does the opposite: it maps an abstract element to the set of concrete vectors that it represents. For example, for Box:\begin{align*} \gamma^{2}(\{0\leq x_{1}\leq 2,1\leq x_{2}\leq 3\})=&\{(0,1), (0,2), (0,3),\\ &(1, 1), (1, 2), (1, 3),\\ & (2, 1), (2, 2), (2, 3), \ldots\}. \end{align*}View SourceRight-click on figure for MathML and additional features.

This only shows the 9 vectors with integer components. We can now define soundness. An abstract transformer T_{f}^{\#} is sound if for all a\in \mathcal{A}^{m}, we have T_{f}(\gamma^{m}(a))\subseteq\gamma^{n}(T_{f}^{\#}(a)), where T_{f} is the concrete transformer. That is, an abstract transformer has to overapproximate the effect of a concrete transformer. For example, the transformers illustrated in Fig. 6 are sound. For instance, if we apply the Box transformer on the box in Fig. 6a, it will produce the box in Fig. 6b. The box in Fig. 6b includes all points that f could compute in principle when given any point included in the concretization of the box in Fig. 6a. Analogous properties hold for the Zonotope and Polyhedra transformers. It is also important that abstract transformers are precise. That is, the abstract output should include as few points as possible. For example, as we can see in Fig. 6b, the output produced by the Box transformer is less precise (it is larger) than the output produced by the Zonotope transformer, which in turn is less precise than the output produced by the Polyhedra transformer.

Property Verification
After obtaining the (abstract) output we can check various properties of interest on the result. In general, an abstract output a=T_{f}^{\#}(X) proves a property T_{f}(X)\subseteq C if \gamma^{n}(a)\subseteq C. If the abstract output proves a property, we know that the property holds for all possible concrete values. However, the property may hold even if it cannot be proven with a given abstract domain. For example, in Fig. 6b, for all concrete points, the property C=\{(y_{1},\ y_{2})\in \mathbb{R}^{2}\vert y_{1}\geq-2\} holds. However, with the Box domain, the abstract output violates C, which means that the Box domain is not precise enough to prove the property. In contrast, the Zonotope and Polyhedra domains are precise enough to prove the property.

In summary, to apply AI successfully, we need to: (a) find a suitable abstract domain, and (b) define abstract transformers that are sound and as precise as possible. In the next section, we introduce abstract transformers for neural networks that are parameterized by the numerical abstract domain. This means that we can explore the precision-scalability trade-off by plugging in different abstract domains.

In this section we present AI2, an abstract interpretation framework for sound analysis of neural networks. We begin by defining abstract transformers for the different kinds of neural network layers. Using these transformers, we then show how to prove robustness properties of neural networks.

A. Abstract Interpretation for CAT Functions
In this section, we show how to overapproximate CAT functions with AI. We illustrate the method on the example in Fig. 7, which shows a simple network that manipulates two-dimensional vectors using a single fully connected layer of the form f(\overline{x})=\mathrm{ReLU}_{2}(\mathrm{ReLU}_{1}\begin{pmatrix} 2 & -1\\ 0 & 1 \end{pmatrix}\cdot\overline{x})). Recall that\begin{align*} \mathrm{ReLU}_{i}(\overline{x}) = &case (x_{i}\geq 0):\overline{x},\\ &case (x_{i} < 0):I_{i\leftarrow 0}\cdot\overline{x}, \end{align*}View SourceRight-click on figure for MathML and additional features. where I_{i\leftarrow 0} is the identity matrix with the i^{\mathrm{th}} row replaced by the zero vector. We overapproximate the network behavior on an abstract input. The input can be obtained directly (see Sec. IV-B) or by abstracting a set of concrete inputs to an abstract element (using the abstraction function \alpha). For our example, we use the concrete inputs (the blue points) from Fig 6. Those concrete inputs are abstracted to the green zonotope z_{0}:[-1,1]^{3}\rightarrow \mathbb{R}^{2}, given as:\begin{equation*} z_{0}(\epsilon_{1},\ \epsilon_{2},\ \epsilon_{3})=(1+0.5\cdot\epsilon_{1}+0.5\cdot\epsilon_{2},2+0.5\cdot\epsilon_{1}+0.5\cdot\epsilon_{3}). \end{equation*}View SourceRight-click on figure for MathML and additional features.

Fig. 7: - Illustration of how ai2 overapproximates neural network states. Blue circles show the concrete values, while green zonotopes show the abstract elements. The gray box shows the steps in one application of the relu transformer (relu1).
Fig. 7:

Illustration of how ai2 overapproximates neural network states. Blue circles show the concrete values, while green zonotopes show the abstract elements. The gray box shows the steps in one application of the relu transformer (relu1).

Show All

Due to abstraction, more (spurious) points may be added. In this example, except the blue points, the entire area of the zonotope is spurious. We then apply abstract transformers to the abstract input. Note that, if a function f can be written as f=f^{\prime\prime}\mathrm{o}f^{\prime}, the concrete transformer for f is T_{f}=T_{f^{\prime\prime}}\circ T_{f^{\prime}}. Similarly, given abstract transformers T_{f^{\prime}}^{\#} and T_{f}^{\#}, an abstract transformer for f is T_{f}^{\#}, \circ T_{f}^{\#}. When a neural network N=f_{\ell}^{\prime}\mathrm{o}\cdots \mathrm{o}f_{1}^{\prime} is a composition of multiple CAT functions f_{i}^{\prime} of the shape f_{i}^{\prime}(\overline{x})=W\cdot\overline{x}+\overline{b} or f_{i}(\overline{x})=case E_{1}:f_{1}(\overline{x}), case E_{k}:f_{k}(\overline{x}), we only have to define abstract transformers for these two kinds of functions. We then obtain the abstract transformer T_{N}^{\#}=T_{f_{\ell}^{\prime}}^{\#}\mathrm{o}\cdots \mathrm{o}T_{f_{1}}^{\#}.

Abstracting Affine Functions
To abstract functions of the form f(\overline{x})=W\cdot\overline{x}+\overline{b}, we assume that the underlying abstract domain supports the operator Aff that overapproximates such functions. We note that for Zonotope and Polyhedra, this operation is supported and exact. Fig. 7 demonstrates Aff as the first step taken for overapproximating the effect of the fully connected layer. Here, the resulting zonotope z_{1}:[-1,1]^{3}\rightarrow \mathbb{R}^{2} is:\begin{align*} &z_{1}(\epsilon_{1},\ \epsilon_{2},\ \epsilon_{3})=\\ &\qquad (2 \cdot(1+0.5\cdot\epsilon_{1}+0.5\cdot\epsilon_{2})-(2+0.5\cdot\epsilon_{1}+0.5\cdot\epsilon_{3}),\\ &\qquad \qquad \qquad \qquad\qquad\qquad \qquad 2+0.5\cdot\epsilon_{1}+0.5\cdot\epsilon_{3})=\\ &\qquad\qquad (0.5\cdot\epsilon_{1}+\epsilon_{2}-0.5\cdot\epsilon_{3)}2+0.5\cdot\epsilon_{1}+0.5\cdot\epsilon_{3}). \end{align*}View SourceRight-click on figure for MathML and additional features.

Abstracting Case Functions
To abstract functions of the form f(\overline{x})= case E_{1}:f_{1}(\overline{x}), \ldots, case E_{k}:f_{k}(\overline{x}), we first split the abstract element a into the different cases (each defined by one of the expressions E_{i}), resulting in k abstract elements a_{1}, \ldots, a_{k}. We then compute the result of T_{f_{i}}^{\#}(a_{i}) for each a_{i}. Finally, we unify the results to a single abstract element. To split and unify, we assume two standard operators for abstract domains: (1) meet with a conjunction of linear constraints and (2) join. The meet (\sqcap) operator is an abstract transformer for set intersection: for an inequality expression E from Fig. 3, \gamma^{n}(a)\cap\{x\in \mathbb{R}^{n}\vert x\models E\}\subseteq\gamma^{n}(a\sqcap\mathrm{E}). The join (\sqcup) operator is an abstract transformer for set union: \gamma^{n}(a_{1})\cup\gamma^{n}(a_{2})\subseteq\gamma^{n}(\alpha_{1}\sqcup a_{2}). We further assume that the abstract domain contains an element \perp, which satisfies \gamma^{n}(\perp)=\{\}, \perp\sqcap E=\perp and \alpha\sqcup\perp=a for a\in A.

For our example in Fig. 7, abstract interpretation continues on Z_{1} using the meet and join operators. To compute the effect of ReLU1, z_{1} is split into two zonotopes z_{2}=z_{1}\sqcap(x_{1}\geq 0) and z_{3}=z_{1}\sqcap(x_{1} < 0). One way to compute a meet between a zonotope and a linear constraint is to modify the intervals of the error terms (see [11]). In our example, the resulting zonotopes are z_{2}: [-1, 1] \times[0,1]\times[-1,1]\rightarrow \mathbb{R}^{2} such that z_{2}(\overline{\epsilon})=z_{1}(\overline{\epsilon}) and z_{3}: [-1, 1]\times[-1,0]\times[-1,1]\rightarrow \mathbb{R}^{2} such that z_{3}(\overline{\epsilon})=z_{1}(\overline{\epsilon}) for \epsilon common to their respective domains. Note that both z_{2} and z_{3} contain small spurious areas, because the intersections of the respective linear constraints with z_{1} are not zonotopes. Therefore, they cannot be captured exactly by the domain. Here, the meet operator \sqcap overapproximates set intersection \cap to get a sound, but not perfectly precise, result.

Then, the two cases of ReLU1 are processed separately. We apply the abstract transformer of f_{1}(\overline{x})=\overline{x} to z_{2} and we apply the abstract transformer of f_{2}(\overline{x})=I_{0\leftarrow 0}\cdot\overline{x} to Z3. The resulting zonotopes are z_{4}=z_{2} and z_{5}:[-1,1]^{2}\rightarrow \mathbb{R}^{2} such that z_{5}(\epsilon_{1},\ \epsilon_{3})=(0,2+0.5\cdot\epsilon_{1}+0.5\cdot\epsilon_{3}). These are then joined to obtain a single zonotope z_{6}. Since Z5 is contained in Z4, we get z_{6}=z_{4} (of course, this need not always be the case). Then, z_{6} is passed to ReLU2. Because z_{6}\sqcap(x_{1} < 0)=\perp, this results in z_{7}=z_{6}. Finally, \gamma^{2}(z_{7}) is our overapproximation of the network outputs for our initial set of points. The abstract element Z_{7} is a finite representation of this infinite set.

Fig. 8: - Abstract transformers for CAT functions.
Fig. 8:

Abstract transformers for CAT functions.

Show All

In summary, we define abstract transformers for every kind of CAT function (summarized in Fig. 8). These definitions are general and are compatible with any abstract domain \mathcal{A} which has a minimum element \perp and supports (1) a meet operator between an abstract element and a conjunction of linear constraints, (2) a join operator between two abstract elements, and (3) an affine transformer. We assume that the operations are sound. We note that these operations are standard or definable with standard operations. By definition of the abstract transformers, we get soundness:

Theorem 1
For any CAT function f with transformer T_{f}:\mathcal{P}(\mathbb{R}^{m})\rightarrow \mathcal{P}(\mathbb{R}^{n}) and any abstract input a\in \mathcal{A}^{m}, \begin{equation*} T_{f}(\gamma^{m}(a))\subseteq\gamma^{n}(T_{f}^{\#}(a)). \end{equation*}View SourceRight-click on figure for MathML and additional features.

Theorem 1 is the key to sound neural network analysis with our abstract transformers, as we explain in the next section.

B. Neural Network Analysis with AI
In this section, we explain how to leverage AI with our abstract transformers to prove properties of neural networks. We focus on robustness properties below, however, the framework can be applied to reason about any safety property.

For robustness, we aim to determine if for a (possibly unbounded) set of inputs, the outputs of a neural network satisfy a given condition. A robustness property for a neural network N:\mathbb{R}^{m}\rightarrow \mathbb{R}^{n} is a pair (X,\ C)\in \mathcal{P}(\mathbb{R}^{m})\times \mathcal{P}(\mathbb{R}^{n}) consisting of a robustness region X and a robustness condition C. We say that the neural network N satisfies a robustness property (X,C) if N(\overline{x})\in C for all \overline{x}\in X.

Local Robustness
This is a property (X, C_{L}) where X is a robustness region and C_{L} contains the outputs that describe the same label \mathrm{L}: \begin{equation*} C_{L}=\left\{\overline{y}\in \mathbb{R}^{n}\left. \right\vert \mathop{\arg\max}_{i\in\{1,\ldots,n\}}(y_{i})=L\right\}. \end{equation*}View SourceRight-click on figure for MathML and additional features.

For example, Fig. 7 shows a neural network and a robustness property (X, C_{2}) for X=\{(0,1),\ (1,1),\ (1,3),\ (2,2)\} and C_{2}=\{\overline{y}\vert arg max (y_{1},\ y_{2})=2\}. In this example, (X,C_{2}) holds. Typically, we will want to check that there is some label L for which (X,\ C_{L}) holds.

We now explain how our abstract transformers can be used to prove a given robustness property (X, C).

Robustness Proofs Using AI
Assume we are given a neural network N:\mathbb{R}^{m}\rightarrow \mathbb{R}^{n}, a robustness property (X, C) and an abstract domain \mathcal{A} (supporting \sqcup,\sqcap with a conjunction of linear constraints, Aff, and \perp) with an abstraction function \alpha and a concretization function \gamma. Further assume that N can be written as a CAT function. Denote by T_{N}^{\#} the abstract transformer of N, as defined in Fig. 8. Then, the following condition is sufficient to prove that N satisfies (X,C): \begin{equation*} \gamma^{n}(T_{N}^{\#}(\alpha^{m}(X)))\subseteq C. \end{equation*}View SourceRight-click on figure for MathML and additional features.

This follows from Theorem 1 and the properties of \alpha and \gamma. Note that there may be abstract domains \mathcal{A} that are not precise enough to prove that N satisfies (X, C), even if \mathrm{N}. in fact satisfies (X,\ C). On the other hand, if we are able to show that some abstract domain \mathcal{A} proves that N. satisfies (X,\ C), we know that it holds.

Proving Containment
To prove the property (X,\ C) given the result a=T_{N}^{\#}(\alpha^{m}(X)) of abstract interpretation, we need to be able to show \gamma^{n}(a)\subseteq C. There is a general method if C is given by a CNF formula \wedge_{i}\vee_{j}l_{i,j} where all literals l_{i,j} are linear constraints: we show that the negated formula \vee_{i}\wedge_{j}\neg l_{i,j} is inconsistent with the abstract element a by checking that a\sqcap(\bigwedge \neg l_{i,j})=\perp for all i.

For our example in Fig. 7, the goal is to check that all inputs are classified as 2. We can represent C using the formula y_{2}\geq y_{1}. Its negation is y_{2}< y_{1}, and it suffices to show that no point in the concretization of the abstract output satisfies this negated constraint. As indeed z_{7}\sqcap(y_{2} < y_{1})=\perp, the property is successfully verified. However, note that we would not be able to prove some other true properties, such as y_{1}\geq 0. This property holds for all concrete outputs, but some points in the concretization of the output Z_{7} do not satisfy it.

SECTION V.

Implementation of AI2
The AI2 framework is implemented in the D programming language and supports any neural network composed of fully connected, convolutional, and max pooling layers.

Properties
AI2 supports properties (X, C) where X is specified by a zonotope and C by a conjunction of linear constraints over the output vector's components. In our experiments, we illustrate the specification of local robustness properties where the region X is defined by a box or a line, both of which are precisely captured by a zonotope.

Abstract Domains
The AI2 system is fully integrated with all abstract domains supported by Apron [20], a popular library for numerical abstract domains, including: Box [7], Zonotope [10], and Polyhedra [8].

Bounded Powerset
We also imnlemented bounded nowerset domains (disjunctive abstractions [33], [36]), which can be instantiated with any of the above abstract domains. An abstract element in the powerset domain \mathcal{P}(\mathcal{A}) of an underlying abstract domain \mathcal{A} is a set of abstract elements from \mathcal{A}, concretizing to the union of the concretizations of the individual elements (i.e., \gamma(A)=\bigcup\nolimits_{{a}\epsilon {A}}\gamma(a) for A\in \mathcal{P}(A)).

The powerset domain can implement a precise join operator using standard set union (potentially pruning redundant elements). However, since the increased precision can become prohibitively costly if many join operations are performed, the bounded powerset domain limits the number of abstract elements in a set to N (for some constant N).

We implemented bounded powerset domains on top of standard powerset domains using a greedy heuristic that repeatedly replaces two abstract elements in a set by their join, until the number of abstract elements in the set is below the bound N, For joining, AI2 heuristically selects two abstract elements that minimize the distance between the centers of their bounding boxes. In our experiments, we denote by ZonotopeN or \mathrm{Z}N the bounded powerset domain with bound N\geq 2 and underlying abstract domain Zonotope.

SECTION VI.

Evaluation of AI2
In this section, we present our empirical evaluation of AI2. Before discussing the results in detail, we summarize our three most important findings:

AI2 can prove useful robustness properties for convolutional networks with 53 000 neurons and large fully connected feedforward networks with 1 800 neurons.

AI2 benefits from more precise abstract domains: Zono-tope enables AI2 to prove substantially more properties over Box. Further, Zonotope N, with N\geq 2, can prove stronger robustness properties than Zonotope alone.

AI2 scales better than the SMT-based Reluplex [21]: AI2 is able to verify robustness properties on large networks with \geq 1200 neurons within few minutes, while Reluplex takes hours to verify the same properties.

In the following, we first describe our experimental setup. Then, we present and discuss our results.

A. Experimental Setup
We now describe the datasets, neural networks, and robustness properties used in our experiments.

Datasets
We used two popular datasets: MNIST [25] and CIFAR-10 [22] (referred to as CIFAR from now on). MNIST consists of 60 000 grayscale images of handwritten digits, whose resolution is 28×28 pixels. The images show white digits on a black background.

CIFAR consists of 60 000 colored photographs with 3 color channels, whose resolution is 32×32 pixels. The images are partitioned into 10 different classes (e.g., airplane or bird). Each photograph has a different background (unlike MNIST).

Neural Networks
We trained convolutional and fully connected feedforward networks on both datasets. All networks were trained using accelerated gradient descent with at least 50 epochs of batch size 128. The training completed when each network had a test set accuracy of at least 0.9.

For the convolutional networks, we used the LeNet architecture [26], which consists of the following sequence of layers: 2 convolutional, 1 max pooling, 2 convolutional, 1 max pooling, and 3 fully connected layers. We write n_{p\times \mathrm{q}} to denote a convolutional layer with n filters of size p\times q, and m to denote a fully connected layer with m neurons. The hidden layers of the MNIST network are 8_{3\times 3},8_{3\times 3},14_{3\times 3},14_{3\times 3},50,50,10, and those of the CIFAR network are 24_{3\times 3},24_{3\times 3},38_{3\times 3},3\mathrm{Z}_{3\times 3},100,100,10. The max pooling layers of both networks have a size of 2×2. We trained our networks using an open-source implementation [37].

We used 7 different architectures of fully connected feedforward networks (FNNs). We write l\times n to denote the FNN architecture with l laycrs, each consisting of n neurons. Note that this determines the network's size; e.g., a 4×50 network has 200 neurons. For each dataset, MNIST and CIFAR, we trained FNN s with the following architectures: 3×20, 6×20, 3×50, 3×100, 6×100, 6×200, and 9×200.

Robustness Properties
In our experiments, we consider local robustness properties (X, C_{L}) where the region X captures changes to lighting conditions. This type of property is inspired by the work of [32], where adversarial examples were found by brightening the pixels of an image.

Formally, we consider robustness regions S_{\overline{x},\delta} that are parameterized by an input \overline{x}\in \mathbb{R}^{m} and a robustness bound \delta\in[0,1]. The robustness region is defined as:\begin{equation*} S_{\overline{x},\delta}=\{\overline{x}^{\prime}\in \mathbb{R}^{m}\vert \forall i\in[1,\ m].\ 1-\delta\leq x_{i}\leq x_{i}^{\prime}\leq 1\vee x_{i}^{\prime}=x_{i}\}. \end{equation*}View Source

For example, the robustness region for \overline{x}=(0.6,0.85,0.9) and bound \delta=0.2 is given by the set:\begin{equation*} \{(0.6,\ x,\ x^{\prime})\in \mathbb{R}^{3}\vert x\in[0.85,1],\ x^{\prime}\in[0.9,1]\}. \end{equation*}View Source

Note that increasing the bound \delta increases the region's size.

In our experiments, we used AI2 to check whether all inputs in a given region S_{\overline{x},\delta} are classified to the label assigned to \overline{x} We consider 6 different robustness bounds \delta, which are drawn from the set \triangle=\{0.001, 0.005, 0.025, 0.045, 0.065, 0.085\}.

We remark that our robustness properties are stronger than those considered in [32]. This is because, in a given robustness region S_{\overline{x},\delta}, each pixel of the image \overline{x} is brightened independently of the other pixels. We remark that this is useful to capture scenarios where only part of the image is brightened (e.g., due to shadowing).

Other Perturbations
Note that AI2 is not limited to certifying robustness against such brightening perturbations. In general, AI2 can be used whenever the set of perturbed inputs can be overapproximated with a set of zonotopes in a precise way (i.e., without adding too many inputs that do not capture actual perturbations to the robustness region). For example, the inputs perturbed by an \ell_{\infty} attack [3] are captured exactly by a single zonotope. Further, rotations and translations have low-dimensional parameter spaces, and therefore can be overapproximated by a set of zonotopes in a precise way.

Benchmarks
We selected 10 images from each dataset. Then, we specified a robustness property for each image and each robustness bound in \triangle, resulting in 60 properties per dataset. We ran AI2 to check whether each neural network satisfies the robustness properties for the respective dataset. We compared the results using different abstract domains, including Box, Zonotope, and Zonotope N with N ranging between 2 and 128.


Fig. 9:

Verified properties by ai2 on the MNIST and CIFAR convolutional networks for each bound \delta\in\Lambda (x-axis).

Show All

We ran all experiments on an Ubuntu 16.04.3 LTS server with two Intel Xeon E5-2690 processors and 512GB of memory. To compare AI2 to existing solutions, we also ran the FNN benchmarks with Reluplex [21]. We did not run convolutional benchmarks with Reluplex as it currently does not support convolutional networks.

B. Discussion of Results
In the following, we first present our results for convolutional networks. Then, we present experiments with different abstract domains and discuss how the domain's precision affects AI2's ability to verify robustness. We also plot AI2's running times for different abstract domains to investigate the trade-off between precision and scalability. Finally, we compare AI2 to Reluplex.

Proving Robustness of Convolutional Networks
We start with our results for convolutional networks. AI2 terminated within 1.5 minutes when verifying properties on the MNIST network and within 1 hour when verifying the CIFAR network.

In Fig. 9, we show the fraction of robustness properties verified by AI2 for each robustness bound. We plot separate bars for Box and Zonotope to illustrate the effect of the domain's precision on AI2's ability to verify robustness.

For both networks, AI2 verified all robustness properties for the smallest bound 0.001 and it verified at least one property for the largest bound 0.085. This demonstrates that AI2 can verify properties of convolutional networks with rather wide robustness regions. Further, the number of verified properties converges to zero as the robustness bound increases. This is expected, as larger robustness regions are more likely to contain adversarial examples.

In Fig. 9a, we observe that Zonotope proves significantly more properties than Box. For example, Box fails to prove any robustness properties with bounds at least 0.025, while Zonotope proves 80% of the properties with bounds 0.025 and 0.045. This indicates that Box is often imprecise and fails to prove properties that the network satisfies.

Similarly, Fig. 9b shows that Zonotope proves more robustness properties than Box also for the CIFAR convolutional network. The difference between these two domains is, however, less significant than that observed for the MNIST network. For example, both Box and Zonotope prove the same properties for bounds 0.065 and 0.085.

Precision of Different Abstract Domains
Next, we demonstrate that more precise abstract domains enable AI2 to prove stronger robustness properties. In this experiment, we consider our 9×200 MNIST and CIFAR networks, which are our largest fully connected feedforward networks. We evaluate the Box, Zonotope, and the Zonotope \mathrm{N} domains for exponentially increasing bounds of \mathrm{N} between 2 and 64. We do not report results for the Polyhedra domain, which takes several days to terminate for our smallest networks.

In Fig. 10, we show the fraction of verified robustness properties as a function of the abstract domain used by AI2. We plot a separate line for each robustness bound. All runs of AI2 in this experiment completed within 1 hour.

The graphs show that Zonotope proves more robustness properties than Box. For the MNIST network, Box proves 11 out of all 60 robustness properties (across all 6 bounds), failing to prove any robustness properties with bounds above 0.005. In contrast, Zonotope proves 43 out of the 60 properties and proves at least 50% of the properties across the 6 robustness bounds. For the CIFAR network, Box proves 25 out of the 60 properties while Zonotope proves 35.


Fig. 10:

Verified properties as a function of the abstract domain used by ai2 for the 9×200 network. Each point represents the fraction of robustness properties for a given bound (as specified in the legend) verified by a given abstract domain (x-axis).

Show All


Fig. 11:

Average running time of ai2 when proving robustness properties on MNIST networks as a function of the abstract domain used by ai2 (x-axis). Axes are scaled logarithmically.

Show All

The data also demonstrates that bounded sets of zonotopes further improve AI2's ability to prove robustness properties. For the MNIST network, Zonotope64 proves more robustness properties than Zonotope for all 4 bounds for which Zonotope fails to prove at least one property (i.e., for bounds \delta\geq 0.025). For the CIFAR network, Zonotope64 proves more properties than Zonotope for 4 out of the 5 the bounds. The only exception is the bound 0.085, where Zonotope64 and Zonotope prove the same set of properties.

Trade-Off Between Precision and Scalability
In Fig. 11, we plot the running time of AI2 as a function of the abstract domain. Each point in the graph represents the average running time of AI2 when proving a robustness property for a given MNIST network (as indicated in the legend). We use a log-log plot to better visualize the trade-off in time.

The data shows that AI2 can efficiently verify robustness of large networks. AI2 terminates within a few minutes for all MNIST FNNs and all considered domains. Further, we observe that AI2 takes less than 10 seconds on average to verify a property with the Zonotope domain.

As expected, the graph demonstrates that more precise domains increase AI2's running time. More importantly, AI2, s running time is polynomial in the bound \mathrm{N} of Zonotope \mathrm{N}, which allows one to adjust AI2's precision by increasing \mathrm{N}.

Comparison to Reluplex
The current state-of-the-art system for verifying properties of neural networks is Reluplex [21]. Reluplex supports FNNs with ReLU activation functions, and its analysis is sound and complete. Reluplex would eventually either verify a given property or return a counterexample.

To compare the performance of Reluplex and AI2, we ran both systems on all MNIST FNN benchmarks. We ran AI2 using Zonotope and Zonotope64. For both Reluplex and AI2, we set a 1 hour timeout for verifying a single property.

Fig. 12 presents our results: Fig. 12a plots the average running time of Reluplex and AI2 and Fig. 12b shows the fraction of robustness properties verified by the systems. The data shows that Reluplex can analyze FNN s with at most 600 neurons efficiently, typically within a few minutes. Overall, both system verified roughly the same set of properties. However, Reluplex crashed during verification of some of the properties. This explains why AI2 was able to prove slightly more properties than Reluplex on the smaller FNN s.


Fig. 12:

Comparing the performance of ai2 to reluplex. Each point is an average of the results for all 60 robustness properties for the MNIST networks. Each point in (a) represents the average time to completion, regardless of the result of the computation. While not shown, the result of the computation could be a failure to verify, timeout, crash, or discovery of a counterexample. Each point in (b) represents the fraction of the 60 robustness properties that were verified.

Show All

For large networks with more than 600 neurons, the running time of Reluplex increases significantly and its analysis often times out. In contrast, AI2 analyzes the large networks within a few minutes and verifies substantially more robustness properties than Reluplex. For example, Zonotope64 proves 57 out of the 60 properties on the 6×200 network, while Reluplex proves 3. Further, Zonotope64 proves 45 out of the 60 properties on the largest 9×200 network, while Reluplex proves none. We remark that while Reluplex did not verify any property on the largest 9×200 network, it did disprove some of the properties and returned counterexamples.

We also ran Reluplex without a predefined timeout to investigate how long it would take to verify properties on the large networks. To this end, we ran Reluplex on properties that AI2 successfully verified. We observed that Reluplex often took more than 24 hours to terminate. Overall, our results indicate that Reluplex does not scale to larger FNN s whereas AI2 succeeds on these networks.

SECTION VII.

Comparing Defenses with AI2
In this section, we illustrate a practical application of AI2: evaluating and comparing neural network defenses. A defense is an algorithm whose goal is to reduce the effectiveness of a certain attack against a specific network, for example, by retraining the network with an altered loss function. Since the discovery of adversarial examples, many works have suggested different kinds of defenses to mitigate this phenomenon (e.g., [12], [27], [42]). A natural metric to compare defenses is the average “size” of the robustness region on some test set. Intuitively, the greater this size is, the more robust the defense.

We compared three state-of-the-art defenses:

GSS [12] extends the loss with a regularization term encoding the fast gradient sign method (FGSM) attack.

Ensemble [42] is similar to GSS, but includes regularization terms from attacks on other models.

MMSTV [27] adds, during training, a perturbation layer before the input layer which applies the FGSMk attack. FGSMk is a multi-step variant of FGSM, also known as projected gradient descent.

All these defenses attempt to reduce the effectiveness of the FGSM attack [12]. This attack consists of taking a network \mathrm{N} and an input \overline{x} and computing a vector \overline{\rho}_{N,\overline{x}} in the input space along which an adversarial example is likely to be found. An adversarial input \overline{a} is then generated by taking a step \epsilon along this vector: \overline{a}=\overline{x}+\epsilon\cdot\overline{\rho}_{{N},\overline{x}}.

We define a new kind of robustness region, called line, that captureg resilience with respect to the FGSM attack. The line robustness region captures all points from \overline{x} to \overline{x}+\delta\cdot\overline{\rho}_{N,\overline{x}} for some robustness bound \delta: \begin{equation*} L_{N,\overline{x},\delta}=\{\overline{x}+\epsilon\cdot\overline{\rho}_{N,\overline{x}}\vert \epsilon\in[0,\ \delta]\}. \end{equation*}View Source

This robustness region is a zonotope and can thus be precisely captured by AI2.

We compared the three state-of-the-art defenses on the MNIST convolutional network described in Section VI; we call this the Original network. We trained the Original network with each of the defenses, which resulted in 3 additional networks: GSS, Ensemble, and MMSTV. We used 40 epochs for GSS, 12 epochs for Ensemble, and 10 000 training steps for MMSTV using their published frameworks.


Fig. 13:

Box-and-whisker plot of the verified bounds for the original, GSS, ensemble, and MMSTV networks. The boxes represent the \delta for the middle 50% of the images, whereas the whiskers represent the minimum and maximum \delta. The inner-lines are the averages.

Show All

We conducted 20 experiments. In each experiment, we randomly selected an image \overline{x} and computed \overline{\rho}_{N,\overline{x}}. Then, for each network, our goal was to find the largest bound \delta for which AI2 proves the region L_{N,\overline{x},\delta} robust. To approximate the largest robustness bound, we ran binary search to depth 6 and ran AI2 with the Zonotope domain for each candidate bound \delta We refer to the largest robustness bound verified by AI2 as the verified bound.

The average verified bounds for the Original, GSS, Ensemble, and MMSTV networks are 0.026, 0.031, 0.042, and 0.209, respectively. Fig. 13 shows a box-and-whisker plot which demonstrates the distribution of the verified bounds for the four networks. The bottom and top of each whisker show the minimum and maximum verified bounds discovered during the 20 experiments. The bottom and top of each whisker's box show the first and third quartiles of the verified bounds.

Our results indicate that MMSTV provides a significant increase in provable robustness against the FGSM attack. In all 20 experiments, the verified bound for the MMSTV network was larger than those found for the Original, GSS, and Ensemble networks. We observe that GSS and Ensemble defend the network in a way that makes it only slightly more provably robust, consistent with observations that these styles of defense are insufficient [16], [27].

SECTION VIII.

Related Work
In this section, we survey the works closely related to ours.

Adversarial Examples
[40] showed that neural networks are vulnerable to small perturbations on inputs. Since then, many works have focused on constructing adversarial examples.

For example, [30] showed how to find adversarial examples without starting from a test point, [41] generated adversarial examples using random perturbations, [35] demonstrated that even intermediate layers are not robust, and [14] generated adversarial examples for malware classification. Other works presented ways to construct adversarial examples during the training phase, thereby increasing the network robustness (see [3], [12], [15], [17], [29], [38]). [1] formalized the notion of robustness in neural networks and defined metrics to evaluate the robustness of a neural network. [32] illustrated how to systematically generate adversarial examples that cover all neurons in the network.

Neural Network Analysis
Many works have studied the robustness of networks. [34] presented an abstraction-refinement approach for FNNs. However, this was shown successful for a network with only 6 neurons. [37] introduced a bounded model checking technique to verify safety of a neural network for the Cart Pole system. [18] showed a verification framework, based on an SMT solver, which verified the robustness with respect to a certain set of functions that can manipulate the input and are minimal (a notion which they define). However, it is unclear how one can obtain such a set. [21] extended the simplex algorithm to verify properties of FNNs with ReLU.

Robustness Analysis of Programs
Many works deal with robustness analysis of programs (e.g., [4], [5], [13], [28]). [28] considered a definition of robustness that is similar to the one in our work, and [5] used a combination of abstract interpretation and SMT-based methods to prove robustness of programs. The programs considered in this literature tend to be small but have complex constructs such as loops and array operations. In contrast, neural networks (which are our focus) are closer to circuits, in that they lack high-level language features but are potentially massive in size.

SECTION IX.

Conclusion and Future Work
We presented AI2, the first system able to certify convolutional and large fully connected networks. The key insight behind AI2 is to phrase the problem of analyzing neural networks in the classic framework of abstract interpretation. To this end, we defined abstract transformers that capture the behavior of common neural network layers and presented a bounded powerset domain that enables a trade-off between precision and scalability. Our experimental results showed that AI2 can effectively handle neural networks that are beyond the reach of existing methods.

We believe AI2 and the approach behind it is a promising step towards ensuring the safety and robustness of AI systems. Currently, we are extending AI2 with additional abstract transformers to support more neural network features. We are also building a library for modeling common perturbations, such as rotations, smoothing, and erosion. We believe these extensions would further improve AI2, s applicability and foster future research in AI safety.