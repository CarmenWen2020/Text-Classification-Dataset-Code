Adversarial examples that fool machine learning models particularly deep neural networks have been a topic of intense research interest with attacks and defenses being developed in a tight back and forth. Most past defenses are best effort and have been shown to be vulnerable to sophisticated attacks. Recently a set of certified defenses have been introduced which provide guarantees of robustness to norm bounded attacks. However these defenses either do not scale to large datasets or are limited in the types of models they can support. This paper presents the first certified defense that both scales to large networks and datasets such as Google s Inception network for ImageNet and applies broadly to arbitrary model types. Our defense called PixelDP is based on a novel connection between robustness against adversarial examples and differential privacy a cryptographically inspired privacy formalism that provides a rigorous generic and flexible foundation for defense. I.IntroductionDeep neural networks DNNs perform exceptionally well on many machine learning tasks including safety and security sensitive applications such as self driving cars malware classification face recognition and critical infrastructure . Robustness against malicious behavior is important in many of these applications yet in recent years it has become clear that DNNs are vulnerable to a broad range of attacks. Among these attacks – broadly surveyed in are adversarial examples the adversary finds small perturbations to correctly classified inputs that cause a DNN to produce an erroneous prediction possibly of the adversary s choosing . Adversarial examples pose serious threats to security critical applications. A classic example is an adversary attaching a small human imperceptible sticker onto a stop sign that causes a self driving car to recognize it as a yield sign. Adversarial examples have also been demonstrated in domains such as reinforcement learning and generative models .Since the initial demonstration of adversarial examples numerous attacks and defenses have been proposed each building on one another. Initially most defenses used best effort approaches and were broken soon after introduction. Model distillation proposed as a robust defense in was subsequently broken in . Other work claimed that adversarial examples are unlikely to fool machine learning ML models in the real world due to the rotation and scaling introduced by even the slightest camera movements. However demonstrated a new attack strategy that is robust to rotation and scaling. While this back and forth has advanced the state of the art recently the community has started to recognize that rigorous theory backed defensive approaches are required to put us off this arms race.Accordingly a new set of certified defenses have emerged over the past year that provide rigorous guarantees of robustness against norm bounded attacks . These works alter the learning methods to both optimize for robustness against attack at training time and permit provable robustness checks at inference time. At present these methods tend to be tied to internal network details such as the type of activation functions and the network architecture. They struggle to generalize across different types of DNNs and have only been evaluated on small networks and datasets.We propose a new and orthogonal approach to certified robustness against adversarial examples that is broadly applicable generic and scalable. We observe for the first time a connection between differential privacy DP a cryptography inspired formalism and a definition of robustness against norm bounded adversarial examples in ML. We leverage this connection to develop PixelDP the first certified defense we are aware of that both scales to large networks and datasets such as Google s Inception network trained on ImageNet and can be adapted broadly to arbitrary DNN architectures. Our approach can even be incorporated with no structural changes in the target network e.g. through a separate auto encoder as described in Section B . We provide a brief overview of our approach below along with the section references that detail the corresponding parts.§II establishes the DP robustness connection formally our first contribution . To give the intuition DP is a framework for randomizing computations running on databases such that a small change in the database removing or altering one row or a small set of rows is guaranteed to result in a bounded change in the distribution over the algorithm s outputs. Separately robustness against adversarial examples can be defined as ensuring that small changes in the input of an ML predictor such as changing a few pixels in an image in the case of an l norm attack will not result in drastic changes to its predictions such as changing its label from a stop to a yield sign . Thus if we think of a DNN s inputs e.g. images as databases in DP parlance and individual features e.g. pixels as rows in DP we observe that randomizing the outputs of a DNN s prediction function to enforce DP on a small number of pixels in an image guarantees robustness of predictions against adversarial examples that can change up to that number of pixels. The connection can be expanded to standard attack norms including l l and l {\infty} norms.§ describes PixelDP the first certified defense against norm bounded adversarial examples based on differential privacy our second contribution . Incorporating DP into the learning procedure to increase robustness to adversarial examples requires is completely different and orthogonal to using DP to preserve the privacy of the training set the focus of prior DP ML literature as § VI explains . A PixelDP DNN includes in its architecture a DP noise layer that randomizes the network s computation to enforce DP bounds on how much the distribution over its predictions can change with small norm bounded changes in the input. At inference time we leverage these DP bounds to implement a certified robustness check for individual predictions. Passing the check for a given input guarantees that no perturbation exists up to a particular size that causes the network to change its prediction. The robustness certificate can be used to either act exclusively on robust predictions or to lower bound the network s accuracy under attack on a test set.§IV presents the first experimental evaluation of a certified adversarial examples defense for the Inception network trained on the ImageNet dataset our third contribution . We additionally evaluate PixelDP on various network architectures for four other datasets CFAR CFAR SVHN MNIST on which previous defenses – both best effort and certified – are usually evaluated. Our results indicate that PixelDP is as effective at defending against attacks as today s state of the art best effort defense and more scalable and broadly applicable than a prior certified defense.Our experience points to DP as a uniquely generic broadly applicable and flexible foundation for certified defense against norm bounded adversarial examples § V § VI . We credit these properties to the post processing property of DP which lets us incorporate the certified defense in a network agnostic way. SECTION II.DP Robustness ConnectionA. Adversarial ML BackgroundAn ML model can be viewed as a function mapping inputs typically a vector of numerical feature values – to an output a label for multiclass classification and a real number for regression . Focusing on multiclass classification we define a model as a function f \mathbb{R}^{n} \rightarrow \mathcal{K} that maps ndimensional inputs to a label in the set \mathcal{K} \{ \ldots \ \ K\} of all possible labels. Such models typically map an input x to a vector of scores y {jF}^{ }x y {} x \ \ldots \ y {K} x such that yk x \in and \sum\nolimits {k }y {k} x . These scores are interpreted as a probability distribution over the labels and the model returns the label with highest probability i.e. f x \arg\max {k\in \mathcal{K}}y {k} x . We denote the function that maps input x to y as Q and call it the scoring function we denote the function that gives the ultimate prediction for input x as f and call it the prediction procedure.Adversarial Examples. Adversarial examples are a class of attack against ML models studied particularly on deep neural networks for multiclass image classification. The attacker constructs a small change to a given fixed input that wildly changes the predicted output. Notationally if the input is x we denote an adversarial version of that input by x α where α is the change or perturbation introduced by the attacker. When x is a vector of pixels for images then xi is the i th pixel in the image and \alpha {i} is the change to the i th pixel.It is natural to constrain the amount of change an attacker is allowed to make to the input and often this is measured by the p norm of the change denoted by \Vert\alpha\Vert {p}. For \leq p\lt \infty the p norm of \alpha is defined by \Vert\alpha\Vert {p} \left \sum {i }^{n} \alpha {i} ^{p}\right ^{/p} for p \infty it is \Vert\alpha\Vert {\infty} \max {i} \alpha {i} . Also commonly used is the norm which is technically not a norm \Vert\alpha\Vert {} \{i \alpha {i} \neq A small norm attack is permitted to arbitrarily change a few entries of the input for example an attack on the image recognition system for self driving cars based on putting a sticker in the field of vision is such an attack . Small p norm attacks for larger values of p including p \infty require the changes to the pixels to be small in an aggregate sense but the changes may be spread out over many or all features. A change in the lighting condition of an image may correspond to such an attack . The latter attacks are generally considered more powerful as they can easily remain invisible to human observers. Other attacks that are not amenable to norm bounding exist but this paper deals exclusively with norm bounded attacks.Let B {p} r \{\alpha\ \in\ \mathbb{R}^{n}\ \ \Vert\alpha\Vert {p}\ \leq\ r\} be the p norm ball of radius r. For a given classification model f and a fixed input x\in \mathbb{R}^{n} an attacker is able to craft a successful adversarial example of size L for a given p norm if they find \alpha \in B {p} L such that f x \alpha \neq f x . The attacker thus tries to find a small change to x that will change the predicted label.Robustness Definition. Intuitively a predictive model may be regarded as robust to adversarial examples if its output is insensitive to small changes to any plausible input that may be encountered in deployment. To formalize this notion we must first establish what qualifies as a plausible input. This is difficult the adversarial examples literature has not settled on such a definition. Instead model robustness is typically assessed on inputs from a test set that are not used in model training – similar to how accuracy is assessed on a test set and not a property on all plausible inputs. We adopt this view of robustness.Next given an input we must establish a definition for insensitivity to small changes to the input. We say a model f is insensitive or robust to attacks of p norm L on a given input x if f x f x \alpha for all \alpha \in B {p} L . If f is a multiclass classification model based on label scores as in § II A this is equivalent to \begin{equation }\forall\alpha \in B {p} L . y {k} x\ \ \alpha {\gt } ii \neq k\max { }y {i} x\ \ \alpha \tag{}\end{equation }View Source\begin{equation }\forall\alpha \in B {p} L . y {k} x\ \ \alpha {\gt } ii \neq k\max { }y {i} x\ \ \alpha \tag{}\end{equation } where k f x . A small change in the input does not alter the scores so much as to change the predicted label.B. DP BackgroundDP is concerned with whether the output of a computation over a database can reveal information about individual records in the database. To prevent such information leakage randomness is introduced into the computation to hide details of individual records.A randomized algorithm A that takes as input a database d and outputs a value in a space O is said to satisfy \epsilon \ \delta DP with respect to a metric \rho over databases if for any databases d and d^{\prime} with \rho\left d d^{\prime}\right \leq and for any subset of possible outputs S\subseteq O we have \begin{equation }P A d \in S \leq e^{\epsilon} P\left A\left d^{\prime}\right \in S\right \delta \tag{}\end{equation }View Source\begin{equation }P A d \in S \leq e^{\epsilon} P\left A\left d^{\prime}\right \in S\right \delta \tag{}\end{equation }Here \epsilon and \delta \in are parameters that quantify the strength of the privacy guarantee. In the standard DP definition the metric \rho is the Hamming metric which simply counts the number of entries that differ in the two databases. For small \epsilon and \delta the standard \epsilon \ \delta DP guarantee implies that changing a single entry in the database cannot change the output distribution very much. DP also applies to general metrics \rho including p norms relevant to norm based adversarial examples.Our approach relies on two key properties of DP. First is the well known post processing property any computation applied to the output of an \epsilon \ \delta \mathrm{D}\mathrm{P} algorithm remains \epsilon \ \delta \mathrm{D}\mathrm{P}. Second is the expected output stability property a rather obvious but not previously enunciated property that we prove in Lemma the expected value of an \epsilon \ \delta DP algorithm with bounded output is not sensitive to small changes in the input.Lemma . Expected Output Stability Bound Suppose a randomized function A with bounded output A x \in b b \in \mathbb{R}^{ } satisfies \epsilon \ \delta DP Then the expected value of its output meets the following property \begin{equation } \forall \alpha \in B {p} \cdot \mathbb{E} A x \leq e^{\epsilon} \mathbb{E} A x \alpha b \delta. \end{equation }View Source\begin{equation } \forall \alpha \in B {p} \cdot \mathbb{E} A x \leq e^{\epsilon} \mathbb{E} A x \alpha b \delta. \end{equation }The expectation is taken over the randomness in A. Proof.Consider any \alpha \in B {p} and let x^{\prime} x \alpha. We write the expected output as \begin{equation }\mathrm{E} A x \int {}^{b}P A x \ {\gt }t dt.\end{equation }View Source\begin{equation }\mathrm{E} A x \int {}^{b}P A x \ {\gt }t dt.\end{equation }We next apply Equation from the \epsilon \ \delta \mathrm{D}\mathrm{P} property \begin{equation }\mathrm{E} A x \ \leq\ e^{\epsilon} \int {}^{b}P A x^{\prime} {\gt }t dt \ \int {}^{b}\delta dt e^{\epsilon}\mathrm{E} A x^{\prime} \int {}^{b}\delta dt.\end{equation }View Source\begin{equation }\mathrm{E} A x \ \leq\ e^{\epsilon} \int {}^{b}P A x^{\prime} {\gt }t dt \ \int {}^{b}\delta dt e^{\epsilon}\mathrm{E} A x^{\prime} \int {}^{b}\delta dt.\end{equation }Since \delta is a constant \int {}^{b}\delta dt b\delta.C. DP Robustness ConnectionThe intuition behind using DP to provide robustness to adversarial examples is to create a DP scoring function such that given an input example the predictions are DP with regards to the features of the input e.g. the pixels of an image . In this setting we can derive stability bounds for the expected output of the DP function using Lemma . The bounds combined with Equation give a rigorous condition or certification for robustness to adversarial examples.Formally regard the feature values e.g. pixels of an input x as the records in a database and consider a randomized scoring function A that on input x outputs scores y {} x \ \ldots \ y {K} x with y {k} x \in and \sum\nolimits {k }y {k} x . We say that A is an \epsilon \ \delta pixel level differentially private or \epsilon \ \delta PixelDP function if it satisfies \epsilon \ \delta \mathrm{D}\mathrm{P} for a given metric . This is formally equivalent to the standard definition of DP but we use this terminology to emphasize the context in which we apply the definition which is fundamentally different than the context in which DP is traditionally applied in ML see § VI for distinction .Lemma directly implies bounds on the expected outcome on an \epsilon \ \delta PixelDP scoring function Corollary .Suppose a randomized function A satisfies \epsilon \ \delta PixelDP with respect to a p norm metric and where A x y {} x \ \ldots \ y {K} x y {k} x \in \begin{equation }\forall k \forall\alpha\in B {p} .\mathrm{E} y {k} x \leq e^{\epsilon}\mathrm{E} y {k} x \alpha \delta. \tag{}\end{equation }View Source\begin{equation }\forall k \forall\alpha\in B {p} .\mathrm{E} y {k} x \leq e^{\epsilon}\mathrm{E} y {k} x \alpha \delta. \tag{}\end{equation } Proof.For any k apply Lemma with b .Our approach is to transform a model s scoring function into a randomized \epsilon \ \delta PixelDP scoring function A x and then have the model s prediction procedure f use A s expected output over the DP noise \mathrm{E} A x as the label probability vector from which to pick the argmax. I.e. f x \arg\max {k\in \mathcal{K}}\mathrm{E} A {k} x . We prove that a model constructed this way allows the following robustness certification to adversarial examples Proposition . Robustness Condition Suppose A satisfies \epsilon \ \delta PixelDP with respect to a p norm metric. For any input x if for some k\in \mathcal{K} \begin{equation } \mathbb{E}\left A {k} x \right \gt e^{ \epsilon} \mathop{\max} \limits {i i \neq k} \mathbb{E}\left A {i} x \right \left e^{\epsilon}\right \delta \tag{}\end{equation }View Source\begin{equation } \mathbb{E}\left A {k} x \right \gt e^{ \epsilon} \mathop{\max} \limits {i i \neq k} \mathbb{E}\left A {i} x \right \left e^{\epsilon}\right \delta \tag{}\end{equation }then the multiclass classification model based on label probability vector y x \mathrm{E} A {} x \ \ldots \ \mathrm{E} A {K} x is robust to attacks \alpha of size \Vert\alpha\Vert {p}\leq on input x. Proof.Consider any \alpha \in B {p} and let x^{\prime} x \alpha. From Equation we have \begin{equation }\mathrm{E} A {k} x \leq e^{\epsilon}\mathrm{E} A {k} x^{\prime} \delta \tag{a}\end{equation }View Source\begin{equation }\mathrm{E} A {k} x \leq e^{\epsilon}\mathrm{E} A {k} x^{\prime} \delta \tag{a}\end{equation } \begin{equation }\mathrm{E} A {i} x^{\prime} \leq e^{\epsilon}\mathrm{E} A {i} x \delta i\neq k. \tag{b}\end{equation }View Source\begin{equation }\mathrm{E} A {i} x^{\prime} \leq e^{\epsilon}\mathrm{E} A {i} x \delta i\neq k. \tag{b}\end{equation }Equation a gives a lower bound on \mathrm{E} A {k} x^{\prime} Equation b gives an upper bound on \max {i\neq k}\mathrm{E} A {i} x^{\prime} . The hypothesis in the proposition statement Equation implies that the lower bound of the expected score for label k is strictly higher than the upper bound for the expected score for any other label which in turn implies the condition from Equation for robustness at x. To spell it out \begin{align } \mathbb{E}\left A {k}\left x^{\prime}\right \right \geq \frac{\mathbb{E}\left A {k} x \right \delta}{e^{\epsilon}} \\ \stackrel{\text { Eg }}{ \gt } \frac{e^{ \epsilon} \max {i i \neq k} \mathbb{E}\left A {i} x \right \left e^{\epsilon}\right \delta \delta}{e^{\epsilon}} \\ e^{\epsilon} \max {i i \neq k} \mathbb{E}\left A {i} x \right \delta \\ { \geq \max {i i \neq k} \mathbb{E}\left A {i}\left x^{\prime}\right \right } \\ \Longrightarrow \mathbb{E}\left A {k}\left x^{\prime}\right \right \gt \max {i i \neq k} \mathbb{E}\left A {i} x \alpha \right \forall \alpha \in B {p} \end{align }View Source\begin{align } \mathbb{E}\left A {k}\left x^{\prime}\right \right \geq \frac{\mathbb{E}\left A {k} x \right \delta}{e^{\epsilon}} \\ \stackrel{\text { Eg }}{ \gt } \frac{e^{ \epsilon} \max {i i \neq k} \mathbb{E}\left A {i} x \right \left e^{\epsilon}\right \delta \delta}{e^{\epsilon}} \\ e^{\epsilon} \max {i i \neq k} \mathbb{E}\left A {i} x \right \delta \\ { \geq \max {i i \neq k} \mathbb{E}\left A {i}\left x^{\prime}\right \right } \\ \Longrightarrow \mathbb{E}\left A {k}\left x^{\prime}\right \right \gt \max {i i \neq k} \mathbb{E}\left A {i} x \alpha \right \forall \alpha \in B {p} \end{align } the very definition of robustness at x Equation . Fig. Architecture. a In blue the original DNN. In red the noise layer that provides the \epsilon \ \delta \mathrm{D}\mathrm{P} guarantees. The noise can be added to the inputs or any of the following layers but the distribution is rescaled by the sensitivity \Delta {p q} of the computation performed by each layer before the noise layer. The DNN is trained with the original loss and optimizer e.g. Momentum stochastic gradient descent . Predictions repeatedly call the \epsilon \ \delta DP DNN to measure its empirical expectation over the scores. b After adding the bounds for the measurement error between the empirical and true expectation green and the stability bounds from Lemma for a given attack size Lattack red the prediction is certified robust to this attack size if the lower bound of the arg max label does not overlap with the upper bound of any other labels.Show AllThe preceding certification test is exact regardless of the value of the \delta parameter of differential privacy there is no failure probability in this test. The test applies only to attacks of p norm size of however all preceding results generalize to attacks of p norm size L i.e. when \Vert\alpha\Vert {p} \leq L by applying group privacy . The next section shows how to apply group privacy § B and generalize the certification test to make it practical § D . SECTION .PixelDP Certified DefenseA. ArchitecturePixelDP is a certified defense against p norm bounded adversarial example attacks built on the preceding DP robustness connection. Fig. a shows an example PixelDP DNN architecture for multi class image classification. The original architecture is shown in blue the changes introduced to make it PixelDP are shown in red. Denote Q the original DNN s scoring function it is a deterministic map from images x to a probability distribution over the K labels Q x y {} x \ \ldots \ y {K} x . The vulnerability to adversarial examples stems from the unbounded sensitivity of Q with respect to p norm changes in the input. Making the DNN \epsilon \ \delta PixelDP involves adding calibrated noise to turn Q into an \epsilon \ \delta \mathrm{D}\mathrm{P} randomized function AQ the expected output of that function will have bounded sensitivity to pnorm changes in the input. We achieve this by introducing a noise layer shown in red in Fig. a that adds zero mean noise to the output of the layer preceding it layerl in Fig. a . The noise is drawn from a Laplace or Gaussian distribution and its standard deviation is proportional to L the p norm attack bound for which we are constructing the network and \Delta the sensitivity of the pre noise computation the grey box in Fig. a with respect to p norm input changes.Training an \epsilon \ \delta PixelDP network is similar to training the original network we use the original loss and optimizer such as stochastic gradient descent. The major difference is that we alter the pre noise computation to constrain its sensitivity with regards to p norm input changes. Denote Q x h g x where g is the pre noise computation and h is the subsequent computation that produces Q x in the original network. We leverage known techniques reviewed in § C to transform g into another function \tilde{g} that has a fixed sensitivity \Delta to p norm input changes. We then add the noise layer to the output of \tilde{g} with a standard deviation scaled by \Delta and L to ensure \epsilon \ \delta PixelDP for p norm changes of size L. Denote the resulting scoring function of the PixelDP network A {Q} x h \tilde{g} x noise \Delta L \epsilon \delta where noise is the function implementing the Laplace/ Gaussian draw. Assuming that the noise layer is placed such that h only processes the DP output of \tilde{g} x without accessing x again \mathrm{i}.\mathrm{e}. no skip layers exist from pre noise to post noise computation the post processing property of DP ensures that A {Q} x also satisfies \epsilon \ \delta PixelDP for p norm changes of size L.Prediction on the \epsilon \ \delta PixelDP scoring function A {Q} x affords the robustness certification in Proposition if the prediction procedure uses the expected scores \mathrm{E} A {Q} x to select the winning label for any input x. Unfortunately due to the potentially complex nature of the post noise computation h we cannot compute this output expectation analytically. We therefore resort to Monte Carlo methods to estimate it at prediction time and develop an approximate version of the robustness certification in Proposition that uses standard techniques from probability theory to account for the estimation error § D . Specifically given input x PixelDP s prediction procedure invokes A {Q} x multiple times with new draws of the noise layer. It then averages the results for each label thereby computing an estimation \hat{\mathrm{E}} A {Q} x of the expected score \mathrm{E} A {Q} x . It then computes an \eta confidence interval for \hat{\mathrm{E}} A {Q} x that holds with probability \eta. Finally it integrates this confidence interval into the stability bound for the expectation of a DP computation Lemma to obtain \eta confidence upper and lower bounds on the change an adversary can make to the average score of any label with a p norm input change of size up to L. Fig. b illustrates the upper and lower bounds applied to the average score of each label by the PixelDP prediction procedure. If the lower bound for the label with the top average score is strictly greater than the upper bound for every other label then with probability \eta the PixelDP network s prediction for input x is robust to arbitrary attacks of p norm size L. The failure probability of this robustness certification \eta can be made arbitrarily small by increasing the number of invocations of A {Q} x .One can use PixelDP s certification check in two ways one can decide only to actuate on predictions that are deemed robust to attacks of a particular size or one can compute on a test set a lower bound of a PixelDP network s accuracy under p norm bounded attack independent of how the attack is implemented. This bound called certified accuracy will hold no matter how effective future generations of the attack are.The remainder of this section details the noise layer training and certified prediction procedures. To simplify notation we will henceforth use A instead of AQ.B. DP Noise LayerThe noise layer enforces \epsilon \ \delta PixelDP by inserting noise inside the DNN using one of two well known DP mechanisms the Laplacian and Gaussian mechanisms. Both rely upon the sensitivity of the pre noise layers function g . The sensitivity of a function g is defined as the maximum change in output that can be produced by a change in the input given some distance metrics for the input and output pnorm and q norm respectively \begin{equation }\Delta {p q} \Delta {p q}^{g} \max {x x x\neq x^{\prime}}\frac{\Vert g x g x^{\prime} \Vert {q}}{ x x^{\prime}\Vert {p}}.\end{equation }View Source\begin{equation }\Delta {p q} \Delta {p q}^{g} \max {x x x\neq x^{\prime}}\frac{\Vert g x g x^{\prime} \Vert {q}}{ x x^{\prime}\Vert {p}}.\end{equation }Assuming we can compute the sensitivity of the prenoise layers addressed shortly the noise layer leverages the Laplace and Gaussian mechanisms as follows. On every invocation of the network on an input x whether for training or prediction the noise layer computes g x Z where the coordinates Z Z {} \ \ldots \ Z {m} are independent random variables from a noise distribution defined by the function noise \Delta \ L \ \epsilon \ \delta . Laplacian mechanism noise \Delta \ L \ \epsilon \ \delta uses the Laplace distribution with mean zero and standard deviation \sigma \sqrt{}\Delta {p }L/\epsilon it gives \epsilon \ \mathrm{D}\mathrm{P}.Gaussian mechanism noise \Delta \ L \ \epsilon \ \delta uses the Gaussian distribution with mean zero and standard deviation \sigma \sqrt{\ln\left \frac{}{\delta}\right }\Delta {p }L/\epsilon it gives \epsilon \ \delta \mathrm{D}\mathrm{P} for \epsilon\leq .Here L denotes the p norm size of the attack against which the PixelDP network provides \epsilon \ \delta \mathrm{D}\mathrm{P} we call it the construction attack bound. The noise formulas show that for a fixed noise standard deviation \sigma the guarantee degrades gracefully attacks twice as big halve the \epsilon in the DP guarantee L\leftarrow L\Rightarrow\epsilon\leftarrow \epsilon . This property is often referred as group privacy in the DP literature .Computing the sensitivity of the pre noise function g depends on where we choose to place the noise layer in the DNN. Because the post processing property of DP carries the \epsilon \ \delta PixelDP guarantee from the noise layer through the end of the network a DNN designer has great flexibility in placing the noise layer anywhere in the DNN as long as no skip connection exists from pre noise to post noise layers. We discuss here several options for noise layer placement and how to compute sensitivity for each. Our methods are not closely tied to particular network architectures and can therefore be applied on a wide variety of networks.Option Noise in the Image. The most straightforward placement of the noise layer is right after the input layer which is equivalent to adding noise to individual pixels of the image. This case makes sensitivity analysis trivial g is the identity function \Delta { } and \Delta { } .Option Noise after First Layer. Another option is to place the noise after the first hidden layer which is usually simple and standard for many DNNs. For example in image classification networks often start with a convolution layer. In other cases DNNs start with fully connected layer. These linear initial layers can be analyzed and their sensitivity computed as follows.For linear layers which consist of a linear operator with matrix form W\in \mathbb{R}^{m n} the sensitivity is the matrix norm defined as \Vert W\Vert {p q} \sup {x \Vert x\Vert {p}\leq }\Vert Wx\Vert {q}. Indeed the definition and linearity of W directly imply that \frac{\Vert Wx\Vert {q}}{\Vert x\Vert {p}} \leq\Vert W\Vert {p q} which means that \Delta {p q} \Vert W\Vert {p q}. We use the following matrix norms \Vert W\Vert { } is the maximum norm of W s columns \Vert W\Vert { } is the maximum norm of W s columns and \Vert W\Vert { } is the maximum singular value of W. For \infty norm attacks we need to bound \Vert W\Vert {\infty } or \Vert W\Vert {\infty } as our DP mechanisms require q\in \{ \}. However tight bounds are computationally hard so we currently use the following bounds \sqrt{n}\Vert W\Vert { } or \sqrt{m}\Vert W\Vert {\infty \infty} where \Vert W\Vert {\infty \infty} is the maximum norm of W s rows. While these bounds are suboptimal and lead to results that are not as good as for norm or norm attacks they allow us to include \infty norm attacks in our frameworks. We leave the study of better approximate bounds to future work.For a convolution layer which is linear but usually not expressed in matrix form we reshape the input e.g. the image as an \mathbb{R}^{nd {in}} vector where n is the input size e.g. number of pixels and din the number of input channels e.g. for the RGB channels of an image . We write the convolution as an \mathbb{R}^{nd {out}\times nd {in}} matrix where each column has all filter maps corresponding to a given input channel and zero values. This way a “column” of a convolution consists of all coefficients in the kernel that correspond to a single input channel. Reshaping the input does not change sensitivity.Option Noise Deeper in the Network. One can consider adding noise later in the network using the fact that when applying two functions in a row f {} f {} x we have \Delta {p q}^{ f\circ f } \leq\Delta {p r}^{ f }\Delta {r q}^{ f }. For instance ReLU has a sensitivity of for p q \in \{ \ \ \infty\} hence a linear layer followed by a ReLU has the same bound on the sensitivity as the linear layer alone. However we find that this approach for sensitivity analysis is difficult to generalize. Combining bounds in this way leads to looser and looser approximations. Moreover layers such as batch normalization which are popular in image classification networks do not appear amenable to such bounds indeed they are assumed away by some previous defenses . Thus our general recommendation is to add the DP noise layer early in the network – where bounding the sensitivity is easy – and taking advantage of DP s post processing property to carry the sensitivity bound through the end of the network.Option Noise in Auto encoder. Pushing this reasoning further we uncover an interesting placement possibility that underscores the broad applicability and flexibility of our approach adding noise “before” the DNN in a separately trained auto encoder. An auto encoder is a special form of DNN trained to predict its own input essentially learning the identity function f x x. Auto encoders are typically used to de noise inputs and are thus a good fit for PixelDP. Given an image dataset we can train \mathrm{a} \epsilon \ \delta PixelDP autoencoder using the previous noise layer options. We stack it before the predictive DNN doing the classification and finetune the predictive DNN by running a few training steps on the combined auto encoder and DNN. Thanks to the decidedly useful post processing property of DP the stacked DNN and auto encoder are \epsilon \ \delta PixelDP.This approach has two advantages. First the auto encoder can be developed independently of the DNN separating the concerns of learning a good PixelDP model and a good predictive DNN. Second PixelDP auto encoders are much smaller than predictive DNNs and are thus much faster to train. We leverage this property to train the first certified model for the large ImageNet dataset using an auto encoder and the pre trained Inception v model a substantial relief in terms of experimental work § IV A .C. Training ProcedureThe soundness of PixelDP s certifications rely only on enforcing DP at prediction time. Theoretically one could remove the noise layer during training. However doing so results in near zero certified accuracy in our experience. Unfortunately training with noise anywhere except in the image itself raises a new challenge left unchecked the training procedure will scale up the sensitivity of the prenoise layers voiding the DP guarantees.To avoid this we alter the pre noise computation to keep its sensitivity constant e.g. \Delta {p q} \leq during training. The specific technique we use depends on the type of sensitivity we need to bound i.e. on the values of p and q. For \Delta { } \Delta { } or \Delta {\infty \infty} we normalize the columns or rows of linear layers and use the regular optimization process with fixed noise variance. For \Delta { } we run the projection step described in after each gradient step from the stochastic gradient descent SGD . This makes the pre noise layers Parseval tight frames enforcing \Delta { } . For the pre noise layers we thus alternate between an SGD step with fixed noise variance and a projection step. Subsequent layers from the original DNN are left unchanged.It is important to note that during training we optimize for a single draw of noise to predict the true label for a training example x. We estimate E A x using multiple draws of noise only at prediction time. We can interpret this as pushing the DNN to increase the margin between the expected score for the true label versus others. Recall from Equation that the bounds on predicted outputs give robustness only when the true label has a large enough margin compared to other labels. By pushing the DNN to give high scores to the true label k at points around x likely under the noise distribution we increase \mathrm{E} A {k} x and decrease \mathrm{E} A {i\neq k} x .D. Certified Prediction ProcedureFor a given input x the prediction procedure in a traditional DNN chooses the argmax label based on the score vector obtained from a single execution of the DNN s deterministic scoring function Q x . In a PixelDP network the prediction procedure differs in two ways. First it chooses the argmax label based on a Monte Carlo estimation of the expected value of the randomized DNN s scoring function \hat{\mathrm{E}} A x . This estimation is obtained by invoking A x multiple times with independent draws in the noise layer. Denote a {k n} x the nth draw from the distribution of the randomized function A on the kth label given x so a {k n \wedge} x \sim A {k} x . In Lemma we replace \mathrm{E} A {k} x with \mathrm{E} A {k} x \frac{}{n}\sum {n}a {k n} x where n is the number of invocations of A x . We compute \eta confidence error bounds to account for the estimation error in our robustness bounds treating each label s score as a random variable in . We use Hoeffding s inequality or Empirical Bernstein bounds to bound the error in \hat{\mathrm{E}} A x . We then apply a union bound so that the bounds for each label are all valid together. For instance using Hoeffding s inequality with probability \eta \hat{\mathrm{E}}^{lb} A x \Delta\hat{\mathrm{E}} A x \sqrt{\frac{}{n}ln\left \frac{k}{ \eta}\right }\leq \mathrm{E} A x \leq\hat{\mathrm{E}} A x \sqrt{\frac{}{n}ln\left \frac{k}{ \eta}\right } \Delta\hat{\mathrm{E}}^{ub} A x .Second PixelDP returns not only the prediction for x \arg\max \hat{\mathrm{E}} A x but also a robustness size certificate for that prediction. To compute the certificate we extend Proposition to account for the measurement error Proposition . Generalized Robustness Condition Suppose A satisfies \epsilon \ \delta PixelDP with respect to changes of size L in p norm metric. Using the notation from Proposition further let \hat{\mathrm{E}}^{ub} A {i} x and \hat{\mathrm{E}}^{lb} A {i} x be the \eta confidence upper and lower bound respectively for the Monte Carlo estimate \hat{\mathrm{E}} A {i} x . For any input x iffor some k\in \mathcal{K} \begin{equation }\hat{\mathbb{E}}^{l b}\left A {k} x \right \gt e^{ \epsilon} \max {i i \neq k} \hat{\mathbb{E}}^{u b}\left A {i} x \right \left e^{\epsilon}\right \delta \end{equation }View Source\begin{equation }\hat{\mathbb{E}}^{l b}\left A {k} x \right \gt e^{ \epsilon} \max {i i \neq k} \hat{\mathbb{E}}^{u b}\left A {i} x \right \left e^{\epsilon}\right \delta \end{equation }then the multiclass classification model based on label probabilities \hat{\mathrm{E}} A {} x \ \ldots \hat{\mathrm{E}} A {K} x is robust to attacks of p norm L on input x with probability \geq\eta.The proof is similar to the one for Proposition and is detailed in Appendix A. Note that the DP bounds are not probabilistic even for \delta \gt the failure probability \eta comes from the Monte Carlo estimate and can be made arbitrarily small with more invocations of A x .Thus far we have described PixelDP certificates as binary with respect to a fixed attack bound L we either meet or do not meet a robustness check for L. In fact our formalism allows for a more nuanced certificate which gives the maximum attack size L {\max} measured in p norm against which the prediction on input x is guaranteed to be robust no attack within this size from x will be able to change the highest probability. L {\max} can differ for different inputs. We compute the robustness size certificate for input x as follows. Recall from B that the DP mechanisms have a noise standard deviation \sigma that grows in \frac{\Delta {P}{} {q}L}{\epsilon}. For a given \sigma used at prediction time we solve for the maximum L for which the robustness condition in Proposition checks out L {\max} \max {L \in \mathbb{R}} L such that\hat{\mathbb{E}}^{l b}\left A {k} x \right \gt e^{ \epsilon} \mathbb{E}^{u b}\left A {i i \neq k} x \right \left e^{\epsilon}\right \delta AND either} \sigma \Delta {p } L / \epsilon \text {and} \delta for Laplace OR\sigma \sqrt{ \ln . / \delta } \Delta {p } L / \epsilon \text {and} \epsilon \leq for Gaussian The prediction on x is robust to attacks up to L {\max} so we award a robustness size certificate of L {\max} for x.We envision two ways of using robustness size certifications. First when it makes sense to only take actions on the subset of robust predictions e.g. a human can intervene for the rest an application can use PixelDP s certified robustness on each prediction. Second when all points must be classified PixelDP gives a lower bound on the accuracy under attack. Like in regular ML the testing set is used as a proxy for the accuracy on new examples. We can certify the minimum accuracy under attacks up to a threshold size T that we call the prediction robustness threshold. T is an inference time parameter that can differ from the construction attack bound parameter L that is used to configure the standard deviation of the DP noise. In this setting the certification is computed only on the testing set and is not required for each prediction. We only need the highest probability label which requires fewer noise draws. § IV E shows that in practice a few hundred draws are sufficient to retain a large fraction of the certified predictions while a few dozen are needed for simple predictions. SECTION IV.EvaluationWe evaluate PixelDP by answering four key questions Q How does DP noise affect model accuracy Q What accuracy can PixelDP certify Q What is PixelDP s accuracy under attack and how does it compare to that of other best effort and certified defenses Q What is PixelDP s computational overhead We answer these questions by evaluating PixelDP on five standard image classification datasets and networks – both large and small – and comparing it with one prior certified defense and one best effort defense . § IV A describes the datasets prior defenses and our evaluation methodology subsequent sections address each question in turn.Evaluation highlights PixelDP provides meaningful certified robustness bounds for reasonable degradation in model accuracy on all datasets and DNNs. To the best of our knowledge these include the first certified bounds for large complex datasets/networks such as the Inception network on ImageNet and Residual Networks on CFAR . There PixelDP gives % certified accuracy for norm attacks up to . at the cost of . and . percentage point accuracy degradation respectively. Comparing PixelDP to the prior certified defense on smaller datasets PixelDP models give higher accuracy on clean examples e.g. .% vs. .% accuracy SVHN dataset and higher robustness to norm attacks e.g. % vs. % accuracy on SVHN for norm attacks of . thanks to the ability to scale to larger models. Comparing PixelDP to the best effort defense on larger models and datasets PixelDP matches its accuracy e.g. % for PixelDP vs. .% on CFAR and robustness to norm bounded attacks.A. MethodologyDatasets. We evaluate PixelDP on image classification tasks from five pubic datasets listed in Table I. The datasets are listed in descending order of size and complexity for classification tasks. MNIST consists of greyscale handwritten digits and is the easiest to classify. SVHN contains small real world digit images cropped from Google Street View photos of house numbers. CFAR and CFAR consist of small color images that are each centered on one object of one of or classes respectively. ImageNet is a large production scale image dataset with over million images spread across classes.Models Baselines and PixelDP. We use existing DNN architectures to train a high performing baseline model for each dataset. Table I shows the accuracy of the baseline models. We then make each of these networks PixelDP with regards to norm and norm bounded attacks. We also did rudimentary evaluation of \infty norm bounded attacks shown in Appendix D. While the PixelDP formalism can support \infty norm attacks our results show that tighter bounds are needed to achieve a practical defense. We leave the development and evaluation of these bounds for future work.Table II shows the PixelDP configurations we used for the norm and norm defenses. The code is available at https //github.com/columbia/pixeldp. Since most of this section focuses on models with norm attack bounds we detail only those configurations here.ImageNet We use as baseline a pre trained version of Inception v available in Tensorflow . To make it PixelDP we use the autoencoder approach from § B which does not require a full retraining of Inception and was instrumental in our support of ImageNet. The encoder has three convolutional layers and tied encoder/decoder weights. The convolution kernels are \times \times \times \times and \times \times with stride . We make the autoencoder PixelDP by adding the DP noise after the first convolution. We then stack the baseline Inception v on the PixelDP autoencoder and fine tune it for k steps keeping the autoencoder weights constant.CIFAR CIFAR SVHN We use the same baseline architecture a state of the art Residual Network ResNet . Specifically we use the Tensorflow implementation of a wide ResNet with the default parameters. To make it PixelDP we slightly alter the architecture to remove the image standardization step. This step makes sensitivity input dependent which is harder to deal with in PixelDP. Interestingly removing this step also increases the baseline s own accuracy for all three datasets. In this section we therefore report the accuracy of the changed networks as baselines.MNIST We train a Convolutional Neural Network CNN with two \times convolutions stride and filters followed by a nodes fully connected layer.Evaluation Metrics. We use two accuracy metrics to evaluate PixelDP models conventional accuracy and certified accuracy. Conventional accuracy or simply accuracy denotes the fraction of a testing set on which a model is correct it is the standard accuracy metric used to evaluate any DNN defended or not. Certified accuracy denotes the fraction of the testing set on which a certified model s predictions are both correct and certified robust for a given prediction robustness threshold it has become a standard metric to evaluate models trained with certified defenses . We also use precision on certified examples which measures the number of correct predictions exclusively on examples that are certified robust for a given prediction robustness threshold. Formally the metrics are defined as follows Conventional accuracy \frac{\Sigma {i }^{n} isCorrect x {i} }{n} where n is the testing set size and isCorrect x {i} denotes a function returning if the prediction on test sample xi returns the correct label and otherwise.Certified accuracy \frac{\Sigma {i }^{n} isCorrect x {i} \ robustSize scores \epsilon \delta L \geq\tau }{n} where robustSize scores \epsilon \delta L returns the certified robustness size which is then compared to the prediction robustness threshold T.Precision on certified examples \frac{\Sigma {i }^{n} isCorrect x {i} \ robustSize p {i} \epsilon \delta L \geq T }{\Sigma {i }^{n}robustSize p {i} \epsilon \delta L \geq T }.For T all predictions are robust so certified accuracy is equivalent to conventional accuracy. Each time we report L or T we use a pixel range.Attack Methodology. Certified accuracy – as provided by PixelDP and other certified defense – constitutes a guaranteed lower bound on accuracy under any norm bounded attack. However the accuracy obtained in practice when faced with a specific attack can be much better. How much better depends on the attack which we evaluate in two steps. We first perform an attack on randomly picked samples as is customary in defense evaluation from the testing set. We then measure conventional accuracy on the attacked test examples.For our evaluation we use the state of the art attack from Carlini and Wagner that we run for iterations of binary search gradient steps without early stopping which we empirically validated to be sufficient and learning rate .. We also adapt the attack to our specific defense following since PixelDP adds noise to the DNN attacks based on optimization may fail due to the high variance of gradients which would not be a sign of the absence of adversarial examples but merely of them being harder to find. We address this concern by averaging the gradients over noise draws at each gradient step. Appendix § C contains more details about the attack including sanity checks and another attack we ran similar to the one used in . Table I Evaluation Datasets and Baseline Models. Last Column Shows the Accuracy of the Baseline Undefended Models. the Datasets Are Sorted Based On Descending Order of Scale Or Complexity. Table II Noise Layers In PixelDP DNNs. For Each DNN We Implement Defenses For Different Attack Bound Norms and DP Mechanisms Table Impact of Pixeldp Noise On Conventional Accuracy. For Each DNN We Show Different Levels of Construction Attack Size L. Conventional Accuracy Degrades with Noise Level.Prior Defenses for Comparison. We use two state of art defenses as comparisons. First we use the empirical defense model provided by the Madry Lab for CFAR . This model is developed in the context of \infty norm attacks. It uses an adversarial training strategy to approximately minimize the worst case error under malicious samples . While inspired by robust optmization theory this methodology is best effort see § VI and supports no formal notion of robustness for individual predictions as we do in PixelDP. However the Madry model performs better under the latest attacks than other best effort defenses it is in fact the only one not yet broken and represents a good comparison point.Second we compare with another approach for certified robustness against \infty norm attacks based on robust optimization. This method does not yet scale to the largest datasets e.g. ImageNet or the more complex DNNs e.g. ResNet Inception both for computational reasons and because not all necessary layers are yet supported e.g. BatchNorm . We thus use their largest released model/dataset namely a CNN with two convolutions and a nodes fully connected layer for the SVHN dataset and compare their robustness guarantees with our own networks robustness guarantees. We call this SVHN CNN model RobustOpt.B. Impact of Noise Q Ql How does DP noise affect the conventional accuracy of our models To answer for each dataset we train up to four . . PixelDP DNN for construction attack bound L \in \{. . . \}. Higher values of L correspond to robustness against larger attacks and larger noise standard deviation \sigma.Table shows the conventional accuracy of these networks and highlights two parts of an answer to Ql. First at fairly low but meaningful construction attack bound e.g. L . all of our DNNs exhibit reasonable accuracy loss – even on ImageNet a dataset on which no guarantees have been made to date ImageNet The Inception v model stacked on the PixelDP auto encoder has an accuracy of .% for L . which is reasonable degradation compared to the baseline of .% for the unprotected network. CFAR Accuracy goes from .% without defense to % with the L . defense. For comparison the Madry model has an accuracy of .% on CFAR . SVHN our L . PixelDP network achieves .% conventional accuracy down from .% for the unprotected network. For comparison the L . RobustOpt network has an accuracy of .% although they use a smaller DNN due to the computationally intensive method.Second as expected constructing the network for larger attacks higher L progressively degrades accuracy. ImageNet Increasing L to . and then . drops the accuracy to .% and .% respectively. CFAR The ResNet with the least noise L\ \ . reaches .% accuracy close to the baseline of .% increasing noise levels L . . . yields % .% and .% respectively. Yet as shown in § IV D PixelDP networks trained with fairly low L values such as L . already provide meaningful empirical protection against larger attacks.C. Certified Accuracy Q Q What accuracy can PixelDP certify on a test set Fig. shows the certified robust accuracy bounds for ImageNet and CFAR models trained with various values of the construction attack bound L. The certified accuracy is shown as a function of the prediction robustness threshold T. We make two observations. First PixelDP yields meaningful robust accuracy bounds even on large networks for ImageNet see Fig. a attesting the scalability of our approach. The L . network has a certified accuracy of % for attacks smaller than . in norm. The L . network has a certified accuracy of % to attacks up to size .. To our knowledge PixelDP is the first defense to yield DNNs with certified bounds on accuracy under norm attacks on datasets of ImageNet s size and for large networks like Inception.Second PixelDP networks constructed for larger attacks higher L hence higher noise tend to yield higher certified accuracy for high thresholds T. For example the ResNet onCFAR see Fig. b constructed with L . has the highest robust accuracy up to T . but the ResNet constructed with L . becomes better past that threshold. Similarly the L . ResNet has higher robust accuracy than the L . ResNet above the . norm prediction robustness threshold. Fig. Certified accuracy varying the construction attack bound L and prediction robustness threshold T on ImageNet auto encoder/Inception and CFAR ResNet norm bounds. Robust accuracy at high Robustness thresholds high T increases with high noise networks high L . Low noise networks are both more accurate and more certifiably robust for low T.Show AllWe ran the same experiments on SVHN CFAR and MNIST models but omit the graphs for space reasons. Our main conclusion – that adding more noise higher L hurts both conventional and low T certified accuracy but enhances the quality of its high T predictions – holds in all cases. Appendix B discusses the impact of some design choices on robust accuracy and Appendix D discusses PixelDP guarantees as compared with previous certified defenses for \infty norm attacks. While PixelDP does not yet yield strong \infty norm bounds it provides meaningful certified accuracy bounds for norm attacks including on much larger and more complex datasets and networks than those supported by previous approaches.D. Accuracy Under Attack Q A standard method to evaluate the strength of a defense is to measure the conventional accuracy of a defended model on malicious samples obtained by running a state of the art attack against samples in a held out testing set . We apply this method to answer three aspects of question Q Can PixelDP help defend complex models on large datasets in practice How does PixelDP s accuracy under attack compare to state of the art defenses How does the accuracy under attack change for certified predictions Accuracy under Attack on ImageNet. We first study conventional accuracy under attack for PixelDP models on ImageNet. Fig. shows this metric for norm attacks on the baseline Inception v model as well as three defended versions with a stacked PixelDP auto encoder trained with construction attack bound L \in \{. . .\}. PixelDP makes the model significantly more robust to attacks. For attacks of size L {attack} . the baseline model s accuracy drops to % whereas the L . PixelDP model s accuracy remains above %. At L {attack} . the baseline model has an accuracy of but the L . PixelDP is still at % while the L . PixelDP model have more that % accuracy. Fig. Accuracy under attack on ImageNet. For the ImageNet auto encoder plus Inception v L \in \{. . .\} norm attacks. The PixelDP auto encoder increases the robustness of Inception against norm attacks.Show AllAccuracy under Attack Compared to Madry. Fig. a compares conventional accuracy of a PixelDP model to that of a Madry model onCFAR as the empirical attack bound increases for norm attacks. For norm attacks our model achieves conventional accuracy on par with or slightly higher than that of the Madry model. Both models are dramatically more robust under this attack compared to the baseline undefended model. For \infty norm attacks our model does not fare as well which is expected as the PixelDP model is trained to defend against norm attacks while the Madry model is optimized for \infty norm attacks. For L {attack} . PixelDP s accuracy is % percentage points lower than Madry s. The gap increases until PixelDP arrives at accuracy for L {attack} . with Madry still having %. Appendix § D details this evaluation.Accuracy under Attack Compared to RobustOpt. Fig. b shows a similar comparison with the RobustOpt defense which provides certified accuracy bounds for \infty norm attacks. We use the SVHN dataset for the comparison as the RobustOpt defense has not yet been applied to larger datasets. Due to our support of larger DNN ResNet PixelDP starts with higher accuracy which it maintains under norm attacks. For attacks of L {attack} . RobustOpt is bellow % accuracy and PixelDP above %. Under \infty norm attacks the behavior is different PixelDP has the advantage up to L {attack} . .% to .% and RobustOpt is better thereafter. For instance at L {attack} . PixelDP has .% accuracy to RobustOpt s .%. Appendix § D details the \infty norm attack evaluation. Fig. Accuracy under norm attack for PixelDP vs. Madry and RobustOpt CIFAR and SVHN. For norm attacks PixelDP is on par with Madry until L {attack} \geq . RobustOpt support only small models and has lower accuracy.Show All Fig. PixelDP certified predictions vs. Madry accuracy under attack CIFAR ResNets norm attack. PixelDP makes fewer but more correct predictions up to L {attack} ..Show AllPrecision on Certified Predictions Under Attack. Another interesting feature of PixelDP is its ability to make certifiably robust predictions. We compute the accuracy of these certified predictions under attack – which we term robust precision – and compare them to predictions of the Madry network that do not provide such a certification. Fig. shows the results of considering only predictions with a certified robustness above . and .. It reflects the benefit to be gained by applications that can leverage our theoretical guarantees to filter out non robust predictions. We observe that PixelDP s robust predictions are substantially more correct than Madry s predictions up to an empirical attack bound of .. For T . PixelDP s robust predictions are .% accurate and up to percentage points more correct under attack for L {attack} \leq .. A robust prediction is given for above % of the data points. The more conservative the robustness test is higher T the more correct PixelDP s predictions are although it makes fewer of them Certified Fraction lines .Thus for applications that can afford to not act on a minority of the predictions PixelDP s robust predictions under norm attack are substantially more precise than Madry s. For applications that need to act on every prediction PixelDP offers on par accuracy under norm attack to Madry s. Interestingly although our defense is trained for norm attacks the first conclusion still holds for \infty norm attacks the second as we saw does not.E. Computational Overhead Q Q What is PixelDP s computational overhead We evaluate overheads for training and prediction. PixelDP adds little overhead for training as the only additions are a random noise tensor and sensitivity computations. On our GPU the CFAR ResNet baseline takes on average . s per training step. PixelDP versions take at most . s per training step .% overhead . This represents a significant benefit over adversarial training e.g. Madry that requires finding good adversarial attacks for each image in the mini batch at each gradient step and over robust optimization e.g. RobustOpt that requires solving a constrained optimization problem at each gradient step. The low training overhead is instrumental to our support of large models and datasets.PixelDP impacts prediction more substantially since it uses multiple noise draws to estimate the label scores. Making a prediction for a single image with noise draw takes . s on average. Making draws brings it only to . s but requires . s and . s. It is possible to use Hoeffding s inequality to bound the number of draws necessary to distinguish the highest score with probability at least \eta given the difference between the top two scores y {\max} y {second \max}. Empirically we found that draws were typically necessary to properly certify a prediction implying a prediction time of . s seconds a \times overhead. This is parallelizable but resource consumption is still substantial. To make simple predictions distinguish the top label when we must make a prediction on all inputs draws are enough in practice reducing the overhead to \times. SECTION V.AnalysisWe make three points about PixelDP s guarantees and applicability. First we emphasize that our Monte Carlo approximation of the function x\rightarrow \mathrm{E} A x is not intended to be a DP procedure. Hence there is no need to apply composition rules from DP because we do not need this randomized procedure to be DP. Rather the Monte Carlo approximation x\mapsto\hat{\mathrm{E}} A x is just that an approximation to a function x \mapsto \mathrm{E} A x whose robustness guarantees come from Lemma . The function x\mapsto\hat{\mathrm{E}} A x does not satisfy DP but because we can control the Monte Carlo estimation error using standard tools from probability theory it is also robust to small changes in the input just like x\rightarrow \mathrm{E} A x .Second Proposition is not a high probability result it is valid with probability even when A is \epsilon \ \delta\ \gt \ \mathrm{D}\mathrm{P}. The \delta parameter can be thought of as a “failure probability” of an \epsilon \ \delta DP mechanism a chance that a small change in input will cause a big change in the probability of some of its outputs. However since we know that A {k} x \in the worst case impact of such failures on the expectation of the output of the \epsilon \ \delta \mathrm{D}\mathrm{P} mechanism is at most \delta as proven in Lemma . Proposition explicitly accounts for this worst case impact term e^{\epsilon} \delta in Equation .Were we able to compute \mathrm{E} A x analytically PixelDP would output deterministic robustness certificates. In practice however the exact value is too complex to compute and hence we approximate it using a Monte Carlo method. This adds probabilistic measurement error bounds making the final certification Proposition a high probability result. However the uncertainty comes exclusively from the Monte Carlo integration – and can be made arbitrarily small with more runs of the PixelDP DNN – and not from the underlying \epsilon \ \delta \mathrm{D}\mathrm{P} mechanism A. Making the uncertainty small gives an adversary a small chance to fool a PixelDP network into thinking that its prediction is robust when it is not. The only ways an attacker can increase that chance is by either submitting the same attack payload many times or gaining control over PixelDP s source of randomness.Third PixelDP applies to any task for which we can measure changes to input in a meaningful p norm and bound the sensitivity to such changes at a given layer in the DNN e.g. sensitivity to a bounded change in a word frequency vector or a change of class for categorical attributes . PixelDP also applies to multiclass classification where the prediction procedure returns several top scoring labels. Finally Lemma can be extended to apply to DP mechanism with bounded output that can also be negative as shown in Appendix E. PixelDP thus directly applies to DNNs for regression tasks i.e. predicting a real value instead of a category as long as the output is bounded or unbounded if \delta . The output can be bounded due to the specific task or by truncating the results to a large range of values and using a comparatively small \delta. SECTION VI.Related WorkOur work relates to a significant body of work in adversarial examples and beyond. Our main contribution to this space is to introduce a new and very different direction for building certified defenses. Previous attempts have built on robust optimization theory. In PixelDP we propose a new approach built on differential privacy theory which exhibits a level of flexibility broad applicability and scalability that exceeds what robust optimization based certified defenses have demonstrated. While the most promising way to defend against adversarial examples is still an open question we observe undebatable benefits unique to our DP based approach such as the post processing guarantee of our defense. In particular the ability to prepend a defense to unmodified networks via a PixelDP auto encoder as we did to defend Inception with no structural changes is unique among certified and best effort defenses.Best effort Defenses. Defenders have used multiple heuristics to empirically increase DNNs robustness. These defenses include model distillation automated detection of adversarial examples application of various input transformations randomization and generative models . Most of these defenses have been broken sometimes months after their publication .The main empirical defense that still holds is Madry et al. based on adversarial training . Madry et al. motivate their approach with robust optimization a rigorous theory. However not all the assumptions are met as this approach runs a best effort attack on each image in the minibatch at each gradient step when the theory requires finding the best possible adversarial attack. And indeed finding this worst case adversarial example for ReLU DNNs used in was proven to be NP hard in . Therefore while this defense works well in practice it gives no theoretical guarantees for individual predictions or for the model s accuracy under attack. PixelDP leverages DP theory to provide guarantees of robustness to arbitrary norm based attacks for individual predictions.Randomization based defenses are closest in method to our work . For example Liu et al. randomizes the entire DNN and predicts using an ensemble of multiple copies of the DNN essentially using draws to roughly estimate the expected argmax prediction. They observe empirically that randomization smoothens the prediction function improving robustness to adversarial examples. However randomization based prior work provides limited formalism that is insufficient to answer important defense design questions where to add noise in what quantities and what formal guarantees can be obtained from randomization The lack of formalism has caused some works to add insufficient amounts of noise e.g. noise not calibrated to pre noise sensitivity which makes them vulnerable to attack . On the contrary inserts randomness into every layer of the DNN our work shows that adding the right amount of calibrated noise at a single layer is sufficient to leverage DP s post processing guarantee and carry the bounds through the end of the network. Our paper formalizes randomization based defenses using DP theory and in doing so helps answer many of these design questions. Our formalism also lets us reason about the guarantees obtained through randomization and enables us to elevate randomization based approaches from the class of best effort defenses to that of certified defenses.Certified Defenses and Robustness Evaluations. PixelDP offers two functions a strategy for learning robust models and a method for evaluating the robustness of these models against adversarial examples. Both of these approaches have been explored in the literature. First several certified defenses modify the neural network training process to minimize the number of robustness violations . These approaches though promising do not yet scale to larger networks like Google Inception . In fact all published certified defenses have been evaluated on small models and datasets and at least in one case the authors directly acknowledge that some components of their defense would be “completely infeasible” on ImageNet . A recent paper presents a certified defense evaluated on the CFAR dataset for multi layer DNNs but smaller than ResNets . Their approach is completely different from ours and based on the current results we see no evidence that it can readily scale to large datasets like ImageNet.Another approach combines robust optimization and adversarial training in a way that gives formal guarantees and has lower computational complexity than previous robust optimization work hence it has the potential to scale better. This approach requires smooth DNNs e.g. no ReLU or max pooling and robustness guarantees are over the expected loss e.g. log loss whereas PixelDP can certify each specific prediction and also provides intuitive metrics like robust accuracy which is not supported by . Finally unlike PixelDP which we evaluated on five datasets of increasing size and complexity this technique was evaluated only on MNIST a small dataset that is notoriously amenable to robust optimization due to being almost black and white . Since the effectiveness of all defenses depends on the model and dataset it is hard to conclude anything about how well it will work on more complex datasets.Second several works seek to formally verify or lower bound the robustness of pre trained ML models against adversarial attacks. Some of these works scale to large networks but they are insufficient from a defense perspective as they provide no scalable way to train robust models.Differentially Private ML. Significant work focuses on making ML algorithms DP to preserve the privacy of training sets . PixelDP is orthogonal to these works differing in goals semantic and algorithms. The only thing we share with DP ML and most other applied DP literature are DP theory and mechanisms. The goal of DP ML is to learn the parameters of a model while ensuring DP with respect to the training data. Public release of model parameters trained using a DP learning algorithm such as DP empirical risk minimization or ERM is guaranteed to not reveal much information about individual training examples. PixelDP s goal is to create a robust predictive model where a small change to any input example does not drastically change the model s prediction on that example. We achieve this by ensuring that the model s scoring function is a DP function with respect to the features of an input example eg pixels . DP ML algorithms e.g. DP ERM do not necessarily produce models that satisfy PixelDP s semantic and our training algorithm for producing PixelDP models does not ensure DP of training data.Previous DP Robustness Connections. Previous work studies generalization properties of DP . It is shown that learning algorithms that satisfy DP with respect to the training data have statistical benefits in terms of out of sample performance or that DP has a deep connection to robustness at the dataset level . Our work is rather different. Our learning algorithm is not DP rather the predictor we learn satisfies DP with respect to the atomic units e.g. pixels of a given test point. SECTION VII.ConclusionWe demonstrated a connection between robustness against adversarial examples and differential privacy theory. We showed how the connection can be leveraged to develop a certified defense against such attacks that is as effective at defending against norm attacks as today s state of theart best effort defense and more scalable and broadly applicable to large networks compared to any prior certified defense. Finally we presented the first evaluation of a certified norm defense on the large scale ImageNet dataset. In addition to offering encouraging results the evaluation highlighted the substantial flexibility of our approach by leveraging a convenient autoencoder based architecture to make the experiments possible with limited resources. ACKNOWLEDGEMENTWe thank our shepherd Abhi Shelat and the anonymous reviewers whose comments helped us improve the paper significantly. This work was funded through NSF CNS CNS and CCF ONR N two Sloan Fellowships a Google Faculty Fellowship and a Microsoft Faculty Fellowship. Appendix SECTION A.Proof of Proposition We briefly re state the Proposition and detail the proof.Proposition.Suppose A is \epsilon \ \delta PixelDP for size L in p norm metric. For any input x if for some k\in \mathcal{K} \begin{equation }\hat{\mathbb{E}}^{l b}\left A {k} x \right \gt e^{ \epsilon} \max {i i \neq k} \hat{\mathbb{E}}^{u b}\left A {i} x \right \left e^{\epsilon}\right \delta \end{equation }View Source\begin{equation }\hat{\mathbb{E}}^{l b}\left A {k} x \right \gt e^{ \epsilon} \max {i i \neq k} \hat{\mathbb{E}}^{u b}\left A {i} x \right \left e^{\epsilon}\right \delta \end{equation }then the multiclass classification model based on label probabilities \hat{\mathrm{E}} A {k} x is robust to attacks of p norm L on input x with probability higher than \eta. Proof.Consider any \alpha\in B {p} L and let x^{\prime} x \alpha. From Equation we have with p\gt \eta that \begin{align }\hat{\mathbb{E}}\left A {k}\left x^{\prime}\right \right \geq\left \hat{\mathbb{E}}\left A {k} x \right \delta\right / e^{\epsilon} \\ \geq\left \hat{\mathbb{E}}^{l b}\left A {k} x \right \delta\right / e^{\epsilon} \\ \mathbb{E}\left A {i i \neq k}\left x^{\prime}\right \right \leq e^{\epsilon} \max {i i \neq k} \hat{\mathbb{E}}^{u b}\left A {i} x \right \delta \quad i \neq k \end{align }View Source\begin{align }\hat{\mathbb{E}}\left A {k}\left x^{\prime}\right \right \geq\left \hat{\mathbb{E}}\left A {k} x \right \delta\right / e^{\epsilon} \\ \geq\left \hat{\mathbb{E}}^{l b}\left A {k} x \right \delta\right / e^{\epsilon} \\ \mathbb{E}\left A {i i \neq k}\left x^{\prime}\right \right \leq e^{\epsilon} \max {i i \neq k} \hat{\mathbb{E}}^{u b}\left A {i} x \right \delta \quad i \neq k \end{align }Starting from the first inequality and using the hypothesis followed by the second inequality we get \begin{align } \hat{\mathbb{E}}^{l b}\left A {k} x \right {\gt }e^{ \epsilon} \max {i i \neq k} \hat{\mathbb{E}}^{u b}\left A {i} x \right \left e^{\epsilon}\right \delta \Rightarrow \\ \hat{\mathbb{E}}\left A {k}\left x^{\prime}\right \right \geq\left \hat{\mathbb{E}}^{l b}\left A {k} x \right \delta\right / e^{\epsilon} \\ {\gt }e^{\epsilon} \max {i i \neq k} \hat{\mathbb{E}}^{u b}\left A {i} x \right \delta \\ {\gt }\hat{\mathbb{E}}\left A {i i \neq k}\left x^{\prime}\right \right \end{align }View Source\begin{align } \hat{\mathbb{E}}^{l b}\left A {k} x \right {\gt }e^{ \epsilon} \max {i i \neq k} \hat{\mathbb{E}}^{u b}\left A {i} x \right \left e^{\epsilon}\right \delta \Rightarrow \\ \hat{\mathbb{E}}\left A {k}\left x^{\prime}\right \right \geq\left \hat{\mathbb{E}}^{l b}\left A {k} x \right \delta\right / e^{\epsilon} \\ {\gt }e^{\epsilon} \max {i i \neq k} \hat{\mathbb{E}}^{u b}\left A {i} x \right \delta \\ {\gt }\hat{\mathbb{E}}\left A {i i \neq k}\left x^{\prime}\right \right \end{align } which is the robustness condition from Equation . SECTION B.Design ChoiceOur theoretical results allow the DP DNN to output any bounded score over labels A {k} x . In the evaluation we used the softmax output of the DNN the typical “probabilities” that DNNs traditionally output. We also experimented with using argmax scores transforming the probabilities in a zero vector with a single for the highest score label. As each dimension of this vector is in our theory applies as is. We observed that argmax scores were a bit less robust empirically lower accuracy under attack . However as shown on Fig. argmax scores yield a higher certified accuracy. This is both because we can use tighter bounds for measurement error using a Clopper Pearson interval and because the arg max pushes the expected scores further apart thus satisfying Proposition more often. Fig. Robust Accuracy argmax Scores. Using argmax scores for certification yields better accuracy bounds see Fig. b both because the scores are further appart and because the measurement error bounds are tighter.Show All Fig. Laplace vs. Gaussian. Certified accuracy for a ResNet on the CIFAR dataset against norm bounded attacks. The Laplace mechanism yields better accuracy for low noise levels but the Gaussian mechanism is better for high noise ResNets.Show AllWe also study the impact of the DP mechanism used on certified accuracy for norm attacks. Both the Laplace and Gaussian mechanisms can be used after the first convolution by respectively controlling the \Delta { } or \Delta { } sensitivity. Fig. shows that for our ResNet the Laplace mechanism is better suited to low levels of noise for L . it yields a slightly higher accuracy .% against .% as well as better certified accuracy with a maximum robustness size of norm . instead of . and a robust accuracy of % against .% at the . threshold. On the other hand when adding more noise e.g. L . the Gaussian mechanism performs better consistently yielding a robust accuracy . percentage point higher. SECTION C.Attack DetailsAll evaluation results § IV are based on the attack from Carlini and Wagner specialized to better attack PixelDP see parameters and adaptation in § IV A . We also implemented variants of the iterative Projected Gradient Descent PGD attack described in modified to average gradients over noise draws per step and performing each attack times with a small random initialization. We implemented two version of this PGD attack. norm Attack The gradients are normalized before applying the step size to ensure progress even when gradients are close to flat. We perform k gradient steps and select a step size of \frac{L}{k}. This heuristic ensures that all feasible points within the norm ball can be reached after k steps. After each step if the attack is larger than L we project it on the norm ball by normalizing it. Under this attack results were qualitatively identical for all experiments. The raw accuracy numbers were a few percentage points higher i.e. the attack was slightly less efficient so we kept the results for the Carlini and Wagner attack.\infty norm Attack We perform \max L .L gradient steps and maintain a constant step of size of . which corresponds to the minimum pixel increment in a discrete pixel range . At the end of each gradient step we clip the size of the perturbation to enforce a perturbation within the \infty norm ball of the given attack size. We used this attack to compare PixelDP with models from Madry and RobustOpt see results in Appendix D .Finally we performed sanity checks suggested in . The authors observe that several heuristic defenses do not ensure the absence of adversarial examples but merely make them harder to find by obfuscating gradients. This phenomenon also referred to as gradient masking makes the defense susceptible to new attacks crafted to circumvent that obfuscation . Although PixelDP provides certified accuracy bounds that are guaranteed to hold regardless of the attack used we followed guidelines from to to rule out obfuscated gradients in our empirical results. We verified three properties that can be symptomatic of problematic attack behavior. First when growing T the accuracy drops to on all models and datasets. Second our attack significantly outperforms random sampling. Third our iterative attack is more powerful than the respective single step attack. SECTION D.∞ Norm AttacksAs far as \infty norm attacks are concerned we acknowledge that the size of the attacks against which our current PixelDP defense can certify accuracy is substantially lower than that of previous certified defenses. Although previous defenses have been demonstrated on MNIST and SVHN only and for smaller DNNs they achieve \infty norm defenses of T {\infty} . with robust accuracy .% and % on MNIST. On SVHN uses T {\infty} . achieving .% of certified accuracy. Using the crude bounds we have between p norms makes a comparison difficult in both directions. Mapping \infty norm bounds in norm gives T {} \geq T {\infty} also yielding very small bounds. On the other hand translating norm guarantees into \infty norm ones using that \Vert x\Vert {} \leq \sqrt{n}\Vert x\Vert {\infty} with n the size of the image would require a norm defense of size T {} . to match the T {\infty} . bound from MNIST an order of magnitude higher than what we can achieve. As comparison points our L . CNN has a robust accuracy of .% at T . and % at T .. We make the same observation on SVHN where we would need a bound at T {} . to match the T {\infty} . bound but our ResNet with L . reaches a similar robust accuracy as RobustOpt for T {} .. This calls for the design \infty norm specific PixelDP mechanisms that could also scale to larger DNNs and datasets. Fig. Accuracy under \infty norm attacks for PixelDP and Madry. The Madry model explicitly trained against \infty norm attacks outperforms PixelDP. The difference increases with the size of the attack.Show All Fig. Accuracy under \infty norm attacks for PixelDP and RobustOpt. PixelDP is better up to L {\infty} . due to its support of larger ResNet models. For attacks of \infty norm above this value RobustOpt is more robust.Show AllOn Figures and we show PixelDP s accuracy under \infty norm attacks compared to the Madry and RobustOpt models both trained specifically against this type of attacks. On CFAR the Madry model outperforms PixelDP for L {attack} . PixelDP s accuracy is % percentage points lower than Madry s. The gap increases until PixelDP arrives at accuracy for L {attack} . with Madry still having %.On SVHN against the RobustOpt model trained with robust optimization against \infty norm attacks PixelDP is better up to L {\infty} . due to its support of larger ResNet models. For attacks of \infty norm above this value RobustOpt is more robust. SECTION E.Extension To RegressionA previous version of this paper contained an incorrect claim in the statement of Lemma for outputs that can be negative. Because the paper focused on classification where DNN scores are in the error had no impact on the claims or experimental results for classification. Lemma below provides a correct version of Lemma for outputs that can be negative showing how PixelDP can be extended to support regression problems Lemma . General Expected Output Stability Bound Suppose a randomized function A with bounded output A x \in a \ b a b\in \mathbb{R} with a\leq \leq b satisfies \epsilon \ \delta DP Let A { } x \max \ A x and A\ x \min \ A x so that A x A { } x A\ x . Then the expected value of its output meets the following property for all \alpha\in B {p} \begin{equation }\mathrm{E} A x \alpha \leq e^{\epsilon}\mathrm{E} A { } x e^{ \epsilon}\mathrm{E} A\ x b\delta e^{ \epsilon}a\delta \mathrm{E} A x \alpha \geq e^{ \epsilon}\mathrm{E} A { } x e^{\epsilon}\mathrm{E} A\ x e^{ \epsilon}b\delta a\delta.\end{equation }View Source\begin{equation }\mathrm{E} A x \alpha \leq e^{\epsilon}\mathrm{E} A { } x e^{ \epsilon}\mathrm{E} A\ x b\delta e^{ \epsilon}a\delta \mathrm{E} A x \alpha \geq e^{ \epsilon}\mathrm{E} A { } x e^{\epsilon}\mathrm{E} A\ x e^{ \epsilon}b\delta a\delta.\end{equation } The expectation is taken over the randomness in A. Proof.Consider any \alpha \in B {p} and let x^{\prime} x \alpha. Observe that \mathrm{E} A { } x \int {}^{b}P A x \gt t dt so by the \epsilon \ \delta \mathrm{D}\mathrm{P} property of A via Equation we have \mathrm{E} A { } x \leq e^{\epsilon}\mathrm{E} A { } x b\delta and \mathrm{E} A { } x^{\prime} \geq e^{ \epsilon}\mathrm{E} A { } x e^{ \epsilon}b\delta. Similarly \mathrm{E} A\ x^{\prime} \leq e^{\epsilon}\mathrm{E} A\ x a\delta and \mathrm{E} A\ x^{\prime} \geq e^{ \epsilon}\mathrm{E} A { } x e^{ \epsilon}a\delta. Putting these four inequalities together concludes the proof.Following Lemma supporting regression problems involves three steps. First if the output is unbounded one must use \epsilon \ \mathrm{D}\mathrm{P} e.g. with the Laplace mechanism . If the output is bounded one may use \epsilon \ \delta \mathrm{D}\mathrm{P}. The output may be bounded either naturally because the specific task has inherent output bounds or by truncating the results to a large range of values and using a comparatively small \delta.Second instead of estimating the expected value of the randomized prediction function we estimate both A { } x and A\ x . We can use Hoeffding s inequality or Empirical Bernstein bounds to bound the error.Third following Lemma we bound A { } x and A\ x separately using the DP Expected Output Stability Bound to obtain a bound on \mathrm{E} A x \mathrm{E} A { } x \mathrm{E} A\ x .