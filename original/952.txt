We present Poligraph, an intrusion-tolerant and decentralized fake news detection system. Poligraph aims to address architectural, system, technical, and social challenges of building a practical, long-term fake news detection platform. We first conduct a case study for fake news detection at authors’ institute, showing that machine learning-based reviews are less accurate but timely, while human reviews, in particular, experts reviews, are more accurate but time-consuming. This justifies the need for combining both approaches. At the core of Poligraph is two-layer consensus allowing seamlessly combining machine learning techniques and human expert determination. We construct the two-layer consensus using Byzantine fault-tolerant (BFT) and asynchronous threshold common coin protocols. We prove the correctness of our system in terms of conventional definitions of security in distributed systems (agreement, total order, and liveness) as well as new review validity (capturing the accuracy of news reviews). We also provide theoretical foundations on parameter selection for our system. We implement Poligraph and evaluate its performance on Amazon EC2 using a variety of news from online publications and social media. We demonstrate Poligraph achieves throughput of more than 5,000 transactions per second and latency as low as 0.05 second. The throughput of Poligraph is only marginally ( 4% – 7% ) slower than that of an unreplicated, single-server implementation. In addition, we conduct a real-world case study for the review of fake and real news among both experts and non-experts, which validates the practicality of our approach.

SECTION I.Introduction
The use of words and semantics to share information in news media and political discourse has considerable power in influencing people’s beliefs and opinions [1]. Fake news is defined as intentionally fabricated information that mimics news media content in form but not in organizational process or intent, lacking the news media’s editorial norms and processes for ensuring the accuracy and credibility of information [2]. The extensive dissemination of fake news has the potential to significantly impact both the individual and society. It is misleading and potentially harmful to readers when news is disconnected from its original source and context [3]. Unfortunately, a person’s ability to distinguish truth from deception has just 54% accuracy on average, or slightly better than random guessing [4]. The majority of U.S. adults (62%) get their news from social media [5], which is the main source of fake news. Currently, fake news about COVID-19 spreads fast and becomes dangerous on social media platforms. Being able to detect fake news has become more important than ever before.

Existing studies in fake news detection can be classified into four categories: experimental studies, research on fake news modeling, machine learning (ML), and deep learning (DL) based detection. In experimental studies, researchers recruit participants to verify the authenticity of the news and assess whether their opinions or beliefs will be influenced by fake news [6]–[7][8]. Fake news modeling focuses on constructing fake news detection models based on the genesis, evolution, and propagation of a fake news article, by cataloging and tracing its characteristics over time [9], [10]. By extending and utilizing various news-related features, researchers have built and trained different machine learning models to predict the probability that a news article is fake, through a variety of classical data science techniques, i.e., classification, and regression [11], [12]. Besides conventional ML approaches, deep learning-based approaches have recently also been studied for fake news detection [13]–[14][15][16][17][18][19][20][21][22][23]. ML based models leverage algorithms such as logistic regression (LR), support vector machine (SVM), and XGBoost to classify news based on news textual and/or visual features while DL based models leverage neural networks like CNN, LSTM, and adversarial network to identify fake news based on news textual and/or visual information. The majority of the works, however, are ML-based, partly because ML approaches usually achieve lower latency (i.e., and therefore higher throughput from system perspective). Furthermore, several companies are also looking for systematic and publicly available solutions for fake news detection. For instance, Facebook is developing its own cryptocurrency solutions with the potential to eliminate fake news and bots [24].

Despite an impressive amount of work on fake news detection techniques as described above, and given that some non-dedicated services that are run by a single provider do exist (e.g., Amazon Mechanical Turk [25]), to the best of our knowledge, there is no public and intrusion-tolerant system for fake news detection ensuring reliability, security, and accuracy.

Besides the basic requirements of being decentralized and defending against arbitrary (Byzantine) failures and malicious attacks, we identify the challenges and requirements of building an intrusion-tolerant fake news detection system.

The Need for ML and Human Review Integration: In our work, we focus on ML based approaches. Compared to DL approaches, ML approaches have lower latency and thus incur a higher throughput in the system we build. Furthermore, most existing approaches for fake news detection are ML based ones. The fake news modeling and ML techniques have limitations that are both general to data science and specific to this domain. As is the case with any ML technique, the results depend on the quality of the training and holdout data, and the overall size of the data set being fed into the ML algorithm. Even for those ML fake news algorithms which have been built upon a robust data set, there remains a challenge: any ML result must be reviewed by a human, preferably a specialist in the field, to validate the results. For instance, social media such as Facebook has a long history of using ML to detect fake news and spam. The ML approaches, however, are shown to have high inaccuracy that can only be addressed by human moderators [30].

Reproducibility: It is desirable to guarantee that news prediction is deterministic and can be verified by anyone over time. Many existing ML systems use a proprietary data set and therefore the prediction is not reproducible. Even if the data set is open-source, one would have to trust the algorithms and the system running the algorithms. Therefore, building a reliable and secure system for an ML system can enhance the reproducibility of the ML model and the data set.

On-Line vs. Off-Line Fake News Detection: Providing real time, on-line prediction is needed for many applications, but the results may be less accurate. An off-line system, however, may provide results that are less responsive but more accurate. While it is desirable to provide efficient trade-offs, no such mechanism exists.

Practical State Transfer for ML: While we know how to maintain and transfer state in replicated systems, it is not clear how to deal with state transfer for ML efficiently. The ML state is complex, and a lightweight solution is needed.

Toward an Evolving, Long-Term Platform: The accuracy of a fake news detection system may be improved as labeled data grows. In addition, the ML needs to be re-trained periodically for better accuracy. This poses additional requirements and challenges of building such a system.

Our Approach and Our Contribution: We have conducted a news review of both real and fake university news among a group of students and news staff at UMBC. We show that machine learning based reviews are less accurate but timely; we also show that with certain expertise in a certain field, collective reviews among a group of human are able to achieve a high enough accuracy to justify the authenticity of the news. The case study bolsters the intuitive viewpoint that one should combine both machine learning techniques and expert review determination to achieve the best of both worlds.

We design and implement Poligraph, a Byzantine fault-tolerant (BFT) distributed system for fake news detection satisfying all system requirements mentioned above. Poligraph ensures integrity and availability, defending against malicious (Byzantine) news feed providers, servers, and human reviewers. Poligraph has a novel system architecture based on a new consensus primitive, two-layer consensus, which can combine the news authenticity results from ML models and human reviewers. We show that two-layer consensus can be efficiently built from BFT and asynchronous threshold PRF protocols [31]. We prove the correctness of our protocol and show how to tune parameters to allow parallel reviews securely and efficiently.

A comparison with existing approaches is shown in Table I. In particular, as the data set maintained by our system grows, its anthropological, forensic, analytic, and data science value increases. In other words, the motivation of our work is not to study the fake news problem itself or enhance the ML approach, but to provide a secure and distributed approach, which can be used as a platform to maintain a growing list of data set. The data set over time will enhance the ML performance.

TABLE I Comparison of Fake News Detection Systems. ○ Not Supported ● Supported

We evaluate the performance of Poligraph on Amazon EC2 using both political and entertainment news from news websites (e.g., Washington Post) and social media platforms (e.g., Twitter) with news labels from PolitiFact.com and GossipCop.com. Via extensive evaluation of Poligraph, we show it achieves throughput of more than 5,000 transactions per second and has latency as low as 0.05 second. The throughput of Poligraph is only marginally (4%–7%) slower than that of an unreplicated, single-server implementation.

SECTION II.Related Work
A. Fake News Detection
In the past two decades, fake news detection has been investigated extensively [6], [9], [11], [17]–[18][19][20][21][22][23], [32]–[33][34]. We classify the past studies into four categories: experimental studies, fake news modeling, ML, and DL based detection. (1) Experimental studies utilize behavioral experiments to reveal the cognitive aspects of people towards fake news detection. For example, Pennycook et al. adopted Amazon’s Mechanical Turk (AMT) [25] to conduct their experiments on whether prior fake news exposure will affect people’s perceived accuracy of fake news. In their research, they confirmed that prior exposure to fake news can increase subsequent perceptions. Specifically, people are more likely to believe fake news articles if they saw those articles before elsewhere, and were even more likely to believe if they saw the article on multiple occasions [6]. (2) Fake news modeling approaches focus on building fake news detection models from news characteristics. For instance, Jin et al. constructed a hierarchical propagation model, consisting of event, sub-event, and message networks, to verify the news credibility on two data sets from Sina Weibo, a popular micro-blogging site. After linking the semantic and social associations of a news event, the model achieves higher accuracy and F1-score [35] than baseline methods [34]. (3) Machine learning based methods apply different models to enhance the accuracy of fake news detection. Several ML models combine the features from previous studies and propose new features to improve the performance of fake news detection [1], [17], [36]. For instance, Jin et al. proposed five new image visual features and seven new image statistical features extracted from news articles to detect fake news. In their experiments, they used SVM, Logistic Regression, KStar, and Random Forest models, utilizing their newly identified features with baseline features from previous studies. They applied their models on the Sina Weibo data set and found their selected features improve fake news detection performance [17]. (4) Recently, deep learning based approaches have also been studied for fake news detection [13]–[14][15][16][17][18][19][20][21][22][23]. Specifically, various deep learning models such as CNN, LSTM, Bi-LSTM, adversarial neural network, and even transfer learning are used to identify fake news. For example, Wang et al. designed an event adversarial neural networks by considering both news textual and visual information, where both text-CNN and image-CNN (e.g., VGG-19) are used to extract the textual and visual feature representation [18].

In our work, we integrate ML-based methods and experimental studies (human reviews to be specific) and provide a generic system architecture to build such a system. Compared to DL-based approaches, we study ML based ones in our work, mainly because ML-based approaches have lower latency and the majority of the works are ML-based ones. Our system architecture, however, is generic so one could simply replace ML-based approaches with DL-based ones.

B. Byzantine Fault Tolerance (BFT)
The goal of BFT consensus is that the correct replicas reach a consensus on the order of client requests. Beginning with PBFT [37], an impressive number of efficient BFT protocols are proposed (e.g., [38]–[39][40][41][42][43][44][45][46][47]). Numerous efforts have been made to improve the performance of BFT using different approaches [41], [48]–[49][50][51][52][53][54][55][56]. While most of them work in partially synchronous environments to guarantee liveness, there exist efficient protocols working in completely asynchronous environments [57], [58]. Our two-layer consensus is general and can be based on any of these BFT protocols.

C. Blockchains
Blockchains can be categorized into two types: permissionless blockchains and permissioned blockchains. A permissionless blockchain allows dynamic membership using Sybil attack resistant mechanisms [59]–[60][61]. In contrast, permissioned blockchains require that servers know the identities of each other but do not have to trust each other [62]–[63][64][65]. BFT is deemed as the model for permissioned blockchains [65] and can also be used to improve permissionless blockchains (hybrid blockchains).

Our two-layer consensus divides the roles of nodes into consensus nodes (replicas) and reviewer nodes. The idea of separating roles of nodes originates from early consensus works such as Lamport’s Paxos crash fault-tolerant consensus [66]. Several previous efforts also separate the roles in the BFT for various purposes [54], [67]–[68][69][70][71].

SECTION III.The Need for Combining Machine Learning and Expert Reviews: A Case Study
Intuitively, one would need machine learning for quick reviews and need human experts for more accurate and detailed new reviews. To justify the need for combining ML techniques and human expert reviews, we have conducted a news review at the authors’ institute (UMBC). We provide interesting results, bolstering the viewpoint that a combination of ML and expert reviews achieve the best of both worlds.

Specifically, we crawled 50 university news articles from the UMBC’s news website. We then use Grover [26] to generate 50 fake news. The 100 news forms a pool of university news.

We conduct news reviews among two groups of people. Group 1 includes only graduate students. Group 2 includes staff from the communications team at UMBC and some of them may have reviewed or edited some news in our news pool. We intentionally chose the two groups of reviewers for our study. For group 1, we aim to validate the conclusion where for news that is not subjective (e.g., university news is usually either fake or authentic), experts can well validate the authenticity of the news. Second, for group 2, we aim to study whether, with some background knowledge, a non-expert can justify the authenticity of the news with decent accuracy. The study for group 2 can also be used to validate our proposed architecture as discussed in great detail later.

For each participant, we randomly select 10 news out of the 100 news pool to review. We have collected 30 valid sets of results. In addition, we also train our LR model using the training data set from PolitiFact.com and use it to predict the authenticity of the 100 news. For the news word embedding, we use doc2vec google news corpus [72] to extract the news features. We include detailed discussion about the implementation of ML model in Sec. VIII. Specifically, each time we randomly select 10 samples to obtain the ML results and we run the same test 30 times. This matches the test we have done for human reviews. We have also conducted a t-test for comparing the average performance between human and ML reviews. The T-stat is calculated as follows:
T−stat=X1¯−X2¯1/N(S12+S22)−−−−−−−−−−−−−√(1)
View Sourcewhere X1¯ , X2¯ , S12 , and S22 represent the average performance (e.g., accuracy) and standard deviations among two samples (human and machine learning), respectively. N reveals the sample size, which is 30 in our situation.

We first report the score for the human review for both groups and doc2vec reviews in Table II. The score refers to the number of correct answers in each experiment. The results for group 2 are highly accurate, in part because the communications team staff are very familiar with the news of the institute. In comparison, the results for students in group 1 are less accurate. The median and mean, however, are well above average. Although none of the student participants is majoring in media or news, the results have shown that, with some familiarity of the news content, people’s justification is well above the 54% random guessing for a general audience [4]. In comparison, non-expert reviews are slightly more accurate than ML reviews and less accurate than expert reviews.

TABLE II Review Scores for Both Human Groups and ML

We also compare accuracy, precision, recall, f1-score, and evaluation time for both human reviews and ML results, where the first four metrics as commonly used automatic fake news detection performance metrics [11], [12], [36]. Specifically, accuracy is the percentage of correctly detected fake and true news among the total number of news examined; precision is defined as the percentage of total news detected as fake that is indeed fake ones; recall is the ratio of fake news that is correctly identified; and F1-score is a harmonic mean of precision and recall.

As shown in Table III, human reviews in general achieve much higher performance for all the criteria we have used. The major drawback is the long review latency. Indeed, humans take on average 7 minutes to review each article, which is significantly higher than a few seconds for ML reviews. This is unavoidable in practice since all the data sets used in supervised ML rely on humans to review them. Among the human reviews, the results by experts (group 2) are consistently higher than that by non-experts (group 1), both of which are consistently better than ML.

TABLE III Review Results of Human Reviewers and ML Reviews

TABLE IV Scalability of Poligraph

To conclude, our results have validated the need for a combination of ML and expert reviews. Specifically, the ML results are less accurate but more timely. Human reviews, in particular, expert reviews, are more accurate but might be time-consuming. With little to median expertise, human results can be used as labels to finalize the results of news.

SECTION IV.System and Threat Model
We consider a generalized distributed review system that is not restricted to fake news detection. The system consists of clients, n servers (replicas), and N reviewers. Clients submit requests to the system and expect results for their requests. The result for a request can be a temporary result or a final result. A temporary result is an initial assessment of a request by replicas according to the current system state, st . A final result combines the temporary result with reviews collected from reviewers. A client may first receive a temporary result and later receive a final result, or receive a final result directly if the request has already been reviewed by reviewers. In our work, the duration between the temporary result and the final result is determined by the time for human reviews.

Each time a new request that has not been reviewed is submitted, a set of I reviewers will be selected to contribute their reviews for the request. Each reviewer provides a binary result with certain data payload. The selected set is called a reviewer set. Let R denote the number of required matching reviews. If R matching reviews are received by servers, the servers combine the temporary server-generated result with the human reviews to form a final result for that request. Client requests and reviews are collectively called transactions. Clients, servers, and reviewers are collectively called nodes.

Nodes can fail arbitrarily (Byzantine failures). We consider a strong adversary which can coordinate Byzantine nodes. We assume f out of n servers may be Byzantine and we require n>3f . We assume F out of N reviewers may be faulty. For simplicity and correctness, we require N>2F in this paper. Namely, among the N reviewers, the majority of them (correct reviewers) will return the same binary review result (denoting if a request is authentic) for the same request. The corresponding review is referred to as a correct review. Faulty reviewers will either be non-responsive or generate biased (incorrect) reviews.

We assume asynchronous environments, making no timing assumptions on message processing or transmission delays. In some cases, the system works under partial synchrony [73], where synchrony holds after some unknown global stabilization time, but the bounds on communication and processing delays are themselves unknown.

Goals: Our system has the following goals.

Agreement: If any correct replica delivers a transaction m , then every correct replica delivers m .

Total Order: If a correct replica has delivered transactions m1,m2,⋯ , ms and another correct replica has delivered m′1,m′2,⋯ , m′s′ , then mi=m′i for 1≤i≤ min(s,s′) .

Liveness-1: If a transaction m is submitted to n−f correct replicas, then all correct replicas will eventually deliver m .

Liveness-2: Temporary Result: For each new request that is sent by a client and has never been processed and finalized by replicas, the client will eventually receive a temporary result from replicas.

Liveness-3: Final Result: The client will eventually receive a final result from replicas for a request.

Validity: The final result is guaranteed to be correct in spite of faulty (non-responsive and biased) reviewers.

A. Fake News Detection
We now restrict the review system to the case of fake news detection. In this system, clients submit news to the servers. Servers run ML models to generate a temporary assessment of whether the news is fake or authentic. Reviewers are experts such as credentialed journalists.

1) Machine Learning Models:
In this work we use supervised machine learning models. Compared to deep learning models, machine learning models are faster and have low latency in detecting fake news. Among the machine learning models, LR, SVM, random forest, naive bayes, and XGBoost are commonly used. Among them, LR and SVM perform the best with the lowest latency in general, as pointed out by prior works [74]. Thus, we focus on LR as the machine learning model. Each model can be initiated with a number of parameters a1,a2,⋯,ak . For example, for a LR model, the input parameters be penalty (the norm used in the penalization), tol (tolerance for stopping criteria), etc.

The parameters determine the ML state. Each machine learning model consists of two phases: training and prediction. The training phase uses a labeled data set to train the ML model and forms a ML state. The prediction phase utilizes trained ML state to make predictions (results) for unlabeled data (news verification requests from clients).

SECTION V.Building Blocks
A. BFT
We use BFT state machine replication (SMR) protocols, where f out of n replicas may fail arbitrarily (Byzantine failures) and a computationally bounded adversary can coordinate faulty replicas. A replica delivers transactions submitted by a client. A client can compute a final result to its submitted transaction from the responses it receives from replicas. Correctness of a secure BFT protocol is specified as follows.

Agreement: If any correct replica delivers a transaction m , then every correct replica delivers m .

Total Order: If a correct replica has delivered m1,m2,⋯,ms and another correct replica has delivered m′1,m′2,⋯,m′s′ , then mi=m′i for 1≤i≤ min(s,s′) .

Liveness: If a transaction m is submitted to n−f correct replicas, then all correct replicas will eventually deliver m .

BFT is a key building block for our system. Both efficient asynchronous and partially synchronous BFT protocols exist. If the underlying BFT protocol is asynchronous (resp., partially asynchronous), our protocols are asynchronous (resp., partially asynchronous).

B. Threshold PRF
We review threshold PRF (e.g., [31]), where a public key is associated with the system and a PRF key is shared among all the servers. A (t,n) threshold PRF scheme Π for a function F consists of the following algorithms (FGen , Eva , Vrf , FCom ).

A probabilistic key generation algorithm FGen takes as input a security parameter l , the number n of total servers, and threshold parameter t , and outputs (pk,vk,sk) , where pk is the public key, vk is the verification key, and sk=(sk1,⋯,skn) is a list of private keys.

A PRF share evaluation algorithm Eva takes a public key pk , a PRF input m , and a private key ski , and outputs a PRF share yi .

A deterministic share verification algorithm Vrf takes as input the verification key vk , a PRF input m , and a PRF share yi , and outputs b∈{0,1} .

A deterministic combining algorithm FCom takes as input the verification key vk , a PRF input m , and a set of t valid PRF shares, and outputs a PRF value y .

We require the threshold PRF value to be unpredictable against an adversary that controls up to t−1 servers. We also require the threshold PRF to be robust in the sense the combined PRF value for m is equal to F(m) .

We use the CKS threshold PRF [31], as described in Figure 1. The protocol is non-interactive, working in asynchronous environments. The CKS threshold PRF is secure under the computational Diffie-Hellman (CDH) assumption in the random oracle model.


Fig. 1.
CKS threshold PRF (FGen , Eva , Vrf , FCom ) for a function F:{0,1}∗→{0,1}l .

Show All

SECTION VI.Poligraph: Intrusion-Tolerant News Detection
A. Motivation and Pitfalls
We build a fake news detection system, Poligraph, combining ML servers and human expert determination. A client may submit news to the system and expect a result representing whether the news is fake. Servers run supervised ML models to generate a temporary assessment of whether the news is fake, along with a confidence score. Meanwhile, servers also invite reviewers, such as credentialed journalists, to review the news. If matching reviews from a majority of reviewers are received, the result for the news is finalized and will be sent to the client. The final result is also viewed as a label of the news item. Thus, the labeled data set maintained by the system can grow over time and can be used to re-train ML models and enhance the accuracy of the ML results in the long run.

Collecting reviews from reviewers, however, is tricky.

First, different from computer nodes, reviewers can be (arbitrarily) slow: reviewers may not be responsive and the review process itself may take time. It is difficult to set up appropriate timers or rely on any timing assumptions. A system working in completely asynchronous environments is desirable.

Second, one cannot ask all reviewers to provide reviews for every news feed. This is prohibitively expensive and slow. We can, however, allow selecting subsets of reviewers to perform parallel reviews for individual news feeds. This again poses various challenges: How can we select subsets of reviewers securely and fairly? What if a subset of reviewers happens to have a malicious majority?

1) Attempts:
Our first attempt is to directly use BFT (e.g., PBFT [37]) or blockchain smart contracts (e.g., Chaincode in Hyperledger Fabric [75]). In this approach, we use a group of servers to run a BFT or blockchain system and treat both clients and reviewers as BFT or blockchain clients.

One may use a specific server, e.g., the leader of the BFT or blockchain system, to select subsets of reviewers. However, this approach does not guarantee the accuracy, because using the leader to select reviewers easily leads to biased results if a faulty leader colludes with certain reviewers. Ideally, reviewers for a news feed should be uniformly chosen at random among all reviewers with specific expertise to ensure fairness and accuracy of review results. Second, the approach impedes liveness. For instance, a faulty leader may simply choose not to send requests to reviewers.

To solve the issue, we use an asynchronous common coin protocol to unbiasedly select reviewer sets. In particular, all replicas jointly and asynchronously generate an uniformly random number corresponding to a random reviewer set. Moreover, each reviewer set should be bound to a specific news feed.

Existing blockchain systems, however, cannot be directly used for our purpose because they cannot efficiently support the data structures we need. Poligraph requires an integration of ML and BFT consensus, involving multiple data formats, e.g., database tables, in-memory hash maps, ML model initialization parameters. These formats are not well supported by existing smart contract and blockchain platforms. Additionally, Poligraph uses threshold PRF to instantiate an asynchronous common coin protocol. Existing blockchains do not support deploying such distributed cryptographic protocols using smart contracts. Therefore, a new system needs to be built.

There is another pitfall. Even if we have a way of fairly selecting subsets of reviewers and even if we can guarantee a majority of reviewers in a subset is correct, there are still potential inconsistencies among the servers. Suppose we have an intuitive protocol requiring servers to take majority votes on reviews. It is possible that replicas agree on the decision (whether or not news is fake) but fail to agree on the human reviews: some correct replicas receive reviews from a subset of reviewers, while some other correct replicas have received reviews from a different subset of reviewers. As these human reviews are used to run ML prediction and further used to perform periodical ML re-training, inconsistencies would occur among correct replicas.

2) Our Approach at a High Level:
We present the concept of two-layer consensus, a new primitive for BFT consisting of a BFT server layer and a review layer. The BFT layer maintains the availability of the service and runs ML models to generate a temporary assessment of the news. The BFT layer also interacts with the review layer and integrates the results. Two-layer consensus provides a two-step validation for the data in client requests, integrates machine review results and expert review results, and enables online vs. offline response trade-offs. We use BFT and asynchronous common coin protocols to realize two-layer consensus and support parallel reviews. We further tune the parameters between the performance and accuracy of the final result. To solve the inconsistency problem, we treat reviews as transactions and explicitly order review transactions. In this way, the exact same reviews should be included to ensure that replicas have the same state and replicas have the same final result integrated from the reviews.

B. Poligraph: A Two-Layer for Fake News Detection
We present Poligraph, a distributed system that relies on two-layer consensus to provide an intrusion-tolerant, reliable, and tamper-proof service for fake news detection.

We believe that abstractly, the two-layer consensus is of separate interest and can be applied to other domains as well. In the rest of this section, we present the workflow of two-layer consensus for fake news detection. The workflow, however, is generic enough to be easily used in other domains.

1) Replica State:
In our two-layer consensus protocol, there are three types of data: transactions (client requests and reviews), temporary results (client requests with pending finalized review results), and final results. The three types of data can be mapped to three tables: a transaction table, a temporary state table, and a final state table. The transaction table and final state table are append-only, and new entries will be appended to the end of the tables. The temporary results are put in the temporary state table, the entries of which will later be moved to the final state table.

Figure 2 depicts the workflow of the protocol. The BFT layer runs a BFT protocol and assigns sequence numbers to both client requests and reviews. Poligraph can be built on top of any BFT protocol. If the BFT protocol is asynchronous (resp., partially asynchronous), our two-layer consensus protocol is asynchronous (resp., partially asynchronous). For instance, if we use HoneyBadgerBFT [57], BEAT [58], our protocol is asynchronous; if we use PBFT [37], our protocol works in partially synchronous environments.


Fig. 2.
The Poligraph workflow.

Show All

2) System Setup:
We set up an (f+1,n) threshold PRF scheme, Π= (FGen , Eva , Vrf , FCom ), where a public key pk and verification keys vk are associated with the system, while a secret key is shared among all servers, with a server pi having a key ski for i∈[1..n] . The keys can be generated either by a trusted dealer or running a distributed key generation algorithm [76], [77]. Let h(⋅) be a hash function.

Step 1 (Client Submits a Request m to the Replicas): The client sends a request of the form ⟨REQUEST,cid,ts,o⟩ to the servers, where cid is the client id, ts is the timestamp, and o is the data payload of a news feed.

Step 2 (Replicas Run the BFT Protocol to Assign a Sequence Number seq to the Request m ): After a replica delivers the request, the request is appended to the transaction table.

The replicas look up the final state table to identify if h(o) exists, i.e., whether o has been stored in the final state table.

If the data payload o already exists in the final state table, replicas directly send a final result to the client. In this case, while the request is new, the data payload for this request has been previously reviewed and finalized. The final result can be directly obtained from the final state table and sent to the client. Replicas complete the request.

If the data payload o has not been reviewed before, each replica runs its ML model to obtain a temporary result of the news in the format of ⟨TEMP,ev,cl⟩ . The value ev is a boolean result representing if the news is authentic. The value cl is the confidence level, where 0<cl<1 . After running the ML model, the replica sends the temporary result to the client and then continues to proceed to the reviewer selection procedures (step 3).

Step 3 (Replicas Agree on a Random Set):

Each replica pi runs Eva on (m,seq) to generate a threshold PRF share yi and broadcasts ⟨BEB,seq,yi⟩ to all other replicas.

Upon receiving f+1 valid PRF shares of the form (m,seq,yj) from replica pj , replica pi runs FCom on (vk,(m,seq),{yj}) and obtains a random output denoting a reviewer set of I invited reviewers.

Step 4/5 (Replicas Collect Reviews From the Reviewers):

Each replica pi sends the review request ⟨REVIEW-REQUEST,seq,o⟩ to the reviewers in the reviewer set. We remove the identity information cid from the request m and reviewer set I and only forward the data payload. This maintains decoupling between clients and reviewers in time and space, providing scalability and (weak) anonymity.

Upon receiving f+1 matching replica review requests with the same (seq,o) , a reviewer reviews the payload request and sends replicas a review ⟨REVIEW,seq,b,rev⟩ , where b denotes a binary review result, and rev is a detailed review.

Step 6 (Replicas Integrate Reviews and Send a Final Result to the Client): A review of the form ⟨REVIEW,seq,b,rev⟩ is ordered as a transaction by replicas and appended to the transaction table. A replica begins integrating reviews after collecting exactly R reviews with the same b , where R is a system threshold parameter that will be specified shortly in Sec. VI-C. The entry in the temporary state table is removed. A new entry with the integrated final result is appended to the final state table. After collecting R reviews, any other reviews for m will not be stored or processed. The corresponding entry is then removed from the temporary state table, and the integrated final result is then sent to the client. Correspondingly, the client accepts the result after receiving f+1 matching replies.

Now it becomes clear why we need to order reviews. In our protocol, the integrated final result for a client request takes as input R reviews of the form (b,rev) . If we do not order reviews, it is possible that replicas agree on the same indicator bit but fail to agree on the reviews. Therefore, the exactly same reviews should be included to ensure that replicas have the same state and the same final result integrated from the reviews. The above argument also explains why we need to choose exactly R reviews for all correct replicas.

Step 7 (ML Model Re-Training): Periodically if a replica appends δ news to the final state table, the replica will copy the δ entries from the final state table to the labeled data set to re-train the ML model. The replica will stop processing new transactions while re-training, and once complete, resumes its normal operations. The step improves accuracy of the ML prediction over the time.

A machine learning model, making predictions (temporary results) for news from client requests, can take a few minutes or even longer. Therefore, we separate the training phase from the prediction phase. Specifically, during system setup, we pre-train the ML model using ground truth data set from our pre-collected labeled news. When the system processes client requests, the ML model processes the news by executing just the prediction phase. This will enable online ML model and reduce the time for training from scratch.

3) The need for Threshold Common Coins:
One may ask if we need to use threshold common coins: would it be possible to use pseudorandom function (PRF) or even a hash function to select reviewers? The idea of using a publicly known PRF or a hash function to generate random coins for BFT, or more generally, for state machine replication, dates back to Rodrigues, Castro, and Liskov [78]. The idea can be used in “benign” applications, say, generating coins for randomized (non-deterministic) algorithms; it cannot be used in scenarios where coins may be manipulated by the adversary.

Our goal is to ensure that the reviewers are chosen in an unpredictable manner, without being controlled by the adversary. If the adversary can predict the reviewers chosen, it can feed into the system with adaptively chosen news feeds and maliciously train the ML model in the favor of the adversary.

C. Proof of Correctness and Parameter Selection
In this section, we prove the correctness of Poligraph and provide theoretical foundations on parameter selection.

Theorem 1:
Two-layer consensus achieves agreement and total order.

Proof:
Agreement and total order follow from the underlying BFT protocol since both requests and reviews are treated as transactions.

Theorem 2:
Two-layer consensus achieves liveness-1 and liveness-2.

Proof:
Liveness-1 easily follows from the livness of the underlying BFT protocol, as we order both expert reviews and client requests. Liveness-2 is immediately implied by liveness 1, as the temporary result does not involve the review layer.

Lemma 3:
Correct replicas agree on the same set of reviewers for any transaction to be reviewed.

Proof:
We observe that transactions are totally ordered by all correct replicas. All correct replicas will take the same input for the common coin protocol and thus obtain the same common coin. Therefore, all correct replicas will select the same set of reviewers.

Theorem 4:
Two-layer consensus achieves liveness-3 under either of the following two conditions: 1) I>2B and B+1≤R≤I−B , where B is the number of faulty reviewers in the reviewer set selected by the replicas; 2) N>2F and F+1≤R≤N−F . (In particular, if the underlying BFT protocol is asynchronous, two-layer consensus achieves liveness-3 in asynchronous environments.)

Proof:
According to Lemma 3, replicas agree on a common set of reviewers. It is straightforward to verify that both conditions guarantee that at least R correct reviews will be received by all replicas. Thus, all correct replicas will make the same review decision on the review. The theorem thus follows from the liveness of the underlying BFT protocol.

We define perfect validity and probabilistic validity. Perfect validity guarantees that the final result received by a client reflects a correct review, i.e., it is the same with that returned by the majority of the reviewers. Probabilistic validity guarantees the same result with an overwhelming probability in the parameter I (and parameters R , N , and F ).

Lemma 5:
It is computationally infeasible for adversaries to distinguish if the I reviewers in the selected reviewer set are uniformly chosen at random from all reviewers.

Proof:
This is implied by the unpredictability property of the underlying threshold PRF protocol.

Theorem 6:
Two-layer consensus achieves perfect validity if F+1≤R≤I−F .

Proof:
Implied by Theorem 4.

Theorem 7:
Two-layer consensus achieves probabilistic accuracy if B+1≤R≤I−B , and N and I are large enough.

Proof:
Let α be the fraction of faulty nodes in N reviewers, i.e., α=FN . Let Xi be a random variable which outputs value one if the i -th review collected from the reviewer set is correct (where i∈{1,⋯,I} ). Let X=∑I1Xi . Clearly, X follows a binomial distribution.
Pr[X≤R]==∑k=0RPr[X=k]∑k=0R(Ik)αI−k(1−α)k.(2)
View SourceRight-click on figure for MathML and additional features.

It is known that the probability decreases exponentially as I increases. That is, given a security parameter λ , there exists an I0 so that ∀I>I0 , Pr[X≤R]≤2−λ .

Examples: For Theorem 7, if we set α=13 , λ=20 , and R=⌊I+12⌋ , we have I0=180 . In words, if 1/3 of the total N reviewers are faulty and if each time we select 180 reviewers and expect 91 matching reviews to finalize the result, then the probability that the review result does not represent a correct review is once every 1 million reviewer set selections.

As another example, we set α=0.1 , λ=10 , and R=⌊I+12⌋ and we have I0=13 . In words, if 10% of the reviewers are faulty and if we select 13 reviewers and expect 7 matching reviews to finalize the result, then the probability that the review result does not represent a correct review is once every thousand reviewer set selections.

Generalizing the Acceptance Condition and the System Assumption: So far we have set the acceptance condition as a simple threshold, i.e., replicas can make a decision after receiving exactly R reviews with the same indicator bit b . One can easily extend the acceptance condition to a more general predicate on the reviews received. It is also easy to make stronger system assumptions regarding the percentage of faulty reviewers.

D. Machine Learning State Transfer
BFT nodes may experience (temporary) failures or attacks, causing some “fall-behind” nodes. While several BFT protocols provide schemes for recovering nodes from failures [37], for systems with ML models integrated, it is not clear how we can efficiently transfer ML state. In Poligraph, we provide an efficient machine learning state transfer protocol to bring fall-behind replicas up to date.

In Poligraph, each replica maintains a system state st and a set of ML state parameters a1,a2,⋯,ak . The system state in Poligraph is defined to be the hash of the transaction history, i.e., the state after processing txj is st=h(txj,h(stj−1)) . In addition, the ML state parameters are initialized with the parameters for the ML model.

As illustrated in Figure 3, we provide a controller module for each replica. The controller is used for consistency checking and state transfer. In case of a replica inconsistency, the controller will trigger the state transfer protocol and re-initiate the ML model, thereby bringing the replica up to date.


Fig. 3.
The ML state transfer architecture.

Show All

The state transfer protocol in Poligraph works as follows.

Step A (Send ML and Replica State): Upon executing a fixed number of transactions, each replica sends a message in the form of ⟨STATE,h(st),seq,a1,a2,⋯,ak⟩ to other replicas, where h(st) is the hash of the state st , seq is the largest sequence number in the transaction table, and a1,a2,⋯,ak are the current ML state parameters.

Step B (Controller Module Collects ⟨STATE⟩ Messages): For a replica pi , if it receives at least 2f+1 matching messages (denoted as ⟨STATE,h(st′),seq,a′1,a′2,⋯,a′k⟩ ), it compares the received state st′ and its local state st .

If st′ matches st , the state is considered as a stable state.

If st′ does not match st , pi starts state transfer. Let seq′ be the last sequence number where pi was consistent with other replicas. To begin state transfer, pi first discards its local state, transactions, and system logs between seq′ and seq . For all the incoming transactions, pi still participates in the consensus. However, it only updates the transaction table and does not process the transactions or reply to the clients. Meanwhile, pi sends a message ⟨RT,seq′,seq⟩ to other replicas and requests the transactions between seq′ and seq . If a replica receives a ⟨RT⟩ message and has executed transactions with sequence numbers greater than or equal to seq , a replica will reply with a ⟨TH,mseq′,⋯,mseq⟩ , where mseq′ to mseq are the transactions with sequence numbers between seq′ and seq . Upon receiving matching ⟨TH⟩ messages from at least 2f+1 replicas, pi has all the transactions with sequence numbers greater than seq′ . Replica pi will append these transactions to its transaction table. Also, pi re-initializes its local ML model using the state parameters a′1,⋯,a′k . Then it processes the transactions with sequence numbers greater than seq′ .

SECTION VII.Discussion
A. The Benefits to Fake News Detection
The purpose of the system is to enhance the quality of the data sets for fake news detection utilizing two-layer BFT consensus. In other words, our goal is not to study or enhance ML algorithms. Instead, due to the growth of the labeled data set (final state table), the accuracy of fake news detection will grow over time.

B. Human Review Considerations
Although our paper provides a scheme to select uniformly random reviewers to review the contents and ensure fairness, we cannot guarantee that reviewers are unbiased. Our assumption is that the majority of the reviewers provide the same review results. Indeed, whether the reviews are biased is related to the types of news, which is orthogonal to the focus of this paper.

C. Considerations About Binary Labels for News
Data set such as the one from PolitiFact.com has several labels, i.e., true, mostly true, half true, mostly false, false, and pants on fire. We simplify them into a binary result, i.e., true or false, for both ML results and human reviews. This is mainly because, in our work, we consider an open framework where non-experts (humans with certain knowledge) can serve as reviewers. Furthermore, there are a large number of reviewers. It is challenging for non-experts to distinguish mostly true from half true and similarly, mostly false from false. Furthermore, differentiating news into multiple categories may create biased results. Therefore, we choose to use binary result in our framework.

D. Generality of Poligraph
Although we focus on fake news detection in Poligraph, the two-layer consensus protocol can be used in data science in general or even applied to other domains. For instance, consider a medical diagnosis problem. A typical workflow is that doctors diagnose manually based on patient data, e.g, an image. If 3 out of 5 doctors have a matching diagnosis, the corresponding image is labeled. The labeled data set is then used for ML training. This is another ideal application for two-layer consensus where doctors become reviewers. Patients get diagnosis results from ML in the first layer and then final results from reviews. The labeled data set grows over time and is stored securely in the system.

E. Feasibility of Poligraph
Poligraph is an open platform for fake news detection where reviewers are not necessarily experts. As shown in our case study, people with certain background knowledge can justify the authenticity of the news much better than others. Therefore, Poligraph is more suitable for news that is not subjective so that human review results are not biased. Reviewers can be humans with expertise (or at least some familarity) in certain domains. In a complete system where news may fall into different domains, reviewers can be tagged by their expertise. News can also be tagged where they are only sent to reviewers with the same tags. However, the considerations of who should serve as reviewers are related to the concrete application if we view Poligraph as a generic architecture.

F. Considerations of the Validity of the Labels
In our work, we consider the labels of the news, once reviewed by reviewers, are final, i.e., if the same news is queried again, the result is directly replied to the client. For some news, the authenticity might change over time, e.g., even for some scientific claims, the authenticity may change. We consider this issue a validity problem of the labels. One approach to address this problem is to set up an expiration time for each final label of the data. We can assign the timestamp to each label and an expiration time/data. After the expiration time, the label can be considered invalid. If the news is queried again, it has to go through all the processes and get reviewed by reviewers again.

SECTION VIII.Implementation
We use ECDSA for authentication and use SHA-256 as the hash function. We implement CKS threshold PRF [31] using the Charm Crypto Python library [79]. We use the NIST P-256 elliptic curve to provide 128-bit security.

We use BFT-SMaRt [80] as the underlying BFT engine, as it is “the most advanced and most widely tested implementation of a BFT consensus protocol” [65]. We extend the BFT-SMaRt library and implement a client-server service. Then we connect the Java library with a Python library in which we implement the ML models and the review layer. During performance evaluation, we code human reviewers where each reviewer node directly sends a pre-defined review to the replicas. We separate our evaluation of reviewers into two parts. When we evaluate the system performance (scalability, throughput, etc.), we implement the reviewers. Specifically, each reviewer script automatically replies with the review result for each news.

We store the data and the state tables in three formats. First, the transactions in the transaction table are written into the database. We use batching to reduce hard disk access and improve the system throughput. We set up a tunable parameter, BatchSize, to provide performance trade-offs. Second, the final state table and temporary state table are stored in memory using hash maps. Last, we store the labeled data set constructed from the final state table in a csv file for the ML model, which is friendly for ML re-training.

We also implement the data preparation and ML models. We first build a data crawler to collect labeled data sets using two python libraries Beautifulsoup [81] and Selenium [82]. We use the labeled data sets to train our ML models to form the initial machine learning state. We then use doc2vec google news corpus [72] to extract the news word embedding as the news features. We select 300 as the feature vector size. Then, we adopt python scikit-learn [83] to implement LR and SVM models. We serialize all the parameters into string format and send them over the network. When a node receives the state, it deserializes the state into the state parameters and initializes the ML model.

For the ML model selection in our system, we use LR. Meanwhile, other machine learning models are available, e.g., random forest and XGBoost [84], [85]. Some prior works also compare different fake news detection models in terms of ML performance. However, depending on the concrete scenarios, the models may achieve different performance. For example, it was shown in [85] that XGBoost achieves the best performance and in [86] that LR achieves the best performance. Therefore, we select the LR model in our implementation because it is a classic ML model, and can achieve low latency and high throughput. To validate the fact that our system can be integrated with any ML model, we also implement SVM model in our system.

Evaluating Poligraph: As shown in Sec. III, we have conducted experiments for human reviewers to demonstrate the need for Poligraph. Besides, We evaluate the system performance of Poligraph. In particular, we implement the behaviors of reviewers and show the results in Sec. IX. The purpose is to evaluate the system performance under extreme conditions, i.e., high concurrency of client requests and a large number of human reviewers. Our evaluation focus on metrics such as latency of each client request (how long does it take for a user of Poligraph to get a ML response to its request), system throughput (how many requests can Poligraph handle at the same time), and ML performance (whether Poligraph can benefit the data science community in enhancing the data quality).

SECTION IX.Evaluation
We deploy and test our protocols on Amazon EC2 utilizing up to 35 nodes (VMs). Each node is a general-purpose t2.medium type with two virtual CPUs and 4GB memory. Among these nodes, we use up to 10 different nodes to run the replicas, 5 to run the reviewers, and 20 to run up to 1,000 clients. We use each node to simultaneously run up to 50 clients. We compare Poligraph with a single-server, unreplicated implementation (denoted as n=1 in the figures).

A. Overview
We evaluate the performance of Poligraph from both system and ML perspectives. From the system perspective, we evaluate both latency (the delay between a client request and response) and throughput (the number of transactions processed per second) using different benchmarks. In addition, we show the peak throughput for parameters such as news length (the number of characters) and δ . Recall δ is a parameter, where after every δ news is appended to the final state table, we update the labeled data set and re-train the ML model. From the ML perspective, we show that as the size of labeled data for training grows over time and reviews are integrated, the accuracy of the ML models’ temporary feedback score improves.

We build several benchmarks for evaluation. In each benchmark, a fraction of client requests has already been reviewed and finalized. We use the notation x% final data set to represent the case where x% of all the client requests are already finalized. For all the experiments, we calculate the average throughput/latency for all of the replicas.

We have evaluated the performance using both political news (from PolitiFact.com) and entertainment news (from GossipCop.com). Our evaluation shows that Poligraph incurs latency of 0.05 second and achieves throughput as high as 5,500 tx/s. In addition, compared with a single-node implementation, the throughput of Poligraph is about 4%–7% lower.

B. Latency for Normal Operations and State Transfer
We assess the latency for f=1 , 2, and 3, where a single client issues one news verification request to the system. We assess the two cases: 1) the news already exists in the final state table; 2) the news has never been reviewed before. As shown in Figure 4a, the latency of the first case is much lower than that of the second case for both political news and entertainment news. This is because in the first case, the replicas directly reply with a final result, while in the second case, the replicas run the ML model to obtain a temporary result and collect reviews from reviewers. Due to the use of ML optimization, the latency is 1 to 2 seconds in the worst case. The latency for entertainment news is higher than that with political news, mainly due to news length. We also report the latency breakdown of the normal case for political news in Figure 4b, where the client sends a news item that has never been processed before. As shown in the figure, the ML and the review collection processes dominate the latency. In practice, the review process can take much longer and becomes the bottleneck of the system. However, the client can get the temporary result immediately after the ML step and the reviews can be collected asynchronously.


Fig. 4.
Performance of Poligraph.

Show All

We evaluate the latency for state transfer. We inject one failure and let one node become inconsistent with other nodes. The nodes will then run the state transfer protocol as described in Sec. VI-D. We also measure the latency breakdown in Figure 4b. The overall latency for state transfer is around 2 seconds on average, and the ML state re-initialization dominated the latency.

C. Throughput
We use three benchmarks to evaluate the throughput and compare them with an unreplicated implementation of the system. We assess the throughput by gradually increasing the number of clients. For all these experiments, we use news with length 150. We use 4 servers and 5 reviewers, and set up BatchSize to 100. The replicas do not re-train their ML models.

As we can see from Figure 4c, all experiments have a similar trend. As the number of clients increases, the system will be saturated with transactions, and the throughput will reach its peak. In almost all experiments, the throughput reaches its peak when the number of clients is greater than 600. However, the higher the fraction of news that is already in the final state table, the higher throughput the system can achieve. This is expected, for the same reason we observe when assessing latency: if a news item has been recorded in the final state table, the final result is directly returned to the client. In the worst case, where 60% of the news has never been reviewed before, the peak throughput can still be higher than 4,000 tx/s. In the best case, where all the news items have are already in the final state table, the peak throughput is close to 5,500 tx/s. In comparison, the peak throughput of the unreplicated version of Poligraph (n=1 ) is close to 6,000 tx/s. In other words, Poligraph is only marginally slower than its unreplicated version.

D. Impact of News Length
The lengths of news in the client requests will affect the performance of the system in two ways: network bandwidth and ML model prediction speed. Since nodes have to exchange several all-to-all messages in the consensus protocol, longer messages will consume higher network bandwidth which results in downgraded throughput. The ML model execution time increases when it has to predict the veracity of longer news items.

We evaluate the peak throughput by varying news lengths in the client requests from 50 to 500. In all experiments, we use 700 clients and no ML re-training is triggered. As shown in Figure 4d, the throughput indeed decreases as the length of news increases. The curves for the two benchmarks are relatively smooth, which demonstrates that Poligraph is capable of processing long news efficiently. Note that for the 0% final data set benchmark, the replicas need to run the ML model for each transaction. The performance degradation, however, is almost similar to that of the 50% final data set benchmark. Therefore, the news length is not a bottleneck to the system performance due to the use of ML optimization.

E. Impact of ML Re-Training
In this experiment, we aim to evaluate the throughput under ML re-training. We use 700 clients, 4 servers, and 5 reviewers. We vary the δ parameter from 50 to 500. We run three benchmarks where news lengths are 50, 150, and 200, separately. As illustrated in Figure 4e, we find that as δ increases, the peak throughput first increases and then decreases. The same trend applies to all three benchmarks. Note that as δ increases, the performance will be affected in two ways. First, the ML model will be re-trained less frequently, during which the protocol will be stalled. Each re-training process, however, will have higher latency. Second, when we update the labeled data set, we simply write all the new data from the final state table to the csv file. The writing time is related to the length of the data. Other operations, such as csv file open and close, are triggered per update. Therefore, as δ increases, csv file operation and ML re-training are triggered less frequently.

As δ first increases from 50 to 200, since re-training is triggered less frequently, the peak throughput will be higher. As the δ further increases, however, the performance degrades. This is because the latency due to csv file update is related to the length of the data. Note that each ML re-training will stall the system performance for a longer period of time. In summary, we find that the optimal performance can be achieved when we update the csv file every 150 to 200 news items.

F. Scalability
We assess the peak throughput of Poligraph using n equals 4 to 10. We use 5 reviewers and set up news length as 150. We use 700 clients and run the 50% final data set benchmark.

We show the performance degradation as the number of BFT servers n increases in Figure 4. As shown in the table, the performance does not degrade significantly when the number of servers increases. The performance degradation of each experiment, when compared with the previous experiment with a smaller n , is 4–5%. We find a similar result when measuring latency, as shown in Figure 4a. We comment that the scalability of the system is related to the underlying BFT protocol though.

G. Machine Learning Performance Improvement
We show that the ML performance improves as a growing labeled data set is generated by Poligraph. We first run the protocol and let clients send requests from data set 1 collected from PolitiFact.com with 14,788 data entries. On the server side, the news in the data set 1 is not stored in the final state table, and the servers will run ML model for every news article. After 4,000 news items have been evaluated by Poligraph and put into the final state table, we re-train the ML model using the 18,788 entries (the data set 2).

We utilize 10 cross-validation methods to run LR and get precision, recall, and f1-score to evaluate the machine learning classification performance.

Figure 4f shows the performance improvement for LR. As expected, we find that the precision, recall, and f1-score are improved. This demonstrates that Poligraph can effectively enhance the accuracy of temporary results from the ML models.

SECTION X.Conclusion
We motivate the need for combing machine learning and expert reviews for fake news detection via a real-world case study. We design and implement Poligraph, an intrusion-tolerant fake news detection system, defending against Byzantine failures and malicious attacks. Poligraph combines machine learning techniques and human expert determination based on two-layer consensus and resolves many key challenges in fake news detection (e.g., online and offline trade-offs, secure parallel reviews, practical state transfer). Our extensive evaluation on Amazon EC2 demonstrates that Poligraph achieves latency as low as 0.05 second and throughput of more than 5,000 tx/s, being only marginally (4%–7%) slower than an unreplicated, single-server implementation.