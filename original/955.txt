We present a privacy-preserving deep learning system in which many learning participants perform neural network-based deep learning over a combined dataset of all, without revealing the participants' local data to a central server. To that end, we revisit the previous work by Shokri and Shmatikov (ACM CCS 2015) and show that, with their method, local data information may be leaked to an honest-but-curious server. We then fix that problem by building an enhanced system with the following properties: 1) no information is leaked to the server and 2) accuracy is kept intact, compared with that of the ordinary deep learning system also over the combined dataset. Our system bridges deep learning and cryptography: we utilize asynchronous stochastic gradient descent as applied to neural networks, in combination with additively homomorphic encryption. We show that our usage of encryption adds tolerable overhead to the ordinary deep learning system.
SECTION I.Introduction
A. Background
In recent years, deep learning (aka, deep machine learning) has produced exciting results in both acamedia and industry, where deep learning systems are approaching and even surpassing human-level accuracy. This is thanks to algorithmic breakthroughs and physical parallel hardware applied to neural networks for processing massive amount of data.

Massive data collection, while vital for deep learning, raises the issue of privacy. Individually, a collected photo can be permanently kept on a company server, outside the owner’s control. Legally, privacy and confidentiality concerns may prevent hospitals and research centers from sharing their medical datasets, barring them from enjoying the advantage of large-scale deep learning over joint datasets.

Shokri and Shmatikov [28] presented a system for privacy-preserving deep learning that allowed local datasets of several participants staying home while the learned model for the neural network over the joint dataset could be obtained by the participants. To achieve the result, the system in [28] needed the following: each learning participant, using local data, first computed gradients of a neural network; then a portion (e.g. 1% ~ 100%) of those gradients must be sent to a parameter cloud server. The server is honest-but-curious. Namely, it is assumed to be curious in extracting the data of individuals; and yet, it is assumed to be honest in operations.

To protect privacy, the system of Shokri and Shmatikov retained an accuracy/privacy tradeoff (see Table I): sharing no local gradients leads to perfect privacy but not desirable accuracy; on the other hand, sharing all local gradients violates privacy but leads to good accuracy. To compromise, sharing a part of local gradients is the main solution in [28] for maintaining the best accuracy possible.

TABLE I Comparison of Techniques

B. Our Contributions
We demonstrate that, in the system of Shokri and Shmatikov [28], even a small portion of the gradients stored over the cloud server can be exploited. Namely, local data can be surreptitiously extracted from those gradients. Illustratively, we show in Section III a few examples on how a small fraction of gradients leaks useful information on data.

We then propose a novel deep learning system to protect the gradients over the honest-but-curious cloud server, using additively homomorphic encryption. All gradients are encrypted and stored on the cloud server. The additive homomorphic property enables the computation across the gradients. Our system is described in Section IV, and depicted in Figure 5, enjoying the following properties on security and accuracy:

Security. Our system leaks no information of participants to the honest-but-curious parameter (cloud) server.

Accuracy. Our system achieves identical accuracy to a corresponding deep learning system (i.e., asynchronous SGD (ASGD)) trained over the joint dataset of all participants.

Fig. 5. - Our system (gradients-encrypted asynchronous SGD) for privacy-preserving deep learning, with a cloud server and 
$N$
 participants.
Fig. 5.
Our system (gradients-encrypted asynchronous SGD) for privacy-preserving deep learning, with a cloud server and N participants.

Show All

In short, our system enjoys the best of both worlds: cryptographic security and deep learning accuracy. See Theorem 1 and Theorem 2 in Section IV.

1) Our Tradeoff:
Protecting the gradients against the cloud server comes with the cost of increased communication between the learning participants and the cloud server. We show in Table II that the increased factors are not too big: less than three for concrete mixed National Institute of Standards and Technology (MNIST) dataset [5] and Street View House Numbers (SVHN) dataset [24]. For example, in the case of MNIST, if each learning participant needs to communicate 0.56 MB1 of plain gradients to the server at each upload or download, then with our system using Learning with Errors (LWE)-based encryption, the corresponding communication cost at each upload or download becomes

2.47 (Table II’s factor) × 0.437 (original MB) ≈1 MB

which needs around 8 milliseconds to be transmitted over a 1 Gbps channel. Technical details are in Sections V and VI.
TABLE II Increased Communication Factor

On the computational side, we estimate that our system employing a multilayer perceptron with 109386 gradients finishes in around 2.25 hours to obtain around 97% accuracy when training and testing over the MNIST dataset, which matches results for the same type of neural network given in [2]. Moreover, using the same convolutional neural network as in [28] for MNIST with 105506 gradients, we estimate that our system finishes in around 7.3 hours (with accuracy around 99% identical to that of [28] thanks to Theorem 2).

2) Discussion on the Tradeoffs:
We show that the trade-off between accuracy/privacy in [28] can be shifted to efficiency/privacy in our system. The accuracy/privacy tradeoff of [28] may make privacy-preserving deep learning less attractive than ordinary deep learning, as accuracy is the main appeal in the field. Our efficiency/privacy tradeoff, keeping ordinary deep learning accuracy intact, can be improved if more processing units and more dedicated programming codes are employed.

3) Threat Model:
Throughout this paper, we will consider the server as an honest-but-curious entity, while the learning participants as honest entities. This is relevant to the scenario in which learning participants are considered as organizations such as financial institutions or hospitals2 acting with responsibilities and by-laws. Below we will discuss other works, particularly [28], in that threat model.

C. Technical Overviews
A succinct comparison is found in Table I. Below we present the underlying technicalities.

1) Asynchronous SGD (ASGD) [16], [27], No Privacy Protection:
Both our system and that of [28] rely on the fact that neural networks can be trained via a variant of SGD called asynchronous SGD [16], [27] with data parallelism and model parallelism. Specifically, first a global weight vector Wglobal for the neural network is initialized randomly. Then, at each iteration, replicas of the neural network are run over local datasets (i.e. data parallelism), and the corresponding local gradient vector Glocal is sent to the cloud server. For each Glocal , the cloud server then updates the global parameters as follows:
Wglobal:=Wglobal−α⋅Glocal(1)
View Sourcewhere α is a learning rate. The updated global parameters Wglobal are broadcast to all replicas, who then use them to replace their old weight parameters. The process of updating and broadcasting Wglobal is repeated until a desired minimum for a pre-defined cost function (based on cross-entropy or squared-error) is reached. For model parallelism, the update at (1) is computed in parallel via components of the vectors Wglobal and Glocal .

2) Shokri-Shmatikov Systems:
The system in [28, Sec. 5] can be called gradients-selective ASGD for the following reasons. In [28, Sec. 5], the update rule at (1) is modified as follows:
Wglobal:=Wglobal−α⋅Gselectivelocal(2)
View Sourcein which vector Gselectivelocal contains selective (say 1% ~ 100%) gradients of Glocal . The update using (2) allows each participant to choose which gradients to share globally, with the hope of reducing the risk of leaking sensitive information on the participant’s local dataset to the cloud server. However, as showed in Section III, a small portion of gradients leaks information to the server.

In [28, Sec. 7], Shokri-Shmatikov showed an additional technique on using differential privacy to counteract indirect leakage from gradients. Their strategy was to add Laplace noises into Gselectivelocal at (2). Due to noises, this method harms learning accuracy which is the main appeal of deep learning.

3) Our System:
Our system can be called gradients-encrypted ASGD for the following reasons. In our system in Section IV, we make use of the following update formula
E(Wglobal):=E(Wglobal)+E(−α⋅Glocal)(3)
View Sourcein which E is homomorphic encryption supporting addition over ciphertexts. The decryption key is only known to the participants and not to the cloud server. Therefore, the honest-but-curious cloud server knows nothing about each Glocal , and hence obtains no information on each local dataset of participants. Nonetheless, as
E(Wglobal)+E(−α⋅Glocal)=E(Wglobal−α⋅Glocal)
View Sourceby the additively homomorphic property of E , each participant will get the correctly updated Wglobal via decryption. Moreover, when the original update at (1) is parallelized via components of the vectors Wglobal and Glocal , our system applies homomorphic encryption to each of the components accordingly.

In addition, to ensure the integrity of the homomorphic ciphertexts, each client will use a secure channel such as TLS/SSL (distinct from each other) to communicate the homomorphic ciphertexts with the server.

4) Extension of Our System:
Our idea of using the encrypted update rule as in (3) can be extended to other SGD-based machine learning methods. For example, our system can readily be used with logistic regression with distributed learning participants who each hold a local dataset. In this case, the only change is that each participant will run the SGD-based logistic regression instead of the neural network of deep learning.

D. More Related Works
Gilad-Bachrach et al. [18] present a system called CryptoNets, which allows homomorphically encrypted data feedforwarding an already-trained neural network. Because CryptoNets assumes that the weights in the neural network have been trained beforehand, the system aims at making prediction for individual data items.

The goal of our paper and [28] differ from that of [18], as our system and Shokri-Shmatikov’s exactly aim at training the weights via multiple data sources, whereas CryptoNets [18] does not.

We consider the cloud server as the adversary in this paper while learning participants are seen as honest entities. Our scenario and adversary model is different from that of Hitaj et al. [20] which examines dishonest learning participants.

Mohassel and Zhang [23] examine privacy-preserving methods for linear regression, logistic regression and neural network training over two servers which are assumed to have not colluded. Their model is different from ours.

In the same research line of only using additively homomorphic encryption, privacy-preserving linear/logistic regression systems have been proposed in [8] and [10].

Using secret sharing, Bonawitz et al. [14] proposes a secure aggregation method and applies it to deep neural networks to aggregate user-provided model updates. In particular, the work [14] tries to deal with many (e.g., 214 = 16384 in the experiments or perhaps more practically) mobile devices so that dropouts (failures in completing the protocol) can be frequent. In contrast, we would like to have multiple (e.g., 10) data providers as organizations such as hospitals or banks each of which holds a large dataset on patients or customers so that dropouts are not considered. For examples, our setting closely captures the situation in medical scenario mentioned in Footnote 2. Another application for our case is that multiple banks and/or credit card companies would like to train over a joint dataset of all (e.g., for the task of fraud detection) but they concern about the curiosity of the central server on their plain and sensitive data.

Due to the tool used (secret sharing in [14] vs. additively homomorphic encryption in ours), the condition for security changes, for example honest majority in [14] vs. computational assumption (LWE) in ours. We do not have any threshold for correctness while [14] does. This diversity is good so that choices are ready for specific applications.

The issue of information leakage from the trained model is also important, and yet it is orthogonal to this work. A common tool to address this issue is differential privacy (often with accuracy decline due to noise added), as used in [6] for deep learning.

E. Improvements on the Conference Version
A preliminary version of this paper was at [26]. This full version generalizes the main system, adding multiple processing units at the honest-but-curious server, and allowing the learning participants to upload and download parts of encrypted gradients. In addition, communication costs are revised and computational costs are given anew.

SECTION II.Preliminaries
A. On Additively Homomorphic Encryption
Definition 1 (Homomorphic Encryption):
Public key additively homomorphic encryption (PHE) schemes consist of the following (possibly probabilistic) poly-time algorithms.

ParamGen(1λ)→pp : λ is the security parameter and the public parameter pp is implicitly fed in following algorithms.

KeyGen(1λ)→(pk,sk) : pk is the public key, while sk is the secret key.

Enc(pk,m)→c : probabilistic encryption algorithm produces c , which is the ciphertext of message m .

Dec(sk,c)→m : decryption algorithm returns message m encrypted in c .

Add(c,c′) : for ciphertexts c and c′ , the output is the encryption of plaintext addition cadd .

DecA(sk,cadd) : decrypting cadd to obtain an addition of plaintexts.

Ciphertext indistinguishability against chosen plaintext attacks [19] (or CPA security for short below) ensures that no bit of information is leaked from ciphertexts.

B. On Deep Machine Learning
1) Some Concepts and Notations:
Deep machine learning can be seen as a set of techniques applied to neural networks. Figure 1 shows a neural network with 5 inputs, 2 hidden layers, and 2 outputs. The node with +1 represents the bias term. The neuron nodes are connected via weight variables. In a deep learning structure of neural network, there can be multiple layers each with thousands of neurons.


Fig. 1.
A neural network with 5 inputs, 2 hidden layers, 2 outputs.

Show All

Each neuron node (except the bias node) is associated with an activation function f . Examples of f in deep learning are f(z)=max{0,z} (rectified linear), f(z)=ez−e−zez+e−z (hyperbolic tangent), and f(z)=(1+e−z)−1 (sigmoid). The output at layer l+1 , denoted as a(l+1) , is computed as a(l+1)=f(W(l)a(l)+b(l)) in which (W(l),b(l)) are the weights connecting layers l and l+1 , and a(l) is the output at layer l .

The learning task is, given a training dataset, to determine these weight variables to minimize a pre-defined cost function such as the cross-entropy or the squared-error cost function [3]. The cost function can be computed over all data items in the training dataset; or over a subset (called mini-batch) of t elements from the training dataset. Denote the cost function for the latter case as J|batch|=t . In the extreme case of t=1 , corresponding to maximum stochasticity, J|batch|=1 is the cost function defined over 1 single data item.

2) Stochastic Gradient Descent (SGD):
Let W be the flattened vector consisting of all weight variables. Namely we take all weights in the neural network and arrange them consecutively to form the vector W . Denote W=(W1,…,Wngd)∈Rngd . Let
G=(δJ|batch|=tδW1,…,δJ|batch|=tδWngd)(4)
View Sourcebe the gradients of the cost function J|batch|=t corresponding to variables W1,…,Wngd . The variable update rule in SGD is as follows, for a learning rate α∈R :
W:=W−α⋅G(5)
View Sourcein which α⋅G is component-wise multiplication, namely α⋅G=(αG1,…,αGngd)∈Rngd . The learning rate α can also be changed adaptively as described in [3] and [17].

3) Asynchronous (aka. Downpour) SGD [16], [27]:
By (4) and (5), as long as the gradients G can be computed, the weights W can be updated. Therefore, the data used in computing G can be distributed (i.e., data parallelism). Moreover, the update process can be parallelized by considering separate components of the vectors (i.e., model parallelism).

Specifically, as in Figure 2, asynchronous SGD uses multiple replicas of a neural network. Before each execution, each replica downloads the newest weights from the parameter server; and each replica is run over a data shard, which is a subset of the training dataset. To use the power of parallel computation when the server has multiple processing units PU1, … , PUnpu , asynchronous SGD splits the weight vector W and gradient vector G into npu parts, namely W=(W(1),…,W(npu)) and G=(G(1),…,G(npu)) , so that the update rule at (5) becomes as follows:
W(i):=W(i)−α⋅G(i)(6)
View SourceRight-click on figure for MathML and additional features.which is computed at processing unit PUi. As the processing units PU1, … , PUnpu can run in parallel, asynchronous SGD significantly increases the scale and speed of deep network training, as experimentally showed in [16].

Fig. 2. - Asynchronous SGD [16], [27].
Fig. 2.
Asynchronous SGD [16], [27].

Show All

SECTION III.Gradients Leak Information
This section shows that a small portion of gradients may reveal information on local data.

Example 1 (One Neuron):
For illustration of how gradients leak information on data, we first use the neural network in Figure 4, with only one neuron. In the figure, real numbers xi (1≤i≤d) are the input data, with a corresponding truth label y ; real numbers Wi (1≤i≤d) are the weight parameters to be learned; and b is the bias. The function f is an activation function (either sigmoid, rectified linear, or hyperbolic tangent as described in Section II-B). The cost function is defined as the distance between the predicted value hW,b(x)=deff(∑di=1Wixi+b) and the truth value y:
J(W,b,x,y)=def(hW,b(x)−y)2
View Sourceand hence the gradients are
ηk=def==δJ(W,b,x,y)δWk=2(hW,b(x)−y)δhW,b(x)δWk2(hW,b(x)−y)δf(∑di=1Wixi+b)δWk2(hW,b(x)−y)f′(∑i=1dWixi+b)⋅xk(7)
View Sourceand
η=def==δJ(W,b,x,y)δb=2(hW,b(x)−y)δhW,b(x)δb2(hW,b(x)−y)δf(∑di=1Wixi+b)δb2(hW,b(x)−y)f′(∑i=1dWixi+b)⋅1.(8)
View SourceThe k -th component xk of x=(x1,…,xd)∈Rd or the truth label y can be inferred from the gradients by one of the following means:

(O1) Observe that ηk/η=xk . Therefore, xk is completely leaked if ηk and η are shared to the cloud server. For example, if 1% of local gradients, chosen randomly as suggested in [28], are shared to the server, then the probability that both ηk and η are shared is (1/100)×(1/100)=1/104 , which is not negligible.

(O2) Observe that the gradient ηk is proportional to the input xk for all 1≤i≤d . Therefore, when x=(x1,…,xd) is an image, one can use the gradients to produce a related “proportional” image, and then obtain the truth value y by guessing.


Fig. 4.
A network with one neuron.

Show All

Example 2 (General Neural Networks, cf. Figure 3(b) ):
The above observations (O1) and (O2) similarly hold for general neural networks, with both cross-entropy and squared-error cost functions [3]. In particular, following [3],
ηik=defδJ(W,b,x,y)δW(1)ik=ξi⋅xk(9)
View Sourcewhere W(1)ik is the weight parameter connecting layer 1’s input xk with hidden node i of layer 2; ξi is a real number.

Fig. 3. - Original data (a) vs. leakage information (b), (c) from a small portion of gradients in a neural network. These images are best seen in color.
Fig. 3.
Original data (a) vs. leakage information (b), (c) from a small portion of gradients in a neural network. These images are best seen in color.

Show All

In Figure 3(b), using a neural network on [1], we demonstrate that gradients at (9) are indeed proportional to the original data, as Figure 3(b) only differs from Figure 3(a) at the value bar. The original data is a 20× 20 image, reshaped into a vector of (x1,…,x400)∈R400 . The vector is an input to a neural network of 1 hidden layer of 25 nodes; and the output layer contains 10 nodes. The total number of gradients in the neural network is
(400+1)×25+(25+1)×10=10285.
View SourceRight-click on figure for MathML and additional features.At (9), we have 1≤k≤400 and 1≤i≤25 . We then use a small part of the gradients at (9), namely (η1k)1≤k≤400 , reshaped into a 20× 20 image, to draw Figure 3(b). It is clear that the part (namely 400/10285≈3.89% ) of the gradients reveals the truth label 0 of the original data.

Example 3 (General Neural Networks, With Regularization, cf. Figure 3(c) ):
In a neural network with regularization, following [3] we have
ηik=defδJ(W,b,x,y)δW(1)ik=ηi=defδJ(W,b,x,y)δb(1)i=ξi⋅xk+λW(1)ikξi
View Sourcewhere notations are as in Example 2 above; b(1)i is the bias associated with node i of layer 2; and \lambda \geq 0 is a regularization term.

As in observation (O1), both \eta _{ik} and \eta _{i} are known to the server with a non-negligible probability. Additionally, in Figure 3(c), we use the following observation:\begin{equation*} \frac {\eta _{ik}}{\eta _{i}} = x_{k} + \frac {\lambda W_{ik}^{(1) }}{\xi _{i}} \end{equation*}
View Sourcewhich is an approximation of the data x_{k} . In Figure 3(c), we take \lambda = 0.1 , and other details identical to Example 2 above. Due to the term \lambda W_{ik}^{(1) }/\xi _{i} , there is noise in the figure, but the truth value (number 0) of the original data can still be seen.

Example 4 (Laplace Noises Added, cf. Figure 3(d) ):
As differential privacy and secrecy are orthogonal, adding Laplace noises to the gradients may not protect the secrecy of those gradients from the curious server. To illustrate, in Figure 3(d), we added Laplace noises of mean 0 and derivation \sigma = 1/100 to gradients \eta _{ik} at (9), namely \begin{equation*} \eta _{ik} = \xi _{i} \cdot x_{k} + Laplace\left ({0,\frac {1}{100}}\right) \end{equation*}
View Sourceso that the image contains noise, and yet the truth value can still be seen. In addition, when the parameter \sigma is larger, \eta _{ik} may be dominated by noise so that the truth label is difficult to be seen, but this may not help in updating the weight variables.

SECTION IV.Our System: Privacy-Preserving Deep Learning Without Accuracy Decline
Our system is depicted in Figure 5, consisting of a common cloud server and N (e.g. = 10 ~ 100) learning participants.

A. Learning Participants
The participants jointly set up the public key pk and secret key sk for an additively homomorphic encryption scheme. The secret key sk is kept confidential from the cloud server, but is known to all learning participants. Each participant establishes a TLS/SSL secure channel, different from each other, to communicate and protect the integrity of the homomorphic ciphertexts.

Then, the participants locally hold their datasets and run replicas of a deep learning-based neural network. The initial (random) weight W_{\mathrm{ global}} to run the local neural network is initialized by Participant 1 who also sends {\mathbf{E}}(W_{\mathrm{ global}}^{(1) }) , {\dots } , {\mathbf{E}}(W_{\mathrm{ global}}^{({n_{pu}})}) to the server initially, in which W_{\mathrm{ global}}^{(i)} is also a vector constituting the i -th part of W_{\mathrm{ global}} . The gradient vector G obtained after each execution of the neural network is split into {n_{pu}} parts, namely G = (G^{(1) }, {\dots }, G^{({n_{pu}})}) , multiplied by the learning rate \alpha , and then encrypted using the public key pk . The resulting encryption {\mathbf{E}}(-\alpha \cdot G^{(i)}) (\forall 1\leq i \leq {n_{pu}} ) from each learning participant is sent to the processing unit PUi of the server. It is also worth noting that the learning rate \alpha can be adaptively changed locally at each learning participant as described in [16].

As seen in Figure 5, each participant 1\leq k \leq N will perform the following steps:

Download the ciphertexts {\mathbf{E}}(W_{\mathrm{ global}}^{(j)}) stored at the processing units PUj of the server for all j\in I_{k}^{({\mathrm{ down}})} \subset [1, {n_{pu}}] . Usually I_{k}^{({\mathrm{ down}})} = [1, {n_{pu}}] , namely the participant will download all encrypted parts of the global weight, but it is possible that I_{k}^{({\mathrm{ down}})} \varsubsetneq [1, {n_{pu}}] if the learning participant has restricted download bandwidth.

Decrypt the above ciphertexts using the secret key sk to obtain W_{\mathrm{ global}}^{(j)} for all j\in I_{k}^{({\mathrm{ down}})} , and replace those values into the corresponding places of W_{\mathrm{ global}} = (W_{\mathrm{ global}}^{(1) }, {\dots }, W_{\mathrm{ global}}^{({n_{pu}})}) .

Get a mini-batch of data from its local dataset.

Using the values of W_{\mathrm{ global}} and data items at steps 2 and 3, compute the gradients G = (G^{(1) }, {\dots }, G^{({n_{pu}})}) with respect to variable W_{\mathrm{ global}} .

Encrypt and send back the ciphertexts {\mathbf{E}}(-\alpha \cdot G^{(i)}) \,\,\forall i \in I_{k}^{({\mathrm{ up}})} \subset [1, {n_{pu}}] to the corresponding processing unit PUi of the server. The subset for uploading I_{k}^{({\mathrm{ up}})} \subset [1, {n_{pu}}] = \{1, {\dots }, {n_{pu}}\} depends on the choice of participant k . For a full upload, I_{k}^{({\mathrm{ up}})} = [1, {n_{pu}}] , so that all encrypted gradients are uploaded to the server.

The downloads and uploads of the encrypted parts of W_{\mathrm{ global}} can be asynchronous in two aspects: the participants are independent with each other; and the processing units are also independent with each other.
B. Cloud Server
The cloud server is a common place to recursively update the encrypted weight parameters. In particular, each processing unit PUi at the server, after receiving any encryption {\mathbf{E}}(-\alpha \cdot G^{(i)}) , computes \begin{equation*} {\mathbf{E}}(W_{\mathrm{ global}}^{(i)}) + {\mathbf{E}}(-\alpha \cdot G^{(i)}) \left ({{\mathrm{ which ~is }}={\mathbf{E}}(W_{\mathrm{ global}}^{(i)} - \alpha \cdot G^{(i)})}\right) \end{equation*}
View Sourcewhere the equality is ensured by the additively homomorphic property of encryption. Therefore, the part W_{\mathrm{ global}}^{(i)} at the processing unit PUi is updated to W_{\mathrm{ global}}^{(i)} - \alpha \cdot G^{(i)} , or notationally W_{\mathrm{ global}}^{(i)}:= W_{\mathrm{ global}}^{(i)} - \alpha \cdot G^{(i)} .

Theorem 1 (Security Against the Cloud Server):
Our system in Figure 5 leaks no information on the datasets to the honest-but-curious cloud server, provided that the underlying homomorphic encryption scheme is CPA-secure.

Proof:
The participants only send encrypted gradients to the cloud server. Therefore, if the encryption scheme is CPA-secure, no bit of information on the data of the participants can be leaked.

Theorem 2 (Accuracy Equivalence to Asynchronous SGD):
Our system in Figure 5, functions as asynchronous SGD described in Section II-B when all ciphertexts are decrypted and I_{k}^{({\mathrm{ up}})} = I_{k}^{({\mathrm{ down}})} = [1, {n_{pu}}]\,\,\forall 1\leq k \leq N (meaning all gradients are uploaded and downloaded). Therefore, our system can achieve the same accuracy as that of asynchronous SGD.

Proof:
After decryption, the update rule of weight parameter becomes W_{\mathrm{ global}}^{(i)}:= W_{\mathrm{ global}}^{(i)} - \alpha \cdot G^{(i)} for any 1\leq i \leq {n_{pu}} in which G^{(i)} is the gradient vector computed from data samples held by participant k (and the downloaded W_{\mathrm{ global}} ). Since the update rule is identical to (6) and each learning participant in our system functions as a replica (as in asynchronous SGD) when encryption is removed, the theorem follows.

Remark 1:
As learning participants connect to the server via distinct TLS/SSL channels, the learning participants will be only able to retrieve the neural network weights but not the gradients, assuming that the server and any leaning participant do not collude. This helps to control the information sharing among learning participants, because deriving any information on data from the model becomes a model inversion problem.

SECTION V.Instantiations of Our System
In this section we use additively homomorphic encryption schemes to instantiate our system in Section IV. We use the following schemes to show two instantiations of our system: LWE-based encryption (modern, potentially post-quantum), and Paillier encryption (classical, smaller key sizes, not post-quantum).

A. Using an LWE-Based Encryption
The mark {\:\stackrel {{\scriptscriptstyle \hspace {0.2em}{\mathrm{ \scriptscriptstyle g}}}}{\leftarrow }\:} is for “sampling randomly from a discrete Gaussian distribution”, so that x {\:\stackrel {{\scriptscriptstyle \hspace {0.2em}{\mathrm{ \scriptscriptstyle g}}}}{\leftarrow }\:} \mathbb {Z} _{(0,s)} means x appears with a probability proportional to \exp (-\pi x^{2}/s^{2}) .

1) LWE-Based Encryption:
We use an additively homomorphic variant [9], [11] of the public key encryption scheme in [21]. The CPA security of this scheme is recalled in Appendix A.

{\sf ParamGen}(1^{\lambda }) : Fix q = q(\lambda)\in \mathbb {Z} ^{+} and l \in \mathbb {Z} ^{+} . Fix p \in \mathbb {Z} ^{+} so that gcd(p,q)=1 . Return pp\,\,= (q,l,p) .

{\sf KeyGen}(1^{\lambda },pp) : Take s = s(\lambda,pp)\in \mathbb {R} ^{+} and {n_{lwe}}\in \mathbb {Z}^{+} . Take R, S {\:\stackrel {{\scriptscriptstyle \hspace {0.2em}{\mathrm{ \scriptscriptstyle g}}}}{\leftarrow }\:} \mathbb {Z} _{(0,s)}^{ {n_{lwe}}\times l} , A {\:\stackrel {{\scriptscriptstyle \hspace {0.2em}{\\$}}}{\leftarrow }\:} \mathbb {Z} _{q}^{ {n_{lwe}}\times {n_{lwe}}} . Compute P = pR-AS \in \mathbb {Z} _{q}^{ {n_{lwe}}\times l} . Return the public key pk = (A, P, {n_{lwe}}, s) , and the secret key sk = S .

{\sf Enc}(pk,m\in \mathbb {Z} _{p}^{1\times l}) : Take e_{1}, e_{2} {\:\stackrel {{\scriptscriptstyle \hspace {0.2em}{\mathrm{ \scriptscriptstyle g}}}}{\leftarrow }\:} \mathbb {Z} _{(0,s)}^{1\times {n_{lwe}} } , e_{3} {\:\stackrel {{\scriptscriptstyle \hspace {0.2em}{\mathrm{ \scriptscriptstyle g}}}}{\leftarrow }\:} \mathbb {Z} _{(0,s)}^{1\times l} . Compute c_{1} = e_{1}\,\,A + pe_{2} \in \mathbb {Z} _{q}^{1\times {n_{lwe}} } , c_{2} = e_{1}\,\,P + pe_{3} + m \in \mathbb {Z} _{q}^{1\times l} . Return c = (c_{1}, c_{2}) .

{\sf Dec}(S, c = (c_{1}, c_{2})) : Compute \overline {m} = c_{1}\,\,S + c_{2}\in \mathbb {Z} _{q}^{1\times l} . Return m = \overline {m} \bmod p .

{\sf Add}(c, c') : For addition, compute and return c_{\mathrm{ add}} = c+ c' \in \mathbb {Z} _{q}^{1\times ({n_{lwe}}+l)} .

2) Data Encoding and Encryption:
A real number a\in \mathbb {R} can be represented, with {\sf prec} bits of precision, by an integer \lfloor a\cdot 2^{\sf prec} \rfloor \in \mathbb {Z} . Let us realize the encryption {\mathbf{E}}(\cdot) in Figure 5. Because both W_{\mathrm{ global}}^{(i)} and \alpha \cdot G^{(i)} are in the space \mathbb {R}^{L_{i}} where L_{i} is the length of the partition so that \sum _{i=1}^{ {n_{pu}}} L_{i} = n_{gd} , it suffices to describe an encryption of a real vector r = (r^{(1) }, {\dots }, r^{(L_{i})}) \in \mathbb {R} ^{L_{i}} . The encryption is, for l = L_{i} , \begin{equation} {\mathbf{E}}(r) = {\sf lweEnc}_{pk}\Big (\overbrace {\underbrace {\lfloor r^{(1) }\cdot 2^{\sf prec} \rfloor }_{\in \mathbb {Z} _{p}}\cdots \underbrace {\lfloor r^{(L_{i})}\cdot 2^{\sf prec} \rfloor }_{\in \mathbb {Z} _{p}}}^{ \mathbb {Z}_{p}^{1\times L_{i}}}\Big).\quad \end{equation}
View SourceFor vectors r,t \in \mathbb {R} ^{L_{i}} , the decryption of \begin{equation} {\mathbf{E}}(r)+ {\mathbf{E}}(-t) \in \mathbb {Z} _{q}^{1\times ({n_{lwe}}+L_{i})} \end{equation}
View Sourcewill yield, for all 1\leq j \leq L_{i} , \begin{equation} \lfloor r^{(j)}\cdot 2^{\sf prec} \rfloor - \lfloor t^{(j)}\cdot 2^{\sf prec} \rfloor \in \mathbb {Z}_{p} \subset \big (-p/2, p/2\big ]\quad \end{equation}
View Sourceand hence \begin{equation} u^{(j)}= \lfloor r^{(j)}\cdot 2^{\sf prec} \rfloor - \lfloor t^{(j)}\cdot 2^{\sf prec} \rfloor \in \mathbb {Z} \end{equation}
View Sourceif p/2 is large enough (see below). The subtraction r^{(j)}-t^{(j)}\in \mathbb {R} is computed via u^{(j)}/2^{\sf prec}\in \mathbb {R} , so that finally r-t\in \mathbb {R} ^{L} is obtained after decryption as desired. To get (13) from (12), it suffices that p/2 > 2\cdot 2^{\sf prec} , as via normalisation, we can assume -1< r^{(j)}, t^{(j)} <1 . In general, to handle {n_{\scriptscriptstyle \rm gradupd}} additive terms without overflow, it is necessary that p/2 > {n_{\scriptscriptstyle \rm gradupd}}\cdot 2^{\sf prec} , or equivalently, \begin{equation} p> {n_{\scriptscriptstyle \rm gradupd}}\cdot 2^{\sf prec+1}. \end{equation}
View Source

Lemma 1 (Choosing Parameters):
When {n_{lwe}}\geq 3000 , s = 8 , it is possible to set \begin{align*} \log _{2}~q\approx&\log _{2}~p + \log _{2} {n_{\scriptscriptstyle \rm gradupd}} \\&+ \log _{2}(167.9\sqrt { {n_{lwe}}} + 33.9) + 1 \end{align*}
View Sourcein which p satisfies (14), and {n_{\scriptscriptstyle \rm gradupd}} is the number of gradient updates at each processing unit of the cloud server in Figure 5. For example, when {n_{lwe}}= 3000 , p = 2^{48} + 1 , {n_{\scriptscriptstyle \rm gradupd}}= 2^{15} , it is possible to set q = 2^{77} .

It is worth noting that the parameters in Lemma 1 is very conservative, due to the consideration of unlikely large noise in the proof. Therefore, {n_{\scriptscriptstyle \rm gradupd}} can be larger than stated. Indeed, we experimentally check with q = 2^{77} , {n_{lwe}}= 3000 , p = 2^{48} + 1 , s=8 and confirm that {n_{\scriptscriptstyle \rm gradupd}} can be twice (i.e. {n_{\scriptscriptstyle \rm gradupd}}= 2\cdot 2^{15} ) of that stated in Lemma 1 without any decryption error.

Theorem 3 (Increased Communication Factor, LWE-Based):
The communication between the server and participants of our system is \begin{equation*} \frac { {n_{pu}} {n_{lwe}} \log _{2}~q}{n_{gd}\cdot {\sf prec} } + \frac {\log _{2}~q}{\sf prec} \end{equation*}
View SourceRight-click on figure for MathML and additional features.time of the communication of the corresponding asynchronous SGD, in which ({n_{lwe}}, p, q) is parameters of the encryption scheme, and n_{gd} is the number of gradient variables represented by {\sf prec} bits.

Proof:
In asynchronous SGD (Section II-B), each replica sends n_{gd} = \sum _{i=1}^{ {n_{pu}}} L_{i} gradients (each of {\sf prec} bits) to the parameter server at each iteration, so that the communication cost for one iteration in bits is \begin{equation*} {\sf PlainBits} = n_{gd}\cdot {\sf prec}. \end{equation*}
View SourceIn our system, we compute the ciphertext length that each participant sends to the cloud parameter server at each iteration. By (10), the ciphertext sent to processing unit PUi is in \mathbb {Z}_{q}^{1\times ({n_{lwe}}+L_{i})} so that its length in bits is ({n_{lwe}}+ L_{i})\log _{2}\,\,q . The length in bits of all ciphertexts sent to the parameter server from the corresponding learning participant at each interaction is at most \begin{align*} {\sf EncryptedBits}=&\sum _{i=1}^{ {n_{pu}}} ({n_{lwe}}+ L_{i})\log _{2}~q\\=&{n_{pu}} {n_{lwe}} \log _{2}~q + n_{gd}\log _{2}~q. \end{align*}
View SourceTherefore, the increased factor is \begin{equation*} \frac {\sf EncryptedBits}{\sf PlainBits} = \frac { {n_{pu}} {n_{lwe}} \log _{2}~q}{n_{gd}\cdot {\sf prec} } + \frac {\log _{2}~q}{\sf prec} \end{equation*}
View Sourceending the proof.

B. Using Paillier Encryption
1) Paillier Encryption:
With public key pk = n (a large positive integer), the encryption of an integer m\in \{0, {\dots }, n-1\} is {\sf PaiEnc}_{pk}(m) = r^{n} (1+n)^{m} \mod n^{2} in which r is chosen randomly from \{0, {\dots }, n-1\} . The encryption is additively homomorphic because the ciphertext product {\sf PaiEnc}_{pk}(m_{1}) {\sf PaiEnc}_{pk}(m_{2}) \mod n^{2} becomes an encryption of m_{1}+m_{2} \mod n . For decryption and CPA security, see the paper [25].

2) Packing Data in Encryption:
As the plaintext space has \log _{2}~n \geq 2048 bits, we can pack many non-negative integers I_{1}, {\dots }, I_{t} each of {\sf prec} bits into each Paillier plaintext as follows.\begin{equation*} {\sf PaiEnc}_{pk}\Big (\overbrace {\underbrace {[I_{1} 0_{\sf pad}]}_{ {\sf prec} + {\sf pad}~ \rm bits}\cdots \underbrace {[I_{t}0_{\sf pad}]}_{ {\sf prec} + {\sf pad}~ \rm bits}}^{\lfloor \log _{2}~n\rfloor ~\rm bits}\Big) \end{equation*}
View Sourcein which 0_{\sf pad} is the zero padding of {\sf pad} bits, which helps to prevent overflows in ciphertext additions. Typically, {\sf pad}\approx \log _{2} {n_{\scriptscriptstyle \rm gradupd}} as we need {n_{\scriptscriptstyle \rm gradupd}} additions of ciphertexts. Moreover, as the number of plaintext bits must be less than \log _{2}~n , it is necessary that t({\sf prec}+ {\sf pad}) \leq \log _{2}\,\,n . Therefore, \begin{equation*} t = \left \lfloor{ \frac {\lfloor \log _{2}~n \rfloor }{\sf prec+ {\sf pad}}}\right \rfloor \end{equation*}
View Sourcewhich is the upper-bound of packing {\sf prec} -bit integers into one Paillier plaintext. To handle both negative and positive integers, we can use the bijection z \in [0, 2^{\sf prec}-1] \mapsto z-\lfloor z /2^{\sf prec}\rceil \cdot 2^{\sf prec} .

As a real number 0\leq r<1 can be represented as an integer of form \lfloor r\cdot 2^{\sf prec}\rfloor , the above packing method can be used to encrypt around \lfloor \log _{2}\,\,n \rfloor /({\sf prec}+ {\sf pad}) real numbers in the range [0, 1) with precision {\sf prec} , tolerating around 2^{\sf pad} ciphertext additions. To realise the encryption {\mathbf{E}}(\cdot) in Figure 5, it suffices to describe an encryption of a real vector r = (r^{(1) }, {\dots }, r^{(L_{i})}) \in \mathbb {R} ^{L_{i}} because both W_{\mathrm{ global}}^{(i)} and \alpha \cdot G^{(i)} \in \mathbb {R} ^{L_{i}} in which \sum _{i=1}^{ {n_{pu}}} L_{i} = n_{gd} . The encryption {\mathbf{E}}(r) consists of approximately \lceil L_{i}/t \rceil Paillier ciphertexts: \begin{align*}&~~{\sf PaiEnc}_{pk}\Big (\overbrace {\underbrace {\lfloor r^{(1) } \cdot 2^{\sf prec}\rfloor 0_{\sf pad}}_{ {\sf prec} + {\sf pad}\,\, \rm bits}\cdots \underbrace {\lfloor r^{(t)} \cdot 2^{\sf prec}\rfloor 0_{\sf pad}}_{ {\sf prec} + {\sf pad}\,\, \rm bits}}^{\lfloor \log _{2}\,\,n\rfloor \,\,\rm bits}\Big), {\dots }, \\& {\sf PaiEnc}_{pk}\Big (\overbrace {\underbrace {\lfloor r^{(L_{i}-t+1)} \cdot 2^{\sf prec}\rfloor 0_{\sf pad}}_{ {\sf prec} + {\sf pad}\,\, \rm bits}\cdots \underbrace {\lfloor r^{(L_{i})} \cdot 2^{\sf prec}\rfloor 0_{\sf pad}}_{ {\sf prec} + {\sf pad}\,\, \rm bits}}^{\lfloor \log _{2}\,\,n\rfloor \,\,\rm bits}\Big).\end{align*}
View SourceRight-click on figure for MathML and additional features.

Theorem 4 (Increased Communication Factor, Paillier-Based):
The communication between the server and participants of our system is \begin{equation*} 2\left ({1+\frac {\sf pad}{\sf prec}}\right) \end{equation*}
View SourceRight-click on figure for MathML and additional features.times of the communication of the corresponding asynchronous SGD, in which {\sf pad} is the number of 0 paddings to tolerate 2^{\sf pad} additions (equal to the number of gradient updates on the server), and {\sf prec} is the bit precision of numbers.

Proof:
In our system in Figure 5, each participant needs to encrypt and send at most \sum _{i=1}^{ {n_{pu}}} L_{i} = n_{gd} gradients to the server, in which L_{i} is the length of G^{(i)} . Therefore, with the above packing method for Paillier encryption, the number of Paillier ciphertexts sent from each participant is around \begin{equation*} \sum _{i=1}^{ {n_{pu}}} \frac {L_{i}}{t} \approx \frac {n_{gd}}{t} \approx \frac {n_{gd} ({\sf prec}+ {\sf pad})}{\lfloor \log _{2}~n \rfloor }. \end{equation*}
View SourceSince each Paillier ciphertext is of 2\lfloor \log _{2}~n \rfloor bits, the number of bits for each communication is around \begin{equation*} {\sf EncryptedBits} \approx 2\lfloor \log _{2}~n \rfloor \cdot \frac {n_{gd}}{t} \approx 2n_{gd}({\sf prec}+ {\sf pad}). \end{equation*}
View SourceOn the other hand, note that each replica in asynchronous SGD needs to send n_{gd} gradients each of {\sf prec} bits to the server, the communication cost in bits is {\sf PlainBits} = n_{gd}\cdot {\sf prec} . Therefore, the increased factor is \begin{equation*} \frac {\sf EncryptedBits}{\sf PlainBits} = \frac {2n_{gd}({\sf prec}+ {\sf pad})}{n_{gd}\cdot {\sf prec} } = 2\left ({1+\frac {\sf pad}{\sf prec}}\right) \end{equation*}
View Sourceas claimed.

For the case of Paillier encryption, we can take {\sf pad}= 15 , so that the increased communication factor becomes, via Theorem 4, \begin{equation*} \frac {\sf EncryptedBits}{\sf PlainBits} = 2\left ({1+\frac {\sf pad}{\sf prec}}\right) = 2\left ({1+\frac {15}{32}}\right)\approx 2.93 \end{equation*}
View Sourcewhich is independent of n_{gd} . This increased communication factor is a little larger than that of the LWE-based one when n_{gd} is big as evaluated below.

SECTION VI.Concrete Evaluations With an LWE-Based Encryption
We take {n_{lwe}}= 3000 , s=8 , p = 2^{48} + 1 , {n_{\scriptscriptstyle \rm gradupd}}= 2^{15} , and q = 2^{77} following Lemma 1. These parameters for ({n_{lwe}}, s, q) conservatively ensure that the LWE assumption has at least 128-bit security according to recent attacks [7], [15], [21], [22]. We will take {n_{pu}}\in \{1,10\} , depending on the number of gradients.

A. Increased Factors in Communication
Let us consider multiple number of gradient parameters n_{gd} :

n_{gd} = 109386 : this number of gradient parameters is used with the dataset MNIST [5]. Specifically, consider a multilayer perceptron with form 784 (input) – 128 (hidden) – 64 (hidden) – 10 (output). The number of gradients for this network is (784+1)128+(128+1)64+(64+1)10 = 109386. We will consider cases where real numbers are represented by 32 bits, so that {\sf prec}= 32 . Theorem 3 tells us that the increased communication factor between our system and the related asynchronous SGD is \begin{equation*} \frac { {n_{pu}} {n_{lwe}} \log _{2}~q}{n_{gd}\cdot {\sf prec} } + \frac {\log _{2}~q}{\sf prec} = \frac {1\cdot 3000\cdot 77}{109386\cdot 32} + \frac {77}{32} \approx 2.47. \end{equation*}
View SourceRight-click on figure for MathML and additional features.

n_{gd} = 402250 : this is used in [28] with the dataset SVHN [24]. The increased communication factor becomes \begin{equation*} \frac { {n_{pu}} {n_{lwe}} \log _{2}~q}{n_{gd}\cdot {\sf prec} } + \frac {\log _{2}~q}{\sf prec} = \frac {1\cdot 3000\cdot 77}{402250\cdot 32} + \frac {77}{32} \approx 2.42. \end{equation*}
View SourceRight-click on figure for MathML and additional features.

n_{gd} = 42\cdot 10^{6} : this number of gradient parameters is used in [16] for speech data. As the number of gradients is large, consider {n_{pu}}= 10 . The increased communication factor becomes \begin{equation*} \frac { {n_{pu}} {n_{lwe}} \log _{2}~q}{n_{gd}\cdot {\sf prec} } + \frac {\log _{2}~q}{\sf prec} = \frac {10\cdot 3000\cdot 77}{42\cdot 10^{6}\cdot 32} + \frac {77}{32} \approx 2.4. \end{equation*}
View SourceRight-click on figure for MathML and additional features.

B. Estimating the Computational Costs
To estimate the running time of our system, we use the following formulas \begin{align} {\mathbf{T}}^{(i)}_{\mathrm{ ours,~one ~run}}=&{\mathbf{T}}^{(i)}_{\mathrm{ original,~one ~run}} + {\mathbf{T}}^{(i)}_{\mathrm{ enc}} + {\mathbf{T}}^{(i)}_{\mathrm{ dec}} \notag \\[-2pt]&+ {\mathbf{T}}^{(i)}_{\mathrm{ upload}} + {\mathbf{T}}^{(i)}_{\mathrm{ download}}+ {\mathbf{T}}_{\mathrm{ add}}, \\[-2pt] {\mathbf{T}}_{\mathrm{ our ~system}}=&\sum _{i=1}^{N} n_{\mathrm{ upload/download}}^{(i)}\! \times \! {\mathbf{T}}^{(i)}_{\mathrm{ ours,one\! ~run}}~\end{align}
View Sourcewhere, in (15), {\mathbf{T}}^{(i)}_{\mathrm{ ours,~one ~run}} is the running time of the participant i when doing the following: waiting for the addition of ciphertexts at the server ({\mathbf{T}}_{\mathrm{ add}} ); downloading the added ciphertext from the server ({\mathbf{T}}^{(i)}_{\mathrm{ download}} ); decrypting the downloaded ciphertext ({\mathbf{T}}^{(i)}_{\mathrm{ dec}} ); training with the downloaded weight ({\mathbf{T}}^{(i)}_{\mathrm{ original,~one ~run}} ); encrypting the resulting gradients ({\mathbf{T}}^{(i)}_{\mathrm{ enc}} ) and sending back that ciphertext to the server ({\mathbf{T}}^{(i)}_{\mathrm{ upload}} ). The total running time of our system {\mathbf{T}}_{\mathrm{ our ~system}} will be the sum of all N participants’ running times, multiplied by the number n_{\mathrm{ repeat}} of repetitions, as expressed in (16).

1) Environment:
Our codes for homomorphic encryption are in C++, and the benchmarks are over an Xeon CPU E5-2660 v3@ 2.60GHz server. To estimate the communication speed between each learning participants and the server, we assume a 1 Gbps network. To measure the running time of MLP, we use the Tensorflow 1.1.0 library [4] over Cuda-8.0 and GPU Tesla K40m.

2) Time of Encryption, Decryption, and Addition:
Table III gives the time for encryption, decryption and addition depending on the number of gradients n_{gd} , using {n_{lwe}}= 3000 , s=8 , p = 2^{48} + 1 , and q = 2^{77} . Figure 6 depicts the time of encryption and decryption using 1 thread and 20 threads of computation respectively.

TABLE III LWE-Based Encryption Scheme ( {n_{lwe}}= 3000 , s=8 , p = 2^{48} + 1 , and q = 2^{77} ) Running Times With Various Numbers of Gradients

Fig. 6. - Computational costs of the LWE-based encryption and decryption when 
${n_{lwe}}= 3000, s = 8, q = 2^{77}$
.
Fig. 6.
Computational costs of the LWE-based encryption and decryption when {n_{lwe}}= 3000, s = 8, q = 2^{77} .

Show All

3) Multilayer Perceptron (MLP):
Consider an MLP with form 784 (input) – 128 (hidden) – 64 (hidden) – 10 (output). The number of gradients for this network is (784 + 1)128 + (128+1)64+(64+1)10 = 109386. For 32-bit precision, these gradients are around 109386\times 32 / (8\times 10^{6}) \approx 0.437 MB in plain. The ciphertext of these gradients is of 0.437\times 2.47 \approx 1.0 MB as computed in Section VI-A which can be sent in around 0.008 seconds (= 8 ms) via an 1 Gbps communication channel. The original running time of this MLP with one data batch of size 50 (MNIST images) is {\mathbf{T}}^{(i)}_{\mathrm{ MLP,~one ~run}} \approx 4.6 (ms). Therefore, \begin{align} {\mathbf{T}}^{(i)}_{\mathrm{ ours,~one ~run}}=&{\mathbf{T}}^{(i)}_{\mathrm{ MLP,~one ~run}} + {\mathbf{T}}^{(i)}_{\mathrm{ enc}} + {\mathbf{T}}^{(i)}_{\mathrm{ dec}} \notag \\&+ {\mathbf{T}}^{(i)}_{\mathrm{ upload}} + {\mathbf{T}}^{(i)}_{\mathrm{ download}}+ {\mathbf{T}}_{\mathrm{ add}} \notag \\\approx&4.6 + 188.9 + 196.0 + 8 + 8 + 19.5/10^{3} \notag \\\approx&405.5 ~({\mathrm{ ms}}). \end{align}
View Source

Suppose that there are totally 2\times 10^{4} times of upload and download from all training participants to the server, namely \sum _{i=1}^{N} n_{\mathrm{ upload/download}}^{(i)} = 2\times 10^{4} . For simplicity suppose {\mathbf{T}}^{(i)}_{\mathrm{ ours,~one ~run}} is the same for all 1\leq i \leq N . In such case, the running time of our system can be estimated as \begin{align*} {\mathbf{T}}_{\mathrm{ our ~system}}=&{\mathbf{T}}^{(i)}_{\mathrm{ ours,~one ~run}} \times 2\times 10^{4} \\=&405.5\times 2\times 10^{4} ~({\mathrm{ ms}}) \approx 2.25 ~({\mathrm{ hours}}). \end{align*}
View SourceRight-click on figure for MathML and additional features.

As seen in Theorem 2, the accuracy of our system can be identical to that of asynchronous SGD. Therefore, it suffices to estimate the accuracy of asynchronous SGD over MNIST containing randomly shuffled 6\times 10^{4} images for training and 10^{4} images for testing. As above, the batch size is 50. The initial weights are randomly selected from a normal distribution of mean 0 and standard deviation 0.1 (via the function random_normal of Tensorflow). The activation function is the relu function in Tensorflow. The Adam optimizer (the AdamOptimizer in Tensorflow) is used for training with input learning rate 10^{-4} , with no drop out. After 2\times 10^{4} iterations elapsed by 2 minutes, our Tensorflow code achieves around 97% accuracy over the testing set.

4) Convolutional Neural Networks (CNN):
Thanks to Theorem 2, our system in Section IV has the same accuracy as asynchronous SGD, so that we can re-use the accuracy results in [28]. In particular, let us consider the CNN [28, Fig. 6] for MNIST with the number of gradients n_{gd} = 105506 . The corresponding accuracy reported in [28, Table 4, Fig. 8] is around 99% when 10% or more (100%) of the gradients are sent to the server. For the discussion of efficiency, let the number of training epoch be 35; the mini-batch size be 32; and the number of learning participant N = 30 (all used in [28]). Note that the number of gradients n_{gd} in this case is almost the same as that in the MLP case, so the speed of encryption and decryption and communication will be identical (or a little lesser). Therefore, we can safely re-use (17) so that \begin{align*} {\mathbf{T}}^{(i)}_{\mathrm{ ours,~one ~run}}=&{\mathbf{T}}^{(i)}_{\mathrm{ CNN,~one ~run}} + {\mathbf{T}}^{(i)}_{\mathrm{ enc}} + {\mathbf{T}}^{(i)}_{\mathrm{ dec}} \\&+ {\mathbf{T}}^{(i)}_{\mathrm{ upload}} + {\mathbf{T}}^{(i)}_{\mathrm{ download}}+ {\mathbf{T}}_{\mathrm{ add}} \\\approx&{\mathbf{T}}^{(i)}_{\mathrm{ CNN,~one ~run}} + 188.9 + 196.0 \\&+ 8 + 8 + 19.5/10^{3} \\\approx&{\mathbf{T}}^{(i)}_{\mathrm{ CNN,~one ~run}} + 401 ~({\mathrm{ ms}}) \end{align*}
View SourceSince the number of images in the training set is 60000 shared between N=30 leaning participants, each participant will hold 60000/30 = 2000 images. The number of mini-batch is 32, so that there will be about 2000/32 uploads and downloads from each participant in each epoch. Therefore there will be (2000/32)\times 30 \times 35 uploads and downloads with 30 participants in 35 epochs. The estimated running time of our system will be \begin{align*} {\mathbf{T}}_{\mathrm{ our ~system}}=&{\mathbf{T}}^{(i)}_{\mathrm{ ours,~one ~run}} \times (2000/32)\times 30 \times 35 \\\approx&\left ({{\mathbf{T}}^{(i)}_{\mathrm{ CNN,~one ~run}} + 401}\right) 65625 ~{\mathrm{ (ms)}}\\\approx&\left ({{\mathbf{T}}^{(i)}_{\mathrm{ CNN,~one ~run}} + 401}\right)\frac {65625}{36\times 10^{5}} {\mathrm{ (hours)}}\\\approx&0.018 {\mathbf{T}}^{(i)}_{\mathrm{ CNN,~one ~run}} + 7.3 ~{\mathrm{ (hours)}} \end{align*}
View SourceRight-click on figure for MathML and additional features.which is around 3.3 times that of the MLP case.

SECTION VII.Conclusion
In this paper, we have shown that sharing gradients even partially over a parameter cloud server as in [28] may leak information. We then proposed a new system that utilizes additively homomorphic encryption to protect the gradients against the curious server. In addition to privacy preservation, our system enjoys the property of not declining the accuracy of deep learning.