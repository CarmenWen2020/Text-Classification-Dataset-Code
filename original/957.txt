Face presentation attack detection (PAD) is essential for securing the widely used face recognition systems. Most of the existing PAD methods do not generalize well to unseen scenarios because labeled training data of the new domain is usually not available. In light of this, we propose an unsupervised domain adaptation with disentangled representation (DR-UDA) approach to improve the generalization capability of PAD into new scenarios. DR-UDA consists of three modules, i.e., ML-Net, UDA-Net and DR-Net. ML-Net aims to learn a discriminative feature representation using the labeled source domain face images via metric learning. UDA-Net performs unsupervised adversarial domain adaptation in order to optimize the source domain and target domain encoders jointly, and obtain a common feature space shared by both domains. As a result, the source domain PAD model can be effectively transferred to the unlabeled target domain for PAD. DR-Net further disentangles the features irrelevant to specific domains by reconstructing the source and target domain face images from the common feature space. Therefore, DR-UDA can learn a disentangled representation space which is generative for face images in both domains and discriminative for live vs. spoof classification. The proposed approach shows promising generalization capability in several public-domain face PAD databases.
SECTION I.Introduction
Face recognition (FR) has been widely used in various applications such as smartphone unlock, access control, and pay-with-face. Since a genuine user’s face image can be easily obtained by malicious users with a smartphone or from the social media, FR systems can be vulnerable to face presentation attacks (PA), e.g., printed photo, photo or video replay, and 3D mask [5]–[6][7]. Therefore, similar to biometric template protection [8], face PAD is a very important step for the existing FR systems, particularly for the face verification scenarios [9]–[10][11]. In recent years, a number of approaches have been proposed to detect various face presentation attacks, e.g., using hand-crafted features, deeply learned features, and auxiliary features.

Assuming that there are inherent disparities between live and spoof faces, most of the early PAD approaches utilized hand-crafted features to perform binary classification (live vs. spoof), e.g., using SVM classifiers [12]–[13][14][15][16][17][18]. These methods were found to be computationally efficient and work well under intra-database testing scenarios. With the success of deep learning such as Convolutional Neural Networks (CNNs) [19] in many computer vision tasks, recent PAD approaches seek to utilize CNNs for end-to-end face PAD or representation learning followed by binary classification models [20], [21]. For example, in [20], the features learned by deep learning show promising performance against the traditional hand-crafted feature based methods under intra-database testing scenarios. However, neither the hand-crafted feature based methods nor the deep feature based methods generalize very well to new application scenarios [11], [20] (see an example of deep feature learned for PAD in Fig. 1). The main reason is that the differences between live and spoof face images may include multiple aspects and different factors, such as skin detail, color distortion, moiré pattern, shape deformation, and texture artifacts. The presence of these factors in two domains (or databases) can be dramatically different; thus simply treating face PAD as a common two-class classification problem may not have good generalization ability. To improve the robustness of PAD methods under new scenarios, some scenario-invariant auxiliary information (such as face depth and heart rhythm) was leveraged to assist in live and spoof face classification [22], [23]. The performance of these methods relies on the accuracy of the estimated auxiliary information to some extent.


Fig. 1.
The t-SNE [1] visualization of the features learned by ResNet-18 [2] for live and spoof face image classification on CASIA [3] and Idiap [4]. The model trained using the training set of CASIA is used for feature extraction on (a) the testing set of CASIA (intra-database testing), and (b) the testing set of Idiap (cross-database testing). We observe that such a straightforward model may not generalize well under cross-database testing scenario.

Show All

Despite the tremendous progress on face PAD research, there are still limitations with existing methods: (i) While most face PAD approaches assume that the training and testing scenarios are similar in data distributions, there are often big disparities between them. This leads to poor generalization ability of PAD methods to practical application scenarios. (ii) There are various types of face presentation attacks; even for photo attack only, there can be printed photo, photo displayed on screen, etc. Therefore, it is not possible to build a labeled training set for each new application scenario, covering all possible presentation attacks. To address these issues, domain adaptation (DA) has been utilized to mitigate the gap between the target domain and the source domain during face PAD [24]–[25][26]. Recently, adversarial adaptation methods [25], [27] have sought to minimize an approximate domain discrepancy distance through an adversarial objective with respect to a domain discriminator because of the success of GAN [28].

In this paper, we focus on improving PAD generalization ability for cross-domain PAD1 and propose a novel end-to-end trainable PAD approach called Unsupervised Domain Adaptation with Disentangled Representation (DR-UDA) to leverage the unlabeled data in the target domain and labeled data in the source domain to build a robust PAD model. For example, in FR based access control systems, a large number of face images can be recorded everyday, which may include both live face images and presentation attacks. However, these recorded face images are unlabeled. We aim to leverage such unlabeled face images in target domain and the labeled data in the source domains to build a PAD model that can generalize well to the target domain. To achieve this goal, we first learn a PAD model from the labeled source domain and then adapt it to the unlabeled target domain by learning a common feature space shared by both the source and target domains. The common feature space is expected to be discriminative for the live and spoof face images and generative for reconstructing the face images in both domains. The proposed approach is end-to-end trainable, and achieves promising results in cross-database face PAD on several public-domain databases (Idiap Replay-Attack (Idiap) [4], CASIA Face Anti-Spoofing (CASIA) [3], MSU-MFSD (MSU) [16], ROSE-YOUTU [24], CASIA-SURF [29] and Oulu-NPU (Oulu) [30], [31]).

The main contributions of this work are three-fold: (i) a novel unlabeled domain adaptation approach that is able to leverage labeled source domain data and unlabeled target domain data to build robust PAD model; (ii) disentangled representation learning for obtaining domain independent features during domain adaptation; and (iii) promising PAD performance in a number of cross-database tests than the state-of-the-art approaches addressing generalized face PAD.

Our preliminary work was described in [32]. Essential improvements in this work include: (i) we propose a disentangled representation learning, which allows more domain independent knowledge to be transferred and used for distinguishing live vs. spoof face images in the unlabeled target domain; (ii) we provide extensive evaluations using more datasets in public domain, e.g., Idiap, CASIA, MSU, ROSE-YOUTU, CASIA-SURF and Oulu databases, and provide comparisons with more baselines for PAD, such as DRCN [33], ADDA [25], DupGAN [34], Auxiliary [26], De-spoof [35] and STASN [36]; and (iii) we have provided more details about our method implementation, experimental evaluation, and related work.

SECTION II.Related Work
In this section, we provide a brief discussion of the related face PAD approaches, which can be generally categorized into hand-crafted feature based methods and deep feature based methods. We also briefly review the domain adaptation methods for PAD.

A. Face PAD
1) Hand-Crafted Feature Based Methods:
Since most FR systems are using RGB sensors, print attack and replay attack become two major ways of presentation attacks. An early work by Li et al. [37] used Fourier spectra analysis to capture the difference between the live and spoof face images. After that, hand-crafted features such as LBP [15], LPQ [12], SURF [12], HoG [14], SIFT [18] and IDA [16] have been widely used with traditional classifiers, such as SVM [42] and LDA [43], for a binary classification. To reduce the influence of illumination variations, some approaches converted face images from RGB color space to other space such as HSV and YCbCr [12], [38], and then extracted the above features for PAD. Besides extracting texture features, a number of methods also explored motion cues of the whole face or individual face components for PAD, such as eye blink [44], mouth movement [45] and head rotation [46]. Optical flow field was computed for discriminating between 2D planes (i.e., a printed photo) and 3D objects (i.e., a 3D live face).

The hand-crafted feature based PAD approaches are usually computationally efficient and explainable, and work well under intra-database testing scenarios. However, their generalization ability to new application scenarios is still not satisfying [20].

The background motion was also utilized for PAD in [47], [48]. Dynamic textures were considered in [7] to extract different facial motions. Liu et al. [5], [6] proposed to estimate rPPG signals from RGB face videos to detect attacks. Motion cue based PAD methods are expected to be more robust than the texture feature based methods under challenging illumination conditions; however, motion based methods often require users’ cooperations, such as blinking eyes and rotating the head following specific instructions. Therefore, the system response time of these methods is usually longer than texture feature based PAD.

2) Deep Feature Based Methods:
Heading into the era of deep learning, researchers are attempting to utilize deep models for face PAD [20], [21], [49] because of the great success of deep learning in many other computer vision tasks. In [21], ImageNet pretrained CaffeNet [50] and VGG-face [51] models were fine-tuned with live and spoof face images for face PAD. Xu et al. [52] adopted Long Short-Term Memory (LSTM) and CNN to obtain spatial-temporal features for PAD. Boulkenafet et al. [30] introduced a new challenging face PAD database namely Oulu-NPU and organized a face PAD competition [31]. Liu et al. [23] designed a novel framework to leverage the auxiliary information of depth and heart rate from the face videos to assist in PAD. Jourabloo et al. [35] inversely decomposed a spoof face into a live face and a noise of spoof, and then utilized the spoof noise for PAD. Wang et al. [53] recovered depth information from a temporal sequence, and used it for PAD. Chen et al. [54] proposed an attention-based method, which applies the complementary features (RGB and MSR) extracted via CNN and then employed the attention based fusion method to fuse these two features. Zhang et al. [29] introduced a large-scale multi-modal face anti-spoofing dataset namely CASIA-SURF, and proposed a multi-modality PAD framework which achieved higher performance than each single modality. Subsequently, in [55]–[56][57] several end-to-end approaches have been proposed to exploit the complementary information contained in RGB, depth and IR, and all reported the promising results on CASIA-SURF. Yang et al. [36] proposed a spatio-temporal attention mechanism to fuse global temporal and local spatial information for PAD. Wang et al. [58] proposed an effective disentangled representation learning for cross-domain presentation attack detection, which consists of a disentangled representation learning module and a multi-domain feature learning module.

While deep feature based PAD methods show strong feature learning ability, and can be trained end-to-end, there are still inherent constraints when the training set and testing set have a big discrepancy. As a result, the generalization ability of deep feature based PAD methods is still not satisfying under new application scenarios.

B. Domain Adaptation for PAD
Domain adaptation (DA) aims to transfer the knowledge or model learned from a source domain to a target domain [59]. DA can be very useful when there is only limited training data in a new application scenario, and thus has received increasing attention in recent years [60].

Long et al. [61] proposed a Deep Adaptation Network (DAN) to map deep features into Reproducing Kernel Hilbert Spaces (RKHS). Then, they performed DA by minimizing the maximum mean discrepancy (MMD) [62]. Muhammad et al. [33] proposed a deep reconstruction-classification network (DRCN) to learn a common representation for both domains through the joint objective of supervised classification of labeled source data and unsupervised reconstruction of unlabeled target data. A number of approaches have utilized adversarial learning proposed in Generative Adversarial Networks (GANs) [28] to reduce the source and target domain discrepancy for better DA [25], [34], [63], [64].

Face PAD also suffers from the cross-domain discrepancy issue, e.g., the distributions of data from the training and testing domains are different w.r.t. facial appearance, pose, illumination, sensor, etc. Therefore, recent studies for face PAD also attempted to utilize domain adaptation and domain generalization to overcome the poor generalization issues. Yang et al. [65] proposed a person-specific face anti-spoofing approach based on a subject-specific domain adaptation method to synthesize virtual features, which assumes that the relationship between live and spoof face images of the same subject can be formulated as a linear transformation. Li et al. [24] proposed an unsupervised domain adaptation PAD framework to transform the source domain feature space to the unlabeled target domain feature space by minimizing MMD. Shao et al. [41] proposed to learn a generalized feature space via a novel multi-adversarial discriminative deep domain generalization framework under a dual-force triplet-mining constraint.

A summary of the representative face presentation attack detection methods designed without domain adaptation and with domain adaptation are given in Table I. On the related note, there are some studies on PAD. Due to limited space, we refer interested readers to reviews in [9]–[10][11], [31]. While the prior work tried to utilize unlabeled target domain data to perform domain generalization, minimizing MMD alone may not fully exploit the useful information from the labeled source domain. In addition, the existing DA based face PAD methods do not consider how to enhance the domain independent feature learning given the labeled source domain data and the unlabeled target domain data.

TABLE I Summary of the Representative Face Presentation Attack Detection Methods With and Without Domain Adaptation

SECTION III.Proposed Approach
We aim to build a robust cross-domain face PAD model when there are only unlabeled face images in the new application domain and labeled live and spoof face images in the source domain. We propose a novel unsupervised adversarial domain adaptation method with disentangled representation (DR-UDA) to exploit the useful information from both domains. The overall framework of our DR-UDA is shown in Fig. 2. In DR-UDA, we aim to learn an effective face PAD model from the labeled source domain, and transfer it to the unlabeled target domain via a common feature space shared by both domains. The common feature space is expected to be discriminative for distinguishing between the live and spoof face images, but indiscriminative for distinguishing between the samples from two domains.


Fig. 2.
The overall diagram of the DR-UDA framework. The DR-UDA consists of three modules, a source domain metric learning network (ML-Net), an unsupervised adversarial domain adaptation module (UDA-Net), and a disentangled representation learning module (DR-Net), respectively.

Show All

The proposed DR-UDA consists of a metric learning module (ML-Net), an unsupervised domain adaptation module (UDA-Net), and a disentangled representation learning module (DR-Net). We detail the three modules in the following sections.

A. ML-Net for Source Domain Metric Learning
Let Xs denote the source domain face images, and Ys denote the corresponding labels (live or spoof). Let Xt denote the target domain face images without known labels. The ML-Net aims to learn a discriminative feature embedding (F ) using the labeled source domain face images (Xs,Ys) , i.e.,
fθS_E(x)∈F,F∈Rd.(1)
View SourceRight-click on figure for MathML and additional features.where x∈Xs , F is a feature space that is expected to be discriminative for live and spoof face images. We use a deep residual neural network, i.e., ResNet-18 [2] with a new joint loss function to learn fθS_E(⋅) , the source encoder with parameter θS_E .

1) Joint Loss:
Instead of using the common cross-entropy loss like previous face PAD methods [21], [52], we propose a novel joint loss consisting of triplet loss [66] and center loss [67] for metric learning to handle the big within-class diversity issue, particularly for the spoof face class, which consists of various spoof types. The center loss is defined as
Lcenter(θS_E)=12∑i=1m∥∥fθS_E(xi)−cyi∥∥22,(2)
View Sourcewhere cyi∈Rd denotes the yi -th class center in feature space. The triplet loss is defined as
Ltriplet(θS_E)=∑(xai,xpi,xni)max(∥∥fθS_E(xai)−fθS_E(xpi)∥∥2−∥∥fθS_E(xai)−fθS_E(xni)∥∥2+α,0),(3)
View Sourcewhere xai , xpi and xni denote anchor sample, positive sample and negative sample, respectively. We want to ensure that an anchor data xai , no matter belonging to live or spoof, can be closer to an image of the same class xpi than an image of different class xni . Minimizing this term results in moving the anchor sample xai towards positive samples xpi while pushing away from negative sample xni in the embedding space. In addition, xai is expected to be closer to xpi than to xni by at least a margin of α in the embedding space. The final joint loss can be represented as
Ljoint=Ltriplet+λLcenter,(4)
View SourceRight-click on figure for MathML and additional features.where λ is a hyperparameter balancing the two losses, and we empirically set λ=1 in our experiments. The live and spoof face images are expected to form two clusters in the embedding space, making it easy for discriminating them using k-nearest neighbors (k-NN) classifier [68] (C ). Compared to SVM [42] and FC layers [19], k-NN classifier can generate a highly convoluted decision boundary as it is driven by the raw training data itself.

2) Triplet Selection:
In order to ensure good network convergence and effective representation learning, it is important to choose the appropriate triplets, particularly the hard ones. Given an anchor sample xai , we determine its hard positive sample xpi by using the following rule
argmaxxpi∥∥fθS_E(xai)−fθS_E(xpi)∥∥2.(5)
View SourceSimilarly, we determine the hard negative sample xni by using the following rule
argminxni∥∥fθS_E(xai)−fθS_E(xni)∥∥2.(6)
View SourceThen, a triplet pair is composed of (xai , xpi , xni ). Each triplet pair is used to compute the loss and update the parameters.

B. UDA-Net for Unsupervised Adversarial Domain Adaptation
Since the face images XT in the target domain are assumed to be unlabeled, supervised learning in terms of live vs. spoof labels in the target domain is not possible. Therefore, we build a target encoder fθT_E(⋅) that can leverage the source domain knowledge and the k-NN classifier learned from the source domain to effectively distinguish between live and spoof face images in the unlabeled target domain. We propose an unsupervised adversarial domain adaptation (UDA-Net) to learn this target domain encoder fθT_E(⋅) . We divide both the source domain feature learning model (fθS ) and target domain feature learning model (fθT ) into two parts: encoder and decoder (see Fig. 2). That is
fθS(xs)=fθT(xt)=Exs∼Xs[fθcls(fθS_E(xs))],Ext∼Xt[fθcls(fθT_E(xt))],(7)
View Sourcewhere the source encoder fθS_E and target encoder fθT_E are used to extract multi-scale features from face images. The k-NN classifier fθcls utilizes the multi-scale features extracted by source encoder fθS_E to predict whether the input face image is live or spoof. Our UDA-Net aims to obtain a feature space which is shared by both the source and target domain encoders. Since this shared feature space is already discriminative for the live and spoof face images in the source domain because of the learning by ML-Net, we can expect it is also discriminative for the unlabeled face images in the target domain after domain adaptation. UDA-Net simultaneously optimizes the learning of fθS_E and fθT_E in an adversarial manner, i.e., the samples from the source domain are indistinguishable from the samples in the target domain in the shared feature space. To achieve this goal, we introduce a discriminator with adversarial loss in our UDA-Net (see Fig. 2).

1) Adversarial Loss:
While fθS_E and fθT_E aim to obtain a shared feature representation space, in which the samples from the source and target domains are indistinguishable with each other, the discriminator D aims to separate the data from the two domains in the embedding space. Here, D is optimized using the conventional adversarial loss [28], i.e.,
LD(θT_E,θD)=−Exs∼Xs[logDθD(fθS_E(xs))]−Ext∼Xt[log(1−DθD(fθT_E(xt)))],(8)
View Sourcewhere θS_E and θT_E denote the parameters for the source and target encoders, respectively. θD denotes the parameters of the discriminator DθD . We fix the parameters of source encoder learned by ML-Net when training UDA-Net and only optimize the parameters of fθT_E and DθD .

We did not optimize UDA-Net by directly using the minimax loss. Instead, we split the objective into two independent objectives, i.e., one for optimizing encoders (fθS_E and fθT_E ), and the other for optimizing the discriminator (DθD ). The loss LD for discriminator DθD remains unchanged. The target encoder loss LE is defined as
LE(θT_E,θD)=−Ext∼Xt[logDθD(fθT_E(xt))].(9)
View Source

In summary, the proposed unsupervised domain adaptation is optimized with the following two objective functions:
minDθDLD(θT_E,θD)=−Exs∼Xs[logDθD(fθS_E(xs))]−Ext∼Xt[log(1−DθD(fθT_E(xt)))],(10)
View Sourceand
minfθT_ELE(θT_E,θD)=−Ext∼Xt[logDθD(fθT_E(xt))].(11)
View Source

C. DR-Net for Disentangled Representation Learning
Given ML-Net and UDA-Net, our core task is to learn a feature embedding (F ), which is shared by the source and target domains and discriminative for face images from both domains. However, since the target domain is unlabeled, unsupervised domain adaptation via UDA-Net alone may not fully exploit the domain-independent features that are useful for robust cross-domain PAD. In light of this, we further propose to disentangle the features that are irrelevant to specific domains (which can be considered as the noises) for cross-domain PAD from the shared feature space F by performing source and target domain reconstruction from the shared feature space F . We denote such a disentangled representation learning module as DR-Net (see Figs 2 and 3).


Fig. 3.
With the face images from source and target domain as the inputs, DR-Net can disentangle the features irrelevant to specific domains (noises) from the common feature space learned by ML-Net and UDA-Net, and obtain a domain independent representation which is both generative and discriminative for cross-domain PAD.

Show All

Specifically, the reconstructions for the source and target domain from shared feature space F can be represented as
X^s=X^t=Exs∼Xs[fθS_D(fθS_E(xs))],fθS_E(x)∈F,Ext∼Xt[fθT_D(fθT_E(xt))],fθT_E(x)∈F,(12)
View Sourcewhere Xs and Xt denote the original face images in source and target domains, and Xs^ and Xt^ denote the corresponding reconstructed face images. fθS_D and fθT_D denote source and target decoders, respectively. To learn fθS_D and fθT_D , and update the target encoder (fθT_E ) in DR-Net, we use a joint of L1 loss and feature-level loss.

1) L1 Loss:
We use an L1 loss to penalize the difference between the reconstructed face image and the input face image, i.e.,
L1(θS_D)=L1(θT_D)=Exs∼Xs[∥xs−fθS_D(fθS_E(xs))∥1],Ext∼Xt[∥xt−fθT_D(fθT_E(xt))∥1].(13)
View SourceWith L1 loss, the two decoders fθS_D and fθT_D are expected to reconstruct the original source and target face images as accurate as possible.

2) Feature-Level Loss:
Motivated by the success of the perceptual loss [69] used for image style transfer and super-resolution, we design a feature-level loss to enhance the training for DR-Net. Specifically, we use the source and target domain encoders fθS_E and fθT_E in ML-Net and UDA-Net to extract features from the original face images and reconstructed face images, which are then used for computing the MSE loss
=Lfea(θS_D)Exs∼Xs∑i=1nMSE[fiθS_E(xs)−fiθS_E(fθS_D(fθS_E(xs)))],(14)
View Sourceand
=Lfea(θT_D)Ext∼Xt∑i=1nMSE[fiθT_E(xt)−fiθT_E(fθT_D(fθT_E(xt)))],(15)
View Sourcewhere fiθS_E(x) and fiθT_E(x) represent the features from the convolutional layer before the i -th pooling layer extracted for image x by ResNet-18.

The overall objective function of DR-Net can be written as
=LREC(θT_E,θS_D,θT_D)Lfea(θS_D)+λ1Lfea(θT_D)+λ2L1(θS_D)+λ3L1(θT_D).(16)
View SourceWe empirically set λ1=1 , λ2=2 and λ3=10 to balance the individual losses in our experiments.

We summarize the effectiveness of ML-Net, UDA-Net and DR-Net in our DR-UDA as follows. ML-Net aims to make the shared feature embedding F to be discriminative for the live vs. spoof face images in the source domain. UDA-Net aims to reduce the difference between the data distributions from source and target domain. With DR-Net included in DR-UDA, we can learn domain-independent features from F previously learned by ML-Net and UDA-Net for cross- domain PAD.

The proposed approach differs from [24] in that the way of domain adaptation between the source and target domains. [24] utilized the Maximum Mean Discrepancy between the source and target domains to perform domain adaptation. However, DR-UDA allows more domain independent knowledge to be transferred by adversarial and disentangled learning when using domain adaptation.

SECTION IV.Experimental Results
A. Databases
We provide evaluations on five public domain face databases for PAD including Idiap REPLAY-ATTACK [4], CASIA Face AntiSpoofing [3], MSU-MFSD [16], ROSE-Youtu [24] and Oulu-NPU [30]. We also use the RGB modality of the CASIA-SURF [29] dataset for cross-database evaluations.

1) Idiap REPLAY-ATTACK:
Idiap REPLAY-ATTACK [4] (Idiap) consists of 1,200 videos from 50 subjects, which were taken by the webcam on a MacBook with the resolution of 320×240 . The videos were captured under two conditions: (i) the controlled condition with uniform background and lighting, and (ii) the adverse condition with complex background and natural lighting. A Canon PowerShot camera was used to capture high-resolution face videos, which were then replayed using iPad 1 (1024×768 ) and iPhone 3GS (480×320 ), and printed on paper.

2) CASIA Face AntiSpoofing:
CASIA Face AntiSpoofing Database [3] (CASIA) consists of 600 videos from 50 subjects, captured using multiple acquisition devices with different resolutions (i.e., Sony NEX-5 with the resolution of 1280×720 , and two webcams with the resolution of 640×480 ). The spoofing attacks include photo warping attack, cutting attack, and replay attack.

3) MSU-MFSD:
MSU-MFSD [16] (MSU) consists of 280 videos from 35 subjects, which were captured using a Laptop camera and a smartphone camera with resolutions of 640×480 and 720×480 , respectively. There are mainly two different spoofing attacks, e.g., printed photo attack and video replay attack.

4) ROSE-Youtu:
ROSE-Youtu [24] consists of 4,225 videos of 20 subjects, which were captured using multiple acquisition devices with different resolutions (Hasee smartphone with resolution of 640×480 , Huawei Smartphone with resolution of 640×480 , iPad 4 with resolution of 640×480 , iPhone 5s with resolution of 1280×720 ), and ZTE smartphone with resolution of 1280×720 ). There are mainly three different spoofing attacks, i.e., printed photo attack, video replay attack, and masking attack.

5) CASIA-SURF:
CASIA-SURF [29] is a recently released multi-modal (RGB, Depth and IR) face database for PAD, which contains 1,000 Chinese subjects with each one live video and six fake videos per subject. The RGB, depth and infrared (IR) modalities were simultaneously captured using the Intel RealSense SR300 camera. The background area of the face was cropped from original videos to make the face PAD task more challenging. They choose one frame out of every ten frames after face detection, and split the database into training, validation and testing sets, which contain 300, 100, and 600 subjects and 148K, 48K, and 295K frames, respectively.

6) Oulu-NPU:
Oulu-NPU [30] consists of 4950 real access and attack videos, which were recorded using the front cameras of six mobile devices (Samsung Galaxy S6 edge, HTC Desire EYE, MEIZU X5, ASUS Zenfone Selfie, Sony XPERIA C5 Ultra Dual and OPPO N3) in three sessions with different illumination conditions and background scenes (Session 1, Session 2 and Session 3). The presentation attack types considered in the OULU-NPU database are print and video replay attacks.

B. Implementation Details
1) Network Structure:
The source and target encoders share the same structure. There are four residual blocks in the encoders and each block has four convolution layers, which have the same settings as the convolution part of ResNet-18 [2]. The input images in the source and target domain are mapped into 512-D embedding feature vectors in disentangled representation space by the source and target encoders. The source and target decoders have the same structure, i.e., with four transposed convolution layers to generate the reconstructed images of 248×248×3 from the 512-D embedding feature space. We use a kernel size of 4 for all transposed convolution layers and use ReLU [70] layer for activation. The domain discriminator consists of two output nodes and three fully connected layers with 256, 256, and 128 hidden units, respectively. Each of the first two layers uses ReLU for activation. Our method is implemented in PyTorch.

2) Training Details:
We use an open source SeetaFace2 algorithm to do face detection and landmark localization. All the detected faces are then normalized to 256×256 based on five facial keypoints (two eye centers, nose, and two mouth corners). We resize the cropped face region to the size of 248×248 and use an open source imgaug3 library to perform data augmentation, i.e., random flipping, rotation, resizing, cropping and color distortion. The DR-UDA is trained end-to-end by minimizing the losses in Eqs. 4, 10, 11 and 16. To make the training manageable, we train our DR-UDA in three stages, by gradually increasing the employed constraints and target domain samples. We train 20, 50, and 50 epochs for the three stages, respectively. At stage-1, we use the source domain samples to train ML-Net optimized with the joint of center loss and triplet loss. We choose SGD with an initial learning rate of 1e−3 , and a batch size of 64 to train the source encoder with fixed parameters used in the following training procedure. At stage-2, we additionally include the images from unlabeled target domain to continue the training of UDA-Net and choose Adam as the optimizers of domain discriminator and target encoder in UDA-Net with initial learning rates of 1e−3 and 1e−5 , respectively. At stage-3, we update the parameters of DR-Net optimized with Adam with an initial learning rate of 1e−3 to obtain a more generative and discriminative feature representation.

C. Experimental Settings
We follow the state-of-the-art face PAD methods [24], [26], [32], [41], and report Half Total Error Rate (HTER) [71] in the cross-database testing. We perform cross-database testing on CASIA, Idiap, MSU and Rose-Youtu. We follow the same testing protocols as that used in [24], [32], i.e., train the model using all the images from dataset A , and test the model on a different dataset B (denoted as A→B ). So, we have twelve cross-database tests in total: C→I , C→M , C→Y , I→C , I→M , I→Y , M→C , M→I , M→Y , Y→I , Y→C , Y→M , in which C, M, I and Y denote CASIA, MSU, Idiap and Rose-Youtu, respectively. We choose ML-Net which is the core module of DR-UDA as one of the baselines. A number of domain adaptation methods, such as DRCN [33], ADDA [25] and DupGAN [34] were engaged in eliminating the gap between source and target domain. Although these methods were not designed specifically for cross-database face PAD, they reported promising generalization ability in digital datasets. So we also use all these methods as our baselines and report their performance on face PAD. In addition, two state-of-the-art methods, i.e., KSA§ [24] and ADA [32] leveraged domain adaptation to improve the cross-database PAD performance, and reported promising results. So we also use them as the state-of-the-art baseline algorithms. There are also many approaches addressing generalized face PAD without using domain adaptation, such as Auxiliary [26], De-spoof [35] and STASN [36]. While these works have not reported all the results under the twelve tests, the comparisons could be included where available, e.g. commonly used C → I and I → C. To further demonstrate the effectiveness of our method, we also use the RGB images in CASIA-SURF [29] as a target domain dataset to evaluate the models learned on Idiap, CASIA, MSU, and Rose-Youtu.

In recent years, Oulu-NPU has been increasingly used in assessing the generalization of face PAD methods. We conduct two experiments to validate the effectiveness of the proposed method on Oulu. In the first experiment, we follow the same testing protocol as that used in [24], [32] and report the cross-database performance under O→I , O→M , O→C , I→O , M→O and C→O . Since MADDG [41] does not show how to train the model using only one source domain dataset, we only compare with [41] using the multiple source domain setting in the second experiment.

In intra-dataset PAD tests, where there is a standard testing protocol provided with the dataset (such as CASIA and Rose-Youtu database), we simply follow their protocols. For the other datasets (such as Idiap and MSU), we split the dataset according to the subjects. The dataset splits for individual datasets are given in Table II.

TABLE II Dataset Splits in Intra-Dataset PAD Tests
Table II- 
Dataset Splits in Intra-Dataset PAD Tests
D. Cross-Database Testing
As shown in Table III, the methods like ADDA [25], DRCN [33], DupGAN [34] and KSA§ [24], which used domain adaptation do not always achieve higher performance compared to the results of ML-Net, Auxiliary [23], De-spoof [35] and STASN [36], which designed for generalized face PAD. For example, under the tests of C→I and I→C , the above DA methods perform worse than the Auxiliary [23], De-spoof [35] and STASN [36]. The possible reason is that these traditional DA methods are classified by simple FC layers optimized with cross-entropy loss. The generalization ability of the representation learned by source domain encoder is poor on the target domain, and thus the results after domain adaptation are still worse than those generalized face PAD methods. More favorably, ML-Net is optimized using the joint of center loss and triplet loss instead of cross-entropy loss, which may benefit from metric learning. We can observe that Auxiliary [23] and STASN [36] achieve the relatively stable and good results under C→I and I→C . This indicates that adding temporal information can make the model more robust for cross-database testing. The baseline method KSA§ [24] works better than ML-Net for most of the twelve tests. This indicates the usefulness and necessity of using DA for cross-database PAD. Our preliminary work ADA [32] performs better than KSA§ [24] under seven of the twelve tests. This suggests that adversarial learning is more effective than Maximum Mean Discrepancy (MMD) for DA, while both are unsupervised DA methods. As shown in Table IV, the proposed method achieves better average testing performance than both KSA§ [24] and our preliminary method ADA under three of four datasets which shows that our domain adaptation approach is able to learn representation with better generalization ability. This suggests that the proposed approach has big potential for PAD under new application scenarios. However, the proposed method may not work well under some cases. For example, when DR-UDA is trained on Idiap, MSU or Rose-Youtu, the HTER error for cross-database testing on CASIA remains high, i.e., the average cross-database testing HTER is more than 20% higher. The possible reason is that the diversity of the spoof attacks in Idiap, MSU MFSD and Rose-Youtu are relatively limited compared with those presented in CASIA.

TABLE III Cross-Database Face PAD Performance (HTER in %) of the Proposed Approach and the Baseline Approaches

TABLE IV Average Cross-Database Testing Performance (HTER in %) on Four Datasets by KSA§ [24], ADA [32] and Proposed Method

In order to demonstrate the effectiveness of the proposed method, we also carry out a statistical t-test. We compare the results by our method and the results by the baseline methods in Table III, and computed the corresponding p-values, as shown in Table V. We notice that the p-values are smaller than 0.05 except for KSA§, which proves that our method and KSA§, which both used domain adaptation, perform better than the other baseline methods, and our method performs slightly better than KSA§.

TABLE V P-values of t-Test Between the Proposed Approach and the Baseline Approaches. Because Three Methods (Auxiliary [23], De-spoof [35], and STASN [36]) Only Reported Results Under Two Test Settings, t-Test is not Performed for These Three Methods for Fair Comparisons With the Other Methods

In addition to the above cross-database tests, we also report cross-dataset PAD performance on CASIA-SURF. Since the cross-database PAD performance on CASIA-SURF was not reported in [24], we only compare our approach with ADDA [25], DRCN [33], DupGAN [34] and ADA [34]. As shown in Table VI, our method again achieves lower PAD error than the other state-of-the-art methods which suggests that the proposed DR-UDA has better generalization ability into new scenarios. We observe that the models trained on the Idiap and MSU achieve higher error than those trained on the CASIA and Rose-Youtu when testing on CASIA-SURF. The possible reason is that the Idiap and MSU do not contain print attacks in which persons hold flat or cut photos. However, these attack types appear in CASIA and Rose-Youtu. In addition, all subjects in CASIA and Rose-Youtu are Asians, while there are no Asians in Idiap and a small portion of Asians in MSU.

TABLE VI Cross-Database Testing Performance (HTER in %) on CASIA-SURF(Denoted as CS) Database by the Proposed Method and the State-of-the-Art Domain Adaptation PAD Methods

We also provide evaluations by using one of the four datasets for testing and the combination of the other three datasets for training. The performance of the proposed approach and the state-of-the-art approaches are shown in Table VII (a). We can see that leveraging bigger training set can usually improve the cross-database testing performance of all the methods. This indicates that collecting a large PAD dataset is an important and effective way for improving the generalization ability of a PAD model. Again, the proposed approach achieves lower error than ADDA [25], DRCN [33], DupGAN [34] and ADA [32].

TABLE VII Cross-Database Testing Performance (HTER in %) of Individual Methods on (a) Idiap, MSU, CASIA, and Rose-Youtu, and (b) Idiap, MSU, CASIA, AND Oulu. In Each Test, One of the Four Datasets is Used for Testing, and the Combination of the Other Three Datasets are Used for Training

We also report cross-database PAD performance on Oulu-NPU, which is a challenging database for generalized PAD methods. For the cross-domain PAD method MADDG [41], since it does not show how to train the model using only one source domain dataset, we only compare with [41] using the multiple source domain setting (see Table VII (b)). We can see that the proposed method performs better than MADDG under three of the four tests. The possible reason is that the proposed method could learn a common feature space shared by both the source and target domains. However, MADDG aims to use the source domain data alone (w/o knowing target domain data) to learn a discriminative feature space, and there might be still domain gaps. We notice that the HTER of [M, C, Y]→I is much lower than [O, C, M]→I . The possible reason is that some live face images in Idipa have very similar background to the spoof face images in Oulu. We also notice that simply mixing multi-source datasets for training may lead to poor performance than using a single source dataset for training under some cases. The possible reasons are that: (1) the proposed approach may not benefit from a simply combined multi-source dataset unless each source dataset is specifically exploited for informative feature learning; and (2) severe data imbalance of the multi-source datasets may suppress the useful information contained in a small dataset. We also follow the same testing protocols as that used in [24], [32] and report cross-database PAD performance on Oulu-NPU (see Table VIII). We can observe that the proposed method performs better than the two baseline methods under all tests.

TABLE VIII Cross-Database Face PAD Performance (HTER in %) of the Proposed Approach and the Baseline Approaches Between Oulu and Other Three Datasets (CASIA, Idiap, and MSU)
Table VIII- 
Cross-Database Face PAD Performance (HTER in %) of the Proposed Approach and the Baseline Approaches Between Oulu and Other Three Datasets (CASIA, Idiap, and MSU)
In this work, we assume that there are unlabeled face images containing both genuine faces and presentation attacks in the target domain. We agree that it is difficult to collect a new dataset containing both genuine and presentation attack samples (even unlabeled) for each new domain. However, we try to replicate the scenarios that when a face recognition system is used for access control, while most of the images are from genuine faces, there might be some chances (e.g., 25%) of either intentional presentation attacks or unintentional presentation attacks just for fun, collected by the face recognition system. We hope to adapt the pre-trained PAD model to the new scenario by using such unlabeled data collected by the face recognition system. This is the main motivation of unsupervised domain adaptation. Accordingly, in our experiments, we have verified the effectiveness of unsupervised domain adaptation when the percentages of live (spoof) samples vary among 0%, 25%, 50%, 75% and 100%. Fig. 6 shows the HTER changes of our approach when keeping the image number of face images of one class (either live or spoof) fixed, and changing the percentage of the other class. From Fig. 6 (a), we can notice that increasing the percentage of live face images does not bring too much gain in reducing the HTER. By contrast, we can notice from Fig. 6 (b) that increasing the percentage of presentation attack face images does bring more gain in reducing the HTER. The reason is that increasing the diversity of presentation attacks is important for improving cross-domain PAD robustness.


Fig. 4.
Examples of correct and incorrect PAD results by the proposed approach in twelve cross-database tests(denoted as (a)-(i)). The label “S, G” (or “G, S”) denotes a spoof (or genuine) face image is incorrectly classified as genuine (or spoof) face image; “G, G” (or “S, S”) denotes a genuine (spoof) face image is correctly classified as genuine (spoof).

Show All

Fig. 5. - Examples of reconstruction results of the target domain by the DR-Net in our DR-UDA in twelve cross-database tests from (a) to (i). The label “S, G” (or “G, S”) denotes a spoof (or genuine) face image is incorrectly classified as genuine (or spoof) face image; “G, G” (or “S, S”) denotes a genuine (spoof) face image is correctly classified as genuine (spoof).
Fig. 5.
Examples of reconstruction results of the target domain by the DR-Net in our DR-UDA in twelve cross-database tests from (a) to (i). The label “S, G” (or “G, S”) denotes a spoof (or genuine) face image is incorrectly classified as genuine (or spoof) face image; “G, G” (or “S, S”) denotes a genuine (spoof) face image is correctly classified as genuine (spoof).

Show All

Fig. 6. - The trend of HTER (in %) by DR-UDA when gradually enlarge the percentage of live (spoof) face samples while keeping the number of spoof (live) images fixed.
Fig. 6.
The trend of HTER (in %) by DR-UDA when gradually enlarge the percentage of live (spoof) face samples while keeping the number of spoof (live) images fixed.

Show All

Fig. 4 shows some examples of PAD results by the proposed approach in the 12 cross-database tests. We notice that most errors are caused by challenging appearance variances, such as over-saturated illumination, similar color distortions observed for both live and spoof face images, etc. In addition, the unseen attack types during training also lead to incorrect classification for the spoof face images in Figs. 4 (f) and (g).

Fig. 5 shows examples of reconstruction results by DR-Net in the target domain. We can observe that the reconstructed face images for Idiap look better than those on the other three databases. At the same time, our DR-UDA achieves lower PAD error on Idiap, as shown in Table IV. We observe that the learned common feature space is not overfitted to the source domain; instead, it is generative in retaining the semantic facial information (shape, illumination, etc.) for the face images in the target domain. Such a capability is useful in improving the generalization ability of our PAD model into new domains.

E. Intra-Database Testing
Since many previous methods for face PAD only reported their performance under intra-database testing scenarios, we also perform intra-database testing on CASIA, Idiap and MSU, respectively. In [24], LPQ, CoALBAP, and deep learning features were reported to have the promising performance in intra-database testing. Therefore, we use the methods studied in [24] as our baselines for intra-database testing. Considering the success of deep learning in different tasks, we also use ResNet-18 [2] and SE-ResNet18 [72] to perform intra-database testing. For fair comparisons, we also use ResNet-18 and SE-ResNet-18 as the backbone network of the ML-Net in our DR-UDA during intra-database testing. We follow [24] and report HTER on Idiap, and EER on CASIA and MSU.

From the results in Table IX, we notice that the [24] using deep features, ResNet [2] and SE-ResNet [72] show stronger representation capacity than the hand-crafted features. We also notice that the ML module using ResNet-18 in our DR-UDA achieves much lower PAD errors than the baseline methods on CASIA, Rose-Youtu and MSU, while our ML module with ‘SE-ResNet18’ achieves the lowest PAD error on Idiap. These results suggest that the proposed approach is more effective in learning a discriminative feature representation for live vs. spoof face image classification. In addition, we can observe that the Squeeze-and-Excitation [72] is also effective in the PAD task.

TABLE IX Intra-Database Testing Performance (HTER in %) of the Proposed Method and the Baseline Approaches

F. Ablation Study
We provide ablation study to investigate the effectiveness of the three components in our DR-UDA, i.e., (i) metric learning for PAD via ML-Net, (ii) adversarial domain adaptation via UDA-Net, and (iii) disentangled representation learning via DR-Net. We study their influences by gradually dropping them from DR-UDA, and denote the corresponding models as ‘w/o ML’, ‘w/o UDA’, ‘w/o DR’, ‘w/o ML&UDA’, ‘w/o ML&DR’, ‘w/o UDA&DR’ and ‘w/o ML&UDA&DR’.

The results of these models under cross-database testing on CASIA, Idiap, MSU and Rose-Youtu are given in Table X and Fig. 8. We can see dropping any of the three components will lead to increased PAD error. This suggests that all the three components are useful for our DR-UDA approach. In addition, we notice that ML-Net has a bigger influence than the other two modules to the generalization abilities of the proposed DR-UDA. For example, in the cross-database testing on Idiap, CASIA and MSU, removing the ML module from DR-UDA leads to 25%, 20% and 15% higher HTER than DR-UDA, respectively. The possible reason is that the UDA-Net and DR-Net modules are fine-tuned in the feature space learned by ML-Net, and thus the feature space is primarily determined by ML-Net. Still, the UDA-Net and DR-Net can contribute to improving the cross-database testing performance. For example, if we visualize the feature representation learned by ML-Net alone, DR-UDA w/o ML-Net and the whole DR-UDA, we can see using UDA-Net and DR-Net together with ML-Net can obtain more robust feature representations that are discriminative for genuine vs. spoof face image classification (see Fig. 7). We also conduct statistical t-test for the proposed approach under ablation study. As shown in Table XI, we observe that the p-values of all tests are smaller than 0.05 except for ‘w/o DR’, which indicates that UDA-Net and ML-Net play more important roles than DR-Net. In addition, compared with the results by discarding all three components, the full method shows signification improvement in cross-dataset testing.

TABLE X Performance (HTER in %) of the Proposed Method Under Ablation Study in Terms of Metric Learning (ML), Unsupervised Domain Adaptation (UDA) and Disentangled Representation Learning (DR)
Table X- 
Performance (HTER in %) of the Proposed Method Under Ablation Study in Terms of Metric Learning (ML), Unsupervised Domain Adaptation (UDA) and Disentangled Representation Learning (DR)
TABLE XI P-Values of t-Test for the Proposed Approach Under Ablation Study in Terms of Metric Learning (ML), Unsupervised Domain Adaptation (UDA) and Disentangled Representation Learning (DR)
Table XI- 
P-Values of t-Test for the Proposed Approach Under Ablation Study in Terms of Metric Learning (ML), Unsupervised Domain Adaptation (UDA) and Disentangled Representation Learning (DR)

Fig. 7.
The t-SNE [1] visualization of the live and spoof face images from MSU, Idiap, CASIA when source domain is Oulu in different feature spaces: (I) learned by ML-Net alone, (II) learned by the DR-UDA w/o DR-Net (ML-Net + UDA-Net), and (III) learned by the whole DR-UDA (ML-Net + UDA-Net + DR-Net).

Show All


Fig. 8.
The increase of average HTER (in %) by DR-UDA when dropping the ML-Net, UDA-Net or DR-Net module for ablation study.

Show All

SECTION V.Conclusion
This paper addresses cross-domain face presentation attack detection (PAD)and proposes an unsupervised adversarial domain adaptation (DR-UDA) method that can leverage unlabeled target domain data and labeled source domain data to build robust PAD model. DR-UDA consists of ML-Net, UDA-Net and DR-Net. ML-Net uses the combination of center loss and triplet loss jointly to learn a feature representation for live vs. spoof face image classification in the source domain. We then adapt this representation to the target domain via UDA-Net and DR-Net, so that this representation can be shared by both the source and target domains, and can be discriminative for live vs. spoof classification. The proposed approach outperforms the state-of-the-art face PAD methods on the a number of the public databases under the challenging cross-database testing scenario. Our future work includes utilizing 3D face prior knowledge and physiological cues to further improve the robustness of PAD models. In addition, we will study how to learn better representations that can further reduce the domain gap during domain adaptation.