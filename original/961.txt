Federated learning (FL) enables a large number of clients to collaboratively train a global model through sharing their gradients in each synchronized epoch of local training. However, a centralized server used to aggregate these gradients can be compromised and forge the result in order to violate privacy or launch other attacks, which incurs the need to verify the integrity of aggregation. In this work, we explore how to design communication-efficient and fast verifiable aggregation in FL. We propose VeriFL, a verifiable aggregation protocol, with O(N) (dimension-independent) communication and O(N+ d) computation for verification in each epoch, where N is the number of clients and d is the dimension of gradient vectors. Since d can be large in some real-world FL applications (e.g., 100K), our dimension-independent communication is especially desirable for clients with limited bandwidth and high-dimensional gradients. In addition, the proposed protocol can be used in the FL setting where secure aggregation is needed or there is a subset of clients dropping out of protocol execution. Experimental results indicate that our protocol is efficient in these settings.

SECTION I.Introduction
Federated learning (FL) [1]–[2][3] has become one of the most popular paradigms for distributed machine learning nowadays. It facilitates the usage of sensitive datasets distributed among a large number of clients, which may be mobile phones, other mobile devices, or sensors, without collecting their data. In FL, to train a global model, a subset of users are instructed to upload their local parameters in each synchronous epoch (Figure 1(a)). To update the global model, a centralized server is adopted to aggregate the parameters received from these users and sends back the updated global model to them. All users will update their local model according to the global one and such training process will continue until model convergence.


Fig. 1.
Federated learning and a trivial attack on it. (a) The framework of federated learning. (b) The attack launched by the adversary corrupting the server and a subset of clients.

Show All

Despite of its appealing functionality, FL has been shown vulnerable to some attacks. For example, the works [4]–[5][6] show that the gradient vector uploaded by a client may leak sensitive information about its private dataset. To address this issue, in [7], the authors proposed a secure aggregation protocol that guarantees the privacy of gradients. However, the recent work [8] shows that, in addition to the gradient privacy, the integrity of aggregation should be protected as well. In particular, the server can easily become a single-point of failure in FL. Without integrity guarantee, once the server gets compromised, the adversary controlling the server can manipulate the global model and cause misclassification of any involved client at its specified data point (Figure 1(b)), which is similar to the consequence of backdoor attacks [9]–[10][11][12]. The lack of the integrity guarantee of aggregation may restrict the commercial application of FL. An example attack is that the corrupted server re-trains the global model updated in this epoch with some poisoned data and returns the re-trained model to honest clients, aiming to cause misclassification in these clients.

We note that attacks by modifying aggregation results (e.g., the aforementioned example attack) can be mitigated using verifiable aggregation protocols. In such protocols for FL, a considered adversary cannot convince an honest client to accept its forged aggregated gradient as a real one with an overwhelming probability (see Definition 3). Certainly, to resist inference attacks exploiting the “non-encrypted” gradients [4]–[5][6], the used verifiable aggregation protocol should be secure as well to guarantee input privacy of each client. Informally speaking, this means that an adversary in the aggregation protocol learns nothing about the gradient of each client from protocol transcripts (see Definition 4).

In this paper, we focus on how to realize a (secure) verifiable aggregation protocol in a communication-efficient and fast way on resource-constrained devices (e.g., the setting of mobile devices in Google’s work [7]). Since the bandwidth in the resource-constrained setting is expensive and a large number of clients are required to train a global model through hundreds of iterations, communication becomes one of the most important considerations in the design of FL protocol. If the designed verifiable aggregation protocol costs a lot in terms of communication and computation, it will take a long period to finish the training process. There are many factors influencing the communication efficiency of a verifiable aggregation protocol, and the number of parameters in the model to be trained is particularly important. So, our first question is, can we design a verifiable aggregation protocol in which the communication of verification is independent of the number of parameters in the model to be trained? In fact, the communication efficiency of FL is always a research hotspot [1], [2], [7], [13].

To the best of our knowledge, there is no previous work that succeeds in designing a communication-efficient verifiable aggregation protocol. The works about communication-efficient FL [7], [13] do not consider how to guarantee the integrity of aggregation. The recent work that achieves the first secure verifiable aggregation [8] uses zero-knowledge proof to enforce honest aggregation. However, its communication cost for verification is linearly dependent on the dimension of gradient vectors. Its linearly-growing and high communication cost makes it impractical in its intended application scenarios, that is FL among mobile devices. Moreover, its computation cost for verification is unsatisfactory due to the use of zero-knowledge. For example, it takes 3.59 MB outgoing communication and 254964 ms computation per client to verify the aggregation of 20K-dimensional parameter vectors among 500 clients. Another trivial solution to achieve verifiable aggregation is using generic multiparty computation (MPC) approaches [14]–[15][16][17][18][19][20][21], but its communication and computational overhead are very large. So, in addition to communication efficiency, our second question is, how to make our verifiable aggregation protocol computation-efficient?

A. Challenge and Contribution
1) Challenge:
To verify the integrity of aggregation, a straightforward idea is to use homomorphic hash to compute the “digest” of gradient vectors to be aggregated. A combined hash can be obtained from these hash values after aggregation, and each client compares it with the hash of the aggregation result (i.e., real hash) to verify the aggregation. Although this idea yields a desirable feature that the verification communication is independent of gradient dimension, we would like to show that this idea provides little security guarantee.

The security challenge comes from the fact that everyone in the above construction has access to the homomorphic hash function. In the simulation-based proof, the simulator did not know the input (i.e., the gradient vector) of each honest client by the time it was asked to simulate its hash value. To simulate it, the simulator has to use a dummy vector. However, this dummy value is different from the real one with an overwhelming probability. Since everyone (including the adversary) can call the homomorphic hash function with the aggregation result, an inconsistency between the combined and real hashes will be found after aggregation and this immediately yields an efficient distinguisher that fails the simulation. That is, the straightforward construction is not secure.

In [8], the authors solve this security challenge by combining homomorphic hash with zero-knowledge proof. However, due to the dimension-wise zero-knowledge proofs, their protocol leads to expensive computation and the communication caused by verification is still linear in the gradient dimension. We note that such dimension-dependence is depressing since the gradient dimension is large in real-world FL applications (e.g., 100K). We are motivated to find another way to achieve verifiable aggregation, overcoming the security challenge with good concrete efficiency in terms of communication and computation.

2) Contribution:
In this paper, we present a communication-efficient and fast protocol, VeriFL, for verifiable aggregation in FL. As shown by experiment, for example, VeriFL achieves 33.24 KB (dimension-independent) outgoing communication and 8899 ms computation per client to verify the aggregation of 100K-dimensional parameter vectors among 500 clients, which outperforms the state-of-the-art work [8] by 110.6× in communication and 28.7× in computation even with 5× larger parameter vectors. More specifically, we make the following contributions:

(Secure) verifiable aggregation. To verify the integrity of aggregation, we combine the linearly homomorphic hash with the commitment scheme to force the aggregation server to use the submitted gradients that are consistent with their previously broadcast hashes. Collision resistance of the hash scheme guarantees that the server cannot have an honest client accept its forged result. In addition, to overcome the aforementioned security issue, we observe that it sufficient to use equivocal commitment scheme to achieve the same security as [8] without using heavy zero-knowledge proof. Based on this observation, we develop a novel verifiable aggregation protocol. Notably, secure aggregation is also considered in our protocol and we show our protocol is composable with the secure protocol in [7] by using double-masking technique and adding extra rounds. The functionality of secure aggregation in our protocol is achieved in the sense that only the aggregation result computed by the server is revealed.

Dimension-independent communication overhead. In our protocol, the communication overhead resulted from integrity verification is independent of the dimension of gradient vectors (i.e., the number of parameters in FL models), which significantly saves the bandwidth of clients. This is achieved by asking each client to commit only the hash of its gradient vector instead of the vector itself. These commitment strings, which facilitate our security proof, have a length independent of the gradient dimension. The linear homomorphism of the hash scheme ensures that the “sum” of hashes equals to the hash of the sum vector. It is helpful to think that linearly homomorphic hash can be used to compress high-dimensional gradient vectors while preserves the properties of addition.

Approximately-halved computation overhead. We succeed in reducing the verification cost of our basic construction in terms of computation by using amortized verification. In our protocol, the most time-consuming operation is the two calls of linearly homomorphic hash that requires O(d) expensive modular exponentiations, in which d is the dimension of gradient vectors. The former call is used to generate the necessary messages for future verification and is inevitable. However, the latter call that performs the integrity verification can be amortized. First, we draw a set of random coefficients to compute the linear combination of the hash aggregations in different epochs. Then, we check whether the combined hash is equal to the hash of the linear combination (using the same coefficients) of aggregation results in different epochs. In this way, we only performs the integrity verification once a batch and the cost of the latter call is amortized by a factor of the batch size. The total computation overhead can be approximately reduced to that of the former hash call.

SECTION II.Federated Learning With Secure Aggregation
A. Federated Learning
The basic framework of FL is summarized as follows. There are N users in FL, each having access to its private dataset Di where |Di|=si . At the beginning of each epoch k∈{1,2,…} , the aggregation server will randomly select a subset of users Sk and send them the parameter vector v⟨k−1⟩ obtained in the previous epoch (note that v⟨0⟩ is initialized randomly). Each selected user Pi will locally minimize the empirical loss over its dataset Di to get its updated (local) model vi⟨k⟩ and upload its gradient Δi⟨k⟩←vi⟨k⟩−vi⟨k−1⟩ to the aggregation server. Upon receiving enough gradients from the users, typically, the server will take a weighted average of these gradients and obtain an updated global model, i.e., v⟨k⟩←v⟨k−1⟩+∑i∈SkβiΔi⟨k⟩ where βi=si/∑i∈Sksi . The training process will continue until model convergence.

B. Secure Aggregation
In [7], the authors build up a secure and dropout-tolerant aggregation protocol based on double-masking technique. More specifically, the double-masking of a gradient vector vi includes two parts: the self-mask generated by the owner Pi itself, and the pairwise-mask generated between Pi and each other client. The double-masked gradient of vi is denoted by pi :
pi=vi+PRG(bi)self−mask+∑j∈U,i<jPRG(maki,j)−∑j∈U,i>jPRG(maki,j)pairwise−mask mod B
View SourceRight-click on figure for MathML and additional features.where B is the modulus for aggregation, bi is a secret seed sampled by the Pi and maki,j=makj,i is a pairwise agreed value between Pi and Pj for each Pj∈U . Rounds of their protocol are briefly summarized in Figure 2.

Fig. 2. - The semi-honest version protocol in [7].
Fig. 2.
The semi-honest version protocol in [7].

Show All

C. Adversarial Model
In this paper, we consider the same adversary as in [8]. The adversary is semi-honest but with the additional power to instruct the corrupted server to forge the aggregation result arbitrarily with the knowledge of the transcripts it has seen before. The adversary can corrupt both the aggregation server and a subset of data parties (i.e., clients). In this semi-honest model, the corrupted parties will provide their gradients honestly. Using the notations to be introduced in the next section, all gradient vectors of parties belong to ZdR and the aggregation result should lie in ZdB for some bound B≥N⋅R , in which N is the number of parties.

In this adversarial model, the adversary may aim either (i) to infer the private gradients of honest parties, or (ii) to convince honest parties of its forged aggregation results. The goal of our protocol is to protect the privacy of each party’s gradient while guarantees the integrity of aggregation. However, we do not consider the adversary that makes queries to the trained model to launch black-box statistical attacks [22]–[23][24][25][26] since it is known hard to prevent the leakage from the output of the functionality implemented by cryptographic protocols. Moreover, such attacks might not work well to precisely infer the sensitive information of honest parties, especially for deep neural networks that generalize well [6].

Note that, in FL, the private inputs of honest parties in the previous round may be approximate to the those in the current round when the model converges. To forge an approximate aggregation result, it suffices for the adversary to choose the inputs of corrupted parties according to the partial sum of the inputs of honest parties in the previous round. This observation can lead to a model replacement attack in [9], which bypasses secure aggregation protocols and is possible since some small perturbations will not significantly affect the behaviour of machine learning models. How to prevent such an attack in FL settings is another interesting and open problem, which is out of the scope of this paper. In this paper, like other works in the literature of multiparty computation, we only focus on how to prevent the adversary from forging an aggregation result different from the exact one. We do not consider the attacks (e.g., model replacement attack) where the adversary chooses its input according to the past protocol transcripts. This is known challenging in multiparty computation since it cannot be distinguished from the case where a corrupted party do use this as its input without some apriori knowledge of input.

SECTION III.VeriFL: Efficient Verifiable Aggregation
A. Notations
We use [n] to denote the set {1,…,n} for some integer n . The set of integers is denoted by Z . The quotient ring of integers modulo a positive integer q is written in Zq . When q is a prime, Zq is a field denoted by Fq . A vector is denoted by a bold lower case letter, e.g., x , and its i -th entry is denoted by x[i] where the index i is counted from one. For some finite set X , its cardinality is |X| . If A is an algorithm, a←A says that a is assigned to be the output of A ; otherwise, if A is a set, a is an element uniformly drawn from the set A . The security parameter is denoted κ and we use negl(κ) to denote the negligible function in κ . We use A(x;y) to denote that an algorithm A is invoked with a public input x and a secret input y . We formulate pseudo-random generator as PRG:{0,1}∗↦ZdB for some modulus B and vector dimension d .

B. Cryptographic Primitives
1) Linear Homomorphic Hash:
A linearly homomorphic hash scheme consists of three polynomial-time algorithms, i.e., LHH=(LHH.HGen,LHH.Hash,LHH.Eval) . We detail the above three algorithms as constructed in [27] by assuming the hardness of discrete logarithm.

LHH.HGen(1κ,1d) : On input the security parameter κ and the dimension d , this algorithm outputs the public parameter LHHpp, including the description of a cyclic group G of prime order q , its generator g∈G and d distinct elements g1,…,gd∈G . For simplicity of presentation, this public parameter will be taken implicitly as the first parameter of LHH.Hash and LHH.Eval .

LHH.Hash(x) : Taking a d -dimensional vector x as input, this algorithm outputs the linearly homomorphic hash of x : h←∏i∈[d]gx[i]i∈G .

LHH.Eval(h1,…,hℓ,α1,…,αℓ) : Taking ℓ hashes and ℓ coefficients of linear combination, this algorithm outputs the linear combination of these ℓ hashes: h∗←∏i∈[ℓ]hαii .

Note that this construction of LHH satisfies the following definition of collision resistance with respect to the collision experiment ExptcollA,LHH .
ExptcollA,LHH(1κ,1d) :

Call LHHpp←LHH.HGen(1κ,1d) .

Send LHHpp to the adversary and wait for its input (x1,x2)←A(LHHpp) where x1,x2∈Fdq are two distinct vectors.

Output 1 iff LHH.Hash(x1)=LHH.Hash(x2) ; other- wise output 0.

Definition 1 (Collision Resistance):
LHH is said to be collision-resistant, if for any PPT adversary A , there exists a negligible function negl(⋅) such that the advantage of A
AdvcollA,LHH(κ):=Pr[ExptcollA,LHH(1κ,1d)=1]≤negl(κ)
View Sourcefor the security parameter κ and the vector dimension d .

2) Commitment:
Commitment is an “envelope” so that a party cannot change the value after they have committed to it while the committed value is kept secret to others before decommitment. The commitment scheme used in this paper is to make the security go through without leading to too much communication overhead for verification. In particular, to formally prove the security of our protocol, we need an equivocal commitment scheme. Roughly speaking, someone possessing the trapdoor of an equivocal commitment scheme can produce commitments that can be opened to different values. However, committers in real world, who have no idea about the trapdoor, can open them to only a single value. Such property enables the simulator in our proof to fool the distinguisher and achieves indistinguishability consequently (see Section IV).

A non-interactive equivocal commitment scheme is defined as a tuple of four polynomial-time algorithms COM=(COM.Setup,COM.Commit,COM.Decommit,COM.Equiv) :

COM.Setup(1κ) : On input the security parameter κ , this algorithm outputs a public parameter COMpp and a trapdoor td. Note that the message space M and the commitment space C are also provided in COMpp. For simplicity of presentation, this public parameter will be taken implicitly as the first parameter of the other algorithms.

COM.Commit(m;r) : This algorithm is run by the committer and takes as input a message to be committed m∈M and a uniform randomness r and outputs a commitment string c∈C , which is to be publicly published. Note that the randomness r serves as the “decommitment string” to open the committed message and it should be kept secret until opening.

COM.Decommit(c,m′,r′) : This algorithm is run by the receiver and takes as input a commitment string c∈C , a claimed committed message m′∈M and the claimed randomness r′ that was used to commit m′ . If c=COM.Commit(m′;r′) then output 1; otherwise output 0.

COM.Equiv(c,(m,r),m′;td) : This algorithm takes as input a commitment string c←COM.Commit(m;r)∈C , a desired arbitrary message m′∈M and the trapdoor td and outputs an valid randomness r′ such that c decommits to m′ .

The equivocality of COM is formally defined as follows.
Definition 2 (Equivocality):
COM is said to be equivocal if for any m∈M and uniform randomness r , it holds that there exists a negligible function negl(⋅) such that
Pr⎡⎣⎢⎢⎢⎢⎢⎢⎢b=0∣∣∣∣∣∣∣∣(COMpp,td)←COM.Setup(1κ),c←COM.Commit(m;r),m′←M,r′←COM.Equiv(c,(m,r),m′;td),b←COM.Decommit(c,m′,r′)⎤⎦⎥⎥⎥⎥⎥⎥⎥≤negl(κ)
View SourceRight-click on figure for MathML and additional features.for the security parameter κ .

3) Symmetric Encryption:
An symmetric encryption scheme is defined to be a tuple SE=(SE.KeyGen,SE.Enc,SE.Dec) .

SE.KeyGen(1κ) : On input the security parameter κ , this algorithm outputs a secret symmetric key k.

SE.Enc(m;k) : This algorithm takes as input a message m to be encrypted and a symmetric key k. The output of this algorithm is the ciphertext c of m under the key k.

SE.Dec(c;k′) : This algorithm takes as input a ciphertext to be decrypted and its corresponding symmetric key k′ . The output of this algorithm is the message m′ such that c←SE.Enc(m′;k′) .

In this paper, we require SE to be IND-CPA secure.
4) Key Agreement:
We use a key agreement scheme to (i) generate pairwise symmetric encryption keys, and (ii) generate pariwise seeds for PRG. A key agreement scheme is defined to be a tuple KA=(KA.Setup,KA.KeyGen,KA.Agree) .

KA.Setup(1κ) : On input the security parameter κ , this algorithm outputs a public parameter KApp. For simplicity of presentation, this public parameter will be taken implicitly as the first parameter of the other algorithms.

KA.KeyGen() : This algorithm generates a key pair (sk,pk) .

KA.Agree(ski,pkj) : This algorithm takes as input a secret key ski and a public key pkj and outputs a private agreed key aki,j .

5) Secret Sharing:
We use secret sharing to deal with dropout in our protocol and preserve input privacy. A secret sharing scheme is defined to be a tuple SS=(SS.Setup,SS.Share,SS.Combine) .

SS.Setup(1κ) : On input the security parameter κ , this algorithm outputs a public parameter SSpp, which includes the message space M . For simplicity of presentation, this public parameter will be taken implicitly as the first parameter of the other algorithms.

SS.Share(t,P,s) : This algorithms takes as input the threshold value t , the set of parties P which is of size N≥t and a secret s∈M . The output of this algorithm is a set of secret shares, denoted by {[[s]]i}Pi∈P , each of which is assigned to a distinct holder Pi∈P .

SS.Combine(t,{[[s]]i}Pi∈P′⊆P) : This algorithm takes as input the threshold value t and the subset of shares {[[s]]i}Pi∈P′⊆P of which the size is not less than t . The output of this algorithm is the original secret s .

C. How to Achieve Efficient Verifiable Aggregation?
1) Basic Verifiable Aggregation:
The integrity of aggregation is achieved by adding the following two steps to the basic FL framework:

Preparation: This step is done by clients before they submit their gradient vectors to the server. In this step, each client is asked to generate: (i) the linearly homomorphic hash of its gradient vector, and (ii) the commitment string of this hash value. More formally, Pi generates
hi←ci←LHH.Hash(vi),COM.Commit(hi;ri),
View Sourcein which hi is the linearly homomorphic hash of vi , ci is the commitment string and ri is a uniformly random string secretly sampled by Pi . Note that (hi,ri) serves as the decommitment string of Pi . Note that, after this step, the commitment string ci will be forwarded to other clients. Pi will not send its gradient vector to the server until it receives from all other clients their ci ’s respectively.

Verification: This step is done by each client after it receives from the server the aggregation result a . In this step, Pi asks each other client Pj for its decommitment string (hj,rj) and checks whether
1=?COM.Decommit(cj,hj,rj),(1)
View Sourcein which ci was received after Preparation step. If the equality test (1) does not hold for some j∈[N]∖{i} , then the aggregation result a will be regarded as forged and Pi terminates with output ⊥ . Otherwise Pi will continue to check the equality of hashes:
LHH.Hash(a)=?LHH.Eval(h1,…,hN,1,…,1N items).(2)
View SourceIf the equality test (2) holds, then the aggregation result a passes the verification and Pi will accept the result; otherwise the result will be regarded as forged and Pi terminates with output ⊥ .

2) Dimension-Independent Communication Overhead:
The reduction in the communication overhead for verification comes from the usage of hash and commitment. Recall that the outgoing message for verification sent by each client Pi in each epoch consists of the commitment string ci and the decommitment string (hi,ri) . As long as we carefully instantiate COM with the one using commitment/decommitment strings of constant length, the outgoing communication for verification is constant and independent of the the dimension of gradient vectors. That is, in each epoch, the outgoing communication cost lead by verification can be made only O(1) and the incoming communication cost is O(N) .

3) Approximately-Halved Computation Overhead:
In our protocol, the most time-consuming operation for verification is running LHH.Hash . To reduce the computation overhead for verification, we allow each client to do verification in an amortized manner. Similar to [14], [15], [28], we draw a set of random coefficients to compute the linear combination of the hash aggregations in different epochs. Then, we check whether the combined hash is equal to the hash of the linear combination (using the same coefficients) of aggregation results in different epochs. More formally, letting ℓ be the preset batch size and a⟨k⟩ be the aggregation result in the epoch k∈[ℓ] , we replace the aforementioned Verification step with the following Amortized- Verification step:

Amortized-Verification: The equality test of commitment (1) remains unchanged for all commitments received in each epoch k∈[ℓ] . However, for k∈[ℓ] , Pi does the following:
αk←h⟨k⟩←Fq,LHH.Eval(h1⟨k⟩,…,hN⟨k⟩,1,…,1N items),
View Sourcein which q is the prime order provided in LHHpp and hi⟨k⟩ is the hash computed by client Pi in the k -th epoch. Then, it verifies the equality
LHH.Hash⎛⎝∑k∈[ℓ]αka⟨k⟩⎞⎠=?LHH.Eval(h⟨1⟩,…,h⟨ℓ⟩,α1,…,αℓ)(3)
View Sourceinstead of the equality (2). If the equality test (3) holds, then all the aggregation results a⟨1⟩,…,a⟨ℓ⟩ pass the verification and Pi will accept these results; otherwise these results will be regarded as forged and Pi terminates with output ⊥ .

In this way, Pi does not need to call LHH.Hash in the equality test (2) every aggregation epoch. That is, the cost for LHH.Hash in the equality test (2) can be amortized by a factor ℓ . Since the cost for LHH.Hash is O(d) , the amortized computation cost for verification in each epoch can be reduced from O(2d) to O(d+d/ℓ) . When ℓ is large enough, the cost can be approximately halved.

4) Privacy Concern and Dropout-Tolerance:
Our verifiable aggregation protocol (i.e., the basic FL framework with Preparation and Amortized- Verification steps) is composable with the secure one Figure 2. The summarized secure verifiable aggregation protocol is given in Figure 3. We modify the ShareKeys round in [7] and the original secure protocol serves as the aggregation phase in this composite protocol. Notably, the dropout-tolerance of [7] is preserved and therefore the aggregation phase in our protocol is robust to client dropout. However, to deal with the potential dropout when clients report their decommitment strings in Decommitting round, an extra DroppedDecommitting round is introduced. This extra round guarantees that all clients that survive dropout are able to run Amortized- Verification in BatchChecking round. Note that, to tolerate dropout, we require the additional O(N) computation of secret sharing in ShareMetadata round. Therefore, the total verification cost in computation is O(N+d) .


Fig. 3.
Our verifiable aggregation protocol with secure aggregation functionality.

Show All

D. Full Version of VeriFL Protocol
The full version of VeriFL Protocol is presented in Figure 4 and Figure 5, where cryptographic primitives are defined in Section III-B.

Fig. 4. - Aggregation phase for privacy-preserving verifiable aggregation protocol.
Fig. 4.
Aggregation phase for privacy-preserving verifiable aggregation protocol.

Show All

Fig. 5. - Verification phase for privacy-preserving verifiable aggregation protocol.
Fig. 5.
Verification phase for privacy-preserving verifiable aggregation protocol.

Show All

SECTION IV.Security Analysis
In this section, we will show that our protocol ensures the integrity of the aggregation results (Definition 3) and the privacy of individual input (Definition 4). Recall that our protocol runs with a set P of N parties and an aggregation server S , in which the building blocks are instantiated with the security parameter κ and the verification is performed in batch of size ℓ . The threshold for dropout is t (i.e., at least t parties survive dropout) and the set of corrupted parties is C⊆P∩{S} such that |C∖{S}|<t . U∗⟨k⟩ is the set U∗ in the k -th aggregation phase. Given fixed N , t , \kappa , \ell and \mathcal {C} , we define M_{ \mathcal {C}} as the polynomial time algorithm for the “next-message” function of corrupted parties in \mathcal {C} . That is, given a party identifier c \in \mathcal {C} , a round index i a transcript T_{i} that has been sent and received so far by all corrupted parties in \mathcal {C} , and the joint randomness r_{ \mathcal {C}} for the execution of corrupted parties, M_{ \mathcal {C}}(c, i, T_{i}, r_{ \mathcal {C}}) outputs the message for the party c in the round i .

Note that some common reference strings (CRS) are required by this protocol. As we will see, this protocol is secure in the CRS-hybrid model [29]. More specifically, according to the CRS functionality, the simulator is allowed to learn the trapdoor of the underlying commitment scheme. Therefore, in the CRS-hybrid model, the simulator of our protocol can obtain this trapdoor by simulating the CRS functionality and then use the trapdoor to make the simulated view consistent with the real one.

Definition 3 gives out the integrity of aggregation considered in this paper. In this definition, we say that integrity is achieved if an adversary who wants to forge the aggregation result after all clients having committed their homomorphic hash values respectively can be detected with an overwhelming probability.

Definition 3 (Integrity of Aggregation):
In the k -th epoch, let \textbf {v}_{H} \langle k \rangle be the partially aggregation result of the inputs of honest parties in \mathcal {U}_{3} \langle k \rangle , and \textbf {v}_{i} \langle k \rangle be the well-formed input of some corrupted party \mathcal {P}_{i} \in \mathcal {U}_{3} \langle k \rangle \cap \mathcal {C} whose hash was computed and committed in ShareMetadata. We say that the integrity of aggregation in a verification batch of size \ell holds, if an adversary can have honest parties accept its forged aggregation result in some epochs of this batch with a negligible probability, i.e., \begin{align*}&{\mathrm{ Pr}}\left[{{ \mathcal {P}_{i}\,\,outputs \perp }}\left| \begin{array}{l|l}{{ \mathcal {P}_{i} \in \mathcal {V}_{2} \setminus \mathcal {C}, for\,\,some\,\,k \in \mathcal {K} \subseteq [\ell], }}\\{{ \boldsymbol {a} \langle k \rangle \leftarrow \boldsymbol {v}_{H} \langle k \rangle + \sum \nolimits_{i \in \mathcal {U}_{3} \langle k \rangle \cap \mathcal {C}} \boldsymbol {v}_{i} \langle k \rangle , }}\\{{ \boldsymbol {a} \langle k \rangle \in \mathbb {Z}_ {B}^ {d}, }}\\{{ \boldsymbol {a}' \langle k \rangle \leftarrow M_{ \mathcal {C}}(\mathcal {S}, k, T_{k}, r_{ \mathcal {C}}), }}\\{{ \boldsymbol {a}' \langle k \rangle \in \mathbb {Z}_ {B}^ {d}, \boldsymbol {a}' \langle k \rangle \neq \boldsymbol {a} \langle k \rangle }}\end{array}\right .\right]\\&\qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \qquad \!\geq 1 - \textsf {negl}(\kappa).\end{align*}
View SourceRight-click on figure for MathML and additional features.

By Definition 4, we define input privacy of each client in the existence of the considered adversary. This definition aims to capture an adversary who corrupts the server and a subset of parties learns nothing from protocol transcript but the partially aggregation result of honest parties. We say that an adversary learns nothing if its view can be simulated by a simulator without any secret internal state of honest parties (e.g., the knowledge of the gradient of a client). More specifically, input privacy is defined in the sense that the view of an adversary corrupting less than t data parties can be simulated only given the partially aggregation result of honest parties.

Definition 4 (Input Privacy):
We say that the input privacy of an honest client holds, if there exists a PPT simulator {\boldsymbol{SIM}} such that for any set of parties \mathcal {P} , threshold t , security parameter \kappa , batch size \ell , set of inputs \{ \textbf {v}_{i} \}_{i \in \mathcal {P}} , \{ \mathcal {U}_{1} \langle k \rangle, \mathcal {U}_{2} \langle k \rangle, \mathcal {U}_{3} \langle k \rangle, \mathcal {U}_{4} \langle k \rangle \}_{k \in [\ell]} and \mathcal {V}_{1} , \mathcal {V}_{2} such that for all j \in ~ {[{3}]} and k \in [\ell -1] \begin{align*} \mathcal {P}\supseteq&\mathcal {U}_{1} \langle 1 \rangle, \\ \mathcal {U}_{j} \langle k \rangle\supseteq&\mathcal {U}_{j+1} \langle k \rangle, \\ \mathcal {U}_{4} \langle k \rangle\supseteq&\mathcal {U}_{1} \langle k+1 \rangle, \\ \mathcal {U}_{4} \langle \ell \rangle\supseteq&\mathcal {V}_{1} \supseteq \mathcal {V}_{2},\end{align*}
View Sourceand set of corrupted parties \mathcal {C} such that \mathcal {C} \subseteq \mathcal {P} \cap \{ \mathcal {S} \} and | \mathcal {C} \setminus \{ \mathcal {S} \} | < t , the output of {\boldsymbol{SIM}} is computationally indistinguishable from the output of \textsf {REAL}_{ \mathcal {C}}^{ \mathcal {P}, t, \kappa } :\begin{align*}&\hspace {-1.4pc}\textsf {REAL}_{ \mathcal {C}}^{ \mathcal {P}, t, \kappa }(\{ \textbf {v}_{i} \}_{i \in \mathcal {P}}, \{ \mathcal {U}_{1} \langle k \rangle, \mathcal {U}_{2} \langle k \rangle, \mathcal {U}_{3} \langle k \rangle, \mathcal {U}_{4} \langle k \rangle \}_{k \in [\ell]}, \mathcal {V}_{1}, \mathcal {V}_{2}) \\\approx _{c}&\textsf {SIM}_{ \mathcal {C}}^{ \mathcal {P}, t, \kappa }(\{ \textbf {v}_{i} \}_{i \in \mathcal {C}}, \{ \textbf {z} \langle k \rangle \}_{k \in [\ell]}, \{ \mathcal {U}_{1} \langle k \rangle, \mathcal {U}_{2} \langle k \rangle, \mathcal {U}_{3} \langle k \rangle, \\&\mathcal {U}_{4} \langle k \rangle \}_{k \in [\ell]}, \mathcal {V}_{1}, \mathcal {V}_{2}),\end{align*}
View Sourcewhere \begin{align*} {{\boldsymbol{z}}} \langle k \rangle = \begin{cases}{{ \sum _{i \in \mathcal {U}_{3} \langle k \rangle \setminus \mathcal {C}} {{\boldsymbol{v}}}_{i}, }}&{{ if | \mathcal {U}_{3} \langle k \rangle | \geq t}}\\\perp , &{otherwise.} \end{cases}\end{align*}
View Source

Lemma 1:
Assume the hardness of discrete logarithm and the security of {\boldsymbol{COM}} . In the Amotized- Verification step, an honest client will accept the aggregation results \textbf {a} \langle 1 \rangle, \ldots, \textbf {a} \langle \ell \rangle if and only if these results are honestly aggregated by the server with an overwhelming probability.

Proof:
Assume there exists a PPT adversary that succeeds in letting some honest party \mathcal {P}_{i} output its forged aggregation results \bar {\textbf {a}} \langle k \rangle \neq \textbf {a} \langle k \rangle = \sum _{i} \textbf {v}_{i} \langle k \rangle for all k \in \mathcal {K} \subseteq [\ell] in BatchChecking. We first observe that, since \mathcal {P}_{i} does not output \perp , the decommitment in BatchChecking should be done successfully. For decommitment strings (h_{j} \langle k \rangle, r_{j} \langle k \rangle) where k \in \mathcal {K} and j \in \mathcal {U}_{3} \langle k \rangle \cap \mathcal {C} , the adversary has the freedom whether to send correct decommitment strings in Decommitting. If it chooses to send malformed decommitment strings, the decommitment in BatchChecking fails with a non-negligible probability since COM is binding. The same argument applies to the case where the adversary sends to \mathcal {P}_{i} its incorrectly reconstructed decommitment strings of (honest) dropped parties in DroppedDecommitting.

The above argument implies that the adversary cannot change the hash values it has committed in ShareMetadata on behalf of corrupted parties without having \mathcal {P}_{i} output \perp . Recall that, by protocol specification, the honestly aggregation result \textbf {a} \langle k \rangle and the final hash h^{*} satisfy \begin{align*} h^{*}=&{\textsf {LHH}}.\textsf {Hash}(\textbf {a}^{*}) = {\textsf {LHH}}.\textsf {Hash} \left ({\sum _{k \in [\ell]} \alpha _{k} \textbf {a} \langle k \rangle }\right) \\=&\prod _{j \in [d]} g_{j}^{\sum _{k \in [\ell]} \alpha _{k} \textbf {a} \langle k \rangle [j]} \\=&\left ({\prod _{j \in [d]} g_{j}^{\sum _{k \in \mathcal {K}} \alpha _{k} \textbf {a} \langle k \rangle [j]} }\right) \cdot \left ({\prod _{j \in [d]} g_{j}^{\sum _{k \in [\ell] \setminus \mathcal {K}} \alpha _{k} \textbf {a} \langle k \rangle [j]} }\right)\end{align*}
View SourceRight-click on figure for MathML and additional features.Meanwhile, with unchanged hashes committed in ShareMetadata, we have another linear combination \overline {\textbf {a}} \leftarrow \sum _{k \in \mathcal {K}} \alpha _{k} \bar {\textbf {a}} \langle k \rangle + \sum _{k \in [\ell] \setminus \mathcal {K}} \alpha _{k} \textbf {a} \langle k \rangle and therefore \begin{align*}&\hspace {-2pc} {\textsf {LHH}}.\textsf {Hash}(\overline {\textbf {a}}) \\=&{\textsf {LHH}}.\textsf {Hash} \left ({\sum _{k \in \mathcal {K}} \alpha _{k} \bar {\textbf {a}} \langle k \rangle + \sum _{k \in [\ell] \setminus \mathcal {K}} \alpha _{k} \textbf {a} \langle k \rangle }\right) \\=&\left ({\prod _{j \in [d]} g_{j}^{\sum _{k \in \mathcal {K}} \alpha _{k} \bar {\textbf {a}} \langle k \rangle [j]} }\right) \cdot \left ({\prod _{j \in [d]} g_{j}^{\sum _{k \in [\ell] \setminus \mathcal {K}} \alpha _{k} \textbf {a} \langle k \rangle [j]} }\right).\end{align*}
View SourceSince \mathcal {P}_{i} does not output \perp , it should hold that h^{*} = {\textsf {LHH}}.\textsf {Hash}(\overline {\textbf {a}}) , i.e., \begin{equation*} \prod _{j \in [d]} g_{j}^{\sum _{k \in \mathcal {K}} \alpha _{k} \textbf {a} \langle k \rangle [j]} = \prod _{j \in [d]} g_{j}^{\sum _{k \in \mathcal {K}} \alpha _{k} \bar {\textbf {a}} \langle k \rangle [j]}.\end{equation*}
View Sourcewhere \textbf {a} \langle k \rangle \neq \bar {\textbf {a}} \langle k \rangle for k \in \mathcal {K} . Notice that the coefficients \alpha _{k} for k \in \mathcal {K} are uniformly drawn from \mathbb {F}_{q} , the event that h^{*} = {\textsf {LHH}}.\textsf {Hash}(\overline {\textbf {a}}) and \begin{equation*} \sum _{k \in \mathcal {K}} \textbf {a} \langle k \rangle [j] = \sum _{k \in \mathcal {K}} \bar {\textbf {a}} \langle k \rangle [j]\end{equation*}
View Sourcein which j \in [d] and \textbf {a} \langle k \rangle \neq \bar {\textbf {a}} \langle k \rangle for k \in \mathcal {K} happens with a probability 1/ | \mathbb {F}_{q} | , which is negligible in the security parameter \kappa . In other words, conditioned on h^{*} = {\textsf {LHH}}.\textsf {Hash}(\overline {\textbf {a}}) , it is hold that \begin{equation*} \sum _{k \in \mathcal {K}} \textbf {a} \langle k \rangle [j] \neq \sum _{k \in \mathcal {K}} \bar {\textbf {a}} \langle k \rangle [j]\end{equation*}
View SourceRight-click on figure for MathML and additional features.for some j \in [d] , which induces a collision of LHH.

It is readily seen that such a PPT adversary can break either the binding property of COM or the collision resistance (Definition 1) of LHH with a non-negligible probability, which is infeasible under the assumption. This completes the proof.

Theorem 1:
Assume the hardness of discrete logarithm and the security of {\boldsymbol{COM}} . VeriFL achieves integrity of aggregation according to Definition 3.

Proof:
This theorem is straightforward from Lemma 1.

Theorem 2:
Assume the security of SE, KA, SS and COM. VeriFL achieves input privacy according to Definition 4 in the CRS-hybrid model.

Proof:
The proof is identical to that for Theorem 6.3 in [7], except that we should additionally take care of the hashes being committed in ShareMetadata. Notice that SIM did not know the real inputs of honest parties by the time it was asked to compute the hash as well as the commitment. The strategy of SIM is to hash a dummy vector (e.g., the one filled with 0’s) and commit this dummy hash value. Give that COM is hiding and the secret sharing scheme hides messages being shared, the joint view w.r.t. the aggregation phase of parties in \mathcal {C} will be indistinguishable from that in \textsf {REAL}_{ \mathcal {C}}^{ \mathcal {P}, t, \kappa } .

It remains to show that the joint view w.r.t. the verification phase of parties in \mathcal {C} can also be simulated by SIM. Note that the simulated commitment is committed to the dummy hash which is different (with an overwhelming probability) to that of the vector sampled by SIM after it obtains the partially aggregation result of honest parties by querying \textbf {z} \langle k \rangle for k \in [\ell] . Recall that COM is equivocal and, in the CRS-hybrid model, SIM has the trapdoor td which is associated with the COMpp output by COM. Setup. So, using the trapdoor td, SIM can equivocate these commitments to the hashes of inputs sampled by it on behalf of honest parties. The equivocality of COM (Definition 2) guarantees that these commitments are consistent with hashes of the simulated inputs of honest parties and therefore can be successfully decommitted with an overwhelming probability. Moreover, since \mathcal {C} forms an unqualified set to reconstruct shared secrets by assumption, SIM can adjust its shares held on behalf of honest parties such that the reconstructed decomittment strings match those obtained from equivocation. That is, the verification phase can be simulated, which completes the proof.

SECTION V.Evaluation
A. Experimental Setup
We show the performance of our protocol based on a prototype implementation. The prototype is written in Java while we use JNI to implement low-level cryptography algorithms. In particular, linearly homomorphic hash LHH is realized using elliptic curve over the NIST P-256 curve. For efficient equivocal commitment COM, we use folklore hash commitment scheme instantiated with SHA-256, which is proved secure in restricted programmable and observable global random oracle model [30]. For secret sharing scheme, we adopt standard t -out-of- N Shamir secret sharing. For key agreement, we use elliptic curve Diffie-Hellman over the NIST P-256 curve with SHA-384. The symmetric encryption is instantiated with AES-OFB mode with 256-bits key and 128-bits IV. For pseudo-random generator, we use AES-CTR mode. In addition, we fix two moduli R= 2^{24} and B= 2^{34} (i.e., the maximum number of clients is B/ R= 2^{10} = 1024 ) and assume all clients will input honestly as in our adversarial model.

We simulate clients and the aggregation server on a 64-bits Ubuntu 16.04 LTS desktop equipped with Intel i7-7700 CPU (3.60 GHz) and 16 GB RAM. The simulation is single-threaded. Given that end-to-end networking time will not significantly influence the asymptotic computation complexity of our protocol and can be calculated using bandwidth, we omit this part in our evaluation.

1) Dropout Cases:
For client dropout, we consider the following two cases of dropout in our protocol:

Case I dropout: In this case, clients drop out of the protocol after sending their metadata to other clients via server in ShareMetadata but before sending their masked gradients to the server in MaskedInputCollection. As discussed in [7], this dropout case will lead to the worst performance of aggregation phase in our protocol for the expensive computation overhead to recover the pairwise-mask.

Case II dropout: In this case, some clients drop out before reporting their decommitment strings to other clients via the aggregation server in Decommitting. The server has to recover these strings of dropped clients by asking all surviving clients to send their shares of these strings and running secret reconstruction algorithm to recover them. This will lead to the most expensive computation overhead in our verification phase.

B. Comparison With VerifyNet [8]: Dimension-Independence
Given that the protocol in [8] does not allow Case II dropout and amortization, we consider only the Case I dropout and set the batch size to 1 in our comparison. In addition, we set the number of clients N= 500 and fix the threshold t= \frac {1}{2} N . All other parameters remain unchanged as above. It is readily seen from Figure 6(a) and Figure 6(b) that, in our protocol, the outgoing communication cost for verification of either each client or the server is independent of the dimension of gradients. However, in [8], the two metrics is linearly dependent on the dimension of gradients, which results in impractical performance when the dimension is large enough. A more comprehensive comparison with respect to the overall overhead is presented in Figure 6. It is easy to see that our protocol outperforms VerifyNet completely. A more encouraging result is that our protocol can be scaled to support high-dimensional gradient vectors (e.g., 100K) with even better performance than that achieved by VerifyNet in dealing with low-dimensional one.

Fig. 6. - Comparison between our protocol and VerifyNet [8] in terms of (i) outgoing communication overhead for verification, (ii) the total computation overhead, and (iii) the total outgoing communication overhead, as the dimension of gradient vectors increases. (a) Outgoing communication overhead for verification per client. (b) Outgoing communication overhead for verification of the server. (c) Computation overhead per client. (d) Outgoing communication overhead per client. (e) Computation overhead of the server. (f) Total outgoing communication overhead of the server.
Fig. 6.
Comparison between our protocol and VerifyNet [8] in terms of (i) outgoing communication overhead for verification, (ii) the total computation overhead, and (iii) the total outgoing communication overhead, as the dimension of gradient vectors increases. (a) Outgoing communication overhead for verification per client. (b) Outgoing communication overhead for verification of the server. (c) Computation overhead per client. (d) Outgoing communication overhead per client. (e) Computation overhead of the server. (f) Total outgoing communication overhead of the server.

Show All

C. Comparison With Secure Aggregation [7]
In Table I, Table II, Table III and Table IV, we give out the additional costs for verification in our protocol. The baseline protocol is [7] that addresses the input privacy of mobile clients. It is easy to see from underlined bold figures that our protocol does not introduce too much additional communication overhead to the baseline secure aggregation protocol per client. For example, when the dimension of gradient vectors is set to 100K, the communication for transferring a gradient is always 488.28 KB, which takes the largest proportion of the overall communication per client. Compared with that, the additional communication caused by verification protocol is insignificant. As for the computation overhead, the one caused by our protocol is also affordable compared with the one in [7]. In the real-world applications of FL, it is not uncommon that each local iteration of machine learning requires tens of seconds. When deploying our protocol to these applications where dropout is not severe (e.g., < 1% occurred naturally as reported in [7]), the overall computation overhead will not blow up since our protocol has similar wall-clock running time with a local iteration of machine learning.

TABLE I Computation Overhead With Respect to Case I Dropout1
Table I- 
Computation Overhead With Respect to Case I Dropout1
TABLE II Outgoing Communication Overhead With Respect to Case I Dropout1

TABLE III Computation Overhead With Respect to Case II Dropout1

TABLE IV Outgoing Communication Overhead With Respect to Case II Dropout1

D. Other Experimental Results
1) Amortized Verification:
In Figure 7, it is clear that, with the growth of batch size, the amortized verification overhead per epoch can be reduced nearly to that of a single {\textsf {LHH}}.\textsf {Hash} call. In other words, with a large enough batch size, we can approximately halve the computation overhead of a client in the sense of amortization. However, in real-world applications where there are dropouts in each epoch, the batch size cannot be set too large given that too large verification batch will lead to severe accumulation of dropouts and therefore the expensive overhead to run secret reconstruction algorithm. We regard this phenomenon as a tradeoff and therefore the batch size should be fine-tuned in real-world applications.

Fig. 7. - Comparison between total computation cost for verification, amortized computation cost for verification and total computation cost per client. The dimension of gradient vectors is set to 100K and the number of clients is 500. Assume no dropout.
Fig. 7.
Comparison between total computation cost for verification, amortized computation cost for verification and total computation cost per client. The dimension of gradient vectors is set to 100K and the number of clients is 500. Assume no dropout.

Show All

2) Number of Clients:
Both the computation and communication cost in client side grows linearly in the number of clients, which results from the fact that the number of secret shares generated in ShareMetadata is proportional to the number of clients. For the wall-clock running time of the server, it grows quadratically in the number of clients. The reason is that, in Unmasking, the server has to run secret reconstruction algorithm to recover the secret mask key of dropped clients and both the threshold t and the number of dropped clients are proportional to the number of clients N .

3) Dropout:
As shown in Figure 8 and Figure 9, it is clear that, in client side, either the computation cost or the communication cost is independent of how many other clients drop out of the protocol execution. However, the two metrics in server side is influenced by the dropout rate, especially the wall-clock running time of the server. We note that the overhead suffered by the server is acceptable.

Fig. 8. - Total computation and outgoing communication overhead, as the number of clients increases. (a) Computation overhead per client. (b) Outgoing communication overhead per client. (c) Computation overhead of the server. (d) Outgoing communication overhead of the server (for a single client). The dimension of gradient vectors is set to 100K and the batch size is 1. Assume Case I dropout.
Fig. 8.
Total computation and outgoing communication overhead, as the number of clients increases. (a) Computation overhead per client. (b) Outgoing communication overhead per client. (c) Computation overhead of the server. (d) Outgoing communication overhead of the server (for a single client). The dimension of gradient vectors is set to 100K and the batch size is 1. Assume Case I dropout.

Show All

Fig. 9. - Total computation and outgoing communication overhead, as the dimension of gradient vectors increases. (a) Computation overhead per client. (b) Outgoing communication overhead per client. (c) Computation overhead of the server. (d) Outgoing communication overhead of the server (for a single client). The number of clients is set to 500 and the batch size is 1. Assume Case I dropout.
Fig. 9.
Total computation and outgoing communication overhead, as the dimension of gradient vectors increases. (a) Computation overhead per client. (b) Outgoing communication overhead per client. (c) Computation overhead of the server. (d) Outgoing communication overhead of the server (for a single client). The number of clients is set to 500 and the batch size is 1. Assume Case I dropout.

Show All

SECTION VI.Related Work
In this section, we briefly discuss the works related to secure verifiable aggregation in federated learning.

A. Generic Maliciously-Secure Multiparty Computation
In general, FL involves an aggregation server and a set of clients and therefore can be regarded as a specific multiparty computation problem. Although there are piles of works [14]–[15][16][17][18][19][20][21], [31] guarantee the integrity of computation in the existence of a malicious (i.e., active) adversary, they are not suitable for FL settings. For garbled circuit-based protocols [19]–[20][21], [31], they deal with the malicious adversary at the cost of expensive communication overhead (e.g., that led by commit-and-prove [18] or cut-and-choose [21], [32] technique) and cannot be deployed on a large scale. For secret sharing-based protocols [14]–[15][16], each client has to divide each entry of its input into (additive) shares and send them to each other client, which results in a rather expensive communication overhead in O(Nd) . In addition, these protocols rely on homomorphic encryption to compute MACs of shares. Their distributed homomorphic decryption sub-protocols cannot tolerate dropout in a threshold manner directly and therefore are not suitable for FL settings where dropout is common.

B. Machine Learning Based on Cryptographic Protocols
There are two lines of the cryptographic research regarding privacy and verifiability issues in machine learning, i.e., secure model prediction and secure model training. For secure model prediction, the goal is to securely query a model without revealing model inputs and the information of the model. All works [33]–[34][35][36][37] require that the model being queried was well-trained in advance, which differs from the motivation of FL.

For secure model training, there are several works [38]–[39][40] being proposed to train a model while guarantee integrity of the derived model and the privacy of users. Unfortunately, these works will result in expensive overhead when applied to FL settings. For example, Mohassel et al. [38] proposed a framework to convert between any two kinds of secret shares and built up general protocols to train a neural network, where each client is supposed to share each entry of its gradient vector. The communication overhead will blow up as the dimension of vector increases. In addition, as the number of clients increases, to preserve the robustness against dropout, the size of the share held by each client blows up as well.

There are also some works [41], [42] based on homomorphic encryption (HE) which address input privacy only. These works rely on a stronger adversarial model where the secret decryption key of encrypted gradients is only known to all clients and these clients are honest. However, in our adversarial model (Section II-C), the adversary can acquire this key from controlled corrupted clients and decrypts all encrypted gradients, which undermines the security of [41], [42]. It is possible to secret share the decryption key (in a threshold manner, e.g., [43], [44]) among all clients to mitigate this issue. Unfortunately, such a construction complicates the decryption of encrypted gradients and, in the FL setting where dropout is common, will significantly increase the size of key shares and the communication between the server and a client as the number of clients increases. So, we believe the protocol based on double-masking is more efficient than that based on HE. In addition, HE is a primitive that can be used to address input privacy only. To achieve both input privacy and verifiability, some other cryptographic primitives (e.g., zero-knowledge proof) are required. This paper finds such a primitive that can be used to replace time-consuming zero-knowledge proof adopted by [8].

To the best of our knowledge, there is only one work [8] for secure verifiable aggregation in FL. However, this work adopts heavy zero-knowledge proof, which results in impractical performance and unaffordable communication overhead.

C. Differential Privacy
A notable fact is that privacy-preserving cryptographic protocols cannot fully prevent privacy disclosure in machine learning. It is known that privacy disclosure can be resulted from the statistical characteristics (e.g., confidence information, prediction outcome) of machine learning algorithms. For example, there are some black-box privacy attacks [22]–[23][24][25][26] leveraging these statistical characteristics. Treating cryptographic protocols as black-box oracles, such black-box attacks can be applied to the machine learning paradigms even if they are securely realized by cryptographic protocols. That is, as long as these protocols implement the functionality of machine learning, the statistical leakage is inevitable. A possible defense is to combine the cryptographic protocols with differential privacy [45]–[46][47][48].

Differentially private federated learning [49]–[50][51][52][53] has been extensively studied in the literature. A few of existing works [49], [53] consider how to combine differentially private mechanisms with multiparty computation to protect not only the statistical characteristics but also the raw inputs of all honest clients. However, these works do not address the verifiability issue in FL. In this work, we are devoted to fix this issue of FL with practical overhead and differential privacy is not the primary goal. A straightforward way to make our protocol differentially private is to add Gaussian or Laplacian noises to original gradients and then regard the noisy gradients as inputs to our protocol, although it might not achieve the best utility of the global model. An interesting direction for future work would be to explore how to design an optimal differentially private mechanism that is compatible with our practical secure verifiable aggregation protocol.

D. Byzantine-Robust Aggregation
A Byzantine client in FL can send arbitrary values to the aggregation server to influence model convergence. To deal with this issue, one popular mitigation is to use Byzantine-robust aggregation, which is mostly related to our aggregation protocol. There are several alternative aggregation mechanisms [54]–[55][56][57] to the standard federated averaging to ensure that convergence is not significantly influenced by Byzantine clients, in which [57] is for non-IID settings. As noted in [58], it is quite challenging to devise a Byzantine-robust aggregation mechanism for non-IID FL datasets.

Byzantine-robust aggregation can be regarded as a supplement to the secure verifiable aggregation based on cryptographic protocols since the latter guarantees only input privacy and the integrity of aggregation with respect to the used inputs. In other words, secure verifiable aggregation cannot prevent Byzantine clients to use malformed inputs to do harm to either the training process or the resulting global model.

However, how to combine Byzantine-robust aggregation [54]–[55][56][57] with secure verifiable aggregation is still an open problem. There are several technical challenges to be addressed by future work. First, these works adopt a completely different adversarial model to that of secure verifiable aggregation (e.g., Section II-C). That is, Byzantine aggregation assume only Byzantine clients that send arbitrary values to an honest aggregation server and does not consider the case where the server is corrupted. Second, these works implicitly assume the aggregation server has access to the plaintext stochastic gradients uploaded by clients, which violates input privacy. Since Byzantine-robust aggregation mechanisms usually adopt more complicated arithmetic (e.g., median or trimmed mean) than federated averaging, it is challenging to devise a tailored dropout-tolerant protocol for these mechanisms to achieve input privacy and verifiability additionally.

SECTION VII.Conclusion
In this paper, we studied how to realize verifiable aggregation in FL in a communication-efficient and fast way, and proposed a protocol named VeriFL. We show by experiments that VeriFL is capable of dealing with (i) high-dimensional gradient vectors, (ii) a large number of clients, and (iii) high dropout rate in real-world applications with practical performance. Notably, in VeriFL, the verification cost in terms of the outgoing communication is independent of the gradient dimension, resulting in 110.6\times improvement in communication even with 5\times larger gradient vectors in our experiments.