Forensic analyses of digital images rely heavily on the traces of in-camera and out-camera processes left on the acquired images. Such traces represent a sort of camera fingerprint. If one is able to recover them, by suppressing the high-level scene content and other disturbances, a number of forensic tasks can be easily accomplished. A notable example is the PRNU pattern, which can be regarded as a device fingerprint, and has received great attention in multimedia forensics. In this paper, we propose a method to extract a camera model fingerprint, called noiseprint, where the scene content is largely suppressed and model-related artifacts are enhanced. This is obtained by means of a Siamese network, which is trained with pairs of image patches coming from the same (label +1) or different (label -1) cameras. Although the noiseprints can be used for a large variety of forensic tasks, in this paper we focus on image forgery localization. Experiments on several datasets widespread in the forensic community show noiseprint-based methods to provide state-of-the-art performance.

SECTION I.Introduction
In the last few years, digital image forensics has been drawing an ever increasing attention in the scientific community and beyond. With cheap and powerful cameras available to virtually anyone in the world, and the ubiquitous diffusion of social networks, images and videos have become a dominant source of information. Unfortunately, they are used not only for innocent purposes, but more and more often to shape and distort people’s opinion for commercial, political or even criminal aims. In this context, image and video manipulations are becoming very common, and increasingly dangerous for individuals and society as a whole.

Driven by these phenomena, in the last decade, a large number of methods have been proposed for forgery detection and localization or camera identification [1], [2]. Some of them rely on semantic or physical inconsistencies [3], [4], but statistical methods, based on pixel-level analyses of the data, are by far the most successful and widespread. Mostly, they exploit the fact that any acquisition device leaves on each captured image distinctive traces, much like a gun barrel leaves peculiar striations on any bullet fired by it.

Statistical methods can follow both a model-based and a data-driven approach. Methods of the first class try to build mathematical models of some specific features and exploit them for forensic purposes. Popular targets of such analyses are lens aberration [5]–[6][7], camera response function [8]–[9][10], color filter array (CFA) [11]–[12][13] or JPEG artifacts [14]–[15][16]. Having models to explain the available evidence has an obvious appeal, but also a number of shortcomings, first of all their usually narrow scope of application.

As an alternative, one can rely on data-driven methods, where models are mostly abandoned, and the algorithms are trained on a suitably large number of examples. Most data-driven methods work on the so-called noise residual, that is, the noise-like signal which remains once the high-level semantic content has been removed. A noise residual can be obtained by subtracting from the image its “clean” version estimated by means of denoising algorithms, or by applying some high-pass filters in the spatial or transform (Fourier, DCT, wavelet) domain [17]–[18][19][20][21][22]. Noise residuals can be also used in a blind context (no external training) to reveal local anomalies that indicate possible image tampering [23]–[24][25][26].

Among all methods based on noise residuals, those relying on the photo-response non-uniformity noise (PRNU) deserve a special attention for their popularity and performance. In the seminal paper by Lukáš et al. [28] it was observed that each individual device leaves a specific mark on all acquired images, the PRNU pattern, due to imperfections in the device manufacturing process. Because of its uniqueness, and stability in time, the PRNU pattern can be regarded as a device fingerprint, and used to carry out multiple forensic tasks. PRNU-based methods have shown excellent performance for source identification [29] and for image forgery detection and localization [28], [30]–[31][32][33]. Note that they can find any type of forgeries, irrespective of their nature, since the lack of PRNU is seen as a possible clue of manipulation. The main drawbacks of PRNU-based methods are i) the need of a large number of images taken from the camera to obtain good estimates and ii) the low power of the signal of interest with respect to noise, which impacts heavily on the performance. In particular, the prevailing source of noise is the high-level image content, which leaks in the PRNU due to imperfect filtering. The latter often overwhelms the information of interest, especially in the presence of saturated, dark or textured areas. This latter is a typical problem of all the methods based on noise residuals.

In this work, to overcome these problems we propose a new method to extract a noise residual. Our explicit goal is to improve the rejection of semantic content and, at the same time, emphasize all the camera-related artifacts, which contribute to the digital history of an image. To this end, we follow a data driven approach and exploit deep learning. A suitable architecture is designed, inspired by Siamese networks, and trained on a large dataset which includes pristine images from many different camera models. During training we only need to know whether the extracted patches come from the same camera and position or not. Therefore, we take advantage of hidden spatial dependencies, also with the help of a suitable spectral loss. Once the training is over, the network is freezed, and can be used with no further supervision on images captured by any camera model, both inside and outside the training set. To any single image the network associates a noise residual, called noiseprint from now on, which shows clear traces of camera artifacts. Therefore, it can be regarded as a camera model fingerprint, much like the PRNU pattern represents a device fingerprint. It can also happen that image manipulations leave traces very evident in the noiseprint, such to allow easy localization even by direct inspection. As an example, Fig. 1 shows two images subject to a splicing forgery, which can be easily detected by visual inspection of their noiseprints. It is worth observing that these artifacts cannot be spotted so clearly using other noise residuals (see Fig. 2).


Fig. 1.
Two forged images (left) with their noiseprints (right). The inconsistencies caused by the manipulation are visible in the extracted noiseprint.

Show All

Fig. 2. - From left to right: the forged image, its noiseprint, the noise residual obtained using a Wavelet-based denoising filter [27] (a tool commonly used for PRNU extraction) and the noise residual obtained through a 3rd order derivative filter (used in the Splicebuster algorithm [26]).
Fig. 2.
From left to right: the forged image, its noiseprint, the noise residual obtained using a Wavelet-based denoising filter [27] (a tool commonly used for PRNU extraction) and the noise residual obtained through a 3rd order derivative filter (used in the Splicebuster algorithm [26]).

Show All

In the rest of the paper, we first analyze related work on noise residuals to better contextualize our proposal (Section II), then describe the proposed architecture and its training (Section III), carry out a thorough comparative performance analysis of a noiseprint-based algorithm for forgery localization (Section IV), provide ideas and examples on possible uses of noiseprints for further forensic tasks (Section V), and eventually draw conclusions (Section VI).

SECTION II.Related Work
A. Exploiting Noise for Image Forensics
The observation that the local noise level within an image may help revealing possible manipulations dates back at least to 2004, with the work of Popescu and Farid [23]. The underlying idea is that each image has an intrinsic uniform amount of noise introduced by the imaging process or by digital compression. Therefore, if two images are spliced together, for example, or some local post-processing is carried out on part of the image to hide traces of tampering, inconsistencies in the noise level may occur, which can be used to reveal the manipulation. In [23] the local noise variance is estimated over partially overlapping blocks based on the second and fourth moments of the data, assuming the kurtosis of signal and noise to be known. Detection of inconsistencies is then left to visual inspection. In [24] the same approach is adopted, but the noise variance is estimated through wavelet decomposition and a segmentation process is carried out to check for homogeneity. In [25], instead, the local noise level is estimated based on a property of natural images, the projection kurtosis concentration, and estimation is formulated as an optimization problem with closed-form solution. Further methods based on noise level inconsistencies have been recently proposed in [34] and [35]. A major appeal of all these unsupervised methods is their generality. They require only a reliable estimator of noise variance to discover possible anomalies, and need no further hypotheses and no training. On the down side, since the noise due to in-camera processing is certainly non-white, using only intensity as a descriptor neglects precious information.

This consideration justifies the quest for better noise descriptors and the use of machine learning in forensics. One of the first methods in this class, proposed back in 2005 [36], exploits statistics extracted from the high-pass wavelet subbands of the image to train a suitable classifier. In [18], the set of wavelet-based features of [36] is augmented with prediction error features computed on a noise residual extracted through denoising. A more accurate discrimination is carried out in [19], [20] by computing both first-order and higher-order Markovian features on DCT or Wavelet coefficients and also on prediction errors. Interestingly, these features were inspired by prior work carried out in steganalysis [37]. This is the same path followed by the popular rich models, which were proposed originally in steganalysis [38], and then applied successfully in image forensics for the detection and localization of various types of manipulations [22], [26], [39], [40]. Like in [18] the rich models rely on noise residuals, but multiple high-pass filters are used to extract them, and discriminative features are built based on the co-occurrence of small local patterns. Even though these methods exhibit a very good performance, they need a large training set to work properly, a condition rarely met in the most challenging real-world cases.

To overcome this limitation, the methods proposed in [26] and [41] exploit rich-model features only to perform unsupervised anomaly detection. In Splicebuster [26] the expectation-maximization algorithm is used to this end, while [41] resorts to an ad hoc autoencoder-based architecture. Eventually, these methods are used for blind forgery detection and localization with no supervision or external training.

The papers by Swaminathan et al. [42], [43] are conceptually related to our noiseprint proposal since they aim at identifying all the possible traces of in-camera and out-camera processing, called intrinsic fingerprints. However, the proposed solution, strongly model-based, is completely different from ours. The traces of interest are estimated on the basis of a suitable camera model. Then, any further post-processing is regarded as a filtering operation whose coefficients can be estimated using blind deconvolution. In the end, inconsistencies in the estimated model parameters suggest that the image has been manipulated in some way. However, correctly modeling such processes is quite difficult, and this approach does not work well in realistic conditions. It is also worth mentioning a recent paper [44] in which traces of camera model artifacts in the noise residual are preserved and collected in the so-called sensor linear pattern, and exploited to find inconsistencies.

B. Using Deep Learning for Image Forensics
Recently, deep learning methods have been applied to image forensics. Interestingly, the first proposed architectures, inspired again by work in steganalysis [45], all focus on suppressing the scene content, forcing the network to work on noise residuals. This is obtained by adding a first layer of high-pass filters, either fixed [46], [47], or trainable [48], [49], or else by recasting a conventional feature extractor as a convolutional neural network (CNN) [50]. A two-stream network is proposed in [51], [52] to exploit both low-level and high-level features, where a first network constrained to work on noise residuals is joined with a general purpose deep CNN (ResNet 101 in [52]). Slightly different CNN-based architectures have been proposed in [53], [54].

All such solutions, however, rely on a training dataset strongly aligned with the test set, which limits their value for real-world problems. Instead, to gain higher robustness, the training phase should be completely independent of the test phase. This requirement inspires a group of recently proposed methods [55]–[56][57] which share some high-level ideas with our own proposal. In [55] a CNN trained for camera model identification is used to analyze pairs of patches: different sources suggest possible image splicing. Results look promising, but only a synthetic dataset is used for experiments, and the performance degrades sharply if the camera models are not present in the training set. A similar approach, based on a similarity network, is followed in [56] for camera model identification. First, the constrained network of [48] is trained to extract high-level camera model features, then, another network is trained to learn the similarity between pairs of such features, with a procedure similar to a Siamese network. A Siamese network is also used in [57] to predict the probability that two image patches share the same value for each EXIF metadata attribute. Once trained, the network can be used on any possible type of image without supervision. This is a very important property that we also pursue in our work. Unlike in [57], however, we do not use metadata information in the training phase, but rely only on the knowledge of the provenance (same camera or not) of pairs of pristine patches.

SECTION III.Proposed Approach
A digital camera carries out a number of processes to convert the input light field into the desired output image. Some of these processes, like data compression, interpolation, and gamma correction, are common to virtually all cameras, although with different implementations. Other processing tools, which are included to offer more advanced functionalities and to attract customers, can vary from model to model. Due to all these internal processing steps, each camera model leaves on each acquired image a number of artifacts which are peculiar of the model itself, and hence can be used to perform forensic analyses. However, such artifacts are very weak and their exploitation requires sophisticated statistical methods. To this end, a typical approach consists in extracting a noise residual of the image, by means of a high-pass filter or a denoiser.

Our goal is to improve the noise residual extraction process, enhancing the camera model artifacts to the point of allowing their direct use for forensic analyses. Accordingly, the product of our system will be an image-size noise residual, just like in PRNU-based methods, a noiseprint image that will bear traces of camera model artifacts, and possibly of some individual device imperfections.

In the following two subsections we describe the noiseprint extraction process, based on the Siamese network concept, and provide implementation details on the network training.

A. Extracting Noiseprints
Our aim is to design a system, Fig. 3(top), which takes a generic image as input and produces a suitable noise residual in output, the image noiseprint. As said before, the noiseprint is desired to contain mostly camera model artifacts. For sure, we would like to remove from it, or strongly attenuate, the high-level scene content, which acts as a disturbance for our purposes. This latter is precisely the goal of the CNN-based denoiser proposed by Zhang et al. [58]. In fact, rather than trying to generate the noiseless version of the image, this denoiser, Fig. 3(bottom), aims at extracting the noise pattern affecting it (by removing the high-level content). Eventually, this is subtracted from the input to obtain the desired clean image. For this reason, this denoiser is obviously a good starting point to develop our own system, so we keep its architecture, and initialize it with the optimal parameters obtained in [58] for AWGN image denoising. Then, we will update such parameters through a suitable training phase.

Fig. 3. - Using CNNs to extract noise residuals. Top: the target CNN processes the input image to generate its noiseprint, a suitable noise residual with enhanced model-based artifacts. Bottom: the CNN proposed in [58] processes the input image to generate its AWGN pattern, a strict-sense noise residual.
Fig. 3.
Using CNNs to extract noise residuals. Top: the target CNN processes the input image to generate its noiseprint, a suitable noise residual with enhanced model-based artifacts. Bottom: the CNN proposed in [58] processes the input image to generate its AWGN pattern, a strict-sense noise residual.

Show All

In [58] these parameters have been obtained by training the CNN with a large number of paired input-output patches, where the input is a noisy image patch and the output its noise content (see Fig. 4). We should therefore resume the training by submitting new paired patches, where the input is a generic image patch, and the output the corresponding noiseprint. The only problem is that we have no model of the image noiseprint therefore we cannot produce the output patches necessary for this training procedure.


Fig. 4.
Training the CNN-based denoiser. To each clean patch yi of the dataset a synthetic AWGN pattern wi is added to create a noisy patch: xi=yi+wi . The (xi,wi) pairs are used to train the CNN. The distance between the residual generated by the CNN, ri=f(xi) , and the true noise pattern, wi , is back-propagated to update the net weights.

Show All

Nonetheless, we have precious information to rely upon. In fact, we know that image patches coming from the same camera model should generate similar noiseprint patches, and image patches coming from different camera models dissimilar noiseprint patches. Leveraging this knowledge, we can train the network to generate the desired noise residual where not only the scene content but all non-discriminative information is discarded, while discriminative features are enhanced.

Consider the Siamese architecture of Fig. 5, formed by the parallel of two identical CNNs, that is two CNNs which have both the same architecture and the same weights. Two different input patches acquired with the same camera model are now fed to the two branches. Since the outputs are expected to be similar, the output of net 1 can take the role of desired output for the input of net 2, and vice-versa, thus providing two reasonable input-output pairs. For both nets, we can therefore compute the error between the real output and the desired output, and back-propagate it to update the network weights. More in general, all pairs formed by the input to one net and the output to its sibling represent useful training data. For positive examples (same model) weights are updated so as to reduce the distance between the outputs, while for negative examples (different models) weights are updated to increase this distance. It is worth emphasizing that negative examples are no less important than positive ones. Indeed, they teach the network to discard irrelevant information, common to all models, and keep in the noiseprint only the most discriminative features.


Fig. 5.
Using a Siamese architecture for training. The output of one CNN takes the role of desired (same model and position) or undesired (different models or positions) reference for the other twin CNN.

Show All

Until now, for the sake of simplicity, we have neglected the following important point. In order for two input patches to merit a positive label, they must come not only from the same camera model but also from the same position in the image. In fact, artifacts generated by in-camera processes are not spatially stationary, just think of JPEG compression with its regular 8×8 grid, or to the regular sampling pattern used for acquiring the three color channels. Therefore, noiseprint patches corresponding to different positions are different themselves (unless the displacement is a multiple of all artifacts’ periods), and input patches drawn from different positions must not be pooled during training, in order not to dilute the artifacts’ strength. An important consequence for forensic analyses is that any image shift, not to talk of rotation, will impact on the corresponding noiseprint, thereby allowing for the detection of many types of manipulations.

When the training process ends, the system is freezed. Consequently, to each input image a noiseprint is deterministically associated, which enhances the camera model artifacts with their model-dependent spatial distribution. Of course, the noiseprint will also contain random disturbances, including traces of the high-level scene. Nonetheless, the enhanced artifacts appear to be much stronger than these disturbances, and such to provide a satisfactory basis for forensic tasks.

B. Implementation
In the previous subsection our aim was to convey the main ideas about the nature of noiseprints and how to extract them. Here we provide crucial implementation details, which allow a fast and accurate training of the system.

1) Initialization:
As already said, we start from the architecture of the denoiser proposed in [58], shown in some more detail in Fig. 6. Like in [58], for complexity issues, the network is trained using minibatches of N patches of K×K pixels, with N=200 and K=48 . It is worth underlining, however, that the system is fully convolutional and hence, once trained, it works on input images of any given size, not just 48×48 patches. Therefore, there is no patch stitching issue.


Fig. 6.
Detailed architecture of the CNN-based denoiser.

Show All

2) Boosting Minibatch Information:
Concerning training, it should be realized that the Siamese architecture is only an abstraction, useful to understand how input-output pairs are generated. In practice, there is only one CNN, which must be trained by submitting a large number of examples. Each minibatch, as said before, includes N=200 patches, which are used to form suitable input-output pairs. However, this does not imply that only N/2 pairs can be formed: in fact, each individual patch can be meaningfully combined with all the others, as proposed in [59], lifting a batch of examples into a dense pairwise matrix. When two patches come from the same model and image position, the pair will have a positive label, otherwise a negative label. Therefore, each minibatch provides O(N2) examples rather than just O(N) , with a significant speed-up of training. In particular, to implement this strategy, our minibatches comprise 50 groups of 4 patches. Each group is internally homogeneous (same model same position) but heterogeneous with respect to the other groups.

3) Distance-Based Logistic Loss:
As for the loss function, L , we adopt the distance based logistic (DBL) proposed in [60]. Let {x1,…,xn} be a minibatch of input patches, and {r1,…,rn} the corresponding residuals output by the net. Then, let dij=∥ri−rj∥2 be the squared Euclidean distance between residuals i and j . We require such distances to be small when (i,j) belong to the same group, and large otherwise. Now, for the generic i -th patch, we can build a suitable probability distribution through softmax processing as
pi(j)=e−dij∑j≠ie−dij(1)
View SourceWith this definition, our original requirement on distances is converted in the requirement that pi(j) be large whenever (i,j) are in the same group, that is, lij=+1 , and small otherwise. This leads us to define the i -th patch loss as
Li=−log∑j:lij=+1pi(j)(2)
View SourceWhen all the probability mass is concentrated in same-group patches, the sum is unitary and the loss is null, while all deviations from this condition cause an increase of the loss. Accordingly, the minibatch loss is defined as the sum of all per-patch losses, hence
L0=∑i⎡⎣−log∑j:lij=+1pi(j)⎤⎦(3)
View Source

4) Regularization:
To encourage diversity of noiseprints, we add a regularization term to the previous DBL loss. Let
Ri(u,v)=F(ri(m,n))(4)
View SourceRight-click on figure for MathML and additional features.be the 2D discrete Fourier transform of patch ri , where (m,n) and (u,v) indicate spatial and spectral discrete coordinates, respectively. The quantity
S(u,v)=1N∑i=1N|Ri(u,v)|2(5)
View SourceRight-click on figure for MathML and additional features.is therefore an estimate of the power spectral density (PSD) of the whole minibatch. For a given camera model, the power spectrum will peak at the fundamental frequencies of artifacts and their combinations. It is expected, however, that different camera models will have different spectral peaks. Actually, since the locations of such peaks are powerful discriminative features, it is desirable that they be uniformly distributed over all frequencies. To this end, we include in the loss function a regularization term given by the log-ratio between geometric and arithmetic means of the PSD components
R==log[SGMSAM][1K2∑u,vlogS(u,v)]−log[1K2∑u,vS(u,v)](6)
View SourceIn fact, the GM/AM ratio is maximized for uniform distribution, and therefore its inclusion encourages the maximum spread of frequency-related features across the model noiseprints. Eventually, the complete loss function reads as
L=L0−λR(7)
View Sourcewith the weight λ to be determined by experiments.

SECTION IV.Experimental Analysis
We now provide some experimental evidence on the potential of noiseprints for forensic analyses. Camera fingerprints can be used for a multiplicity of goals, as proven by the large body of literature on the applications of PRNU patterns. In Section V we provide some insights into the possible uses of noiseprints. However, we leave a detailed investigation of all these cases for future work, and focus, instead, on just one major forensic task, the localization of image manipulations, irrespective of their nature. To analyze performance in depth, we carry out an extensive experimental analysis, considering 9 datasets of very different characteristics, and comparing results, under several performance criteria, with all the most promising reference techniques. In the rest of this Section, we first present our noiseprint-based localization method, then describe the reference methods, the datasets, and the performance metrics, provide details on the training of the noiseprint extractor, and finally present the results of preliminary ablation studies and of comparative experiments.

A. Forgery Localization Based on Noiseprints
In the presence of localized image manipulations, the image noiseprint shows often clear traces of the manipulation, allowing direct visual detection and localization. However, this is not always the case, and an automatic localization tool is necessary to support the analyst’s work. In particular, we look for a localization algorithm which takes the image and its noiseprint as input, and outputs a real-valued heatmap which, for each pixel, provides information on the likelihood that it has been manipulated.

Here, we use the very same blind localization algorithm proposed for Splicebuster [26]. By so doing, we obtain an objective measure of the improvement granted by adopting the image noiseprint in place of the third-order image residual used in [26]. The algorithm assumes that the pristine and manipulated parts of the image are characterized by different models. Accordingly, it looks for anomalies w.r.t. the dominant pristine model to locate the manipulated part. To each pixel of a regular sampling grid, a feature vector is associated, accounting for the spatial co-occurrences of residuals. These vectors are then fed to the expectation-maximization (EM) algorithm, which learns the two models together with the corresponding segmentation map. The interested reader is referred to [26] for a more detailed description. However, it is worth emphasizing the blind nature of this localization algorithm, which relies only on the given image with no need of prior information.

B. Reference Methods
We consider only reference methods that do not need specific datasets for training or fine tuning, nor do they use metadata or other prior information on the test data. Besides being more general, these methods are less sensitive to dataset-related polarizations, allowing a fair comparison. Most of these methods can be considered state-of-the-art in the field, except for a few ones, like the error level analysis (ELA) included for their diffusion among practitioners. They can be roughly grouped in three classes according to the features they exploit: i) JPEG artifacts [14], [61]–[62][63][64][65], ii) CFA artifacts [12], [13], iii) inconsistencies in the spatial distribution of features [24]–[25][26], [57], [66], [67]. Table I lists all methods under comparison together with a link to the available source or executable code. To save space, we use compact acronyms, for example EXIF-SC to mean EXIF self-consistency algorithm [57]. Our own proposed noiseprint-based localization algorithm will be referred to simply as Noiseprint (capital letter) from now on.

TABLE I Reference Methods

C. Datasets
To assess performance we use 9 datasets, which are listed in Table II together with their main features. Some of them focus only on splicing, like the DSO-1 dataset [4], the VIPP dataset [14], created to evaluate double JPEG compression artifacts, and the FaceSwap dataset [51], where only automatic face manipulation have been created using code available on-line.1 All other datasets, instead, present a wide variety of manipulations, sometimes cascaded on one another on the same image.

TABLE II Datasets

They also present very different characteristics in terms of number of cameras, resolution and format. For example, the dataset proposed by Korus and Huang [68] comprises only raw images of the same resolution, acquired by only four different cameras. This low variability can induce some polarizations of the results. On the contrary, the NIMBLE2 and MFC3 datasets designed by NIST for algorithm development and evaluation in the context of the Medifor program, are extremely variable, beyond what can be found in real practice. Therefore, they can be considered very challenging benchmarks for all methods under test.

D. Performance Measures
Forgery localization can be regarded as a binary classification problem. Pixels belong to one of two classes, pristine (background or negative) or forged (foreground or positive), and a decision must be made for each of them. All performance metrics rely on four basic quantities

TP (true positive): # positive pixels declared positive;

TN (true negative): # negative pixels declared negative;

FP (false positive): # negative pixels declared positive;

FN (false negative): # positive pixels declared negative;

Since the last two items correspond to errors, a natural performance measure is the overall accuracy
A=TP+TNTP+TN+FP+FN(8)
View SourceHowever, often there are many more negative than positive pixels, and errors on positive pixels impact very little on accuracy, which becomes a poor indicator of performance. This is exactly the case of forgery localization, where the manipulated area is often much smaller than the background.
To address this problem a number of other metrics have been proposed. Precision and recall, defined as
precision=TPTP+FPrecall=TPTP+FN
View Sourceput emphasis on the positive (forged) class, measuring, respectively, the method’s ability to avoid false alarms and detect forged pixels. These quantities are summarized in a single index by their harmonic mean, the F1 measure
F1=21precision+1recall=2TP2TP+FN+FP
View SourceAnother popular metric is the Matthews Correlation Coefficient (MCC), that is, the cross correlation coefficient between the decision map and the ground truth, computed as
MCC=TP×TN−FP×FN(TP+FP)(TP+FN)(TN+FP)(TN+FN)−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−√
View Sourcewhich is robust to unbalanced classes.

Both F1 and MCC work on a binary decision map. However, most methods provide a continuous-valued heatmap, which is converted to a binary map by thresholding. Therefore, to free performance assessment from the threshold selection problem, for both F1 and MCC the maximum over all possible thresholds is taken. An alternative approach is followed with the Average Precision (AP) which is computed as the area under the precision-recall curve, and therefore can be regarded as an average of performance measures over all thresholds. In order to carry out a solid assessment of performance, we consider all three measures, F1, MCC, and AP.

Note that, in some cases, the output heatmap may have an inverted polarity w.r.t. the ground truth (see Fig. 7). Since this is immaterial for the semantics of the map, but may disrupt performance, we always consider both the original and inverted heatmaps, and keep the best of the two. Finally, we exclude from the evaluation pixels near the foreground-background boundary, where all methods are very unreliable due to limited resolution.


Fig. 7.
From left to right: forged images, ground truth, heatmaps from the method proposed in [65] and [24]. The forged area can be dark or light, this does not really matter, since the approaches look for image inconsistencies.

Show All

E. Training Procedure
For the proposed method the network used to extract all noiseprints is trained on a large variety of models. To this end, we formed a large dataset, including both cameras and smartphones, using various publicly available datasets, plus some other private cameras. In detail, we used

44 cameras from the Dresden dataset [69],

32 from Socrates dataset [70],

32 from VISION [71],

17 from our private dataset,

totaling 125 individual cameras from 70 different models and 19 brands. For the experiments, this dataset is split, on a per-camera basis, in training and validation sets, comprising 100 and 25 cameras, respectively. We note explicitly that the datasets used to form the training and validation sets are not used in the test phase.
All images are originally in JPEG format at the native compression factor and resolution, without any post-processing. The network is initialized with the weights of the denoising network of [58]. During training, each minibatch contains 200 patches of 48×48 pixels extracted from 100 different images of 25 different cameras. In each batch, there are 50 sets, each one formed by 4 patches with same camera and position. Training is performed using the ADAM optimizer. All hyper-parameters (learning rate, number of iterations, and weight of regularization term) are chosen using the validation set.

Considering the major impact of JPEG compression on performance, we use multiple networks, one for each JPEG quality factor (QF). The basic network is trained using all the images at native quality, without any further compression. In addition, 46 more networks are trained for JPEG QF ranging from 55 to 100, using exactly the same images as in the basic case, but now compressed to the specific quality factor. In the test phase, we estimate the QF of the input image and then apply the corresponding network. The basic network is instead used for all uncompressed images (e.g., TIF, PNG) or images with no estimate of the QF. The QF itself is estimated based on the quantization matrix of the luminance component, available in the image header. All trained nets are publicly available online (https://grip-unina.github.io/noiseprint/).

F. Preliminary Analyses
Before proceeding to comparative experiments, we carry out some preliminary analyses to support our key design choices. More precisely, we study how such choices impact on Noiseprint’s forgery localization ability. To this end, Table III shows results in terms of MCC, F1 and AP for two selected datasets, DSO-1 and Korus. The first row displays the results obtained with the proposed Noiseprint method. On the following row, instead, we can appreciate the effect of removing the regularization term in the loss. All indicators decrease, sometimes quite significantly, especially for the Korus dataset. A further experiment concerns the same-location constraint. We re-trained the network by labeling pairs of patches based only on source model, irrespective of spatial location. Again, a significant performance loss is observed for all indicators with respect to the proposed solution. Finally, we train the network using RAW images, with no JPEG compression. To this end, we use the MIT-Adobe FiveK Dataset4 described in [72], comprising 25 different camera models with 16 images per model. This is a relatively small training set, but RAW images are much rarer than JPEG images. Results are reported on the last row, and show a performance impairment on both datasets. On DSO-1 a huge loss is observed because the images, though saved in PNG format, were originally JPEG compressed. The lack of JPEG-related information in the training set impacts heavily on performance. Instead, the TIF images of the Korus dataset are truly uncompressed, but the new training set, more aligned with the test data, does not improve performance, probably also because of its smaller size.

TABLE III Comparing Training Modalities

In our next experiment, we explore the impact of multiple QF-adapted networks on performance. Table IV shows results for two different datasets, NIMBLE16 and NIMBLE17-eval, characterized by the presence of images JPEG compressed at a wide variety of quality factors. Results indicate that using the fixed default network, trained on very high-quality images, causes a limited but consistent loss. So we will use the adaptive rule in order to optimize performance.

TABLE IV Fixed vs. Adaptive QF
Table IV- 
Fixed vs. Adaptive QF
Finally, we explore the impact of image resizing on DSO-1 dataset, where all the images have a fixed resolution. Fig. 8 plots the F1 indicator when test images are resized at various scales. A significant performance loss is observed as soon as resizing is applied, which grows slowly with increasing/decreasing scale, suggesting that some forms of scale adaptivity could improve robustness. In any case, similar effects are observed for all other methods, which keep performing worse than Noiseprint.

Fig. 8. - Impact of resizing on the F1 indicator for the DSO-1 dataset.
Fig. 8.
Impact of resizing on the F1 indicator for the DSO-1 dataset.

Show All

G. Comparative Experiments
We now present and discuss experimental results for all 9 datasets, 15 methods under comparison, and 3 performance metrics. Our choice is to consider one metric at a time, in order to allow a synoptic view of the performance of all methods over all datasets. Under this respect, it is worth noting in advance that, although numbers change significantly from one metric to another, the relative ranking of methods remains pretty much the same. Therefore, in the tables V–VII we report results in terms of MCC, F1 and AP, respectively. We complement each performance value with the corresponding rank on the dataset, in parentheses, using red for the three best methods, blue for the others. The last two columns show the average performance and average ranking over all datasets.

TABLE V Experimental Results: MCC (Matthews Correlation Coefficient)
Table V- 
Experimental Results: MCC (Matthews Correlation Coefficient)
TABLE VI Experimental Results: F1 (F-Measure)
Table VI- 
Experimental Results: F1 (F-Measure)
TABLE VII Experimental Results: AP (Average Precision)
Table VII- 
Experimental Results: AP (Average Precision)
We begin our analysis from these latter quantities which allow for a first large-scale assessment. The proposed noiseprint-based method provides the best average performance, MCC = 0.403, which is 10% better than the second best (Splicebuster) and much better than all the others, which go from 0.101 to 0.333. This is the effect of a uniformly good performance over all datasets. Noiseprint ranks always among the best three methods (red), with an average rank of 1.7, testifying of a remarkable robustness across datasets with wildly different characteristics. The comparison with Splicebuster (average MCC = 0.365, average ranking = 2.7) is especially meaningful, since the two methods differ only in the input noise residual, obtained through high-pass filtering in Splicebuster and given by noiseprint here. It is also worth noting that the third best technique, based on EXIF metadata inconsistencies, also looks for similarity among patches and uses a deep CNN, further supporting the soundness of the proposed approach. However, it needs a huge training set of 400000 images, 200 times larger than the training set used for Noiseprint.

Some specific cases deserve a deeper analysis. On the Korus dataset, for example, Noiseprint ranks only third, after CFA1 and Splicebuster. However, this is a dataset of raw images (not JPEG) while Noiseprint is trained on JPEG-compressed images. On this dataset, CFA-based methods perform especially well, since two of the four cameras (the two Nikon) fit very well the model developed in [13]. All this said, Noiseprint keeps providing a good performance, while methods based on JPEG artifacts show a dramatic impairment. Conversely, on the VIPP dataset, JPEG-based methods exhibit a boost in performance, especially ADQ1 and ADQ2 which look for double JPEG compression artifacts. Indeed, the VIPP dataset was built originally to expose this very type of artifacts. In this case as well, Noiseprint ranks among the best methods. These examples ring an alarm bell on the use of polarized dataset. In fact, these are precious tools to study a specific phenomenon but cannot be taken as reliable predictors of performance in uncontrolled scenarios. For this latter task, datasets should be much more varied and, even better, multiple independent datasets should be considered at once.

The NIMBLE and MFC datasets, developed by NIST under the Medifor program, fit very well this latter profile. They are characterized by a large variety of forgeries (e.g., splicing, copy-move, removal through inpainting, local blurring, contrast enhancement) often cascaded on the same image. Therefore, they represent very challenging testbeds, especially for robustness. In fact, all methods exhibit a worse average performance on these datasets than on the first four. Noteworthy, Noiseprint ranks always first or second on these datasets and, when second, just inches away from the best (0.324 vs. 0328, or 0.295 vs. 0.297). Since also Splicebuster and EXIF-SC perform quite well, it seems safe to say that spatial inconsistency is the key for good results. A relatively good performance is also ensured by ADQ2, CAGI, and NOI2.

Table VI and Table VII report experimental results for the F1 and AP metrics, with the same structure as Table V. We will keep comments to a minimum here, since the numbers, and especially the relative ranking, change very little by replacing one metric with another. The most remarkable variation is a slight improvement in the ranking of EXIF-SC on the AP metric, which is now the same as Splicebuster on the average. Noiseprint keeps providing the best average performance, with a remarkable stability across all datasets. Readers familiar with the F1 measure may notice the impressive 0.78 obtained on DSO-1, but this is a simple dataset, with large splicings and uncompressed images. Still, this shows that in favorable conditions, near-perfect localization is possible.

For completeness, in Fig. 9 we also present results in terms of receiver operating characteristic (ROC) curves, plotting, for each value of the false positive rate (FPR), the average true positive rate (TPR) over all tested images. For the sake of readability, we show curves only for the six best methods for each specific dataset. Even in this restricted set, Noiseprint curves are always among the best. It is also worth underlining that, due to the intrinsic inbalancing of the two classes, ROC curves are more relevant in the low FPR region.

Fig. 9. - ROC curves for all 9 datasets. For each dataset, only the curves for the best 6 methods are shown.
Fig. 9.
ROC curves for all 9 datasets. For each dataset, only the curves for the best 6 methods are shown.

Show All

A better insight on the actual quality of results can be gathered by the visual inspection of the examples of Fig. 10. Note that these examples were chosen from all datasets to highlight cases where Noiseprint provides much better results than the competitors. It is worth noting that the correct localization in these examples comes together with an accurate delineation of contours and rare false alarms. This is not always the case, of course, as is shown in Fig. 11. In general, errors are due to the leakage of high-level content into the noiseprint. This happens especially in the presence of strongly textured areas, since the leaked regular patterns, with almost periodic structures, are misinterpreted as traces of an alien noiseprint. A further critical case is given by very small images, with data too scarce to allow correct interpretation. Finally, global image processing, like compression, resizing, and blurring, tend to reduce the noiseprint strength and hence impair the performance of subsequent steps.

Fig. 10. - Examples from all the test datasets. From left to right: forged image, ground truth, heatmaps from the six best performing methods: ADQ2, CAGI, NOI2, EXIF-SC, Splicebuster, Noiseprint. Note that some heatmaps are color-inverted, for example, the NOI2 map for the Nim.16 image.
Fig. 10.
Examples from all the test datasets. From left to right: forged image, ground truth, heatmaps from the six best performing methods: ADQ2, CAGI, NOI2, EXIF-SC, Splicebuster, Noiseprint. Note that some heatmaps are color-inverted, for example, the NOI2 map for the Nim.16 image.

Show All

Fig. 11. - Noiseprint failure examples. Top: original image, middle: ground truth, bottom: heatmap. Problems are mostly due to strongly textured regions, whose leaks in the noise residual are interpreted as an alien noiseprint.
Fig. 11.
Noiseprint failure examples. Top: original image, middle: ground truth, bottom: heatmap. Problems are mostly due to strongly textured regions, whose leaks in the noise residual are interpreted as an alien noiseprint.

Show All

SECTION V.Further Noiseprint-Based Forensic Analyses
The main goal of this work was to present the noiseprint idea and its implementation. Forgery localization was selected to prove the potential of this approach on a well-studied forensic problem, where plenty of reference methods and datasets are available. It should be clear, however, that a strong camera model fingerprint can be used for many other applications in the forensic field. This Section is meant to highlight some possible applications, providing some very basic experimental evidence with no claim of completeness.

A. Camera Model Identification
A first obvious application is camera model identification. If the image noiseprint bears traces of camera-model artifacts, it should allow one to perform model identification, just like the PRNU noise pattern can be used for device identification. Accordingly, we carried out a very basic source identification experiment. We took 3 different camera models (Nikon D70, Nikon D200 and Smartphone OnePlus) with 2 devices per model and 100 training images per device. These were used to estimate the ideal reference patterns of the six devices, by averaging the extracted noiseprints. Then, we took 50 more out-of-training images per device and, using only the central crop, attributed them to the model whose reference pattern had minimum Euclidean distance w.r.t. the image noiseprint. The resulting model identification accuracy was 100%, both with 1024×1024 and 128×128 crops.

Armed with such good results, we went one step further, repeating the whole procedure on a per-device basis. That is, we extracted 6 reference patterns, one per device, and tried to perform device identification. The resulting confusion matrices are reported in Table VIII and Table IX for 1024×1024 and 128×128 crops, respectively. Together with the noiseprint-based results we also show, for reference, those obtained using the PRNU approach of [28] on the same data. In both cases, the noiseprint-based analysis keeps identifying the model perfectly. Interestingly, it ensures also a decent device identification, with accuracies of 75.3% and 61.7%, respectively, well above the 50% expected with random choice. This suggests that also some device-specific artifacts leak in the noiseprint. Of course, the PRNU-based method provides much higher accuracies, 91.3% and 69.7%, respectively. However, when errors occur, they are evenly distributed on all devices, with no obvious bias towards the same-model device.

TABLE VIII Confusion Matrices for Camera Identificationfor 1024×1024 Image Crops
Table VIII- 
Confusion Matrices for Camera Identificationfor 
$1024\times1024$
 Image Crops
TABLE IX Confusion Matrices for Camera Identificationfor 128×128 Image Crops
Table IX- 
Confusion Matrices for Camera Identificationfor 
$128\times128$
 Image Crops
To further support our claim, that noiseprints actually characterize camera models, let us consider the experiments described by Fig. 12. Each scatter plot is generated by taking two noiseprints of same-size images and plotting a point for each pixel, using the value of the first[second] noiseprint as abscissa[ordinate]. The resulting clouds allows for a visual assessment of noiseprint correlation. In particular, top-row plots refer to plain (single-image) noiseprints, while in bottom-row plots, to reduce the impact of the scene, averages of 25 homologous noiseprints are used. From left to right, instead, we consider same-device, same-model (different device), and different model cases. While the left and middle clouds are clearly skewed, testifying of some correlation between the noiseprints under test, this is not the case for the right clouds. Hence, noiseprints appear to be correlated if and only if they refer to the same model, and the correlation does not increase if the same device is used. Of course, all phenomena are more obvious when using the cleaner average noiseprints. Similar results, not shown for brevity, emerged for other camera models.

Fig. 12. - Noiseprint-vs.-noiseprint scatter plots. Top: single-image noiseprints. Bottom: average noiseprints over 25 homologous images.
Fig. 12.
Noiseprint-vs.-noiseprint scatter plots. Top: single-image noiseprints. Bottom: average noiseprints over 25 homologous images.

Show All

B. Identification of Demosaicing Algorithm
Among the features captured by a noiseprint, we expect to find artifacts related to demosaicing. If this is true, it should be possible to use noiseprints to tell apart images demosaiced by different algorithms. Aimed at testing this conjecture, we designed the following experiment. From the RAISE dataset [73], we took 100 RAW images of size 2048×2048 , all acquired by the same device, a Nikon D7000, to ensure that they share the same digital history. Then, we applied five different well-known demosaicing algorithms to generate the corresponding RGB images, and extracted their noiseprints. Lacking 100 more suitable (with the same white balancing) images of the same camera, we used a leave-one-out strategy, averaging 99 noiseprints per group to generate reference patterns and classifying the remaining one, with a minimum distance rule. The confusion matrix of Table X testifies of a very good classification ability, with an overall accuracy of 85.2%. Moreover, using smaller 512×512 crops, the accuracy decreases to 83.40%, showing that identification is possible even with fewer data.

TABLE X Confusion Matrix for Demosaicing Identification
Table X- 
Confusion Matrix for Demosaicing Identification
C. Identification of JPEG Quality Factor
Following the same line of reasoning, we carried out an experiment to test the noiseprint ability to keep traces of JPEG compression. We considered the same 100 images as before, used a fixed demosaicing algorithm (Linear) to convert them to RGB, and generated six versions JPEG compressed at different quality factors, from 80 to 99, very close to one another. Again, we used a leave-one-out strategy to estimate reference patterns and classify the remaining image. The resulting confusion matrix, reported in Table XI shows near-perfect classification. Overall accuracy remains very high, 99.00% also when only 512×512 -pixel crops are used.

TABLE XI Confusion Matrix for QF Identification
Table XI- 
Confusion Matrix for QF Identification
D. Identification of Real-to-Integer Conversion: JPEG Dimples
In [16] it was shown that JPEG compression generates some peculiar artifacts, called dimples, related with the real-to-integer conversion in the quantization phase. In particular, different dimples are generated for FLOOR, ROUND and CEIL conversion. Using always the same images, with linear demosaicing, we carried out JPEG compression with QF=98 and the three types of conversion. Even for crops as small as 128×128 pixels, we obtained an overall classification accuracies of 99%.

E. Detection of Unconventional Image Manipulations
Let us now consider a less widespread type of manipulation, like seam carving. In Fig. 13 we show, on the left, some images subject to seam carving [74], horizontal, vertical, or both, and, on the right, the corresponding noiseprint heatmaps obtained as described in Section IV-A. In the heatmaps, traces of the inserted seams are clearly visible, allowing easy detection of the manipulation by a human observer.

Fig. 13. - Seam-carved images (left part) and corresponding noiseprint heatmaps (right part). Top-left: original image; diagonal: vertical/horizontal seam carving; bottom-right: vertical and horizontal seam carving.
Fig. 13.
Seam-carved images (left part) and corresponding noiseprint heatmaps (right part). Top-left: original image; diagonal: vertical/horizontal seam carving; bottom-right: vertical and horizontal seam carving.

Show All

Image inpainting has seen huge progresses with the advent of deep learning. A recently proposed [75] method based on generative adversarial networks has proven to produce results with a remarkably natural appearance in the presence of quite complex scenes. In Fig. 14 we show two such examples, together with the noiseprints extracted from inpainted images. A visual inspection of the noiseprints (with suitable zoom) reveals a clear textural change in correspondence of the inpainted area. Converting such information into an automatic algorithm for inpainting detection should not be difficult.

Fig. 14. - Noiseprints of images inpainted by an advanced GAN-based method.
Fig. 14.
Noiseprints of images inpainted by an advanced GAN-based method.

Show All

F. Localization of Splicing in Remote Sensing Images
We conclude this short review going back to image splicing. In this case, however, the target images shown in Fig. 15, were acquired by sensors mounted on board a satellite, which have quite different characteristics w.r.t. common camera sensors, and their own peculiar processing chain. Nonetheless, the associated noiseprints reveal quite clearly the manipulations, which are captured with great accuracy in the heatmaps.

Fig. 15. - In first line, there is a image of Washington by WorldView2 satellite (https://www.digitalglobe.com/resources/product-samples/washington-dc-usa). In the second line a image of Caserta by Ikonos satellite.
Fig. 15.
In first line, there is a image of Washington by WorldView2 satellite (https://www.digitalglobe.com/resources/product-samples/washington-dc-usa). In the second line a image of Caserta by Ikonos satellite.

Show All

SECTION VI.Conclusions
In this paper we proposed a deep learning method to extract a noise residual, called noiseprint, where the scene content is largely suppressed and model-related artifacts are enhanced. Therefore, a noiseprint bears traces of the ideal camera model fingerprint much like a PRNU residual bears traces of the ideal device fingerprint. In noiseprints, however, the signal of interest is much stronger than in PRNU residuals, allowing for the reliable accomplishment of many forensic tasks. Experiments on forgery localization provide support to this statement, but many more forensic applications can be envisioned. For example, completely synthetic images can be characterized by artificial fingerprints [76] and the use of a siamese network procedure can be helpful to highlight them.

Despite the promising results, one must keep in mind that no tool can solve all forensic tasks by itself. As an example, noiseprints can be useful for camera model identification, but cannot help for device identification. Therefore, the fusion of noiseprint-based tools with other approaches is a further topic of interest for future research.