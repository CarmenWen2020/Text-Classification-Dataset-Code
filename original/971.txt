This paper proposes a multi-agent deep reinforcement learning-based buffer-aided relay selection scheme for an intelligent reflecting surface (IRS)-assisted secure cooperative network in the presence of an eavesdropper. We consider a practical phase model where both phase shift and reflection amplitude are discrete variables to vary the reflection coefficients of the IRS. Furthermore, we introduce the buffer-aided relay to enhance the secrecy performance, but the use of the buffer leads to the cost of delay. Thus, we aim to maximize either the average secrecy rate with a delay constraint or the throughput with both delay and secrecy constraints, by jointly optimizing the buffer-aided relay selection and the IRS reflection coefficients. To obtain the solution of these two optimization problems, we divide each of the problems into two sub-tasks and then develop a distributed multi-agent reinforcement learning scheme for the two cooperative sub-tasks, each relay node represents an agent in the distributed learning. We apply the distributed reinforcement learning scheme to optimize the IRS reflection coefficients, and then utilize an agent on the source to learn the optimal relay selection based on the optimal IRS reflection coefficients in each iteration. Simulation results show that the proposed learning-based scheme uses an iterative approach to learn from the environment for approximating an optimal solution via the exploration of multiple agents, which outperforms the benchmark schemes.

SECTION I.Introduction
With the development of the fifth-generation (5G) wireless communication, physical layer security has been widely studied to provide secure wireless communications in recent years [1]. Unlike cryptographic techniques, physical layer security exploits the dynamics of fading channels for achieving the perfect secrecy performance and dose not require encryption keys [2]–[3][4]. Security is also particularly relevant for cooperative communication networks, which has been investigated in [3], [5], [6]. In [3], the secrecy rate performance of full-duplex (FD) decode-and-forward (DF) cooperative networks was studied with a self-interference cancellation technology. The authors in [5] proposed two linear precoding schemes to improve the secrecy rate performance in half-duplex (HD) amplify-and-forward (AF) relaying systems. To maximize the diversity gain, relay selection was also proposed in cooperative networks to reduce the secrecy outage probability in [6]. To further enhance the secrecy performance, a novel max-ratio buffer-aided relay selection was proposed to select the link with the largest signal-to-interference-ratio (SIR) in cooperative networks with buffering technology [7]. Then, the trade-off between the average delay and secrecy rate for the max-ratio scheme was investigated in a buffer-aided cooperative network [8]. In [9], the max-ratio and state-based schemes were amalgamated to reduce the secrecy outage probability and average delay for buffer-aided cooperative networks. Furthermore, in [10], the average secrecy rate in an energy-harvesting based buffer-aided cooperative network was enhanced by an adaptive transmission algorithm considering power constraints, buffer and energy storage. Although using buffer improves outage performance, it increases the instantaneous delay, which is a key issue in Internet of Things (IoT) networks [11]. A buffer-state-based probabilistic relay selection method was proposed to enhance the outage performance with delay constraint in [12]. In [13], the delay constrained throughput was investigated via deep reinforcement learning (DRL). However, physical layer security has not been considered in delay-constraint buffer-aided cooperative networks. This motivates us to study security communication systems to satisfy instantaneous delay constraints.

On the other hand, intelligent reflecting surface (IRS) is an emerging technique for beyond 5G wireless communications [14]–[15][16]. IRS is an array which consists of low-cost passive reflecting elements, each of which can be appropriately reconfigured to control its reflection coefficient independently to provide controllable phase shift and amplitude for reflecting or refracting the signals to the intended receiver. Therefore, the IRS-assisted secure networks have attracted much attention recently [17]–[18][19]. In [17], a joint design of the transmit covariance and the IRS phase shifts was proposed to maximize the secrecy rate via an alternating optimization-based algorithm. A joint beamforming vectors and IRS phase shifts optimization scheme based on the combination of the alternating optimization and semidefinite relaxation (SDR) methods were proposed to enhance the secrecy rate in [18]. In [19], a successive convex approximation based algorithm was proposed to improve the secrecy rate by considering the trajectory and power control of an unmanned-aerial-vehicle (UAV), and the phase shifts of the IRS. The effect of main parameters on secrecy performance in IRS-assisted wireless networks was investigated in [20]. To improve the average secrecy rate, an alternating optimization method was proposed to jointly optimize the UAV’s trajectory, the beamforming matrix and the transmit power for an IRS-assisted UAV communication network in [21]. However, the above works focused on point-to-point communications. Therefore, to amalgamate the benefits of both IRS and cooperative communications, the authors in [22] and [23] investigated the IRS phase shifts optimization to maximize the achievable rate for hybrid IRS with HD and FD relay networks. Two hybrid relay and IRS-assisted transmission schemes were proposed in [24] to achieve higher error performance and achievable rate, compared with IRS-only and relay-only transmissions. In [25], to further enhance the transmission quality, relay selection was investigated in IRS-assisted cooperative networks. Motivated by this, this paper proposes an IRS-assisted secure buffer-aided relay network. Moreover, we consider the practical phase shift model [26], where both the reflection amplitude and the phase shift vary with the reflection coefficient. Therefore, the design of the IRS reflection coefficient vectors and buffer-aided relay selection will be given to maximize the secrecy rate and throughput with secure and delay constraints in this paper.

The traditional optimization algorithms in most IRS-assisted networks require huge amount of computational resource with low-efficiency. The DRL algorithm was therefore introduced to solve the complicated optimization problems [27]. In [28], a joint optimization of transmit beamforming vectors and IRS continuous phase shifts was proposed via DRL to enhance the average sum rate. Furthermore, the DRL algorithm was applied to jointly optimize the beamforming vectors and IRS continuous phase shifts to enhance the secrecy rate for IRS-assisted networks in [29]. However, the phase-dependent amplitude variation has not been considered. Moreover, joint optimization of the buffer-aided relay selection and IRS reflection coefficients is a much more complicated high-dimensional problem for the proposed system. To solve this, we introduce the multi-agent DRL (MA-DRL) algorithm as in [30] to solve two related sub-problems, namely, buffer-aided relay selection and IRS reflection coefficients optimization. Considering the limitations of storage, computation ability, delay and energy for a single device, we further apply distributed DRL in [31] on multiple relay nodes to reduce the training cost and improve the convergence efficiency for IRS reflection coefficients optimization. The main contributions of this paper are summarized as follows:

We propose a joint buffer-aided relay selection and IRS reflection coefficient optimization scheme in IRS-assisted secure cooperative networks. Two optimization problems are considered: maximizing the average secrecy rate with the delay constraint and maximizing the throughput with the delay and secrecy constraints.

We introduce the MA-DRL method to solve the complicated optimization problem. In terms of optimizing the IRS reflection coefficients, an asynchronous distributed framework is proposed to apply multi-agent on relay nodes to train the global model without sharing the local data. On the other hand, the agent at the source for solving the buffer-aided relay selection problem learns its strategy by using the IRS reflection coefficients optimization solution from the distributed framework.

Simulation results show that the proposed MA-DRL algorithm can apply multi-agents to explore the environment to learn approximate solutions. Based on the rewards, the agents can build a global model to solve the two related sub-problems to generate the joint optimization strategy and achieve higher performance than the benchmark.

The rest of this paper is organized as follows: The system model and problem formulation are introduced in Section II. In Section III, MA-DRL scheme is proposed. Section IV provides the simulation results and verifies the performance of the proposed scheme. Finally, Section V concludes this paper.

SECTION II.System Model and Problem Formulation
A. System Model
In this paper, we focus on a two-hop IRS-assisted secure relay network which is consisted of one source S , one IRS I , one destination D , which is equipped with N reflecting elements, and K half-duplex (HD) randomize-and-forward1 relays Rk(k∈{1,2,…,K}) , each of which is equipped with a data buffer of finite size L . The system model is shown in Fig. 1. Based on the buffer-aided relaying technique, a S→Rk transmission is considered available when the buffer of Rk is not full, and a Rk→D transmission is considered available when the buffer of Rk is not empty. Furthermore, there is an untrusted node2 E which can eavesdrop the signal sent from the S and R . All nodes are assumed to be equipped with a single antenna. The IRS can control the reflection coefficients to change the phase shift and reflection amplitude for each IRS reflecting element independently. We assume that S is the controller of the network3 to determine the relay selection and IRS reflection coefficients, and can receive the required information from all relay nodes, while each relay node Rk can receive the channel state information (CSI) of channels for the transmissions to and from itself,4 and receives the related CSI of the eavesdropper5 from S . Furthermore, we assume no direct link between nodes S and D due to severe blocking or deep fading. The channels of S→Rk and Rk→D links are assumed to experience Non-Line-of-Sight (NLoS) Rayleigh fading, and the channels from and to I are assumed to experience Rician fading [44]. Therefore, the channel coefficient hij between nodes i and j is given by
hij=⎧⎩⎨⎪⎪⎪⎪h¯ij,κijκij+1−−−−−−√H^ij+1κij+1−−−−−−√h^ij,RayleighRician,(1)
View Sourcewhere κij is the Rician factor for hij . We assume h¯ij=g¯ijd−α¯/2ij in NLoS Rayleigh channels, where ij∈{SRk , SE,RkD,RkE} , g¯ij is modeled by a complex Gaussian fading with zero-mean and unit-variance, dij is the distance between nodes i and j , α¯ is the path loss exponent of NLoS Rayleigh fading links. On the other hand, we assume H^ij=g^ijd−α^L/2ij and h^ij=g^ijd−α^N/2ij are the Line-of-Sight (LoS) component and the NLoS component in Rician fading channels, respectively, where ij∈{SI,IRk,IE,RkI,ID} , α^L and α^N are the path loss exponents for the LoS and NLoS Rician fading channel, respectively, g^ij is modelled by a complex Gaussian fading with zero-mean and unit-variance, and g^ij is given by
g^ij=β−−√[1,e−jπsin∂ij,…,e−jπ(M−1)sin∂ij]T,(2)
View Sourcewhere β denotes the signal attenuation for the reference distance d0=1 m [45], ∂ij∈[0,2π] denotes the angle of arrival (AoA) or the angle of departure (AoD) of the corresponding signal.


Fig. 1.
System model of the secure hybrid buffer-aided relay and IRS network.

Show All

At a given time slot, when a S→Rk link is selected, S transmits the signal xS to both Rk and I . Then, I reflects the signal to Rk . The received signal at Rk is thus given by
ySRk=P−−√(hSRk+hHIRkΘhSI)xS+nRk,(3)
View Sourcewhere P is the transmit power at all nodes, nRk denotes the additive-white-Gaussian-noise (AWGN) with variance σ2n at Rk , Θ=diag(η1ejθ1,η2ejθ2,…,ηNejθN) denotes the diagonal reflection matrix for I , with θn∈[0,2π ) and ηm∈[0,1] denoting the phase shift and reflection amplitude of the n th IRS reflecting element, respectively. Without loss of generality, we denote v=[v1,v2,…,vN] as the reflection coefficient vector for I , ηn=|vn| and θn=arg(vn) for the n th IRS reflecting element. We adopt the practical IRS phase shift model to achieve the discrete reflection amplitudes and phase shifts based on the reflection coefficients from [26, Fig. 3(b)], where the effective resistance R is 2Ω . Moreover, in order to support the practical implementation [46], we assume discrete phase shifts for IRS reflecting elements. The range of discrete phase shifts is
χ≜{0,2πλ,…,(λ−1)2πλ},(4)
View SourceRight-click on figure for MathML and additional features.where λ is the number of quantization levels.

Then, based on (3), the channel capacity for a S→Rk link is given by
CSRk=log2⎛⎝1+P∣∣hSRk+hHIRkΘhSI∣∣2σ2n⎞⎠.(5)
View Source

Notice that E can also receive signals from S during the S→Rk transmission. Therefore, the received signal at E can be expressed as
ySE=P−−√(hSE+hHIEΘhSI)xS+nE,(6)
View SourceRight-click on figure for MathML and additional features.where nE denotes the AWGN with variance σ2n at E . The channel capacity for the S→E link is given by
CSE=log2(1+P∣∣hSE+hHIEΘhSI∣∣2σ2n).(7)
View SourceRight-click on figure for MathML and additional features.

Similarly, when a Rk→D link is selected, the received signals at D and E can be expressed as
yRkD=yRkE=P−−√(hRkD+hHIDΘhRkI)xRk+nD,P−−√(hRkE+hHIEΘhRkI)xRk+nE,(8)
View Sourcerespectively, where xRk denotes the signal from relay node Rk . Therefore, the channel capacities for the Rk→D and Rk→E links can be expressed as
CRkD=CRkE=log2(1+P∣∣hRkD+hHIDΘhRkI∣∣2σ2n),log2(1+P∣∣hRkE+hHIEΘhRkI∣∣2σ2n),(9)
View Sourcerespectively. Then, the secrecy rates for the S⟶Rk and Rk⟶D links can be expressed as
Cs(SRk)=Cs(RkD)=[CSRk−CSE]+,[CRkD−CRkE]+,(10)
View Sourcerespectively, where [x]+=max(0,x) .

B. Problem Formulation
We assume the link between nodes i and j is available for single-packet transmission when its capacity satisfies C(ij)≥ω , where ω denotes the target rate. Moreover, a secure transmission between i and j happens when Cs(ij)≥ς , where S denotes the target secrecy rate. Because the use of buffer leads to delay of data transmissions, we consider the delay constraint in this paper. The delay Δ of a given packet is defined as the time of transmitting this packet from S to D . To be specific, if a packet is transmitted successfully from S to Rk at time slot t , and then arriving at D at time slot t+1 successfully, it takes two time slots to arrive at D and its delay is Δ=2 . Furthermore, at a given time slot, a relay Rk is selected to transmit one packet from its buffer to the destination, or receive one packet from the source and save it in its buffer. Notice that only one link can be selected for transmission at a given time slot. Simultaneously, the IRS reflection coefficient vector v is used to alter the reflected signal to boost the signal quality collaboratively. We use F=0 to specify that S→Rk transmission is selected and F=1 to denote that Rk→D transmission is chosen. We aim to design an algorithm to obtain solutions to two different optimization problems in IRS-assisted secure cooperative networks. Thus, we have the following two cases:

To find out the optimal k , F and v to maximize the achievable average secrecy rate6 with delay constraint for the end-to-end transmission, the optimization problem can be formulated as
O(1)=maxk(t),F(t),v(t)1T∑t=1T(F(t)μ(Δ(t)≤ΔT)×μ(CRkD(t)≥ω)μ(CSRk(t−Δ(t))≥ω)×min{Cs(SRk)(t−Δ(t)),Cs(RkD)(t)}),s.t. k(t)∈{1,2,…,K},F(t)∈{0,1},v(t)=[v1(t),v2(t),…,vN(t)],θn(t)=arg(vn(t))∈χ,∀m,ηn(t)=|vn(t)|,lk(t)>0,when F(t)=1,lk(t)<L,when F(t)=0,(11)(11a)(11b)(11c)(11d)(11e)(11f)(11g)
View Sourcewhere T denotes the number of total time slots, μ(⋅)=1 if the input event is true and μ(.)=0 otherwise, and Δ(t) denotes the delay of the corresponding packet which is transmitted at time slot t , and ΔT denotes the target delay, and lk denotes the buffer state of relay Rk . (11f) and (11g) ensure that the buffer should not be empty for Rk→D transmissions and not full for S→Rk transmissions constraints.

To find out the optimal k , F and v to achieve the maximum throughput with delay and secrecy constraint, the optimization problem can be formulated as
O(2)=maxk(t),F(t),v(t)1T∑t=1T(F(t)μ(Δ(t)≤ΔT)×μ(Cij(t)≥ω)μ(Cs(ij)(t)≥ς)),s.t. k(t)∈{1,2,…,K},F(t)∈{0,1},v(t)=[v1(t),v2(t),…,vN(t)],θn(t)=arg(vn(t))∈χ,∀m,ηn(t)=|vn(t)|,lk(t)>0,whenF(t)=1,lk(t)<L,whenF(t)=0,(12)(12a)(12b)(12c)(12d)(12e)(12f)(12g)
View Sourcewhere the constraints are similar to that in (11). Due to the discrete integer decision variables in the function and discrete set in each constraint, the functions and constraints are non-convex in (11) and (12). Thus, they are non-convex optimization problems and too complex to be solved by an exhaustive search method in large-scale networks. Besides, many traditional algorithms require high computational complexity to solve the IRS phase shift optimization problem with fixed reflection amplitude [27], and traditional methods for IRS-assisted wireless communications only consider the continuous ideal IRS model. Furthermore, considering the buffering technology provides more choices for relay selection at each time slot, and maximizing the throughput with secrecy and delay constraints at D in T time slots is a challenge for traditional optimization algorithms in buffer-aided cooperative networks. Therefore, we introduce the MA-DRL algorithm to obtain the feasible k , F and v to solve these two optimization problems. The DRL algorithm can be used to optimize the IRS coefficients with discrete practice phase shift model, and buffer-aided relay selection with secrecy and delay constraints. Moreover, a multi-agent framework is applied to reduce the training cost for each single node and communication cost for the proposed network, compared with the centralized DRL training.

SECTION III.MA-DRL-Based Algorithm
Considering both the optimization problems in (11) and (12) which contain variables k and F for buffer-aided relay selection, and v for adjusting IRS reflection coefficients, we split each of the optimization problems into two sub-tasks: 1) IRS reflection coefficient optimization, and 2) the buffer-aided relay selection, to reduce the space of exploration for the proposed algorithm. We propose a multi-agent framework for DRL in secure IRS-assisted buffer-aided cooperative networks, as shown in Fig. 2. The framework of the proposed scheme consists of a controller S and K relay nodes participating in the training process. Considering the large state-action space for IRS reflection coefficient optimization in large-scale IRS systems, we apply the distributed DRL algorithm for IRS reflection coefficient optimization to reduce the training cost of each device for data processing and improve the convergence efficiency. Each relay node represents an agent which learns the solution for IRS reflection coefficient optimization to build its local model (local solution for the IRS reflection coefficient optimization problem), then an agent on S updates the global model (global solution for the IRS reflection coefficient optimization problem) based on the local models from all relay nodes. In each iteration, the proposed approach first learns to address the task of optimizing the IRS reflection coefficient via the asynchronous distributed DRL algorithm, and subsequently train the model for buffer-aided relay selection based on the optimized solution from the former task, finally aggregates the results from the two sub-tasks to generate the joint optimization model (joint solution for the optimization problem in (11) or (12)). Thus, rather than directly exploring the large space of the whole environment, MA-DRL can obtain the optimized K , F and v efficiently for (11) and (12) with low computational complexity.

Fig. 2. - The framework of multi-agent DRL in the proposed network.
Fig. 2.
The framework of multi-agent DRL in the proposed network.

Show All

A. Asynchronous Distributed DRL Algorithm for IRS Reflection Coefficient Optimization
Considering the fact that the complexity of the exhaustive search algorithm is KLM for searching the optimal IRS reflection coefficients for each iteration, the space of exploration for DRL is huge in large scale networks. Therefore, distributed DRL is introduced to solve this problem. The asynchronous method is considered to improve the convergence efficiency among all nodes participating in the training process. We will first introduce the framework of the distributed DRL and the elements of DRL, then give the details of DRL algorithm.

1) The Distributed Learning Framework and Basic Elements of DRL:
As shown in Fig. 3, the process of the asynchronous distributed learning framework has the following steps.

Step 1: Each of the relay nodes can train and update its own model in parallel. For example, the local agent on relay node Rk trains its local model based on its local data, which contains its buffer state and CSI information. To be specific, relay node Rk explores its environment to train the local deep neural network (DNN) model via DRL. After WL local iterations of training, Rk updates its local model and obtains the accumulated gradients.

Step 2: After updating the local model, the agent on Rk uploads its accumulated gradients to the controller S for updating the global model of IRS reflection coefficient optimization. Considering the differentiation between the computational resources of all relay nodes, we apply the asynchronous updating method to improve the efficiency of convergence for the global model. Thus, each local model uploads its accumulated gradients to update the global model on S asynchronously.

Step 3: After updating the global model on S in Step 2, the corresponding relay node Rk downloads the global model from S to update its local model.


Fig. 3.
The framework of the asynchronous distributed DRL network for IRS reflection coefficient optimization.

Show All

To be specific, there is an agent on each relay node to learn from its own environments with the local data via DRL, then train its local model to find out the local solution of IRS reflection coefficient optimization. In terms of training local models, the DRL algorithm generally consists of states, actions and rewards, which are defined for Case 1 and Case 2 as follows.

Case 1: To achieve the maximum average secrecy rate with delay constraint.

State: In the IRS-assisted transmissions, we define the state s(t)={hij(t),hHIj(t)hiI(t),hiE(t),hHIE(t)hiI(t)} at time slot t , to describe the dynamic IRS-assisted network according to (5), (7) and (9). In DRL, the environment can transit from one state to another possible future state by taking an action, which will be defined as follows.

Action: For a secure IRS-assisted buffer-aided relay network, an action for the task of IRS reflection coefficient optimization is to control v in (11) to reflect or refract the signal to the receiver. Therefore, we define the action a(t)=[v1(t),v2(t),…,vN(t)] at time slot t to provide controllable phase shifts and reflection amplitudes. Based on the action and state, the DRL algorithm can learn to make decisions for the proposed network with the pre-defined rewards, which will be introduced to help the DRL algorithm converge.

Reward: The objective of this sub-task is to adjust the IRS reflection coefficients to improve the secure transmission quality with a delay constraint. To be specific, the design of IRS reflection coefficients can improve the secure transmission between nodes i and j with the delay constraint. In other words, the requirements of C(ij)≥ω and Δ≤ΔT are satisfied in Case 1. The reward in DRL is designed to help the agent learn the solution of the task, and we give the agent a positive reward when the channel rate of the transmission satisfies the requirements based on the current state-action pair. To be specific, in Case 1, when the transmission requirements are satisfied, a packet is transmitted to D within the target delay and the agent can receive a positive reward. Moreover, the agent is designed to receive a negative reward when the corresponding transmission cannot support the requirements. Therefore, the proposed DRL based algorithm can map the relationship between the states and actions using the rewards.

Case 2: To achieve the maximum throughput with delay and secrecy constraints.

State: In Case 2, the states are designed as the same as in Case 1.

Action: In Case 2, the actions are designed as the same as in Case 1.

Reward: In Case 2, the goal is to achieve the maximum throughput with delay and secrecy constraints. Therefore, the secure transmission in Case 2 requires C(ij)≥ω , Cs(ij)≥ς , and Δ≤ΔT . On the other hand, in Case 2, when the requirements are satisfied, to encourage the agent giving higher priority to selecting high-security links, the positive reward is designed based on the secrecy rate of the transmitted packet. Moreover, the agent receives a negative reward when the requirements are not satisfied. This design can help to find out the optimal state-action pair for reinforcement learning.

Moreover, unlike traditional reinforcement learning, to combine the advantage of value-based or policy-based algorithms for efficient convergence, we will introduce a distributed asynchronous advantage actor-critic (A3C) algorithm [47] without sharing the global training environment, as the DRL-based solution for the task of IRS reflection coefficient optimization.

2) The Distributed Asynchronous Advantage Actor-Critic Algorithm:
To enhance the convergence performance and robustness of training in DRL, we apply the A3C algorithm to train the local models for each agent on relay nodes. In the A3C algorithm, there is an actor network and a critic network to evaluate the relation and advantage of each state-action pair, respectively. For example, at time slot t , the input for both the actor and critic networks is the current state s(t) . At the same time, the outputs are the probabilities for each corresponding action-state pair and the evaluation value Q for estimating the advantage of the state, respectively. Therefore, the estimation value V for s(t) is given by
V(s(t))=rs(t),a(t)+ρrs(t+1),a(t+1)+…+ρT−1×rs(t+T−1),a(t+T−1)+ρTQ(s(t+T);θc),(13)
View SourceRight-click on figure for MathML and additional features.where ρ is the discount factor of the critic network, rs(t),a(t) denotes the reward for the (s(t),a(t)) pair, θc is the DNN weights set of the critic network, the actions are estimated from the actor network. Then, we can obtain the advantage of the (s(t),a(t)) pair as
A(s(t))=V(s(t))−Q(s(t+N);θc),(14)
View SourceRight-click on figure for MathML and additional features.which is used to help the agent learn the advantage (or the disadvantage) of the corresponding state-action pairs from the actor network.

We introduce the DNN to form the actor and critic networks. The actor network θa is designed to estimate actions for optimizing a policy π to achieve the maximum throughput with delay and secrecy constraints. In the A3C algorithm, the estimated action for a given state is determined with the maximum probability value from the results of the actor network, where the loss function of the actor network can be expressed as
φA(t)=logπ(a(t),s(t);θa)(V(s(t))−Q(s(t);θc)).(15)
View Source

On the other hand, the critic network is used to calculate the loss for the actor network, to evaluate the advantage or the disadvantage of the current policy π in the actor network. The critic network can be trained by the corresponding loss function, which is given by
φC(t)=(V(s(t))−Q(s(t);θc))2.(16)
View Source

To calculate the gradients based on the loss functions in (15) and (16), we apply the RMSProp method [48] in DNN. In A3C, each agent in the proposed distributed DRL algorithm performs the advantage actor-critic (A2C) algorithm asynchronously in its thread, then upload its accumulated gradients to update the global model. To be specific, each relay node has its own agent to perform A2C asynchronously to train its local model and obtain the accumulated gradients as in Step 1. In each local iteration, the local agent generates a training sample as {s(t),a(t),rs(t),a(t)} to calculate the gradients. After WL local iterations, the relay node will upload its accumulated gradients to update the global model on S as in Step 2, and then download the global model to update its local model as in Step 3. Besides, the output of the global model on S can provide a solution for adjusting the reflection coefficients of IRS elements to meet the requirements of the channel rate and secrecy rate. This result can help the agent on S to solve the other sub-task, which is for buffer-aided relay selection.

B. DRL Algorithm for Buffer-Aided Relay Selection
Now we introduce DRL to select the best buffer-aided relay with delay and secrecy constraints. By considering the long term benefit in buffer-aided relay networks, we apply a DRL agent on the source S to solve the buffer-aided relay selection. The states, actions and rewards in this sub-task are defined as follows.

Case 1: To achieve the maximum average secrecy rate with a delay constraint.

State: We introduce A3C algorithm to solve the IRS reflection coefficients problem. The channel capacities for S→Rk , S→E and Rk→D/E can be obtained from (5), (7), (9), respectively, and the secrecy rate can be obtained based on (10). We assume that a valid transmission satisfies C(ij)≥ω in Case 1. Thus, the channel state of relay Rk can be described as ck∈{1,2,3,4} , where ck denotes the availability of S→Rk and Rk→D transmissions as

ck=1 : none of the two transmission is valid,

ck=2 : S→Rk is valid,

ck=3 : Rk→D is valid,

ck=4 : both transmissions are valid.

Therefore, it is easy for each relay to find out its channel states for secure transmission. Furthermore, the buffer state lk(t) of relay Rk denotes the number of packets in relay node Rk ’s buffer at time slot t . Thus, we form the state of DRL for buffer-aided relay selection as sr(t)={c1(t),c2(t),…,cK(t),l1(t),l2(t),…,lK(t)} , which includes the buffer states and the channel states of the proposed network.
Action: At a given time slot, the buffer-aided relay network needs to select a link for transmission. Considering that there are 2K links for transmission in the proposed network, we design the action as ar(t)={k,F} , where k∈{1,2,…,K} , F=0 means to select S→Rk transmission and F=1 denotes the fact that Rk→D transmission is selected. Therefore, the action is used to determine the variables k and F of the optimization problem in (12).

Reward: This sub-task aims to select the best link in the buffer-aided cooperative network to achieve the maximum average secrecy rate with a delay constraint. To be specific, the optimization of relay selection can help avoid invalid transmissions and address the delay issue. Therefore, we design the positive reward for the proposed scheme similar to that in Section III-A.1. Based on this reward, the DRL algorithm can learn to achieve the maximum delay constrained average secrecy rate for Case 1. Furthermore, we assign a negative reward for selecting an invalid transmission, unless all possible transmissions are invalid at a given time slot. This design can encourage the agent to avoid taking invalid actions and improve the convergence efficiency.

Case 2: To achieve the maximum throughput with delay and secrecy constraints.

State: In Case 2, the state are designed as the same as in Case 1, except that a valid transmission between nodes i and j is assumed to satisfy C(ij)≥ω and Cs(ij)≥ς .

Action: In Case 2, we design the actions as the same as in Case 1.

Reward: In Case 2, we aim to achieve the maximum throughput with delay and secrecy constraints. Therefore, the reward is designed to encourage the agent selecting links with high secrecy rate. To be specific, we give the positive reward to the agent based on the secrecy rate of the transmitted packet, while the negative reward is used when the requirements of the transmission are not satisfied.

Similar to the task of optimizing IRS reflection coefficients, we apply the A2C algorithm on S to address the problem of buffer-aided relay selection. Then, after training the models for two sub-tasks, we can form the states s(t) and sr(t) as the input, the variables k , F , and {\boldsymbol {v}} as the outputs, to train a global model for joint design of buffer-aided relay selection and IRS reflection coefficients. Furthermore, the space of exploration for the DRL algorithm is significantly reduced from 2K \times \lambda ^{N} to 2K + \lambda ^{N} by splitting each of the optimization problems into the two sub-tasks. Thus, the convergence efficiency of the DRL can be enhanced by the proposed algorithm. The pseudo-code of MA-DRL is summarized in Algorithm 1. Notice that the proposed algorithm is designed for both cases, but the states, actions and rewards are different in the two cases, as mentioned before.

Algorithm 1 Ma-DRL
Initialize the variables.

Initialize the actor network \theta _{a}^{r} and critic network \theta _{c}^{r} for buffer-aided relay selection.

Train the sub-task for IRS reflection coefficient optimization:

Initialize the global actor network \theta _{a} and critic network \theta _{c} .

Initialize the local actor network \theta ^{'}_{a} and critic network \theta ^{'}_{c} for each relay node.

repeat for each relay node thread:

Synchronize the local networks \theta ^{'}_{a} = \theta _{a} and \theta ^{'}_{c} = \theta _{c} .

for t = 1, 2, \ldots, W_{L} do

Use the policy \pi \big (a(t),s(t);\theta ^{'} \big) to select the action a(t) .

Obtain the reward r_{s(t),a(t)} .

Save the sample \{ s(t), a(t), r_{s(t),a(t)}\} .

end for

\begin{aligned} V(s(t)) = \begin{cases} 0, {\mathrm{ for~ final ~convergence }}\\ Q\big (s(t);\theta ^{'}_{c} \big), {\mathrm{ otherwise}} \end{cases} \end{aligned}

for t = W_{L}-1, W_{L}-2, \ldots, 1 do

V(s(t)) = r_{s(t),a(t)} + \rho V(s(t+1)) .

Get the gradients \nu (t) and \nu _{c}(t) for \theta ^{'} and \theta ^{'}_{c} based on (15) and (16) via RMSProp.

Accumulate gradients \nu for \theta ^{'} : \nu = \nu + \nu (t) .

Accumulate gradients \nu _{c} for \theta ^{'}_{c} : \nu _{c} = \nu _{c} + \nu _{c}(t) .

end for

Asynchronous update \theta _{a} with \nu and \theta _{c} with \nu _{c} .

Train the sub-task for buffer-aided relay selection:

for t = 1, 2, \ldots, W_{L} do

Use the policy {\pi }_{r} \big (a_{r}(t),s_{r}(t);\theta _{a}^{r} \big) to select the action a(t) .

Obtain the reward r_{r{s_{r}(t),a_{r}(t)}} .

Save the sample \{s_{r}(t),a_{r}(t),r_{r{s_{r}(t),a_{r}(t)}}\} .

end for

\begin{aligned} V(s_{r}(t)) = \begin{cases} 0, {\mathrm{ for ~final ~convergence }}\\ Q\big (s_{r}(t);\theta ^{r}_{c} \big), {\mathrm{ otherwise}} \end{cases} \end{aligned}

for t = W_{L}-1, W_{L}-2, \ldots, 1 do

V(s_{r}(t)) = r_{r}{s_{r}(t),a_{r}(t)} + \rho V(s_{r}(t+1)) .

Obtain the gradients \nu _{r}(t) and \nu _{r_{c}}(t) for \theta _{a}^{r} and \theta _{c}^{r} based on (15) and (16) via RMSProp.

Accumulate gradients \nu _{r} for \theta ^{'} : \nu = \nu + \nu (t) .

Accumulate gradients \nu _{r_{c}} for \theta ^{'}_{c} : \nu _{c} = \nu _{c} + \nu _{c}(t) .

end for

Update \theta _{a}^{r} with \nu _{r} and \theta _{c}^{r} with \nu _{r_{c}} .

\nu _{r} = 0 and \nu _{r_{c}} = 0 .

Update the global model for joint optimization.

until final convergence.

SECTION IV.Simulation Results
In this section, we analyze the performance of the proposed scheme via simulations. For comparison, we consider selecting the max-ratio buffer-aided relay selection scheme [7] with random IRS reflection coefficients as the benchmark. Unless otherwise stated, the parameters for the proposed network and algorithm are shown in Table I. Furthermore, the locations of S , I , D , R_{1} , R_{2} , R_{3} , R_{4} , R_{5} and E are (0, 0) m, (2, 24) m, (0, 40) m, (0, 20) m, (−0.88, 18.32) m, (−4.1, 21.92) m, (0.88, 19.52) m, (5.28, 18.8) m and (−28.4, 10.2) m, respectively.7 The AOD or AOA \partial _{ij} between nodes i and j is randomly distributed within [0,2 \pi ) [22], [44], [49]. We build both the actor network and critic network with 256, 128 and 128 neurons for DRL algorithms in the two tasks. We run the simulations on GPU Geforce GTX-2080 with the deep learning library TensorFlow.

TABLE I Simulation Parameters

Fig. 4 shows the average secrecy rate with delay constraint versus training iterations for the proposed scheme in Case 1. According to the results, it is clear that the average secrecy rate with delay constraint increases with the number of training iterations. The proposed MA-DRL algorithm achieves approximately 0.4 bps/Hz when B = 5 bits after 7,000 iterations, while the case with B = 2 bits obtains a solution of 0.35 bps/Hz after 5,000 iterations. Thus, the average secrecy rate increases with the quantization bits, resulting from the increase in the quantization levels as B increases. Moreover, more quantization bits lead to slower convergence due to the larger exploration space for reinforcement learning. Due to exploration mode of DRL in training, the curve of the convergence is not very stable. In terms of the implementation for IRS, the quantization bits vary in different scenarios. This result shows that the proposed reinforcement learning-based algorithm can learn from the environment to find the optimal solution in dynamic networks.

Fig. 4. - Average secrecy rate versus training iterations in Case 1.
Fig. 4.
Average secrecy rate versus training iterations in Case 1.

Show All

The comparison of the average secrecy rate with delay constraint between the proposed scheme and the benchmark in Case 1 is provided in Fig. 5. First, we can see that the average secrecy rate increases when the target delay becomes larger. This is because a larger value of target delay leads to decreasing the average delay of packets. Secondly, the results illustrate that the proposed learning-based algorithm can analyze the network to optimize its solution. For example, the proposed MA-DRL algorithm achieves approximately 0.37 bps/Hz when the target delay is \varsigma = 10 time slots and the transmit power to noise ratio is P/{\sigma }_{n}^{2} = 40 dB, while the max-ratio scheme only achieves 0.07 bps/Hz. In addition, compared with the case of P/{\sigma }_{n}^{2} = 35 dB, high signal-to-noise ratio (SNR) has more performance gain when the target delay is small. The reason is that if the target delay is large, each packet can be stored in the corresponding buffer for a long period, the relay selection algorithm has more possible choices at a given time slot to reduce the impact of low SNR. Moreover, the proposed MA-DRL achieves more performance gain with a larger value of IRS elements N , while the performance of the benchmark max-ratio with different N is similar. The reason is that the proposed algorithm optimizes the IRS reflection coefficients to improve the signal quality, while max-ratio selects random IRS reflection coefficients.

Fig. 5. - Average secrecy rate versus target delay in Case 1.
Fig. 5.
Average secrecy rate versus target delay in Case 1.

Show All

As we can see in Fig. 6, the average secrecy rate in each result decreases as the target rate increases due to the increasing outage probability. Furthermore, the proposed algorithm performs much better than the benchmark, and the reason is that the MA-DRL algorithm can learn to obtain the solution, which includes the buffer-aided relay selection and IRS reflection coefficients optimization under all constraints, while max-ratio only considers the buffer-aided relay selection in secure transmissions. Results show that the proposed MA-DRL algorithm achieves approximately 0.25 bps/Hz when the target rate \omega = 3 bps/Hz and the number of relay K = 5 , while the max-ratio scheme only achieves 0.1 bps/Hz. Moreover, MA-DRL also performs much better than max-ratio when K = 2 ; this result shows the learning-based algorithm can adjust its variables for different environments to find out the solution.

Fig. 6. - Average secrecy rate versus target rate in Case 1.
Fig. 6.
Average secrecy rate versus target rate in Case 1.

Show All

The results in Fig. 7 show the investigation of throughput with delay and secrecy constraints under increasing training iterations for the proposed scheme in Case 2. As we can see, the MA-DRL converges to a good solution after thousands of training iterations, and we also compare the convergence performance of MA-DRL under different quantization bits. The proposed MA-DRL algorithm achieves approximately 0.34 packets/time slot when B = 5 bits after 6,500 iterations, while the case with B = 2 bits obtain a solution of 0.3 packets/time slot after 5,000 iterations. This result shows that due to the quantization error and the space of exploration, a larger value of quantization bits leads to better throughput in IRS-assisted networks, but slower convergence.

Fig. 7. - Throughput versus training iterations in Case 2.
Fig. 7.
Throughput versus training iterations in Case 2.

Show All

Fig. 8 indicates the impact of target delay on the throughput with the secrecy constraint in Case 2. From this figure, we can see that the throughput increases with the target delay because the packets can stay in the corresponding buffer for more time slots, leading to the decreasing of the outage probability. Furthermore, the proposed MA-DRL obtains a better solution from learning experience, compared with the benchmark. The proposed MA-DRL algorithm achieves approximately 0.33 packets/time slot when the target delay \varsigma = 10 time slots and the transmit power to noise ratio P/{\sigma }_{n}^{2} = 40 dB, while the max-ratio scheme only achieves 0.18 packets/time slot. In addition, the comparison between P/{\sigma }_{n}^{2} = 40 dB and P/{\sigma }_{n}^{2} = 35 dB shows that higher SNR leads to better performance, due to the decreasing of average delay of packets. Moreover, with a larger value of IRS element N , the proposed algorithm obtains better throughput with constraints, while it is difficult for the benchmark to get gain from the increasing number of N . This result shows the benefit of IRS reflection coefficient optimization in IRS-assisted cooperative networks.

Fig. 8. - Throughput versus target delay.
Fig. 8.
Throughput versus target delay.

Show All

In Fig. 9, we analyze the performance of the proposed scheme and the benchmark under different target rate. It can be seen that in all results, the throughput decreases as the target rate increases. This is because a larger value of target rate leads to more outages in the proposed network. As expected, MA-DRL can optimize the solution from learning experience and achieve better result than the benchmark. As we can see, MA-DRL algorithm achieves approximately 0.21 packets/time slot when the target rate \omega = 3 bps/Hz and the number of relay K = 5 , while the max-ratio scheme only achieves 0.08 packets/time slot. Moreover, when the number of relay K varies, the proposed MA-DRL algorithm always improves the performance compared with max-ratio. This comparison shows the MA-DRL algorithm exhibits robust performance in dynamic networks.

Fig. 9. - Throughput versus target rate.
Fig. 9.
Throughput versus target rate.

Show All

The results in Fig. 10 show that all performance decrease as the target secrecy rate increases due to the increasing secrecy outage probability. The proposed MA-DRL algorithm learns from the environments with different target secrecy rates, and obtains a solution of approximately 0.34 packets/time slot when the target secrecy rate \varsigma = 0.5 bps/Hz and the transmit power to noise ratio P/{\sigma }_{n}^{2} = 40 dB, while the max-ratio scheme only achieves 0.17 packets/time slot. As the target rate, high SNR helps achieve high throughput with delay and secrecy constraints when the target secrecy rate is high. Moreover, compared with the max-ratio scheme, the proposed MA-DRL algorithm can learn to reduce the impact of low SNR efficiently and gain better performance. The MA-DRL algorithm not only optimizes the buffer-aided relay selection rule to improve the throughput but also adjusts the IRS reflection coefficients to reduce the secrecy outage probability for the selected transmission.

Fig. 10. - Throughput versus target secrecy rate.
Fig. 10.
Throughput versus target secrecy rate.

Show All

SECTION V.Conclusion
In this paper, the multi-agent DRL-based joint design of relay selection and IRS reflection coefficients was investigated in IRS-assisted secure buffer-aided cooperative networks. For practical implementation, discrete IRS phase shifts and reflection amplitudes were considered. Two optimization problems were considered, namely, maximizing the average secrecy rate with the delay constraint, and maximizing the throughput with the delay and secrecy constraints. To obtain the solution, we split each of the optimization problems into two sub-tasks to reduce the space of exploration in DRL, and combine the solutions from sub-tasks as a joint optimization scheme. Considering the limitation of computation ability for wireless devices, we applied the distributed framework to address the sub-task of IRS reflection coefficient optimization by sharing the accumulated gradients instead of the sharing training data. The simulation results showed the benefits of jointly optimizing buffer-aided relay selection and IRS reflection coefficients, and provided a possible way for solving optimization problems in future wireless networks. Finally, we note that the proposed scheme can also be applied with the 3D mmWave channel model. This is a worthy research direction and would be left as our future work.