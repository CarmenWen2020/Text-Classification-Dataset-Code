Iris pattern recognition has significantly improved the biometric authentication field due to its high stability and uniqueness. Such physical characteristics have played an essential role in security applications and other related areas. However, presentation attacks, also known as spoofing techniques, can bypass biometric authentication systems using artefacts such as printed images, artificial eyes, textured contact lenses, etc. Many liveness detection methods that improve the robustness of these systems have been proposed. The first International Iris Liveness Detection competition, where the effectiveness of liveness detection methods is evaluated, was first launched in 2013, and its latest iteration was held in 2020. In this paper, we present the approach that won the LivDet-Iris 2020 competition using two-class scenarios (bona fide iris images vs. presentation attack iris images). Additionally, we propose new three-class and four-class scenarios that complement the competition results. These methods use a serial architecture based on a MobileNetV2 modification, trained from scratch to classify bona fide iris images versus presentation attack images. The bona fide class consists of live iris images, whereas the attack presentation instrument classes consist of cadaver, printed, and contact lenses images, for a total of four species. All the images were pre-processed and weighted per class to present a fair evaluation. This approach is primarily focused on detecting the bona fide class over improving the detection of presentation attack instruments. For the two, three, and four classes scenarios BPCER 10 values of 0.99%, 0.16%, and 0.83% were obtained respectively, whereas for the BPCER 20 values of 3.09%, 0.16%, and 3.77% were obtained, with the best model overall being the proposed 3-class serial model. This work reaches competitive results according to the reported results in the LivDet-Iris 2020 competition.
SECTION I.Introduction
Iris recognition systems has been shown to be robust over time, affordable, non-invasive, and touchless; these strengths will allow it to grow in the market in the coming years [1]. Iris recognition systems are usually based on near-infrared (NIR) lighting and sensors, and have been shown to be susceptible to Presentation Attack Instruments (PAI) [2], where PAI refers to a biometric characteristic or object used in a presentation attack. Presentation Attack Detection (PAD) refers to the ability of a biometric system to recognize PAIs, that would otherwise fool the system into recognizing an illegitimate user as a genuine one, by means of presenting a synthetic forged version of the original biometric trait to the capture device. The biometric community, including researchers and vendors, have thrown themselves into the challenging task of proposing and developing efficient protection mechanisms against this threat [3], where PAD methods have been suggested as a solution to this vulnerability. Attacks are not restricted to merely theoretical or academic scenarios anymore, as they are starting to be carried out against real-life operations. One example is the hacking of Samsung Galaxy S8 devices with the iris unlock system, using a regular printer and a contact lens. This case has been reported to the public from hacking groups attempting to get recognition for real criminal cases, including from live biometric demonstrations at conferences.1 An ideal PAD technique should be able to detect all of these attacks, along with any new or unknown PAI species that may be developed in the future [4]. PAD for iris recognition systems is a very dynamic topic, as it has been shown in past editions of the LivDet competition, revealing that there are still open problems to get efficient methods for usage in capturing devices. This paper contributes to improving the state of the art, adds a new database and also explains the methodology used for the winning team.

In order to improve PAD methods, a few competitions and databases have been created, such as the LivDet-Iris.2 The goal of the Liveness Detection Competition (LivDet-Iris) is to compare biometric liveness detection methodologies, using a standardized testing protocol and large quantities of attack presentation (spoofed) and bona fide presentation samples. This competition has shown that there are still challenges for the detection of iris presentation attacks, mainly when unknown materials or capture devices are used to generate the attacks [5]. The results show that even with latest advances in presentation attacks, printed iris PAIs, as well as patterned contact lenses PAIs, are still difficult for software-based systems to detect according with the quality of the images. In LivDet-2017 [5], printed iris images were easier to be differentiated from bona fide images in comparison to patterned contact lenses, as it was also shown in the previous competitions. Some properties of the samples (images) are unknown during training, making the challenge a difficult task, as the winning algorithm did not recognize from 11% to 38% of the attack images, depending on the database. Therefore, the PAD techniques are still an open challenge in NIR, and it has been even less explored in VIS periocular images and multiple capture devices.

The results from the LivDet-2020 [6] competition indicate that iris PAD is still far from a fully solved research problem. Large differences in accuracy among baseline algorithms, which were trained with significantly different data, stress the importance of access to large and diversified training datasets, encompassing a large number of PAI species. The winning team (our method) also achieved the lowest Bona Fide Classification Error Rate (BPCER) of 0.46%, out of all nine algorithms in the three categories. This aligns well with the operational goal of PAD algorithms to correctly detect bona fide presentations (i.e., and not to contribute to system’s False Non-Match Rate), and capture as many attacks as possible.

One of the main challenges to improve PAD systems is the quantity and quality of the data available. Printed images are easy to reproduce with different kinds of paper. Conversely, post-mortem images [7], and PAI species such as contact lenses, cosmetic lenses, plastic lenses, all sourced from different brands, are hard to get. Therefore a subject-disjoint dataset containing different iris patterns is difficult to achieve. Alternatively, these datasets can be synthetically created using deep learning techniques [8], [9].

In this work, a serial, two-stage architecture for classification of bona fide, presentation attack, high-quality printed, and digitally displayed images of LivDet-2020, plus three complementary databases were explored using deep learning techniques. The main contributions of this work can be summarized a follows:

Architecture: A serial, two-stage architecture is proposed. This consists of a modified MobileNetV2 model (“MobileNetv2a”), trained from scratch, which is utilized to differentiate between bona fide presentation and presentation attack. A second MobileNet named “MobileNetv2b”, trained from scratch for four scenarios, which is then used to detect printed/contact-lenses/cadaver impostor attacks by identifying the physical source of the images. See Figure 3.

Network inputs: A strong set of experiments of serial and parallel structures of DNNs was evaluated with two, three, and four classes, using NIR images. Bona fide versus contact lenses, print-out, cadavers, electronics and prosthetic displays were used as input to the network. Also, separate and exhaustive experiments were realized using one of these four types of input, and the results were analyzed.

Weights: Balanced class weights were used in order to correctly represent the number of images per class. Most of the spoofing databases are unbalanced according to PA scenarios. Weighted classes help to balance the dataset and to get realistic results.

Database: This paper presents two new databases, one database to increase the number of bona fide images (10,000), and a second database to increase the number of printed PAIs with high-quality images (1,800). Both databases will be available to other researchers upon request, for research purposes only (See Section III).

Data-Augmentation (DA): An aggressive DA technique to train the modified MobileNetV2a and MobileNetV2b networks was used. These images allow the network to learn more challenging scenarios considering blurring, Gaussian noise, coarse occlusion, crop, and others.

Winning method: Focused on the correct classification of bona fide images instead of the identification of several PAI species, the serial approach presented in this work reached first place in the LivDet-2020 competition. Furthermore, the current proposal with three and fourth classes complement our results presented in the competition, featuring more challenging scenarios.

Not self-reported: The two-stage algorithm presented in this paper was evaluated by the organizers of the competition in an independent test on unknown data; the test data was not available for the participants.


Fig. 1.
Example images of all presentation attack instruments in the database. Images c, and d show examples of the new PAI species included.

Show All


Fig. 2.
Examples of the aggressive data augmentation applied randomly to bona fide and attack presentation images. Left: original images, rotation, blurring, Gaussian noise filter, and Filter Edge Enhance.

Show All


Fig. 3.
Proposed Two-stage serial framework for Presentation Attack Detection. AP: Attack Presentation. BP: Bona fide Presentation. The system has one input to the serial framework.

Show All

SECTION II.Related Work
Multiple PAD methods for iris recognition systems have been proposed in the scientific literature, given the increased adoption of these systems for a variety of different operations, which increases the threats of attacks on these sensitive systems.

Zou et al. [10] have presented a novel algorithm, 4DCycle-GAN, for expanding spoofed iris image databases, by synthesizing artificial iris images wearing textured contact lenses. The proposed 4DCycle-GAN follows the Cycle Consistent Adversarial Networks (Cycle-GAN) framework, which translates between one kind of image (bona fide iris images) to another kind (textured contact lenses iris images). Despite the improvements on Conditional Generative Adversarial Networks, there are still some open problems that limit its application for image generation. Therefore, the method helps to create and increase the number of images based on conditional GANs while preserving the information in the images of each PAI in the NIR spectrum.

Hu et al. [11] investigated the use of regional features in iris PAD (RegionalPAD). Features are extracted from local neighborhoods, based on spatial pyramid (multi-level resolution) and relational measures (convolution on features with variable-size kernels). Several feature extractors, such as Local Binary Patterns (LBP) [12], Local Phase Quantization (LPQ) [13], and intensity correlogram are examined. They used a three-scale LBP-based feature, since it achieves the best performance, as pointed out by the original authors.

Gragnaniello et al. [14] proposes that the sclera region also contains important information about iris liveness (SIDPAD). Hence, the authors extract features from both the iris and sclera regions. The two regions are first segmented, and scale-invariant local descriptors (SID) are applied. A bag-of-feature method is then used to accumulate the features. A linear Support Vector Machine (SVM) is used to perform the final prediction. Also, in [15], domain-specific knowledge of iris PAD is incorporated into the design of their model (DACNN). With the domain knowledge, a compact network architecture is obtained, and regularization terms are added to the loss function to enforce high-pass/low-pass behavior. The authors demonstrate that the method can detect both face and iris presentation attacks.

SpoofNets [16] are based on GoogleNet, and consist of four convolutional layers and one inception module. The inception module is composed by layers of convolutional filters of dimensions 1×1 , 3×3 , and 5×5 , executed in parallel. It has the advantage of reducing the complexity and improving the efficiency of the architecture, once the filters of dimension 1×1 help reduce the number of features before executing layers of convolution with filters of higher dimensions.

Boyd et al. [17] chose the ResNet50 architecture as a backbone to explore whether iris-specific feature extractors perform better than models trained for non-iris tasks. They demonstrated three types of networks: off-the-shelf networks, fine-tuned, and networks trained from scratch, with five different sets of weights for iris recognition. They concluded that fine-tuning an existing network to the specific iris domain performed better than training from scratch.

Yadav et al. [18], used a combination of handcrafted and deep-learning-based features for iris PAD. They fused multi-level Haralick features with VGG16 features to encode the iris textural patterns. The VGG16 features were extracted from the last fully connected layer, with a size of 4,096, and then reduced to a lower dimensional vector by Principal Component Analysis (PCA).

Nguyen et al. [19] proposed a PAD method by combining features extracted from local and global iris regions. First, they trained multiple VGG19 [20] networks from scratch for different iris regions. Then, the features were separately extracted from the last fully connected layer, before the classification layer of the trained models. The experimental results showed that the PAD performance was improved by fusing the features based on both feature-level and score-level fusion rules.

Kuehlkamp et al. [21] propose an approach for combining two techniques for iris PAD: CNNs and Ensemble Learning. Extensive experimentation was conducted using the most challenging datasets publicly available. The experiments included cross-sensor and cross-dataset evaluations. Results show a varying ability for different BSIF+CNN representations to capture different aspects of the input images. This method outperform the results presented in the LivDet-Iris 2017 competition.

Our approach, presented in the LivDet-Iris 2020 competition, reached the first place with an Average Classification Error Rate (ACER) of 29.78%. This method achieved also the lowest Bona Fide Classification Error Rate (BPCER) of 0.46% out of all nine algorithms in the three categories. This paper shows the relevance of focusing mainly on the bona fide images as a “first-filter”. However, a broad space for improvement was detected in the identification of the PAI species, specially in cadaver and printed iris images. An Attack Presentation Classification Error Rate (APCER) of 9.87% was reached for the electronic display PAI species, which is lower than all competing algorithms (53.08% and 83.95%) by a large margin.

Based on previous results, this current paper proposes a new framework to improve the detection performance of PAIs per species in order to get a strong PAD method.

The rest of the article is organized as follows: Section II summarizes the related works on Presentation Attack Detection. The ISO metrics are explained in Section IV. The database description is explained in Section III. The experimental framework is then presented in Section V, and the results are discussed in Section VI. We conclude the article in Section VII.

SECTION III.Databases
For this work, the LivDet-Iris 2020 competition database was used. In addition, three sets of complementary databases of iris images were also utilized. First, a database of NIR bona fide images, captured using an Iritech TD100 iris sensor with a resolution of 640×480 pixels, called “Iris-CL1”. A second database, called “iris-printed-CL1”, containing high-quality presentation attack images of printed PAIs was created. The goal of this database is to increase the challenge of the printed irises species, due to the noticeable visible patterns in the printed images from the LivDet-Iris 2020 database, which makes them trivial to distinguish from bona fide images. See Figure 1. The iris-printed-CL1 database contains 1,800 images captured with two smartphone devices (900 images each one): a Nokia 9 PureView device, with an image resolution of 1280×957 pixels, and a Motorola Moto G4 Play device, with an image resolution of 1280×960 pixels. Only the red channel was used. These new datasets will be available to others researchers upon request. Figure 1 present new images of printed species.

The third database is the Warsaw-BioBase-Post-Mortem-Iris v3.0 database [7]. This database contains a total of 1,094 NIR images (collected with an IriShield M2120U), and 785 visible-light images (obtained with Olympus TG-3), collected from 42 post-mortem subjects. This database was not fully available for the competition.

The LivDet-Iris 2020 database included five different PAI species, each with a different level of challenge: printed eyes, textured contact lens, electronic display, fake/prosthetic eyes, printed with add-ons, and a small number of cadaver eyes. The printed image dataset is of a very low resolution. No specific training dataset was prepared for the competition. A total of 11,918 images were made available.

The competition was different from previous editions in regards to the training dataset. the participants were encouraged to use all the data available to them (both publicly available and proprietary) to make their solutions as effective and robust as possible. The entirety of previous LivDet-Iris benchmarks were also made publicly available [5], [22], [23]. Additionally, the competition organizers shared five examples of each PAI (samples which were not used later in evaluations) to help the competitors familiarize themselves with the test data format (pixel resolution, bits per pixel used to code the intensity, etc.).

Table I shows a summary of the all databases available for training in LivDet-Iris 2020. The datasets of presentation attack instruments (PAIs) were specifically created for the development of PAD methods. With the evolving of PAIs, the datasets include new challenges. A detailed technical summary of the available datasets can be found in [17], [24]. It is essential to point out that the test dataset was sequestered by the organizers and was not available for the competitors.

TABLE I Training Dataset Summary – 11,918 Images. Fa, PrD, Pr, Represent: Fake/Prosthetic Display and Printed Add-Ons. BP: Bona Fide Presentation. AP: Attack Presentation

Table II shows a summary of all datasets available from LivDet-Iris 2020, plus Cadaver images, iris-CL1, and iris-printed-CL2. The new total count of images available is 27,964. This is more than two times the number of images shown in Table I.

TABLE II Summary of the New, Complete Database, With 27,964 Images Divided in Train, Test, and Validation

A. Data Augmentation
An aggressive data augmentation (DA) method was applied when training the modified MobileNetV2 networks. All the images were normalized using a histogram equalization algorithm. A large number of images, with several operations such as affine transformations, perspective transformations, contrast changes, Gaussian noise, random dropout of image regions, cropping/padding, and blurring were included in the train dataset. These DA operations are based on the imgaug library [25], which is optimized for high performance. This improves the quality of the training results by using very challenging images. Examples of some augmented images are shown in Figure 2.

SECTION IV.Metrics
The ISO/IEC 30107–3 standard3 presents methodologies for the evaluation of the performance of PAD algorithms for biometric systems. The APCER metric measures the proportion of attack presentations—for each different PAI—incorrectly classified as bona fide (genuine) presentations. This metric is calculated for each PAI, where ultimately the worst-case scenario is considered. Equation 1 details how to compute the APCER metric, in which the value of NPAIS corresponds to the number of attack presentation images, where RESi for the i th image is 1 if the algorithm classifies it as an attack presentation (spoofed image), or 0 if it is classified as a bona fide presentation (real image) [26].
APCERPAIS=1−(1NPAIS)∑i=1NPAISRESi(1)
View Source

Additionally, the BPCER metric measures the proportion of bona fide (live images) presentations mistakenly classified as attacks presentations to the biometric capture device, or the ratio between false rejection to total genuine attempts. The BPCER metric is formulated according to equation 2, where NBF corresponds to the number of bona fide (live) presentation images, and RESi takes identical values of those of the APCER metric.
BPCER=∑NBFi=1RESiNBF(2)
View Source

These metrics effectively measure to what degree the algorithm confuses presentations of spoofed images with real images, and vice versa. Furthermore, the Average Classification Error Rate (ACER) is also used. This is computed by averaging the APCER and BPCER metrics, as shown in equation 3. Whereas the ACER metric evaluates the overall system performance, it has been deprecated in the ISO/IEC 30107-3, and is computed mainly for the purpose comparing with the state of the art. The APCER, BPCER, and ACER metrics are dependent on a decision threshold.
ACER=APCER+BPCER2(3)
View Source

A Detection Error Trade-off (DET) curve is also reported for all the experiments. In the DET curve, the Equal Error Rate (EER) value represents the trade-off when the APCER is equal to the BPCER. Values in this curve are presented as percentages. Additionally, two different operational points are reported, according to ISO/IEC 30107-3. BPCER10 which corresponds to the BPCER when the APCER is fixed at 10%, and BPCER20 which is the BPCER when the APCER is fixed at 5%. BPCER10 and BPCER20 are independent of decision thresholds.

SECTION V.Methodology
In this section, we introduce our baseline by utilizing a fine-tuned network, and a new network trained from scratch. Then, the detailed description of the used convolutional layers is presented.

A. Networks
MobileNetV2 [27] is based on a streamlined architecture to build lightweight deep neural networks. This allows for usage in environments with limited resources, such as mobile applications, while achieving state-of-the-art performance for tasks such as classification. MobileNetV2 trades the basic Depthwise Separable Convolution building block of MobileNetV1 [28] for a Bottle Residual block, introducing a 1×1 Expansion layer which increases the dimensionality of the input tensor before passing it to the lightweight 3×3 Depthwise Convolution layer to filter the features. Finally, the 1×1 Pointwise Convolution layer is changed for a 1×1 Projection layer, which bottlenecks the data by projecting it into a tensor of lower dimensionality. This effectively allows the basic Bottle Residual block to apply its filtering step on a high dimensional tensor given by the Expansion layer, while outputting a low dimensional tensor using the Projection layer. Furthermore, MobileNetV2 adds inverted residual connections between the bottlenecks, inverting the concept proposed by networks such as ResNet [29]. These MobileNetV2 features help to speed up feature learning and improve network accuracy over its predecessor, while also reducing the amount of parameters of the network. In this work, a modified MobileNetV2 was used to detect bona fide and attack presentation images.

For this work, ImageNet [30] weights were initially used for transfer learning. However, the results of fine-tuning the network would worsen proportionally to the amount of layers that were frozen. Therefore training the networks from scratch resulted in a better classification performance overall. This is explained in more detail in Section VI.

In addition, two different network architectures are used in this work: MobileNetV2a and a modified MobileNetV2b. MobileNetV2a was trained from scratch, based on bona fide and fake species only, whereas MobileNetV2b was introduced based on bona fide, patterned contact lenses, printed, and cadaver species. See Figure 3.

B. Image Pre-Processing
All the images in the database were pre-processed using a contrast limited adaptive histogram equalization algorithm (See sub-section V-C) to improve the gray-scale intensity. Later, a weighted factor per each class was applied (See sub-section V-D). Also, a higher number of filters was applied, using the MobileNetV2 alpha parameter, from the standard 1.0 up to 1.4. Both methods are leveraged to create a two-stage classifier that can detect bona fide and attack presentation scenarios. All the images were resized to 224×224 and 448×448 according to the experiments.

C. Contrast Limited Adaptive Histogram Equalization (CLAHE)
In order to improve the quality of the images and highlight texture-related features, the CLAHE algorithm was applied. This algorithm divides an input image into an M×N grid. Afterwards, it applies equalization to each cell in the grid, enhancing global contrast and definition in the output image. All the images were divided in 8×8 sized cells.

D. Class Weights
A weight factor was estimated for each class according to the number of images of the class, helping to balance the database. Class weights are applied to the loss function, this favours under-representation and penalizes over-representation of classes by re- scaling the gradient steps during training. See Equation 4.
Weighti=NsamplesNclasses×samplesi(4)
View SourceRight-click on figure for MathML and additional features.where Weighti is the weight for class i , Nsamples is the total number of images in the database, Nclasses is the total number of classes in the database, and samplesi is the number of samples of class i . The weight values associated to each class are the following:

Class 0, Cadaver: 4.4162

Class 1, Bona fide: 0.5787

Class 2, Pattern: 1.0133

Class 3, Printed: 0.9443

E. Network Parameters
The number of trainable parameters and number of multiply-adds can be modified by using MobileNetV2’s alpha parameter, which increases/decreases the number of filters in each layer of the network. This alpha value is known as the depth multiplier in the original MobileNet implementation:

If alpha=1.0 , the default number of filters from the original MobileNet paper are used at each layer.

If alpha<1.0 , proportionally decreases the number of filters in each layer.

If alpha>1.0 , proportionally increases the number of filters in each layer.

For the experiments in Section VI, the value of the alpha parameter was set between 1.0 or 1.4, depending on the experiment. Furthermore, two image input sizes were tested: 224×224 and 448×448 . The networks for all experiments were trained with a limit of 200 epochs. Categorical cross-entropy was used as the loss function. Adam optimization [31] was also utilized.

SECTION VI.Experiments and Results
The approach presented in this work takes into account the variability of the attack presentation images, and the number of images per class. These images present a problem for the classifier because the PAI species are not equally represented (for instance only five images of cadaver eyes were available for LivDet-Iris 2020). Considering this imbalance, our strategy is primarily focused on classifying bona fide images with high precision first, and attack presentation images second. Therefore, our first approach was training a network with only two classes. Then, a second network was trained from scratch with three and four classes, increasing the number of filters (alpha 1.4) and weighting each class according to the numbers of images per species. To study these limitations and improve performance for these aforementioned scenarios, five experiments were developed in order to analyze the best hyper-parameter configuration of MobileNetV2. A combination of serial and parallel DNNs was used, trained from scratch. A grid search was used to determine the learning rate, number of epochs, global pooling operation, alpha value, and input size of images. All the experiments employ the CLAHE algorithm and the class weight balancing operation. All the networks were trained with a limit of 200 epochs, using an early stopping method in case the measured performance would stop improving. The input size of the image was 224×224 and 448×448 pixels. All the experiments used the same number of images.

A. Experiment 1
A traditional MobileNetv2 network was used, trained with fine-tuning techniques. Several tests were performed, sequentially freezing an additional MobileNetV2 block in each one, from the bottom of the network to the top. For this experiment the images were grouped in two classes: Bona fide and Fake. The Fake dataset encompasses all the different PAI species: Contact Lenses (CL), Printout (Pr), Electronic displays (EDs), Prosthetic Display (PD) and Cadaver Eyes (CE).

B. Experiment 2
A modified MobileNetv2a network was trained from scratch. For this experiment, the images were again grouped in two classes: BP (Bona fide presentations) and AP (Attack presentation with various PAI). The AP dataset is comprised of all PAI classes: Contact Lenses (CL), Printout (Pr), Electronic displays (EDs), Prosthetic Display (PD) and Cadaver Eyes (CE).

C. Experiment 3
For this experiment, a modified MobileNetv2b network was trained from scratch. The images were grouped in three classes this time: Bona fide, Contact lenses (patterned) and Printouts.

D. Experiment 4
A modified MobileNetv2b network were trained from scratch. The images in this experiment were grouped into four classes: Bona fide, Contact lenses, Printouts, and Cadaver.

E. Experiment 5
This experiment evaluates the feasibility of our proposed two-stage method against unknown/unseen PAI species, where these species are not part of the PAD algorithm training set. Three networks of two stages were trained, using a leave-one-PAI-species-out cross-validation protocol. The first model was trained using bona fide and printed images, and was evaluated using patterned contact lenses and cadaver PAI species. The second model was trained using bona fide and patterned contact lenses, and was evaluated using cadaver and printed PAI species. The last model was trained using bona fide and cadaver images, and was evaluated on patterned contact lenses and printed PAI species images.

F. Results
In this section, we report the best results for each experiment. Adam optimization performed better than SGD and RMSprop. The best initial learning rate was 1×10−5 . Global max pooling performs better than global average pooling. An alpha value of 1.0 performed better with two class scenarios, with an input image size of 224×224 , whereas an alpha value of 1.4 with an input image size of 448×448 performed better for three and four class scenarios.

Table III shows an overview of the results for two class scenarios trained with fine-tuning, and two, three, and four class scenarios trained from scratch, which correspond to Experiments 1 to 4, respectively. Experiment 1 describes the fine-tuning approach. Experiments 2, 3 and 4 describe the two, three and four classes trained from-scratch respectively. For the Experiment 1, only the results for layers 10, 19, and 28 are included, due to the degradation in performance that was proportional to the amount of bottom layers from the network that were frozen. We infer this is probably because the pre-trained ImageNet [30] weights were not trained using images of spoof NIR eyes, or anything similar. Model names are IDs, which correspond to the curves shown in Figures 4 to 6. Overall, for fine-tuning, the best results were obtained when freezing only the first MobileNetV2 block (2C_MOD7), using Adam optimization, resulting in a BPCER10 of 0.99% and BPCER20 of 3.09%. Please note that for model 3C_MOD5, the BPCER value corresponding to the highest APCER value is reported as BPCER10 and BPCER20, this is due that APCER values of 5% and 10% are never reached by this model, as can be seen in Figure 5.

TABLE III Summary of the Results for Two, Three, and Four Classes. In Bold are Highlighted the Best Results. POOL: Global Pooling Operation Used. FT: Fine Tuning Training; Number of Blocks Frozen. Networks Trained From Scratch Appear as “NONE”. ACER, BPCER10 and BPCER20 are Reported for All the Experiments. Model ID Format is “xC_MODy”, Where “x” is the Number of Classes and “y” is the Model Number


Fig. 4.
Results of the PAD method with two classes for model 2C_MOD7. Left: confusion matrix for two class test, attack presentation (fake) and bona fide (live). Right: DET curve for the best results. The number in parenthesis corresponds to the EER in percentage. The black dashed lines indicate two operational points for BPCER10 and BPCER20.

Show All


Fig. 5.
Results of the PAD method with three classes for model 3C_MOD5. Left: confusion matrix considering each PAI independently. The second confusion matrix considers the bona fide class versus the fusion of all PAI species. The number in parenthesis corresponds to the EER in percentage. The black dashed lines indicate two operational points for BPCER10 and BPCER20.

Show All


Fig. 6.
Results of the PAD method with four classes for 4C_MOD3. Left: confusion matrix considering each PAI species independently. The second confusion matrix considers the bona fide class versus the fusion of all PAI species. The number in parenthesis corresponds to the EER in percentage. Similarly, The black dashed lines indicate two operational points for BPCER10 and BPCER20.

Show All

Figure 4 shows the best result for two class scenarios, trained from scratch. This allows us to focus on identifying bona fide presentation (live) images versus attack presentation (fake) images. In this figure, a confusion matrix considering these two classes is shown. Additionally, a Detection Error Trade-off (DET) curve is presented. Several approaches were tested, where the best result reaches an EER of only 4.04% (brown curve), a BPCER10 of 0.99%, and a BPCER20 of 3.09%, respectively. The best model uses an alpha value of 1.4, an initial learning rate of 1×10−5 , and the Adam optimization algorithm.

Figure 5 shows the best result for three class scenarios: live, printed, and patterned contact lenses images. In this figure, a confusion matrix considering these three classes is shown. Furthermore, a confusion matrix showing bona fide presentation vs. attack presentation classes is presented. In this case, the attack presentation class encompasses both printed and patterned contact lenses PAI species. Additionally, a Detection Error Trade-off (DET) curve is also shown. The best result reaches an EER of only 0.33% (orange curve). For BPCER10 and BPCER20, a result of 0.16% is shown on Table III. This corresponds to the BPCER reached at the maximum possible APCER, which is 0.83%. As it can be seen in this DET curve, the 3C_MOD5 model never intersects with the black dashed lines. This model uses an alpha value of 1.4, an initial learning rate of 1×10−5 , and the Adam optimization algorithm.

Figure 6 shows the best result for four class scenarios: live, printed, patterned contact lenses, and post-mortem (cadaver). Likewise, two confusion matrices, one showing four classes, and the other grouping all PAI species under the “attack” class, are presented. A Detection Error Trade-off (DET) curve is also shown. The best result for this experiment reaches an EER of only 4.53% (green curve), a BPCER10 of 0.83%, and a BPCER20 of 3.77%. The best model uses an alpha value of 1.4, an initial learning rate of 1×10−5 , and the Adam optimization algorithm.

Figure 7 shows the performance for each PAI species for model 4C_MOD3. For the printed, cadaver, and pattern species Equal Error Rates of 0.74%, 4.02%, and 4.53% (as shown in Figure 6) were obtained, respectively.

Fig. 7. - DET curve for each PAI for the best four classes model.
Fig. 7.
DET curve for each PAI for the best four classes model.

Show All

Figure 8 shows the performance for unknown/unseen PAI species. Our proposal was evaluated using a leave-one-PAI-species-out cross-validation protocol. To that end, three two-stage networks were trained, according to Section VI-E. The Equal Error Rates reached are in the range of 2.11% to 8.89% for unseen PAI species. The best results show the robustness of our method in detecting unknown PAI species.

Fig. 8. - DET curves for models trained using a leave-one-PAI-species-out cross-validation protocol. Models were trained using bona fide and printed PAI images, bona fide and patterned contact lenses PAI images, and lastly bona fide and post-mortem PAI images, respectively. Each model was evaluated using the PAI species that were not presented to them during training, effectively testing each model against unseen PAI species. The EER for each curve in percentage is shown in parenthesis. The black dashed lines indicate two operational points for BPCER10 and BPCER20.
Fig. 8.
DET curves for models trained using a leave-one-PAI-species-out cross-validation protocol. Models were trained using bona fide and printed PAI images, bona fide and patterned contact lenses PAI images, and lastly bona fide and post-mortem PAI images, respectively. Each model was evaluated using the PAI species that were not presented to them during training, effectively testing each model against unseen PAI species. The EER for each curve in percentage is shown in parenthesis. The black dashed lines indicate two operational points for BPCER10 and BPCER20.

Show All

Finally, Table IV shows a comparison with the state-of-the-art methods, where our two-stage submitted proposal reached the best results on the LivDet-Iris 2020 Competition.

TABLE IV Comparison With the State of the Art. Results are Shown in %. All the Methods Were Evaluated in the Same Test Set
Table IV- Comparison With the State of the Art. Results are Shown in %. All the Methods Were Evaluated in the Same Test Set
Additionally, Figure 9 shows two density plots—in linear and logarithmic scale for the ordinate respectively—showing the distribution of attack presentation scores versus bona fide scores for the best two-classes model 2C_MOD7, shown previously in Figure 4. The decision threshold is defined as 0.75 for demonstration purposes. This operating point can be adjusted depending on requirements and use case, subject to the trade-off between APCER and BPCER.

Fig. 9. - Distribution of attack presentation scores versus bona fide scores. Top: linear scale. Bottom: logarithmic scale. The abscissa is shown in linear scale for both. The decision threshold is shown as the black dashed line.
Fig. 9.
Distribution of attack presentation scores versus bona fide scores. Top: linear scale. Bottom: logarithmic scale. The abscissa is shown in linear scale for both. The decision threshold is shown as the black dashed line.

Show All

SECTION VII.Conclusion
Existing studies in the iris PAD literature are based on the assumption that the system encounters a specific iris presentation attack. However, this may not be the case in real-world scenarios, where the iris recognition system may have to handle multiple kinds of presentation attacks, including unseen species. We propose a framework focused on detecting bona fide images to address this challenge, which means optimising the models for a lower BPCER score. For this approach, we developed the largest iris presentation attack database by combining several other databases. This database is also available to other researchers by request.4

When trained from scratch, our suggested networks allow us to complement the results of the LivDet-Iris 2020 competition by using more challenging PAI species. When using fine-tuning, model performance worsens in proportion to the number of layers from the network that were frozen. Nonetheless, results using fine-tuning are competitive with the literature.

According to our results, an image input size of 224×224 is enough to classify bona fide images successfully. However, the results were improved for presentation attack instruments when using an image input size of 448×448 . This result shows that the extra detail from higher resolution images contains relevant features for PAI species classification.

Overall the best result reached was with three scenarios, obtaining a BPCER10 of 0.16% and an EER of 0.33%.

This work reached competitive results according to the reported results in the LivDet-Iris 2020 competition.

For future work, newer lightweight model architectures such as MobileNetV3 [32], and EfficientNets [33] should be tested, and new PAI species should be included, considering, for example, synthetic images.

This work serves as the latest evaluation of iris PAD on a large spectrum of presentation attack instruments.