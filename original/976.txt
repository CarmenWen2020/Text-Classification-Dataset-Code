Federated learning (FL), as a type of distributed machine learning, is capable of significantly preserving clients' private data from being exposed to adversaries. Nevertheless, private information can still be divulged by analyzing uploaded parameters from clients, e.g., weights trained in deep neural networks. In this paper, to effectively prevent information leakage, we propose a novel framework based on the concept of differential privacy (DP), in which artificial noise is added to parameters at the clients' side before aggregating, namely, noising before model aggregation FL (NbAFL). First, we prove that the NbAFL can satisfy DP under distinct protection levels by properly adapting different variances of artificial noise. Then we develop a theoretical convergence bound on the loss function of the trained FL model in the NbAFL. Specifically, the theoretical bound reveals the following three key properties: 1) there is a tradeoff between convergence performance and privacy protection levels, i.e., better convergence performance leads to a lower protection level; 2) given a fixed privacy protection level, increasing the number N of overall clients participating in FL can improve the convergence performance; and 3) there is an optimal number aggregation times (communication rounds) in terms of convergence performance for a given protection level. Furthermore, we propose a K-client random scheduling strategy, where K (1 ≤ K <; N) clients are randomly selected from the N overall clients to participate in each aggregation. We also develop a corresponding convergence bound for the loss function in this case and the K-client random scheduling strategy also retains the above three properties. Moreover, we find that there is an optimal K that achieves the best convergence performance at a fixed privacy level. Evaluations demonstrate that our theoretical results are consistent with simulations, thereby facilitating the design of various privacy-preserving FL algorithms with different tradeoff requirements on convergence performance and privacy levels.
SECTION I.Introduction
It is anticipated that big data-driven artificial intelligence (AI) will soon be applied in many aspects of our daily life, including medical care, agriculture, transportation systems, etc. At the same time, the rapid growth of Internet-of-Things (IoT) applications calls for data mining and learning securely and reliably in distributed systems [1]–[2][3]. When integrating AI into a variety of IoT applications, distributed machine learning (ML) is preferred for many data processing tasks by defining parametrized functions from inputs to outputs as compositions of basic building blocks [4], [5]. Federated learning (FL) is a recent advance in distributed ML in which data are acquired and processed locally at the client side, and then the updated ML parameters are transmitted to a central server for aggregation [6]–[7][8]. The goal of FL is to fit a model generated by an empirical risk minimization (ERM) objective. However, FL also poses several key challenges, such as private information leakage, expensive communication costs between servers and clients, and device variability [9]–[10][11][12][13][14][15].

Generally, distributed stochastic gradient descent (SGD) is adopted in FL for training ML models. In [16], [17], bounds for FL convergence performance were developed based on distributed SGD, with a one-step local update before global aggregation. The work in [18] considered partially global aggregation, where after each local update step, parameter aggregation is performed over a non-empty subset of the set of clients. In order to analyze the convergence, the federated proximal algorithm (FedProx) was proposed [19] by adding regularization on each local loss function. The work in [20] obtained a convergence bound for SGD based FL that incorporates non-independent-and-identically-distributed (non-i.i.d.) data distributions among clients.

At the same time, with the ever increasing awareness of data security of personal information, privacy preservation has become significant issue, especially for big data applications and distributed learning systems. One prominent advantage of FL is that it enables local training without personal data exchange between the server and clients, thereby protecting clients’ data from being eavesdropped upon by hidden adversaries. Nevertheless, private information can still be divulged to some extent by analyzing the differences of parameters trained and uploaded by the clients, e.g., weights trained in neural networks [21]–[22][23].

A natural approach to preventing information leakage is to add artificial noise, one prominent example of which is differential privacy (DP) [24], [25]. Existing works on DP based learning algorithms include local DP (LDP) [26]–[27][28], DP based distributed SGD [29], [30] and DP meta learning [31]. In LDP, each client perturbs its information locally and only sends a randomized version to a server, thereby protecting both the clients and server against private information leakage. The work in [27] proposed solutions to building up an LDP-compliant SGD, which powers a variety of important ML tasks. The work in [28] considered distributed estimation at the server over uploaded data from clients while providing protections on these data with LDP. The work in [32] introduced an algorithm for user-level differentially private training of large neural networks, in particular a complex sequence model for next-word prediction. The work in [33] developed a chain abstraction model on tensors to efficiently override operations (or encode new ones) such as sending/sharing a tensor between workers, and then provided the elements to implement recently proposed DP and multiparty computation protocols using this framework. The work in [29] improved the computational efficiency of DP based SGD by tracking detailed information about the privacy loss, and obtained accurate estimates of the overall privacy loss. The work in [30] proposed novel DP based SGD algorithms and analyzed their performance bounds which were shown to be related to privacy levels and the sizes of datasets. Also, the work in [31] focused on the class of gradient-based parameter-transfer methods and developed a DP based meta learning algorithm that not only satisfies the privacy requirement but also retains provable learning performance in convex settings.

More specifically, DP based FL approaches are usually devoted to capturing the tradeoff between privacy and convergence performance in the training process. The work in [34] proposed an FL algorithm with the consideration on preserving clients’ privacy. This algorithm can achieve good training performance at a given privacy level, especially when there is a sufficiently large number of participating clients. The work in [35] presented an alternative approach that utilizes both DP and secure multiparty computation (SMC) to prevent differential attacks. However, the above two works on DP-based FL design have not taken into account privacy protection during the parameter uploading stage, i.e., the clients’ private information can be potentially intercepted by hidden adversaries when uploading the training results to the server. Moreover, these two works only showed empirical results using simulations, but lacked theoretical analysis on the FL system, such as the tradeoffs between privacy, convergence performance, and convergence rate. To the authors’ knowledge, a theoretical analysis on convergence behavior of FL with privacy-preserving noise perturbations has not yet been considered in existing studies, which will be the major focus of this work. Compared with conventional works, such as [34], [35], which focus mainly on simulation results, our theoretical performance analysis is more efficient for finding the optimal parameters, e.g., the number of chosen clients K and the number of maximum aggregation times T , to achieve the minimum loss function.

In this paper, to effectively prevent information leakage, we propose a novel framework based on the concept of DP, in which each client perturbs its trained parameters locally by purposely adding noise before uploading them to the server for aggregation, namely, noising before model aggregation FL (NbAFL). To the best of the authors’ knowledge, this is the first piece of work of its kind that provides a theoretical analysis of the convergence properties of differentially private FL algorithms.

The main contributions of this paper are summarized as follows:

We prove that the proposed NbAFL scheme satisfies the requirement of DP in terms of global data under a certain noise perturbation level with Gaussian noise by properly adapting their variances.

We develop a convergence bound on the loss function of the trained FL model in the NbAFL with artificial Gaussian noise. Our developed bound reveals the following three key properties: 1) there is a tradeoff between the convergence performance and privacy protection levels, i.e., better convergence performance leads to a lower protection level; 2) increasing the number N of overall clients participating in FL can improve the convergence performance, given a fixed privacy protection level; and 3) there is an optimal number of maximum aggregation times in terms of convergence performance for a given protection level.

We propose a K -client random scheduling strategy, where K (1≤K<N ) clients are randomly selected from the N overall clients to participate in each aggregation. We also develop a corresponding convergence bound on the loss function in this case. From our analysis, the K -client random scheduling strategy retains the above three properties. Also, we find that there exists an optimal value of K that achieves the best convergence performance at a fixed privacy level.

We conduct extensive simulations based on real-world datasets to validate the properties of our theoretical bound in NbAFL. Evaluations demonstrate that our theoretical results are consistent with simulations. Therefore, our analytical results are helpful for the design on privacy-preserving FL architectures with different tradeoff requirements on convergence performance and privacy levels.

The remainder of this paper is organized as follows. In Section II, we introduce background on FL, DP and a conventional DP-based FL algorithm. In Section III, we detail the proposed NbAFL and analyze the privacy performance based on DP. In Section IV, we analyze the convergence bound of NbAFL and reveal the relationship between privacy levels, convergence performance, the number of clients, and the number of global aggregations. In Section V, we propose the K -client random scheduling scheme and develop the convergence bound. We show the analytical results and simulations in Section VI. We conclude the paper in Section VII. A summary of basic concepts and notations is provided in Tab. I.

TABLE I Summary of Main Notation
Table I- 
Summary of Main Notation
SECTION II.Preliminaries
In this section, we will present preliminaries and related background knowledge on FL and DP. Also, we introduce the threat model that will be discussed in our following analysis.

A. Federated Learning
Let us consider a general FL system consisting of one server and N clients, as depicted in Fig. 1. Let Di denote the local database held by the client Ci , where i∈{1,2,…,N} . At the server, the goal is to learn a model over data that resides at the N associated clients. An active client, participating in the local training, needs to find a vector w of an AI model to minimize a certain loss function. Formally, the server aggregates the weights received from the N clients as
w=∑i=1Npiwi,(1)
View Sourcewhere wi is the parameter vector trained at the i -th client, w is the parameter vector after aggregating at the server, N is the number of clients, pi=|Di||D|≥0 with ∑Ni=1pi=1 , and |D|=∑Ni=1|Di| is the total size of all data samples. Such an optimization problem can be formulated as
w∗=argminw∑i=1NpiFi(w,Di),(2)
View Sourcewhere Fi(⋅) is the local loss function of the i -th client. Generally, the local loss function Fi(⋅) is given by local empirical risks. The training process of such a FL system usually contains the following four steps:

Step 1: Local training: All active clients locally compute training gradients or parameters and send locally trained ML parameters to the server;

Step 2: Model aggregating: The server performs secure aggregation over the uploaded parameters from N clients without learning local information;

Step 3: Parameters broadcasting: The server broadcasts the aggregated parameters to the N clients;

Step 4: Model updating: All clients update their respective models with the aggregated parameters and test the performance of the updated models.


Fig. 1.
A FL training model with hidden adversaries who can eavesdrop trained parameters from both the clients and the server.

Show All

In the FL process, the N clients with the same data structure collaboratively learn a ML model with the help of a cloud server. After a sufficient number of local training and update exchanges between the server and its associated clients, the solution to the optimization problem (2) is able to converge to that of the global optimal learning model.

B. Threat Model
The server in this paper is assumed to be honest. However, there are external adversaries targeting at clients’ private information. Although the individual dataset Di of the i -th client is kept locally in FL, the intermediate parameter wi needs to be shared with the server, which may reveal the clients’ private information as demonstrated by model inversion attacks. For example, authors in [36] demonstrated a model-inversion attack that recovers images from a facial recognition system. In addition, the privacy leakage can also happen in the broadcasting (through downlink channels) phase by analyzing the global parameter w .

We also assume that uplink channels are more secure than downlink broadcasting channels, since clients can be assigned to different channels (e.g., time slots, frequency bands) dynamically in each uploading time, while downlink channels are broadcasting. Hence, we assume that there are at most L (L≤T ) exposures of uploaded parameters from each client in the uplink1 and T exposures of aggregated parameters in the downlink, where T is the number of aggregation times.

C. Differential Privacy
(ϵ,δ) -DP provides a strong criterion for privacy preservation of distributed data processing systems. Here, ϵ>0 is the distinguishable bound of all outputs on neighboring datasets Di,D′i in a database, and δ represents the event that the ratio of the probabilities for two adjacent datasets Di,D′i cannot be bounded by eϵ after adding a privacy preserving mechanism. With an arbitrarily given δ , a privacy preserving mechanism with a larger ϵ gives a clearer distinguishability of neighboring datasets and hence a higher risk of privacy violation. Now, we will formally define DP as follows.

Definition 1 ((ϵ,δ) -DP [24]):
A randomized mechanism M:X→R with domain X and range R satisfies (ϵ,δ) -DP, if for all measurable sets S⊆R and for any two adjacent databases Di,D′i∈X ,
Pr[M(Di)∈S]≤eϵPr[M(D′i)∈S]+δ.(3)
View Source

For numerical data, a Gaussian mechanism defined in [24] can be used to guarantee (ϵ,δ) -DP. According to [24], we present the following DP mechanism by adding artificial Gaussian noise.

In order to ensure that the given noise distribution n∼N(0,σ2) preserves (ϵ,δ) -DP, where N represents the Gaussian distribution, we choose noise scale σ≥cΔs/ϵ and the constant c≥2ln(1.25/δ)−−−−−−−−−√ for ϵ∈(0,1) . In this result, n is the value of an additive noise sample for a data in the dateset, Δs is the sensitivity of the function s given by Δs=maxDi,D′i∥s(Di)−s(D′i)∥ , and s is a real-valued function.

Considering the above DP mechanism, choosing an appropriate level of noise remains a significant research problem, which will affect the privacy guarantee of clients and the convergence rate of the FL process.

SECTION III.Federated Learning With Differential Privacy
In this section, we first introduce the concept of global DP and analyze the DP performance in the context of FL. Then we propose the NbAFL scheme that can satisfy the DP requirement by adding proper noisy perturbations at both the clients and the server.

A. Global Differential Privacy
Here, we define a global (ϵ,δ) -DP requirement for both uplink and downlink channels. From the uplink perspective, using a clipping technique, we can ensure that ∥wi∥≤C , where wi denotes training parameters from the i -th client without perturbation and C is a clipping threshold for bounding wi . We assume that the batch size in the local training is equal to the number of training samples and then define local training process in the i -th client by
sDiU≜=wi=argminwFi(w,Di)1|Di|∑j=1|Di|argminwFi(w,Di,j),(4)
View Sourcewhere Di is the i -th client’s database and Di,j is the j -th sample in Di . Thus, the sensitivity of sDiU can be expressed as
ΔsDiU==maxDi,D′i∥sDiU−sD′iU∥maxDi,D′i∥∥∥∥1|Di|∑j=1|Di|argminwFi(w,Di,j)−1|D′i|∑j=1|D′i|argminwFi(w,D′i,j)∥∥∥∥=2C|Di|,(5)
View Sourcewhere D′i is an adjacent dataset to Di which has the same size but only differ by one sample, and D′i,j is the j -th sample in D′i . From the above result, a global sensitivity in the uplink channel can be defined by
ΔsU≜max{ΔsDiU},∀i.(6)
View SourceTo achieve a small global sensitivity, the ideal condition is that all the clients use sufficient local datasets for training. Hence, we define the minimum size of the local datasets by m and then obtain ΔsU=2Cm . To ensure (ϵ,δ) -DP for each client in the uplink in one exposure, we set the noise scale, represented by the standard deviation of the additive Gaussian noise, as σU=cΔsU/ϵ . Considering L exposures of local parameters, we need to set σU=cLΔsU/ϵ due to the linear relation between ϵ and σU in the Gaussian mechanism.

From the downlink perspective, the aggregation operation for Di can be expressed as
sDiD≜w=p1w1+…+piwi+…+pNwN,(7)
View Sourcewhere 1≤i≤N and w is the aggregated parameters at the server to be broadcast to the clients. Regarding the sensitivity of sDiD , i.e., ΔsDiD , we have the following lemma.

Lemma 1 (Sensitivity for the Aggregation Operation):
In FL training process, the sensitivity for Di after the aggregation operation sDiD is given by
ΔsDiD=2Cpim.(8)
View Source

Proof:
See Appendix A.

Remark 1:
From the above lemma, to achieve a small global sensitivity in the downlink channel which is defined by
ΔsD≜max{ΔsDiD}=max{2Cpim},∀i,(9)
View Sourcethe ideal condition is that all the clients should use the same size of local datasets for training, i.e., pi=1/N .

From the above remark, when setting pi=1/N , ∀i , we can obtain the optimal value of the sensitivity ΔsD . So here we should add noise at the client side first and then decide whether or not to add noise at the server to satisfy the (ϵ,δ) -DP criterion in the downlink channel.

Theorem 1 (DP Guarantee for Downlink Channels):
To ensure (ϵ,δ) -DP in the downlink channels with T aggregations, the standard deviation of Gaussian noise terms nD that are added to the aggregated parameter w by the server can be given as
σD={2cCT2−L2N√mNϵ0T>LN−−√,T≤LN−−√.(10)
View Source

Proof:
See Appendix B.

Theorem 1 shows that to satisfy a (ϵ,δ) -DP requirement for the downlink channels, additional noise terms nD need to be added by the server. With a certain L , the standard deviation of additional noise depends on the relationship between the number of aggregation times T and the number of clients N . The intuition is that a larger T can lead to a higher chance of information leakage, while a larger number of clients is helpful for hiding their private information. This theorem also provides the variance value of the noise terms that should be added to the aggregated parameters. Based on the above results, we propose the following NbAFL algorithm.

B. Proposed NbAFL
Algorithm 1 outlines our NbAFL for training an effective model with a global (ϵ,δ) -DP requirement. We denote by μ the presetting constant of the proximal term and by w(0) the initiate global parameter. At the beginning of this algorithm, the server broadcasts the required privacy level parameters (ϵ,δ) are set and the initiate global parameter w(0) are sent to clients. In the t -th aggregation, N active clients respectively train the parameters by using local databases with preset termination conditions. After completing the local training, the i -th client, ∀i , will add noise to the trained parameters w(t)i , and upload the noised parameters w˜(t)i to the server for aggregation.


Algorithm 1
Noising Before Aggregation FL

Show All

Then the server update the global parameters w(t) by aggregating the local parameters integrated with different weights. The additive noise terms n(t)D are added to this w(t) according to Theorem 1 before being broadcast to the clients. Based on the received global parameters w˜(t) , each client will estimate the accuracy by using local testing databases and start the next round of training process based on these received parameters. The FL process completes after the aggregation time reaches a preset number T and the algorithm returns w˜(T) .

Now, let us focus on the privacy preservation performance of the NbAFL. First, the set of all local parameters are received by the server. Owing to the local perturbations in the NbAFL, it will be difficult for malicious adversaries to infer the information at the i -client from its uploaded parameters w˜i . After the model aggregation, the aggregated parameters w will be sent back to clients via broadcast channels. This poses threats on clients’s privacy as potential adversaries may reveal sensitive information about individual clients from w . In this case, additive noise may be posed to w based on Theorem 1.

SECTION IV.Convergence Analysis on NbAFL
In this section, we are ready to analyze the convergence performance of the proposed NbAFL. First, we analyze the expected increment of adjacent aggregations in the loss function with Gaussian noise. Then, we focus on deriving the convergence property under the global (ϵ,δ) -DP requirement.

For the convenience of the analysis, we make the following assumptions on the loss function and network parameters.

Assumption 1:
We make assumptions on the global loss function F(⋅) defined by F(⋅)≜∑Ni=1piFi(⋅) , and the i -th local loss function Fi(⋅) as follows:

Fi(w) is convex;

Fi(w) satisfies the Polyak-Lojasiewicz condition with the positive parameter l , which implies that F(w)−F(w∗)≤12l∥∇F(w)∥2 , where w∗ is the optimal result;

F(w(0))−F(w∗)=Θ ;

Fi(w) is β -Lipschitz, i.e., ∥Fi(w)−Fi(w′)∥≤β∥w−w′∥ , for any w , w′ ;

Fi(w) is ρ -Lipschitz smooth, i.e., ∥∇Fi(w)−∇Fi(w′)∥≤ρ∥w−w′∥ , for any w , w′ , where ρ is a constant determined by the practical loss function;

For any i and w , ∥∇Fi(w)−∇F(w)∥≤εi , where εi is the divergence metric.

Similar to the gradient divergence, the divergence metric εi is the metric to capture the divergence between the gradients of the local loss functions and that of the aggregated loss function, which is essential for analyzing SGD. The divergence is related to how the data is distributed at different nodes. Using Assumption 1 and assume ∥∇F(w)∥ to be uniformly away from zero, we then have the following lemma.

Lemma 2 (B -Dissimilarity of Various Clients):
For a given ML parameter w , there exists B satisfying
E{∥∇Fi(w)∥2}≤∥∇F(w)∥2B2,∀i.(11)
View Source

Proof:
See Appendix C.

Lemma 2 comes from the assumption of the divergence metric and demonstrates the statistical heterogeneity of all clients. As mentioned earlier, the values of ρ and B are determined by the specific global loss function F(w) in practice and the training parameters w . With the above preparation, we are now ready to analyze the convergence property of NbAFL. First, we present the following lemma to derive an expected increment bound on the loss function during each iteration of parameters with artificial noise.

Lemma 3 (Expected Increment in the Loss Function):
After receiving updates, from the t -th to the (t+1) -th aggregation, the expected difference in the loss function can be upper-bounded by
≤E{F(w˜(t+1))−F(w˜(t))}λ2E{∥∇F(w˜(t))∥2}+λ1E{∥n(t+1)∥∥∇F(w˜(t))∥}+λ0E{∥n(t+1)∥2},(12)
View SourceRight-click on figure for MathML and additional features.where λ0=ρ2,λ1=1μ+ρBμ , λ2=−1μ+ρBμ2+ρB22μ2 and n(t) are the equivalent noise terms imposed on the parameters after the t -th aggregation, given by n(t)=∑Ni=1pin(t)i+n(t)D .

Proof:
See Appendix D.

In this lemma, the value of an additive noise sample n in vector n(t) satisfies the following Gaussian distribution n∼N(0,σ2A) . Also, we can obtain σA=σ2D+σ2U/N−−−−−−−−−√ from Section III. From the right hand side (RHS) of the above inequality, we can see that it is crucial to select a proper proximal term μ to achieve a low upper-bound. It is clear that artificial noise with a large σA may improve the DP performance in terms privacy protection. However, from the RHS of (12), a large σA may enlarge the expected difference of the loss function between two consecutive aggregations, leading to a deterioration of convergence performance.

Furthermore, to satisfy the global (ϵ,δ) -DP, by using Theorem 1, we have
σA={cTΔsDϵcLΔsUN√ϵT>LN−−√,T≤LN−−√.(13)
View SourceNext, we will analyze the convergence property of NbAFL with the (ϵ,δ) -DP requirement.

Theorem 2 (Convergence Upper Bound of the NbAFL):
With required protection level ϵ , the convergence upper bound of Algorithm 1 after T aggregations is given by
E{F(w˜(T))−F(w∗)}≤PTΘ+(κ1Tϵ+κ0T2ϵ2)(1−PT),(14)
View Sourcewhere P=1+2lλ2,κ1=λ1βcCm(1−P)2Nπ−−−√ and κ0=λ0c2C2m2(1−P)N .

Proof:
See Appendix D.

Theorem 2 reveals an important relationship between privacy and utility by taking into account the protection level ϵ and the number of aggregation times T . As the number of aggregation times T increases, the first term of the upper bound decreases but the second term increases. Furthermore, By viewing T as a continuous variable and by writing the RHS of (14) as h(T) , we have
d2h(T)d2T=(Θ−κ1Tϵ−κ0T2ϵ2)PTln2P−2(κ1ϵ+2κ0Tϵ2)PTlnP+2κ0ϵ2(1−PT).(15)
View SourceIt can be seen that the second term and third term of on the RHS of (15) are always positive. When N and ϵ are set to be large enough, we can see that κ1 and κ0 are small, and thus the first term can also be positive. In this case, we have d2h(T)/d2T>0 and the upper bound is convex for T .

Remark 2:
As can be seen from this theorem, the expected gap between the achieved loss function F(w˜(T)) and the minimum one F(w∗) is a decreasing function of ϵ . By increasing ϵ , i.e., relaxing the privacy protection level, the performance of NbAFL algorithm will improve. This is reasonable because the variance of artificial noise terms decreases, thereby improving the convergence performance.

Remark 3:
The number of clients N will also affect its iterative convergence performance, i.e., a larger N would achieve a better convergence performance. This is because a lager N leads to a lower variance of the artificial noise terms.

Remark 4:
There is an optimal number of maximum aggregation times T in terms of convergence performance for given ϵ and N . In more detail, a larger T may lead to a higher variance of artificial noise, and thus may pose a negative impact on convergence performance. On the other hand, more iterations can generally boost the convergence performance if noise levels are not large enough. In this sense, there is a tradeoff on choosing a proper T .

SECTION V.K -Client Random Scheduling Policy
In this section, we consider the case where only K(K<N) clients are selected to participate in the aggregation process, namely K -client random scheduling.

We now discuss how to add artificial noise in the K -client random scheduling to satisfy a global (ϵ,δ) -DP. It is obvious that in the uplink channels, each of the K scheduled clients should add noise terms with scale σU=cLΔsU/ϵ for achieving (ϵ,δ) -DP. This is equivalent to the noise scale in the all-clients selection case in Section III, since each client only considers its own privacy for uplink channels in both cases. However, the derivation of the noise scale in the downlink will be different for the K -client random scheduling. As an extension of Theorem 1, we present the following lemma in the case of K -client random scheduling on how to obtain σD .

Lemma 4 (DP Guarantee for K -Client Random Scheduling):
In the NbAFL algorithm with K -client random scheduling, to satisfy a global (ϵ,δ) -DP, and the standard deviation σD of additive Gaussian noise terms for downlink channels should be set as
σD=⎧⎩⎨⎪⎪2cCT2b2−L2K√mKϵ0T>ϵγ,T≤ϵγ,(16)
View Sourcewhere b=−Tϵln(1−NK+NKe−ϵT) and γ=−ln(1−KN+KNe−ϵLK√) .

Proof:
See Appendix F.

Lemma 4 recalculates σD by considering the number of chosen clients K . Generally, the number of clients N is fixed, we thus focus on the effect of K . Based on the DP analysis in Lemma 4, we can obtain the following theorem.

Theorem 3 (Convergence Under K -Client Random Scheduling):
With required protection level ϵ and the number of chosen clients K , for any Θ>0 , the convergence upper bound after T aggregation times is given by
≤E{F(v˜T)−F(w∗)}QTΘ+1−QT1−Q(cCα1β−mKln(1−NK+NKe−ϵT)2π−−√+c2C2α0m2K2ln2(1−NK+NKe−ϵT)).(17)
View Sourcewhere Q=1+2lμ2(ρB22+ρB+ρB2K+2ρB2K√+μBK√−μ) , α0=2ρKN+ρ,α1=1+2ρBμ+2ρBK√μN , and v˜(T)=∑Ki=1pi(w(T)i+n(T)i)+n(T)D .

Proof:
See Appendix G.

The above theorem provides the convergence upper bound between F(v˜T) and F(w∗) under K -random scheduling. Using K -client random scheduling, we can obtain an important relationship between privacy and utility by taking into account the protection level ϵ , the number of aggregation times T and the number of chosen clients K .

Remark 5:
From the bound derived in Theorem 3, we conclude that there is an optimal K in between 0 and N that achieves the optimal convergence performance. That is, by finding a proper K , the K -client random scheduling policy is superior to the one that all N clients participate in the FL aggregations.

SECTION VI.Simulation Results
In this section, we evaluate the proposed NbAFL by using multi-layer perception (MLP) and real-world federated datasets. In order to characterize the convergence property of NbAFL, we conduct experiments by varying the protection levels of ϵ , the number of clients N , the number of maximum aggregation times T and the number of chosen clients K .

We conduct experiments on the standard MNIST dataset for handwritten digit recognition consisting of 60000 training examples and 10000 testing examples [37]. Each example is a 28×28 size gray-level image. Our baseline model uses a a MLP network with a single hidden layer containing 256 hidden units. In this feed-forward neural network, we use a ReLU units and softmax of 10 classes (corresponding to the 10 digits). For the optimizer of networks, we set the learning rate to 0.002. Then, we evaluate this MLP for the multi-class classification task with the standard MNIST dataset, namely, recognizing from 0 to 9, where each client has 100 training samples locally. This setting is in line with the ideal condition in Remark 1.

We can note that parameter clipping C is a popular ingredient of SGD and ML for non-privacy reasons. A proper value of clipping threshold C should be considered for the DP based FL framework. In the following experiments (except subsection B), we utilize the method in [29] and choose C by taking the median of the norms of the unclipped parameters over the course of training. The values of ρ , β , l and B are determined by the specific loss function, and we will use estimated values in our simulations [20].

A. Performance Evaluation on Protection Levels
In Figs. 2, we choose various protection levels ϵ=50 , ϵ=60 and ϵ=100 to show the results of the loss function in NbAFL. Furthermore, we also include a non-private approach to compare with our NbAFL. In this experiment, we set N=50 , T=25 and δ=0.01 , and compute the values of the loss function as a function of the aggregation times t . As shown in Fig. 2, values of the loss function in NbAFL are decreasing as we relax the privacy guarantees (increasing ϵ ). Such observation results are in line with Remark 2. We also choose high protection levels ϵ=6 , ϵ=8 and ϵ=10 for this experiment, where each client has 512 training samples locally. We set N=50 , T=25 and δ=0.01 . From Fig. 3, we can draw a similar conclusion as in Remark 2 that values of the loss function in NbAFL are decreasing as we relax the privacy guarantees.


Fig. 2.
The comparison of training loss with various protection levels for 50 clients using ϵ=50 , ϵ=60 and ϵ=100 , respectively.

Show All

Fig. 3. - The comparison of training loss with various protection level for 50 clients using 
$\epsilon = 6$
, 
$\epsilon = 8$
 and 
$\epsilon = 10$
, respectively.
Fig. 3.
The comparison of training loss with various protection level for 50 clients using ϵ=6 , ϵ=8 and ϵ=10 , respectively.

Show All

Considering the K -client random scheduling, in Fig. 4, we investigate the performances with various protection levels ϵ=50 , ϵ=60 and ϵ=100 . For simulation parameters, we set N=50 , K=20 , T=25 , and δ=0.01 . As shown in Figs. 4, the convergence performance under the K -client random scheduling is improved with an increasing ϵ .


Fig. 4.
The comparison of training loss with various privacy levels for 50 clients using ϵ=50 , ϵ=60 and ϵ=100 , respectively.

Show All

B. Impact of the Clipping Threshold C
In Fig. 5, we choose various clipping thresholds C=10,15,20 and 25 to show the results of the loss function for 50 clients using ϵ=60 in NbAFL. As shown in Fig. 5, when C=20 , the convergence performance of NbAFL can obtain the best value. We can note that limiting the parameter norm has two opposing effects. On the one hand, if the clipping threshold C is too small, clipping destroys the intended gradient direction of parameters. On the other hand, increasing the norm bound C forces us to add more noise to the parameters because of its effect on the sensitivity.


Fig. 5.
The comparison of training loss with various clipping thresholds for 50 clients using ϵ=60 .

Show All

C. Impact of the Number of Clients N
Figs. 6 compares the convergence performance of NbAFL under required protection level ϵ=60 and δ=10−2 as a function of clients’ number, N . In this experiment, we set N=50 , N=60 , N=80 and N=100 . We notice that the performance among different numbers of clients is governed by Remark 3. This is because more clients not only provide larger global datasets for training, but also bring down the standard deviation of additive noise due to the aggregation.


Fig. 6.
The value of the loss function with various numbers of clients under ϵ=60 under NbAFL Algorithm with 50 clients.

Show All

D. Impact of the Number of Maximum Aggregation Times T
In Fig. 7, we show the experimental results of training loss as a function of maximum aggregation times with various privacy levels ϵ=50 , 60, 80 and 100 under NbAFL algorithm. This observation is in line with Remark 4, and the reason comes from the fact that a lower privacy level decreases the standard deviation of additive noise terms and the server can obtain better quality ML model parameters from the clients. Fig. 7 also implies that an optimal number of maximum aggregation times increases almost with respect to the increasing ϵ .


Fig. 7.
The convergence upper bounds with various privacy levels ϵ=50 , 60 and 100 under 50-clients’ NbAFL algorithm.

Show All


Fig. 8.
The comparison of the loss function between experimental and theoretical results with the various aggregation times under NbAFL Algorithm with 50 clients.

Show All

In Fig. 9, we plot the values of the loss function in the normalized NbAFL using solid lines and the K -random scheduling based NbAFL using dotted lines with various numbers of maximum aggregation times. This figure shows that the value of loss function is a convex function of maximum aggregation times for a given protection level under NbAFL algorithm, which validates Remark 4. From Fig. 9, we can also see that for a given ϵ , K -client random scheduling based NbAFL algorithm has a better convergence performance than the normalized NbAFL algorithm for a larger T . This is because that K -client random scheduling can bring down the variance of artificial noise with little performance loss.


Fig. 9.
The value of the loss function with various privacy levels ϵ=60 and ϵ=100 under NbAFL Algorithm with 50 clients.

Show All

E. Impact of the Number of Chosen Clients K
In Fig. 10, we plot values of the loss function with various numbers of chosen clients K under the random scheduling policy in NbAFL. The number of clients is N=50 , and K clients are randomly chosen to participate in training and aggregation in each iteration. In this experiment, we set ϵ=50 , ϵ=60 , ϵ=100 and δ=0.01 . Meanwhile, we also exhibit the performance of the non-private approach with various numbers of chosen clients K . Note that an optimal K which further improves the convergence performance exists for various protection levels, due to a trade-off between enhance privacy protection and involving larger global training datasets in each model updating round. This observation is in line with Remark 5. The figure shows that in NbAFL, for a given protection level ϵ , the K -client random scheduling can obtain a better tradeoff than the normal selection policy.


Fig. 10.
The value of the loss function with various numbers of chosen clients under ϵ=50,60,100 under NbAFL Algorithm and non-private approach with 50 clients.

Show All

SECTION VII.Conclusions
In this paper, we have focused on information leakage in SGD based FL. We have first defined a global (ϵ,δ) -DP requirement for both uplink and downlink channels, and developed variances of artificial noise terms at clients and server sides. Then, we have proposed a novel framework based on the concept of global (ϵ,δ) -DP, named NbAFL. We have developed theoretically a convergence bound on the loss function of the trained FL model in the NbAFL. Using this convergence bound, we have obtained the following results: 1) there is a tradeoff between the convergence performance and privacy protection levels, i.e., better convergence performance leads to a lower protection level; 2) increasing the number N of overall clients participating in FL can improve the convergence performance, given a fixed privacy protection level; and 3) there is an optimal number of maximum aggregation times in terms of convergence performance for a given protection level. Furthermore, we have proposed a K -client random scheduling strategy and also developed a corresponding convergence bound on the loss function in this case. In addition to the above three properties. we find that there exists an optimal value of K that achieves the best convergence performance at a fixed privacy level. Extensive simulation results confirm the correctness of our analysis. Therefore, our analytical results are helpful for the design on privacy-preserving FL architectures with different tradeoff requirements on convergence performance and privacy levels.

We can note that the size and the distribution of data both greatly affect the quality of the FL training. As a future work, it is of great interest to analytically evaluate the convergence performance of NbAFL with varying size and distribution of data at client sides.