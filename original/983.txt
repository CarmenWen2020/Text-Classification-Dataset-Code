With the rapid development of face manipulation technology, it is difficult for human eyes to distinguish fake face images. On the contrary, Convolutional Neural Network (CNN) discriminators can quickly reach high accuracy in identifying fake/real face images. In this study, we explore the behavior of CNN models in distinguish fake/real faces. We find multi-scale texture difference information plays an important role in face forgery detection. Motivated by the above observation, we propose a new Multi-scale Texture Difference model coined as MTD-Net for robust face forgery detection, which leverages central difference convolution (CDC) and atrous spatial pyramid pooling (ASPP). CDC combines the pixel intensity information and the pixel gradient information to give a stationary description of texture difference information. Simultaneously, based on the ASPP, multi-scale information fusion can keep the texture features from being destroyed. Experimental results on several databases, Faceforensics++, DeeperForensics-1.0, Celeb-DF and DFDC prove that our MTD-Net outperforms existing approaches. The MTD-Net is more robust to image distortion, e.g., JPEG compression and blur, which is urgently needed in the wild world.
SECTION I.Introduction
The rapid development of face manipulation techniques [1]–[2][3] has fueled the sharp increase of forgery face images and videos. These techniques, e.g., DeepFakes [2], Face2Face [3], especially learning-driven generative models, such as Generative Adversarial Nets (GAN) [4], can create lifelike forgery face images and videos that cannot be distinguished even by human eyes. Some vivid examples are shown in Fig. 1. However, it is perilous that these methods are used for malicious purposes, e.g., fake news, reputation infringement, even political purposes. Thus, it is extremely crucial to develop more effective approaches to face forgery detection.

Fig. 1. - Some vivid examples of real and fake images in the four databases, Faceforensics++, DeeperForensics-1.0, Celeb-DF and DFDC. The manipulation methods used in these databases are all based on deep learning. The real images are in the top row, and the fake images are in the bottom row.
Fig. 1.
Some vivid examples of real and fake images in the four databases, Faceforensics++, DeeperForensics-1.0, Celeb-DF and DFDC. The manipulation methods used in these databases are all based on deep learning. The real images are in the top row, and the fake images are in the bottom row.

Show All

In previous research, various methods [5]–[6][7] have been proposed for the most common traditional manipulations, such as splicing, copy-move, and removal. Most works utilize hand-crafted features, e.g., illumination color [5], color filter array patterns [6], and blur type inconsistency [7], to classify a specific patch in an image as tampered or not. These features magnify the difference between real and fake images but focus on a single tampering technique. These methods have laid a solid theoretical foundation for image forensics research and inspired face forgery detection based on deep learning. However, in the face of today’s widespread fraud, the direct use of these methods may be ineffective.

Deep learning has been broadly used in computer vision fields, such as object detection [8] in recent years. Scholars have also proposed some methods based on deep learning [9]–[10][11][12] to detect face forgery. Considering the influencing factors (distortion and compression) in the real environment, an explainable and robust model must adapt to this challenge.

Our work is inspired by two motivations. On the one hand, forgery clues may appear in different areas on the face. On the other hand, we want to know the behavior of CNN in identifying fake/real face images. Regarding the first motivation, we show some examples in Fig. 2, which indicate that forgery clues may appear in different areas on the face. Forgery clues are visually salient in occasional cases, for example, the lack of facial depth, the asymmetry of the eyebrows and the boundaries of a crop area shown in Fig. 2(a), Fig. 2(b), and Fig. 2(c). Human beings can quickly spot and utilize these clues to determine examples as “fake” even without further careful view. Nevertheless, human beings may give predictions with less confidence in most cases, as the clues mentioned above may be unnoticeable. For instance, no salient clue appears in Fig. 2(d), Fig. 2(e), and Fig. 1. Moreover, the fake faces look more vivid, and thus human beings cannot distinguish the difference with only a glance. However, CNN models can give an accurate prediction under these circumstances. Thus, to explore the behavior of CNN models may provide us useful information in facial forgery detection, which exposes our second motivation.

Fig. 2. - Some examples of face forgery clues. (a) The lack of facial depth. (b) Asymmetry of the eyebrows. (c) The boundaries of a crop area can be seen on the right of the image. The examples in (d) and (e) are vivid such that no obvious face forgery clues can be seen.
Fig. 2.
Some examples of face forgery clues. (a) The lack of facial depth. (b) Asymmetry of the eyebrows. (c) The boundaries of a crop area can be seen on the right of the image. The examples in (d) and (e) are vivid such that no obvious face forgery clues can be seen.

Show All

Under these two motivations, this study starts with the learning focus on CNN’s behavior in the face forgery detection task, and we find that multi-scale texture difference information plays an important role in face forgery detection. Thus, a Multi-scale Texture Difference model (MTD-Net) is proposed to face forgery detection. Firstly, we use MTCNN [13] to crop faces from video frames and use a crop (extended by a factor of 1:3) around the center of the tracked face. Then, the crop faces are put into a module to extract the texture difference features. The pixel intensity information and the pixel gradient information are used to represent texture difference information. A special convolution operation is used to combine the intensity and gradient information. Next, we extract multi-scale information through a module. Finally, we fuse the extracted texture difference features at different scales for classification. The major contributions of our work can be outlined as follows:

By studying the behavior of CNN, we realize that CNN has different regions of interest for real and fake images on face forgery detection, and the texture difference statistics of fake faces are different from real faces on multiple scales.

We use the pixel intensity information and the pixel gradient information to give a stationary description of texture difference information. To extract the practical features, we leverage the advantage of a special convolution operation based on central difference convolution (CDC) [14]. To the best of our awareness, in the domain of face forgery detection, this is the first attempt to introduce special convolution operations for feature extraction and information fusion.

In the process of combining multi-scale information, the use of atrous spatial pyramid pooling (ASPP) [15] and image-level pooling ensures that the information of the original features is not destroyed. Moreover, the loss of details due to the influence of dilated convolution is avoided.

We propose a Multi-scale Texture Difference model (MTD-Net) for face forgery detection. Our model aims to extract and fuse the multi-scale texture difference information. Extensive experiments are conducted on three benchmark databases, Faceforensics++ [10], DeeperForensics-1.0 [16], Celeb-DF [17], and DFDC [18] to evaluate our method. Experimental results prove that our method performs better than other methods. Moreover, our method performs more robust in realistic data with high compression and mixed distortion. The proposed method effectively enhances the robustness and feasibility of a face forgery detection system.

The rest of this paper is arranged as follows: related works are briefly summarized in Section II. Empirical studies and analysis of CNN’s behavior in the face forgery detection task are illustrated in Section III. The proposed method details are described in Section IV, and experiments and conclusions are presented in Section V and VI, respectively.

SECTION II.Related Work
A. Facial Manipulation Techniques
There are currently many public databases of real faces, such as CelebA, CASIA-WebFace, etc. The face data in these databases are obtained from public data used for research and does not invade personal privacy. As the facial manipulation techniques based on deep learning methods become more mature, researchers have established some public facial manipulation databases. For example, Faceforensics++, DeeperForensics-1.0, Celeb-DF, DFDC, etc. The quality of these facial manipulation databases is different, and the facial manipulation techniques used are also different in detail. However, they are all based on the data generated by the following facial manipulation techniques.

1) Deepfakes:
The term Deepfakes has become the name of a specific facial manipulation technique. This technique aims to replace the faces of the target sequence with the faces of source videos or images. Currently, there are many open-source implementations of this method, such as in GitHub [2] and Fakeapp. This method uses a shared encoder and two automatic decoders. The shared encoder is trained to encode the features of the source face and the target face, and the automatic decoder is trained to reconstruct the training images of the source face and the target face. After that, the decoders corresponding to the source face and the target face are exchanged, and the resulting model can swap the source face and the target face.

2) NeuralTextures:
NeuralTextures [19] is a facial manipulation technology based on the neural textures rendering method to display face reproduction. It learns the neural texture of the target face from the original video data and trains the rendering network corresponding to the target face. During the training process, the network uses a photometric reconstruction loss in combination with an adversarial loss. And the database Faceforensics++ uses a patch-based GAN-loss as used in Pix2Pix [20] to achieve a perfect reconstruction effect.

3) FaceSwap:
FaceSwap [1] is a graphics-based method that can transfer facial regions from the source image to the target image. It extracts facial areas based on sparsely detected facial landmarks. The process uses these landmarks and blends shapes to fit the 3D template model. To get better results, researchers have proposed some methods combined with GAN. Researchers have added adversarial loss and perceptual loss to Variational Auto-Encoder (VAE) [21], which effectively improved the generation effect and solved the image blur and video jitter. Wang et al. [22] used CycleGAN [23] to improve the Pix2Pix method, which significantly enhanced the clarity and details of generated face images.

4) Face2Face:
Face2Face [3] is a face reproduction system that can reproduce the facial expressions of the characters in the source video onto the faces of the characters in the target video while maintaining the identity information of the target character. In other words, Face2Face can exchange only the facial expressions of the characters. This method is based on two video input streams, manually select the keyframes in the video stream, extract the face information of the character, reproduce the face through calculation, transfer the expression through the mapping relationship, and re-synthesize under different conditions (lighting and expression) human face.

B. Forensics Methods
Inspired by the field of traditional image forensics and steganalysis, scholars have proposed some methods based on standard statistical features to detect facial tampering. In recent years, detection methods based on deep learning have shown their advantages in face forgery detection.

In traditional image forensics methods, most methods are based on statistical data or based on hand-designed functions. For the conventional image tampering mentioned in [24], scholars have proposed a series of effective detection methods. Lukas et al. [25] used the uniqueness of the correlation between the camera’s fixed pattern noise and the source device to detect tampered images. Cozzolino et al. [26] proposed to detect image splicing through summarized noise statistics. Stamm and Bayar [27] proposed constrained convolutional neural networks to suppress image content. This method provided an important foundation for subsequent forensic research. Light sources were used to calculate the direction of scene lighting in [28] and the inconsistencies in lighting were used to determine whether the image has tampered. Other methods analyzed, for example, JPEG compression artifacts [29], [30] and traces of resampling [31]. Fridrich and Kodovsky [32] used a hand-made function to scan features with a pixel radius of 2 along with the horizontal and vertical directions of the image using a high-pass filter, and used these features to train a linear Support Vector Machine (SVM) classifier. Some detection methods based on neural networks were proposed, for example, setting noise patterns on EXIF entries [33] or directly searching for unqualified noise [34]. Zhou et al. [35] proposed a dual-stream network which can detect tampered images and distinguish the tampering methods. These methods have laid a solid theoretical foundation for image forensics research and inspired forged image detection based on deep learning. However, with the improvement of GANs, the application of traditional methods in deep forgery detection becomes more and more difficult.

Since the forgery faces become more and more realistic, some works [36], [37] utilized deep networks to learn discriminative features or find manipulation traces to detect face forgery. Li et al. [38] found that human eyes of GAN-based videos did not blink. A simple deepfakes detection network MesoNet was created in [39], which gave some interpretable conclusions. Rossler et al. [10] introduced an effective Xception-Net as a binary face forgery image detector. Li et al. [40] focused on artifacts or splicing traces produced to generate tampered images and achieved great results. Amerini et al. [41] used the difference of optical the flow field as a clue to identify deepfakes video and original video, considering possible anomalies in the time dimension of the sequence. Durall et al. [42] transferred the research field from the spatial domain to the frequency domain, used the power spectrum as a feature for forensic image forensics. Two features based on frequency domain design were proposed in [43], called Frequency-aware Decomposition (FAD) and Local Frequency Statistics (LFS). Tolosana et al. [44] provided an exhaustive analysis of both 1st and 2nd DeepFakes generations in terms of facial regions and fake detection performance, which provided a good reference for follow-up research. Liu et al. [45] focused on global texture and introduced the gram module into the network. This method significantly improved the robustness and provided a direction for subsequent research. Kumar et al. [46] proposed a approach based on metric learning, which provided an important foundation for subsequent classifier research in face forgery detection. Some recent works used biological information [47], such as heartbeat information [48] to detect deepfakes videos.

SECTION III.Empirical Studies and Analysis
This section starts with the behavior of CNN models in distinguishing facial forgery, and we realize that CNN has different interest areas for real and fake images through visualization experiments. Based on the above observation, some experiments are designed for further analysis.

A. Visual Perception
The CNN models can already achieve excellent results in forgery detection tasks. However, the data-driven CNN models lack interpretability. Moreover, the robustness of the data-driven CNN models is generally very poor, and the performance on the distorted data such as (distortion and compression) is also not good enough. Thus, to understand the behavior of CNN in forgery detection tasks deeply, we first design a visualization experiment to understand the interest areas of the CNN models in the forgery detection task.

The contribution map CMc is calculated to reflect which parts of the input the CNN model’s attention is on. A simple CNN model, ResNet-18 [50], was trained on real and fake face images from Faceforensics++ (DFc23). Assuming there is a face image I , we put it into our trained ResNet-18 model. Then, we get the prediction c (the value of c is 0 or 1, indicating the fake or real face image) and corresponding output value before the softmax layer yc . We take out K feature maps from the last convolutional layer. The pixel value at position (i,j) in the kth feature map is defined as Ik(i,j) . The weights of the kth feature map to the class c can be obtained using Gradient-weighted Class Activation Mapping (Grad-CAM) [49]:
wck=∑i∑j∂yc∂Ik(i,j)(1)
View SourceRight-click on figure for MathML and additional features.

The contribution map CMc is as follows,
CMc=1K∑kwckIk(2)
View Sourcethe contribution maps are shown in Fig. 3.


Fig. 3.
The visual images obtained using the method in [49]. The red regions represent excellent attention, and the blue regions represent insufficient attention. The left side is the input image, and the right side is the Grad-CAM image. (a) Real face images. (b) Fake face images. (c) Areas comparison between the real images and fake images.

Show All

We find that the CNN model has different focus areas for real and fake face images in face forgery detection. For real face images, the CNN model pays more attention to the mouth and nose regions. For fake face images, the CNN model focuses more on the eyes and eyebrows regions. By visually observing the focus areas of fake image, the intuitive difference between the real and fake face images can be easily found.

As shown in Fig. 3(c), the texture of the eyes regions in the real face image is obvious, while the surface of the eyes regions in the fake face image is very blurry. This situation also occurs in eyebrows areas with great attention. We expand the observation scope and find that this “fuzzy” phenomenon only appears in the facial area; in other words, the tampered area. We can even observe the border caused by texture differences, marked in Fig. 3(b). The above observations prompt us to study further whether the fake face is different from the real face in terms of texture difference statistics.

B. Regional Differences
Texture is an image feature formed by the repeated occurrence of gray pixel levels in space. To study the texture differences between real face images and fake face images, we use Gray Level Co-occurrence Matrix (GLCM) [51] to perform a qualitative analysis.

The GLCM Pdθ∈R256×256 represents the co-occurrence of measured pixel values under a given offset parameterized by distance d and angle θ . Pdθ(i,j) means how often a pixel with value i and a pixel at offset (d,θ) with pixel value j co-exist. In the qualitative analysis, we calculate Pdθ separately on face images and then calculate the statistics that can intuitively represent texture information based on Pdθ . We select parameter d∈{1,2,5,10,15,20} , parameter θ∈{0,π/2,π,3π/2} . The texture contrast is chosen as a statistic to measure texture information more intuitively. The texture contrast C is calculated under different distance offsets. Various d and θ combinations ensure that the complete pixel relationship information of the face image. The formula for texture contrast is expressed as follows,
C=1N∑i,j=0255∑θ=03π/2|i−j|2Pθd(i,j)(3)
View Sourcewhere N is a normalization factor and N=256×256×4 . i,j represents the intensity of the pixels. A low C value indicates blurry texture. On the contrary, larger C reflects stronger texture contrast and clearer visual effects. The contrast component C is shown in TABLE I.

TABLE I The Value of C With Different Distance d . Real Faces Retain Larger Texture Contrast Than Fake Faces at allMeasured Distances

We use 1,000 face images from the Faceforensics++ database (c23), randomly selected for each category, to calculate the parameter C . It can be seen that at all distances d , real faces retain larger contrast than fake faces. We notice that CNN-based generators usually normalize the values of associated pixels during the generation process, which results in images generated based on CNN that cannot restore the texture contrast as strong as the real images. This difference is reflected in multiple sizes of d , and as d increases, the difference in texture becomes more apparent. In this paper, we only conduct a quantitative analysis of the texture contrast. The results show that multi-scale texture difference information can play a role in face forgery detection.

SECTION IV.Proposed Method
In this work, we propose a multi-scale texture difference model inspired by how CNN models can behave to anticipate whether a face image is real or fake. In Section IV-A, we describe how the CDC work to extract texture difference features. Subsequently, in Section IV-B, we illustrate the extraction of multi-scale information from texture difference feature map FT . Finally, in Section IV-C, the whole network architecture and the training process are presented.

A. Texture Difference Features Extraction
The texture difference features extraction aims to exploit textural discriminative information. In previous work, Gram-Net [45] used the gram matrix to extract global texture difference information of the full image generated by GAN. However, a new description of texture difference information is needed for the face of locally generated images. In our work, following the idea of CDCN [14], we use the pixel intensity information and the pixel gradient information to give a stationary description of texture difference information.

The most basic vanilla convolution is used to extract the pixel intensity information. The process of vanilla convolution extracting the next-level feature map Fl+1 from the feature map Fl is as follows,
Fl+1(pc)=∑pr∈rw(pr)Fl(pc+pr)(4)
View Sourcewhere pc is a current location on both input feature map Fl and feature map Fl+1 . r is the local receptive field region for vanilla convolution operation, and pr enumerates the locations in r . For instance, pr∈{(−1,−1),(−1,0),⋯,(0,1),(1,1)} with the 3×3 kernel and dilation 1, and w(pr) stands for the weight.

For the pixel gradient information, we use CDC to give a stationary description. The CDC combines the idea of difference with the convolution operation to enhance the ability of features to express pixel gradient information. The process of CDC extracting the next-level feature map Fl+1 from the feature map Fl is as follows.
Fl+1(pc)=∑pr∈rw(pr)(Fl(pc+pr)−Fl(pc))(5)
View SourceRight-click on figure for MathML and additional features.

The details are shown in Fig. 4. Then, we combine vanilla convolution and the CDC with a parameter α∈[0,1] .
Fl+1(pc)=(1−α)∑pr∈rw(pr)Fl(pc+pr)+α∑pr∈rw(pr)(Fl(pc+pr)−Fl(pc))(6)
View Source

Fig. 4. - The process of CDC.
Fig. 4.
The process of CDC.

Show All

The formula after decomposition and merger is as follows.
Fl+1(pc)=∑pr∈rw(pr)Fl(pc+pr)−α∑pr∈rw(pr)Fl(pc)(7)
View SourceRight-click on figure for MathML and additional features.

We call the generalized central difference convolution described by Eq. 7 as the full version of CDC, and refer to the work [14] in face anti-spoofing (FAS), we set α=0.7 . It is worth noting that CDC does not increase the number of network parameters in the specific implementation process. We replaced the convolutions of ResNet-18 by central difference convolutions, and trained the model under the same setting mentioned in Section III-A. We extracted the features of the last convolutional layer in the convergent model. As shown in Fig. 5(a), (b), and (c), from left to right are the fake image, the features extracted by CDC, and the features extracted by VC. It can be seen from Fig. 5(b), the edges of the character’s scarf represent the real area in the image, the features of CDC are stronger than the features of VC. And the eyes of the character represent the fake area in the Fig. 5(c), the features of CDC reflect the actual information of the image more than the features of VC. The feature map extracted using CDC can better reflect the essential features of forged face images.


Fig. 5.
The visual image obtained using the method in [52]. We can clearly see that the extracted features are different. From left to right are the fake image, the features extracted by CDC, and the features extracted by VC. (a) Full face. (b) Edge of scarf (real area). (c) Eyes (fake area).

Show All

B. Multi-Scale Information Extraction
Multi-scale information is expected to exploit discriminative information from different-scale features. We use a special convolution operation, dilated convolution [53], to extract different-scale features from the feature map FT , which is the output of texture difference features module. The process of dilated convolution is as follows,
Md(p)=∑t∈rFT(p+dt)w(t)(8)
View Sourcewhere Md(p) is the output of dilated convolution, and p is a current location on M . w(t) is a 3×3 kernel, t is a location in the kernel. p is the location in the feature map FT , and d is the dilation rate. Inspired by the previous work [15], we use dilated convolution with different dilation rates d∈{6,12,18} to extract different-scale information.

To avoid the loss of local texture information, we refer to the work [15] introduces ASPP and image pooling to better integrate different-scale information. The process of image pooling is as follows,
MImage=U(δ(FT))(9)
View Sourcewhere δ is the 1×1 convolution, and U is the upsampling operation. The output of the final multi-scale information fusion is as follows.
M=δ(concat(MImage,[δ(FT),M6,M12,M18]))(10)
View Source

C. The Networks for Face Manipulation Detection
Our work aims to train a function Yfake/real=P(xface) , which gives an accurate prediction whether the input face image real or fake. So we propose a multi-scale texture difference model MTD-Net.

The proposed network is comprised of the texture difference feature module and multi-scale information module. As shown in Fig. 6, the texture difference feature module is built based on the a simple 18-layer ResNet, since the shortcuts can combine intuitive features near the bottom layer and abstract features near the top layer. The convolutions of the basic ResNet-18 are replaced by CDC to make full use of the texture difference features. In addition, to extract enough features for face forgery, we adjust the size of the feature map by setting dilation rate to 2 in block2. The extracted features are integrated in the next module.


Fig. 6.
The architecture of our MTD-Net: (a) the architecture of our network, comprised of a texture difference feature module (marked with the purple dotted line) and a multi-scale information module (marked with the green dotted line), (b) the block1 in texture difference feature module, (c) the block2 in the in texture difference feature module, and (d) the ASPP block in the multi-scale information module. In particular, the “k” represents kernel size, the “n” represents the number of channels, the “s” represents stride size, and the “d” represents the dilation rate.

Show All

After the texture difference feature module, we propose a multi-scale information module, which consists of ASPP block, to extract multi-scale information. The structure of the module is inspired by the work in [15]. The ASPP block performs parallel dilated convolution with different dilation rates to the input feature map in order to capture different scale information, and then fuses them together. Then, a 1×1 convolution layer is used to learn the adaptive re-calibration of the extracted features. After the multi-scale information module, we use utilize global average pooling to squeeze the spatial information into channel statistics and sent the feature information to the fully connected layer for final classification. The detail settings for the full MTD-Net are shown in Fig. 6 (a), (b), (c), and (d). The visual images are shown in Fig. 7 (a), (b), and (c). We can clearly see that compared with the basic network ResNet-18, the proposed MTD-Net uses a larger range features to make judgments, which includes the real area and the fake area. The experimental results also show that MTD-Net have a good effect on face forgery detection.


Fig. 7.
The visual images obtained using the method in [49]. The red regions represent excellent attention, and the blue regions represent insufficient attention. (a) The input face images. (b) ResNet-18. (c) MTD-Net.

Show All

Considering the difference between the two tasks, we use random initialization for the lays and blocks in face forgery detection, which is different from the specific initialization used for the dilated convolution in the image segmentation [53]. The networks are optimized via Adam [54]. We set the base learning rate as 0.001 and use Cosine [55] learning rate scheduler, and the momentum is set as 0.9. The batch size is set as 64 for about 50 epochs training. For the loss function, we choose the cross-entropy function, which is often used as the loss of binary classification task functions,
L(yt,yp)=−(yt×log(yp)+(1−yt)log(1−yp))(11)
View SourceRight-click on figure for MathML and additional features.we assume that the true label of y is yt , and the probability of yt=1 is yp , where label ∈{0,1} .

SECTION V.Experiments and Results
In this section, extensive experiments are conducted to verify the effectiveness of our proposed method. We first briefly introduce three benchmark databases. Then, implementation details are illustrated. Finally, we present and discuss our experimental results.

A. Databases
In this subsection, to assess the effectiveness of our method, an experimental evaluation on Faceforensics++, DeeperForensics-1.0, Celeb-DF and DFDC is provided. Fig. 1 shows some examples.

1) Faceforensics++:
Faceforensics++ [10] is a large-scale facial manipulation database that consists of real portrait videos and fake portrait videos. Most real portrait videos are collected from YouTube with the consent of the subjects. Each real portrait video undergoes four manipulation methods, i.e., Deepfakes, FaceSwap, Face2Face, and NeuralTexture, to generate four fake videos. Each manipulation method contains 1,000 videos. Output videos are developed with three quality levels, i.e., raw, c23, and c40, corresponding to high quality, medium quality, and low quality, respectively. We followed the set of previous work [10] to partition the database to compare with other methods. For 1,000 videos in each sub-database, we used 720 videos for training, 140 videos for validation, and 140 videos for testing. We sampled 270 frames from each training video, and 100 frames from each validation and testing video. Our performance report was achieved on c23 and c40.

2) DeeperForensics-1.0:
DeeperForensics-1.0 [16] is a large-scale database for real-world face forgery detection. The database collects face data from 100 individuals and takes 1,000 refined YouTube videos collected by Faceforensics++ as target videos. Each face of the ordered 100 identities is swapped onto ten target videos by an end-to-end process. Besides, videos add various perturbations, which are mentioned in Image Quality Assessment (IQA) [56], [57], to simulate videos in real scenes. We followed the set in previous work [16] to partition the database. For 1,000 videos in each sub-database, videos were split into training, validation, and testing with 7: 1: 2. We sampled 270 frames from each training video and 100 frames from each validation and testing video. Our performance report was achieved on std, std/random, and std/mix.

3) Celeb-DF:
Celeb-DF [17] is a new large-scale and challenging deepfakes video database. The Celeb-DF database aims to generate fake videos of better visual quality compared with the previous database. This database contains 590 real videos extracted from YouTube, matching celebrities of various gender, age, and ethnic groups. These videos exhibit an extensive range of aspects, such as face sizes, lighting conditions, and backgrounds. As for fake videos, a total of 5,639 videos are created swapping faces using deepfakes technology. The final videos are in MPEG4.0 format. We followed the set in previous work [59] to partition the database and sampled 32 frames for each video. We use the test set provided by the database itself, and we randomly select 15% of the videos as validation set, with the remaining 85% for training.

4) DFDC:
DFDC [18] is a large publicly-available face swap video database, with over 100,000 total clips sourced from 3,426 paid actors, produced with several Deepfake, GAN-based, and non-learned methods. We followed the set in previous work [60] to partition the database and sampled 32 frames for each video. We split DFDC according to its folder structure, using the first 35 folders for training, folders from 36 to 40 for validation and the last 10 folders for testing.

B. Experimental Setups
In this subsection, the experimental settings of our method is presented so that the other researchers can reproduce our results.

1) Setting:
The hardware conditions of the experiments are Intel (R) Xeon (R) CPU E5-2620 V4 and two NVIDIA GTX Titan XP GPUs. The input crop face images are resized to a fixed size of 224×224 pixels before inputting into the network. The model updates and saves through the hyperparameters setting.

2) Evaluation Metrics:
We report the Accuracy score (ACC) and Area Under the Receiver Operating Characteristic Curve (AUC) as our evaluation metrics.

a) ACC:
We use the frame-level ACC as a significant evaluation of Faceforensics++ and DeeperForensics-1.0. The ACC formula is as follows,
ACC=Stp/ALLtest(12)
View Sourcewhere Stp is the number of images classified correctly, ALLtest is the number of pictures participating in the test.

b) AUC:
AUC is used as another evaluation metric for Celeb-DF and DFDC. We computed binarizing the network output with different thresholds to calculate the frame-level AUC.

C. Performance Evaluation
In this section, the performance of the proposed method is analyzed and compared with other state-of-the-art ones. We test the performance of the proposed method on three databases. The results are shown in TABLE II.

TABLE II Comparative Analysis of Detection Performance With the Other Recent Methods. Each Detection Method in the Table is Trained and Tested Under the Condition of Face Images

In the comparison part, we chose some classic methods, which are based on frame-level detection. Durall et al. [42] proposed a method using the high-frequency information difference to train SVM for identification. Rahmouni et al. [36] adopted different CNN architectures with a global pooling layer that computes four statistics (mean, variance, maximum, and minimum). MesoNet [39] is a CNN-based network to detect face forgery. XceptionNet [10] is a traditional CNN trained on ImageNet based on separable convolutions with residual connections. DSP-FWA [58] employed a dual spatial pyramid strategy to tackle multi-scale issues, and the use of multi-scale information provides important inspiration for follow-up research. Liu et al. [45] focused on global texture and introduced the gram module into the network. Qian et al. [43] proposed a method based on frequency domain. Bondi et al. [59] used EfficientNet B4 for more accurate detection, which provided a good training strategy in face forgery detection. Bonettini et al. [60] proposed a model based on EfficientNet, which used attention layers. In this paper, we do not use any unique training method for the fair comparison, i.e., special loss. The input of these methods is only a single face image without any additional input, i.e., masks or other information. For all methods whose source codes are opened to the public, we conducted experiments for them by ourselves.

On the Faceforensics++ database, the proposed MTD-Net achieved great results. The ACC in some categories exceeds the reference methods in c23, i.e., F2F, FS, NT. The ACC in all categories exceeds the reference methods in c40. A possible explanation for this result might be that the MTD-Net performs better on low-quality data. Moreover, the ACC of the proposed method decreases less than other reference methods when the compression level increases from c23 to c40.

On the other three databases, the proposed method can generally achieve great performance. On DeeperForensics-1.0 database, our MTD-Net gets the best results in std/random, and this finding suggests that the proposed method shows stronger robustness on distorted data. Our MTD-Net ranks second among all methods on Celeb-DF and DFDC, indicating that our method has good generalization performance in face forgery detection. From the TABLE II, one can see that the proposed MTD-Net obtains the best overall performance.

D. Module Comparison Evaluation
This section validates the contribution of multi-scale information and texture difference features to the proposed method. For comparison purposes, we use the most basic network as ResNet-18. ResNet-18 does not include any multi-scale information module or texture difference features module. For convenience, we refer to this model as “Resnet” in the following. On this basis, the version with multi-scale information is represented as Res_MS, and the version with texture difference features as Res_CDC. An intuitive algorithm configuration is shown in Fig. 8.

Fig. 8. - Algorithmic configuration in module comparison. VC represents the feature extraction layer in ResNet-18 using vanilla convolution. CDC defines the feature extraction layer in ResNet-18 using CDC. MS represents multi-scale extraction and fusion. (a) Resnet. (b) Res_CDC. (c) Res_MS.
Fig. 8.
Algorithmic configuration in module comparison. VC represents the feature extraction layer in ResNet-18 using vanilla convolution. CDC defines the feature extraction layer in ResNet-18 using CDC. MS represents multi-scale extraction and fusion. (a) Resnet. (b) Res_CDC. (c) Res_MS.

Show All

During the module comparison experiment, we first designed independent experiments to discuss the role of multi-scale information and texture difference features, respectively. Then, we conducted module reduction experiments on three databases. The experiment configuration under the same investigation remains unchanged. Only the modules that need to be compared are added or deleted.

1) The Benefits of Multi-Scale Information:
To further prove the benefits of multi-scale information in face forgery detection, we designed an experiment on full images in Faceforensics++ c23. The information of “full image” is more prosperous, and the “full image” scan can better reflect the advantages of multi-scale information. The setting is the same as that of performance evaluations. As shown in the last row of TABLE III, the ACC of Res_MS is much higher than Xception. Moreover, the ACC of each category has been dramatically improved compared with the primary network Resnet. For face images, we only compared with the primary network in this paper. The results are shown in TABLE VI. We can see the difference between column 2 and column 4 in TABLE VI, proving that multi-scale information is beneficial in face forgery detection.

TABLE III Comparative Analysis of the Benefits of Multi-Scale Information. Each Detection Method in the Table is Trained and Tested Under the Condition of Full Images
Table III- 
Comparative Analysis of the Benefits of Multi-Scale Information. Each Detection Method in the Table is Trained and Tested Under the Condition of Full Images
TABLE IV Comparative Analysis of the Benefits of Texture Difference Features. Each Detection Method in the Table is Trained and Tested Under the Condition of Face Images
Table IV- 
Comparative Analysis of the Benefits of Texture Difference Features. Each Detection Method in the Table is Trained and Tested Under the Condition of Face Images
TABLE V Comparative Analysis of the Benefits of Texture Difference Features. Each Detection Method in the Table is Trained and Tested Under the Condition of Face Images
Table V- 
Comparative Analysis of the Benefits of Texture Difference Features. Each Detection Method in the Table is Trained and Tested Under the Condition of Face Images
TABLE VI Comparative Analysis of Module Comparison Evaluation. Each Detection Method in the Table is Trained and Tested Under the Condition of Face Images
Table VI- 
Comparative Analysis of Module Comparison Evaluation. Each Detection Method in the Table is Trained and Tested Under the Condition of Face Images
2) The Benefits of Texture Difference Features:
To verify the texture difference features in face forgery detection, we designed independent experiments on Faceforensics++ c23 and DeeperForensics-1.0. In this subsection, we aim to evaluate the robustness of the texture difference features. The experiments were conducted on face images, and the setting was the same as that of performance evaluations.

On the Faceforensics++ c23, we did verification tests on different categories. As shown in TABLE IV, texture difference features help improve ACC compared with the primary network Resnet. To further evaluate the feasibility of texture difference features, an experiment was designed on distorted data in DeeperForensics-1.0. The detailed configuration is shown in TABLE V. The hyperparameters setting is the same as that of the previous work [16]. We used manipulated data in the standard set (std), manipulated videos with random-type, random-level distortions (std/random), and manipulated videos with a mixture of three random-level, random-type distortions (std/mix).

In TABLE V, the ACC of the two methods is high when the models are trained and tested on the standard database. It is expected that the models perform well on high-quality databases. It is worth noting that the ACC of Resnet decreases when we choose distorted databases. However, the ACC of Res_CDC increases on the distorted databases, and it performs better than the Resnet. It seems possible that these results are due to the texture difference features have better representation, which is more robust.

To further evaluate the performance, we swapped the training data and the test data. The detailed configuration is shown in TABLE V. The ACC of Resnet is significantly affected, while the impact on Res_CDC is small. The experimental results reflect the robustness of the texture difference features. Moreover, in rows 3 and 4, the effects on both methods are small. Initial observations suggest that there may be a link between the quality of training data and accuracy.

3) The Benefits of Fusion Information:
In this subsection, we prove the performance of the proposed method by jointly exploiting texture difference features and multi-scale information. Module comparison evaluations were conducted on Faceforensics++ c23, std (DeeperForensics-1.0) and Celeb-DF. The results are shown in TABLE VI, where the proposed represents the complete method MTD-Net.

As shown in TABLE VI, the results with only multi-scale information exceed the results with superior texture difference features. One possible implication is that multi-scale information may be more efficient than texture difference features. Intuitively, human are likely to give a reliable judgment with a glance at the apparent artifacts, such as the asymmetry of the eyebrows or the boundaries of crop area. The artifacts are located in different locations and have different scales, corresponding to multi-scale information. However, when the discrimination artifacts are not noticeable, the subtle texture difference features may play a role, which we found when discussing the behavior CNN model. Besides, by fusing texture difference and multi-scale information, the proposed method can further achieve better performance. This improvement supports our motivation that “multi-scale texture difference features” can provide more details to improve the classification.

E. Cross-Database Experiment
To further verify the performance of the proposed method, a cross-database experiment was designed. We trained and test on F++ c23 (DF, F2F, FW, NT and Real), std/random (DeeperForensics-1.0) and Celeb-DF. We followed the set in previous work [59] to partition the database. We only consider the benefits of the network structure, and the input of the network is only a single face image, without any other information. To comparison with other methods easily, we use AUC as an evaluation indicator. Detailed configurations and results are shown in TABLE VII.

TABLE VII Comparative Analysis of Cross-Databases Experiment. Each Detection Method in the Table is Trainedand Tested Under the Condition of Face Images
Table VII- 
Comparative Analysis of Cross-Databases Experiment. Each Detection Method in the Table is Trainedand Tested Under the Condition of Face Images
From the experimental results, we can see that if the training set and testing set are the same data distribution, the detection effects of all methods are excellent. However, the AUC is significantly reduced when the training set and the testing set belong to different data distributions.

We can still dig out useful information from the results. The model trained with F++c23 is more effective on the test set of other databases than the model trained on std/random and Celeb-DF. Experimental results show that training the model with data that has more fake category distributions might improve generalization ability. Moreover, training the model with a high degree of distortion might increase the robustness of the model. Our MTD-Net has a strong ability to deal with data distortion. MTD-Net can still reflect certain advantages in cross-database experiments.

The distribution difference between the databases is big. The face forgery databases must consider different methods of the forgery and evaluate the effects of environmental distortion. How to propose a highly generalized method in this data environment is also our future research direction.

SECTION VI.Conclusion and Prospection
Our study provided a method of face forgery detection based on multi-scale texture difference information. The texture differences between the real and fake face images were found by the traditional texture representation GLCM. Based on the finding, we first attempted to introduce particular convolution operations-CDC for feature extraction and information fusion. Meanwhile, the advantage of ASPP and image-level pooling was also leveraged to fuse multi-scale information. The evaluation experiments were conducted on the Faceforensics++, DeeperForensics-1.0, Celeb-DF and DFDC databases compared with the recent state-of-the-art model. Experiments prove that our method can achieve high accuracy and also perform well in combating distortion. However, it is still necessary to train a new model for an unknown face manipulation method. In future research work, we will commit to forming a universal approach and promoting face forgery detection.