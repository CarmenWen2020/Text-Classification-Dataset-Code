In this paper, we propose a lightweight privacy-preserving Faster R-CNN framework (SecRCNN) for object detection in medical images. Faster R-CNN is one of the most outstanding deep learning models for object detection. Using SecRCNN, healthcare centers can efficiently complete privacy-preserving computations of Faster R-CNN via the additive secret sharing technique and edge computing. To implement SecRCNN, we design a series of interactive protocols to perform the three stages of Faster R-CNN, namely feature map extraction, region proposal and regression and classification. To improve the efficiency of SecRCNN, we improve the existing secure computation sub-protocols involved in SecRCNN, including division, exponentiation and logarithm. The newly proposed sub-protocols can dramatically reduce the number of messages exchanged during the iterative approximation process based on the coordinate rotation digital computer algorithm. Moreover, the effectiveness, efficiency and security of SecRCNN are demonstrated through comprehensive theoretical analysis and extensive experiments. The experimental findings show that the communication overhead in computing division, logarithm and exponentiation decreases to 36.19%, 73.82% and 43.37%, respectively.

SECTION I.Introduction
Faster region conventional neural network (R-CNN) based object detection has been considered to be an ideal approach to assist medical diagnosis. Doctors can utilize the automatic detection results of medical images to obtain further insights into the patient-specific pathological features and make a more accurate diagnosis. Some typical applications include anatomical object localization [1], cell tracking [2] and bone age estimation [3]. The detection accuracy of most applications far exceeds that of experienced clinicians. However, the computing power and storage capacity requirements for training such a Faster R-CNN based object detection model are quite staggering. For a single patient, the recording of magnetic resonance imaging (MRI) or computed tomography (CT) required to be processed and stored can be ten minutes or more, and the duration to record data for 1000 patients can reach more than 130 hours [4]. Also, the quality of medical images is higher than the images for general use. Therefore, instead of building their own local server, healthcare centers prefer to outsource their medical images to cloud servers and build the Faster R-CNN model using cloud computing technology. Moreover, to provide a timely response and steady communication for the field diagnostic test, the connection channels with the cloud server should have low communication latency and high robustness against network fluctuations. To achieve this goal, edge computing is proposed to build a “short bridge” between the data owner and the cloud server. Research shows that for deep learning based face recognition, the response latency can be decreased from 900 ms to 169 ms with the help of edge computing [5].

In addition, the high performance of Faster R-CNN based object detection depends heavily on the quality of large-scale training data. A single healthcare center’s data are usually not enough to train a high-performance model [6]. Consequently, the cooperation of multiple healthcare centers through data sharing becomes imperative under the situation. Nevertheless, the problem arsing from such collaboration is that the medical images may be leaked to others during the data sharing process. Medical images are considered to be private information of patients. No patient wants these images to be revealed to others, except for the healthcare center where the patient is treated. At the same time, medical images are valuable commercial resources for the healthcare center. Taking the cancer diagnosis as an example, at least two days are required to process only one patient’s pathological images with metaiodobenzylguanidine (a common technology for collecting pathological images from patients with cancer).1 A healthcare center may spend years building its pathological database and cannot be willing to share the data with others. Consequently, for patient privacy and smooth cooperation between healthcare centers, it is necessary to build an efficient privacy-preserving framework for Faster R-CNN based object detection of medical images.

For medical image privacy, current research mostly concentrates on data storage privacy and cannot support online calculations in encrypted format [7]. The problem of the method is that when applying the medical image data into Faster R-CNN, we still have to query and download the data to a local server, which can dramatically reduce data availability and computational efficiency. To overcome this problem, schemes based on homomorphic encryption (HE) [8] and garbled circuit (GC) [9] have been proposed. However, HE and GC are both computation-intensive and memory-intensive algorithms. For most real-world applications, the overheads caused by these methods are almost intolerable. Additionally, differential privacy (DP) is also a popular technique for the privacy preservation of deep learning models. Implementing DP only requires few computations to generate random perturbations. However, the accuracy reduction caused by the introduction of random perturbations is quite considerable [10].

To address the above problems, we propose an additive secret sharing based Faster R-CNN framework (SecRCNN) for privacy-preserving object detection of medical images. The main contributions of this work are listed as follows.

We propose SecRCNN, the first privacy-preserving Faster R-CNN framework for medical image object detection. SecRCNN allows multiple healthcare centers to securely share their medical image data and collaborate to build a high-performance Faster R-CNN model to assist in clinical diagnosis. During the cooperation process, no healthcare center has to worry about their own data revealed to other healthcare centers or the cloud server.

Several additive secret sharing based sub-protocols are designed to complete the division, exponentiation and logarithm operations in SecRCNN. Compared with existing protocols, the newly proposed protocols not only realize the corresponding functions safely and accurately, but also dramatically reduce communication costs.

A series of interactive protocols are proposed to complete the secure computation of the feature extraction network, the region proposal network and the classification & bounding box regression of Faster R-CNN without revealing the original data.

A comprehensive analysis is presented to prove the correctness and security of SecRCNN. The experimental findings further indicate that SecRCNN is efficient and only introduces little computation errors on the final output. The decreases of communication overhead for computing division, logarithm and exponentiation reach 36.19%, 73.82% and 43.37%, respectively.

The remainder of this paper is organized as follows. In Section II, we describe the preliminaries of SecRCNN. In Section III, the system model of SecRCNN is described. Then, we introduce the sub-protocols for division, exponentiation and logarithm in Section IV, followed by the interactive protocols utilized to complete the object detection task of SecRCNN in Section V. The security analysis and performance evaluation are presented in Sections VI and VII. Related work is discussed in Section VIII. Section IX concludes this paper.

SECTION II.Preliminaries
A. Faster Region Conventional Neural Network
Faster R-CNN is a deep learning architecture that outperforms most previous techniques in object detection and classification [11]. Given a set of medical images I , the first stage of Faster R-CNN is to extract fixed size feature maps m by a common neural network such as VGG-16 [12]. Based on m , the region proposal network (RPN) [11] then produces a predicted bounding box to label the boundaries of the target object and determine its class. The objective loss function of Faster R-CNN corresponds to the addition of the classification loss and the bounding box regression loss, and is given as follows:
Loss=1Nc∑i=0NcLLog(pi,p∗i)+λ1Nr∑i=0Nrp∗iSmoothL1(ti,t∗i),(1)
View Sourcewhere Nc and Nr are the mini-batch size and the number of anchor locations; i is the index of anchors; pi is the prediction probability of whether the anchor i is the target object; p∗i is the ground-truth label; λ is a constant which is set to 10 by default. Given an input x , smooth L1 function SmoothL1 works for the bounding box regression loss by computing
SmoothL1(x)={σ2x22,|x|−12σ2,|x|<1σ2otherwise,(2)
View Sourcewhere ti and t∗i are the coordinates of the predicted bounding box and the ground-truth bounding box, respectively. Let us denote the four types of coordinates as x , y , w and h . Then, ti and t∗i can be defined as follows:
tx=tw=t∗x=t∗w=(Tx−Gx)/Gw,ty=(Ty−Gy)/Gh.log(Tw/Gw),th=log(Th/Gh).(T∗x−Gx)/Gw,t∗y=(T∗y−Gy)/Gh.log(T∗w/Gw),t∗h=log(T∗h/Gh).(3)
View Source

Here, x , y , w and h represent the coordinates, width and height of the bounding box. T , G and T∗ are the predicted box, the anchor box and the ground-truth box [11], respectively.

B. Basic Definitions
Basic definitions of the data format, data split and secure computation protocols in SecRCNN are given as follows.

Data Format: In SecRCNN, we adopt a modified fixed-point format [13] to store and transmit random shares. In this format, an arbitrary fixed-point number x is represented as x=(−1)s⋅x^⋅10−c , where s∈{0,1} is the sign of x . For the original data (i.e., medical image pixels) and the random share, x^ belongs to two different groups with different prime orders, which are Zq={0,1,…,q−1} and Zp={0,1,…,p−1} . Under most circumstances, the image pixels are integers ranged from 0 to 255. To guarantee the security level, we usually set p to be a large prime, q>255 and p≫q . c>2 is a fixed integer that controls the representation precision; c is a public parameter. According to the common statistical theorem [14], multiplying the uniformly random value (−1)s⋅x^ by a constant 10−c does not change its original distribution, which means that the multiplication does not lead to more information about the original data revealed to the adversary. Thus, the publication of c does not influence the security of SecRCNN. Note that all random shares are represented as the modified fixed-point data format in the paper. For brevity, we do not specifically indicate the format again.

Data Split: Data split is then used to split medical images into random shares. For most cases, each pixel of an image is stored as a number of integers, e.g., grey values or RGB values. However, pixels of the processed images are sometimes stored as floating point numbers. For unity and security, the pixels are uniformly transformed into the fixed-point format while performing data split. Given an arbitrary medical image pixel x^0 , it is first rounded to c decimals. The rounded result is a fixed-point number and stored as x=(−1)s⋅x^⋅10−c . To split the pixel, the data owner then uniformly selects a random value x^′ from Zp and a random sign s′∈{0,1} . Next, he computes x1^=(−1)s⋅x^−(−1)s′⋅x^′ . If x1^<0 , s′′=1 ; otherwise, s′′=0 . Finally, x^′′ is obtained by calculating the absolute value |x1^|modp . Thus, x is split into two random shares x′=(−1)s′⋅x^′⋅10−c and x′′=(−1)s′′⋅x^′′⋅10−c . To recover the original value, we can simply compute x=x′+x′′=(−1)s′⋅x^′⋅10−c+(−1)s′′⋅x^′′⋅10−c . An example of a medical image split is given in Appendix.

Secret Sharing Protocol: All secret sharing protocols in this paper satisfy the following formal definition. Suppose that Gp,q(X,S) is an arbitrary secret sharing protocol. Given the random shares of inputs X={(x1′,x1′′),(x2′,x2′′),…} and two edge servers S={S1,S2} , Gp,q outputs two random shares f′ and f′′ of the computation result. To recover the computation result f , one needs to compute f=f′+f′′ .

C. Basic Secure Protocols
The following sub-protocols, proposed in [15], are the basic components to complete the secure linear and nonlinear functions of SecRCNN. The sub-protocols are operated between two edge servers. Their implementation details are presented in Section IX-B.

Secure Comparison Protocol (SCmp): given the random shares of two inputs (μ′,μ′′) and (υ′,υ′′) , it performs secure comparison and outputs (f′,f′′) , where f′+f′′=0 if μ>υ ; otherwise, f′+f′′=1 . During execution, two intermediate values are exchanged between the two edge servers.

Secure Addition Protocol (SAdd): given the random shares of two inputs (μ′,μ′′) and (υ′,υ′′) , it performs secure addition and outputs (f′,f′′) , where f′+f′′=μ+υ . During execution, no intermediate values are exchanged.

Secure Multiplication Protocol (SMul): given the random shares of two inputs (μ′,μ′′) and (υ′,υ′′) , it performs secure multiplication and outputs (f′,f′′) , where f′+f′′=μ⋅υ . During execution, four intermediate values are exchanged between the two edge servers.

SECTION III.System Model & Attack Model
A. System Model
As illustrated in Fig.1, four types of participants comprise SecRCNN, namely healthcare centers H={H1,H2,…,Hn} , two edge servers S1 and S2 , and the trusted third party T .

H1,H2,…,Hn are healthcare centers, which would like to cooperate to train a Faster R-CNN based medical image object detection model. They are not willing to share their patients’ pathology image database with others. Therefore, in SecRCNN, we randomly split the database I1,I2,…,In into {(I′1,I′′1),(I′2,I′′2),…,(I′n,I′′n)} . The encrypted data D′={I′k|k∈{1,2,…,n}} and D′′={I′′k|k∈{1,2,…,n}} are then sent to the two edge servers for storage and computation.

S1 and S2 are two outsourced edge servers’ which are responsible for the intensive computation task. In SecRCNN, they complete all the computations of Faster R-CNN without knowing any plaintext of medical images. The final outputs O′ and O′′ are simultaneously sent to H1,H2,…,Hn using secure communication channels. The healthcare centers can obtain the original output by computing O=O′+O′′ .

T is a trusted third server, which serves as a random value generator.


Fig. 1.
System model of SecRCNN.

Show All

B. Attack Model
In SecRCNN, we adopt the curious-but-honest model. According to the model, H={H1,H2,…,Hn} , S1 and S2 are all curious-but-honest parties. Literally, they follow the steps to complete the protocols, yet never refuse to know the data belonging to others that can benefit themselves.

Furthermore, we assume that there is a simulator ζ , which can generate uniformly random values and obtain the real view V1 of the secret sharing protocol. Based on V1 , ζ tries to generate a simulated view V2 in polynomial time. For adversary A , a successful attack means that A can find a probabilistic polynomial algorithm to distinguish V1 and V2 . Also, we hypothesize that the edge servers cannot collude with each other or be simultaneously corrupted. This hypothesis is essential because the plaintext data can be recovered by simply adding the corresponding two random shares together. In addition, we assume that there is a trusted third server that is responsible for generating uniformly random values. Note that the above assumptions are commonly used in the additive secret sharing based privacy-preserving schemes [16], [17].

SECTION IV.Secret Sharing Based Sub-Protocols
Diving to the bottom, the object detection with Faster R-CNN is completed by a series of mathematical operations. In SecRCNN, to avoid medical image data revealed to the cloud servers, all the operations have to be completed in a privacy-preserving way. Therefore, based on the Coordinate Rotation Digital Computer Algorithm (CORDIC) and the additive secret sharing technique, we design several secure computation sub-protocols, which are secure division protocol (SDiv), secure logarithm protocol (SLog) and secure exponentiation protocol (SExp). Compared with existing protocols [18], the newly proposed protocols can reduce the communication overhead while maintaining a low number of computation errors.

A. Secure Iteration of CORDIC
In the section, we implement the secure iteration process of CORDIC. CORDIC was first proposed by Volder et al. in [19]. There are three models of CORDIC iterative methods that we consider, namely secure linear vectoring mode (LiVec), secure hyperbolic vectoring mode (HypVec) and secure hyperbolic rotation mode (HypRot). These models work on different types of coordinate systems (rectangular coordinates for vectoring or polar coordinates for rotation) and make the coordinates rotate on different graphs (linear or hyperbolic). Based on different operation modes, CORDIC can be utilized to approximate several mathematical functions with only addition and shift operations.

Secure Linear Vectoring Mode Iteration. To complete the computation of Livec, we have to introduce four variants: μi,vi,ϑi and τi . μi and vi represent the coordinates of the target vector after i times of vector rotation. ϑi denotes the sum of the phase position after i times of vector rotation. τi determines the rotation direction. As shown in Protocol 1, the value of μi remains equal to μ1 during the iterative process. τi is computed by invoking SCmp. After getting the comparison result, S1 and S2 locally update v′i+1=v′i+τ′⋅μ′1⋅2−i,v′′i+1=v′′i+τ′′⋅μ′′1⋅2−i,ϑ′i+1=ϑ′i−τ′i⋅2−i and ϑ′′i+1=ϑ′′i−τ′′i⋅2−i . It can be discovered that there are only two times intermediate values exchanges in LiVec caused by conducting SCmp. The mathematical expression of the whole calculation process is:
⎧⎩⎨μi+1=μ1vi+1=vi+τi⋅μ1⋅2−iϑi+1=ϑi−τi⋅2−i,(4)
View Sourceand
τi=2SCmp(vi)−1={1,−1,vi≤0vi>0.(5)
View Source

Protocol 1 Secure Linear Vectoring Mode Iteration
S1 has input μ′1 , υ′1 and ϑ′1 ; S2 has input μ′′1 , υ′′1 and ϑ′′1 ; The maximum iteration number is m

S1 outputs ϑ′m ; S2 outputs ϑ′′m ;

Set a public known index i←1 .

while i≤m do

(τ′i,τ′′i)← 2SCmp(υi,0)−1 .

υi+1′←′i+τ′⋅μ1′⋅2−i and υi+1′′←′′i+τ′′⋅μ1′′⋅2−i .

ϑi+1′←′i−τ′i⋅2−i and ϑi+1′′←′′i−τ′′i⋅2−i .

go for next iteration.

end while

Secure Hyperbolic Vectoring Mode Iteration. Similar to LiVec, the computation of LiVec also needs four variants. As illustrated in Protocol 2, intermediate value exchanges occur only two times. Nevertheless, since the vector in LiVec rotates on a hyperbola, the trigonometric function tanh is introduced to compute ϑi+1 . The local update process of μi+1 and ϑi becomes μ′i+1=μ′i+τ′⋅υ′i⋅2−i , μ′′i+1=μ′′i+τ′′⋅υ′′i⋅2−i , ϑ′i+1=ϑ′i−τ′i⋅tanh−12−i and ϑ′′i+1=ϑ′′i−τ′′i⋅tanh−12−i . Note that if the iteration number satisfies i=3k+1 and k∈N+ , the current iteration has to be repeated to ensure convergence. The whole computation process can be expressed as
⎧⎩⎨⎪⎪μi+1=μi+τi⋅υ⋅2−1υi+1=υi+τi⋅μi⋅2−iϑi+1=ϑi−τi⋅tanh−12−i,(6)
View SourceRight-click on figure for MathML and additional features.and
τi=2SCmp(υi)−1={1,−1,υi≤0υi>0.(7)
View Source

Protocol 2 Secure Hyperbolic Vectoring Mode Iteration
S1 has input μ′1 , υ′1 and ϑ′1 ; S2 has input μ′′1 , υ′′1 and ϑ′′1 ; The maximum iteration number is m

S1 outputs ϑ′m ; S2 outputs ϑ′′m ;

Set a public known index i←1 .

while i≤m do

(τ′i,τ′′i)← 2SCmp(υi,0)−1 .

μ′i+1←μ′i+τ′⋅υ′i⋅2−i and μ′′i+1←μ′′i+τ′′⋅υ′′i⋅2−i .

υ′i+1←υ′i+τ′⋅μ′i⋅2−i and υ′′i+1←υ′′i+τ′′⋅μ′′1⋅2−i .

ϑ′i+1←ϑ′i−τ′i⋅tanh−12−i and ϑ′′i+1←ϑ′′i−τ′′i⋅tanh−12−i .

if i%3 is 1 then

do the ith iteration again.

else

go for next iteration.

end if

end while

Secure Hyperbolic Rotation Mode Iteration. To calculate SExp, we propose HypRot, as shown in Protocol 3. Similar to LiVec, the iterative process of HypRot is completed according to Eq. 6. The difference is that HypRot updates τi via the following equation.
τi=−2SCmp(ϑi)+1={1,−1,ϑi≥0ϑi<0.(8)
View Source

Protocol 3 Secure Hyperbolic Rotation Mode Iteration
S1 has input μ′1 , υ′1 and ϑ′1 ; S2 has input μ′′1 , υ′′1 and ϑ′′1 ; The maximum iteration number is m

S1 outputs μ′m and υ′m ; S2 outputs μ′′m and υ′′m ;

Set a public known index i←1 .

while i<m do

(−τ′i,−τ′′i)←− 2SCmp(ϑi,0)+1 .

μ′i+1←μ′i+τ′⋅υ′i⋅2−i and μ′′i+1←μ′′i+τ′′⋅υ′′i⋅2−i .

υ′i+1←υ′i+τ′⋅μ′i⋅2−i and υ′′i+1←υ′′i+τ′′⋅μ′′1⋅2−i .

ϑ′i+1←ϑ′i−τ′i⋅tanh−12−i and ϑ′′i+1←ϑ′′i−τ′′i⋅tanh−12−i .

if i%3 is 1 then

do the ith iteration again.

else

go for next iteration.

end if

end while

B. Secure Division Protocol
As shown in Protocol 4, given two random share pairings (μ′,μ′′) and (υ′,υ′′) , SDiv outputs (f′,f′′) , where μ=μ′+μ′′ and υ=υ′+υ′′ . f′ and f′′ are two random shares of the division result, i.e., f′+f′′=υ/μ . In the process, S1 and S2 have to compute LiVec(μ,1,0) . However, to ensure convergence, μ must be greater than 1 and less than 2. For this, we invoke the single-precision representation method SME in [18] to meet the requirement. By calculating η⋅2ϵ← SME(μ) , the input of LiVec is ensured to satisfy 2>η>1 . ϵ is the exponent of μ . Moreover, suppose the input size is n . Compared with the existing protocol [18], the exchanged number of messages decreases from 8n to 2n .

Protocol 4 Secure Division Protocol
S1 has input μ′1 , υ′1 ; S2 has input μ′′1 , υ′′1 ; The maximum iteration number m

S1 outputs f′ ; S2 outputs f′′ ;

(η′,η′′,ϵ)← SME(μ) .

(α′,α′′)← LiVec(η,1,0,m) .

α′=α′⋅2ϵ and α′′=α′′⋅2ϵ .

(f′,f′′)← SMul(α,υ) .

S1 and S2 return f′ and f′′ , respectively.

Secure Division Protocol. In SDiv, S1 and S2 first convert μ into the single-precision format by invoking (η′,η′′,ϵ)← SME(μ) , which ensures the inputs of LiVec within the valid range. Then, the reciprocal of η=η′+η′′ is computed by utilizing (α′,α′′)← LiVec(η,1,0) . Finally, the two edge servers update α′=α′⋅2ϵ and α′′=α′′⋅2ϵ and continue to calculate (f′,f′′)← SMul(α,υ) . An example of SDiv is presented in Appendix for a better understanding of the workflow of our secret sharing protocols. For brevity, the examples of the other two protocols are omitted.

C. Secure Natural Logarithm Protocol
Given the random share pairing (μ′,μ′′) , Protocol 5 outputs (f′,f′′) , where μ=μ′+μ′′ . f′ and f′′ are two random shares of the logarithm result, i.e., f′+f′′=log(μ) . Here, S1 and S2 compute HypVec(μ+1,μ−1,0) to complete the iterative approximation. To guarantee convergence, the condition for loop termination in SME becomes that the mantissa η has to be between 0.1069 and 9.3573. Compared with the existing protocol [18], the number of messages required to be exchanged is reduced from 12n to 2n .

Protocol 5 Secure Natural Logarithm Protocol
S1 has input μ′1 ; S2 has input μ′′1 ; The maximum iteration number m

S1 outputs f′ ; S2 outputs f′′ ;

(η′,η′′,ϵ)← SME(μ) .

(α′,α′′)← HypVec(η+1,η−1,0,m) .

f′←2α′+ϵ⋅log2 and f′′←2α′′ .

S1 and S2 return f′ and f′′ , respectively.

Secure Natural Logarithm Protocol. First, the two edge servers compute (η′,η′′,ϵ)← SME(μ) to restrict the input of LiVec into the valid range. Then, S1 and S2 collaboratively invoke the secure CORDIC based method to calculate (α′,α′′)← HypVec(η+1,η−1,0) . Finally, they update f′←2α′+ϵ⋅log2 and f′′←2α′′ .

D. Improved Secure Natural Exponentiation Protocol
As shown in Protocol 6, SExp implements the iterative approximation process of natural exponentiation based on HypRot. Similar to LiVec, there is also an input range restriction for HypRot which is [−1.1181, 1.1181]. f′ and f′′ are two random shares of the exponentiation result, i.e., f′+f′′=eμ . Compared with the existing protocol [18], the exchanged number of messages is reduced from 4n to 2n .

Protocol 6 Secure Natural Exponentiation Protocol
S1 has input μ′1 ; S2 has input μ′′1 ; The maximum iteration number m

S1 outputs f′ ; S2 outputs f′′ ;

μ′=α′+β′ and μ′′=α′′+β′′ , where α′ and α′′ are integers and 0<β′<1 , 0<β′′<1 .

[(γ′,γ′′),(δ′,δ′′)]← HypRot(1/Rn,0,β,m) , where Rn=∏n−1i=11−2−2i−−−−−−√ .

S1 computes a←eα′ and splits it into random shares a←a′+a′′ .

S2 computes b←eα′′ and splits it into random shares b←b′+b′′ .

S1 sends a′′ to S2 and S2 sends b′ to S1 .

(ϱ′,ϱ′′)← SMul(a,b) .

(f′,f′′)← SMul(ϱ,γ+δ) .

S1 and S2 return f′ and f′′ , respectively.

Secure Natural Exponentiation Protocol. The two edge servers first split the inputs into μ′=α′+β′ and μ′′=α′′+β′′ , where α′ and α′′ are integers, and β′ and β′′ are the decimal parts of μ . By computing (γ′,γ′′,δ′,δ′′)← HypRot(1/Rn,0,β,m) , we have γ′+γ′′+δ′+δ′′=eβ . Here, 1/Rn=∏n−1i=11−2−2i−−−−−−√≈0.8281 , when n→∞ . S1 and S2 then compute a=eα′ , b=eα′′ , and split them into random shares (a′,a′′) and (b′,b′′) . Subsequently, a′′ and b′ are exchanged between S1 and S2 . Finally, SMul is invoked twice for computing a⋅b and ϱ⋅(γ+δ) , where ϱ=ϱ′+ϱ′′ is the multiplication result of a and b . The outputs of the second invocation for SMul, f′ and f′′ , are two random shares of the natural exponentiation result of μ .

SECTION V.Privacy-Preserving Object Detection of Medical Images
In this section, we provide the implementation details of SecRCNN and discuss the feasibility of extending SecRCNN to a multiparty setting.

A. High-Level Overview of SecRCNN
Before introducing the implementation details of SecRCNN, we first give its high-level overview for a better understanding of its workflow, shown in Fig.2. SecRCNN is composed of three stages, namely secure feature map extraction, secure region proposal and secure regression and classification. The overall goal of SecRCNN is to implement object detection of medical images without revealing any information to the original images revealed to the edge servers. To achieve this goal, SecRCNN first splits medical image pixels into random shares and uploads the random shares to the edge servers. Then, all subsequent computations are performed on secretly shared pixels using our secure protocols. An overview of each stage is below.


Fig. 2.
Privacy-preserving faster R-CNN for object detection of medical images.

Show All

Secure Feature Map Extraction. To ensure no plaintext image pixel information is revealed to the edge servers, the feature map extraction of medical images is completed via the secure feature extraction network (SVGG). The input of SVGG is the secretly shared pixel maps (RGB or grey values) of medical images with a fixed size, i.e., D′ and D′′ , which are uploaded by the healthcare centers. SVGG can be based on an arbitrary ImageNet [20], but the involved three basic neural layers have to be implemented with the three secure protocols, which are secure convolutional layer (SCL), secure ReLU layer (SRL) and secure pooling layer (SPL). In this paper, we choose VGG as the feature extractor. After completing SVGG, S1 and S2 obtain the shared feature maps F′′ and F′′ , respectively.

Secure Region Proposal. Two parts comprise the secure region proposal stage, namely the anchor generation layer (AGL) and the secure PRN network (SPRN). The goal of the region proposal stage is to operate anchor recommendation without leaking any medical image feature information. To achieve this goal, SecRCNN first calculates the AGL with a specially designed intersection over union (IoU) algorithm and gets a series of candidate anchors. Then, it recommends good anchors (i.e., proposal regions) by invoking two SCLs, one secure softmax function (SSM) and one secure non-maximum suppression protocol (SNMS). Finally, the recommended proposal regions are pushed to the next stage.

Secure Regression & Classification. In the last stage, the destination of SecRCNN is to complete the bounding box regression and object classification task without disclosing the image feature information of the recommended proposal regions. Both the regression and classification processes are accomplished by a modified SPRN, in which the two SCLs are substituted with two SFLs. During the process, the input proposal regions are reshaped to a fixed size by the secure regions of interest protocol (SROI).

B. Secure Feature Map Extraction
Before the medical images are sent to SRPN, they are applied to SVGG to extract the feature map. SVGG consists of three kinds of layers: SCL, SRL and SPL. Given the input matrix x=(x0,0,x0,1,…,xw,h) , SCL lets S1 and S2 compute ϕ′=∑w−1i=0∑h−1j=0ω′i,jx′i,j+b and ϕ′′=∑w−1i=0∑h−1j=0ω′′i,jx′′i,j , where ωi,j and b are publicly known weight parameter and bias. The addition here is performed locally by invoking SAdd. (ϕ′,ϕ′′) is the output of SCL.

In SRL, we first utilize s=(− SCmp(xi,j,0)+2)/2 to determine the sign of the input. If s=0 , SRL outputs (x′i,j,x′′i,j) ; otherwise, it outputs (αx′i,j,αx′′i,j) . α is a common learnable parameter. Specially, for convenience of parallel computing, extra computations are operated on the output of SCmp to make s to be only 0 or 1. As for SPL, each convolutional sliding window ξk outputs ({x}_{i}', {x}_{i}'') = \mathop {\arg \max } \xi _{k}({x}) by letting the two edge servers compute (- SCmp({x}_{i} - {x}_{j}, 0) + 2)/2 for several times. As shown in Fig.3, 13 SCLs, 13 SRLs and 4 SPLs are deployed to implement SVGG. Interested readers can refer to [15] for the details of the three secure neural layers.


Fig. 3.
Privacy-preserving VGG feature extraction architecture.

Show All

C. Secure Region Proposal
Given the medical image feature map {\mathcal {F}} output by SVGG, SRPN produces a set of candidate regions called anchors. The anchors are ranked and filtered in SRPN for use in the next stage.

1) Anchor Generation Layer:
The AGL scans the medical image feature map and outputs nine kinds of anchor boxes with varying sizes {8, 16, 32} and aspect ratios {0.5, 1, 2}. Let {\mathcal {F}}' and {\mathcal {F}}'' be two secretly shared feature maps possessed by S_{1} and S_{2} . We set the default stride length to 16 and the size of {\mathcal {F}} to w\times h and place it at every grid location of {\mathcal {F}}' and {\mathcal {F}}'' . The two edge servers can separately obtain the sets of anchor boxes {\mathcal {B}}' = \{{b}_{1}', {b}_{2}', \ldots, {b}_{n}'\} and {\mathcal {B}}'' = \{{b}_{1}'', {b}_{2}'', \ldots, {b}_{n}''\} , where n = \frac {w}{16}\times \frac {h}{16} \times 9 and the original anchor box is {b}_{i} = {b}_{i}' + {b}_{i}'' .

To filter and label the generated anchor boxes, we have to calculate the IoU of anchors. As shown in Fig.4, the computation process of IoU does not need data exchange and can be locally completed by letting S_{1} and S_{2} compute \begin{equation*} IoU({b}_{i}, {b}_{gt}) = \frac {Area_{overlap} ({b}_{i}', {b}_{gt}')}{Area_{union} ({b}_{i}', {b}_{gt}')},\tag{9}\end{equation*}
View Sourceand \begin{equation*} IoU({b}_{i}, {b}_{gt}) = \frac {Area_{overlap} ({b}_{i}'', {b}_{gt}'')}{Area_{union} ({b}_{i}'', {b}_{gt}'')}.\tag{10}\end{equation*}
View Source{b}_{gt} is the ground-truth bounding box feature map. Given the positive IoU threshold \Theta _{pos} and negative IoU threshold \Theta _{neg} , the anchor boxes that are beyond the boundary or satisfy \Theta _{neg} < IoU_{b_{i}} < \Theta _{pos} are ignored in the following computation. The boxes satisfying IoU({b}_{i}, {b}_{gt}) > \Theta _{pos} are masked as “positive”. The others are masked as “negative”.


Fig. 4.
The calculation for Intersection over Union in S_{1} and S_{2} .

Show All


Fig. 5.
Privacy-preserving region proposal layer.

Show All

2) Region Proposal Network:
In this step, SRPN filters {\mathcal {B}} and recommends the good regions for the ROI layer. To train SRPN and rank the region proposals, two SCLs are invoked. One is used to compute the probability {O}_{cls} of each anchor being foreground or background. The other is used to compute the regression coefficients {C}_{reg} . The two layers are simultaneously operated and have the same kernel size (1\times 1) . However, the one for classification has n\times 2 output channels, and the other has n\times 4 output channels. Then, {O}_{cls} is further processed by SSM to get the final scores {P}_{cls} . SSM can be expressed as \begin{align*} {p}_{i}'= & SSM(i) = {\texttt {SDiv}}({\texttt {SExp}}({O}_{cls}(i)'), \\& \qquad \qquad \qquad \,{\texttt {SExp}}({O}_{cls}'(0)) + {\texttt {SExp}}({O}_{cls}'(1))),\tag{11}\end{align*}
View Sourceand \begin{align*} {p}_{i}''= & SSM(i) = {\texttt {SDiv}}({\texttt {SExp}}({O}_{cls}(i)''), \\& \qquad \qquad \qquad \,{\texttt {SExp}}({O}_{cls}''(0)) + {\texttt {SExp}}({O}_{cls}''(1))),\tag{12}\end{align*}
View Sourcewhere {p}_{i} = {p}_{i}' + {p}_{i}'' = {P}_{cls}(i) is a probability vector.

Furthermore, to reduce the number of candidate anchor boxes and improve efficiency, the SNMS is deployed to select the anchor boxes whose scores are lower than others. As illustrated in Protocol 7, SNMS can be simply implemented by the SCmp based sort function and the secure IoU function.

Protocol 7 Secure Non-Maximum Suppression Protocol
S_{1} has the set of candidate regions {\Gamma}_{0}' and corresponding scores {\Omega}_{0}' ; S_{2} has the set of candidate regions {\Gamma}_{0}'' and corresponding scores {\Omega}_{0}'' ; common parameter IoU threshold is \Theta ;

S_{1} outputs the filtered regions {\Gamma}' ; S_{2} outputs the filtered regions {\Gamma}'' ;

if {\Gamma}_{0} = {\Gamma}_{0}' + {\Gamma}_{0}''\not\subset \mathcal{B} or {\Omega}_{0} = {\Omega}_{0}' + {\Omega}_{0}''\not\subset S_{cls} then

Return error.

end if

Update (\Omega_{0}', \Omega_{0}'', {\Gamma_{0}}', {\Gamma_{0}}'')\gets ~sort(\Omega_{0}, {\Gamma_{0}}) .

Set the retained candidate regions (\Gamma', \Gamma'') = \varnothing .

while {\Omega}_{0} is not empty do

(\omega_{0}', \omega_{0}'')\gets {\Omega}(0) and ({\tau}_{0}', {\tau}_{0}'')\gets {\Gamma}(0) .

Update (\Gamma', \Gamma'')\gets + {\tau}_{0} .

Delete \tau{0} from {\Gamma} and {\omega}_{0} from {\Omega} .

for all {\tau}_{i}\in{\Gamma} do

cmp =

if IoU({\tau_{i}},{\tau_{0}})>\Theta then

Delete \tau{i} from {\Gamma} and {\omega}_{i} from {\Omega} .

end if

end for

end while

s_{1} and s_{2} return \Gamma' and \Gamma'' , respectively.

3) Loss Function:
As mentioned before, the loss function of RPN is composed of two parts, the log loss part for classification and the smooth L_{1} part for box bounding regression. To calculate log loss, S_{1} and S_{2} have to compute \begin{align*} L_{Log}({p}_{i}', {y}_{i}') = &{\texttt {SMul}}({y}_{i}', {\texttt {SLog}}({p}_{i}')) \\& \qquad \,\,+\, {\texttt {SMul}}(1 - {y}_{i}', {\texttt {SLog}}(1 - {p}_{i}')),\tag{13}\end{align*}
View SourceRight-click on figure for MathML and additional features.and \begin{align*} L_{Log}({p}_{i}'', {y}_{i}'') = &{\texttt {SMul}}({y}_{i}'', {\texttt {SLog}}({p}_{i}'')) \\& \qquad \qquad \,\,+\, {\texttt {SMul}}({y}_{i}'', {\texttt {SLog}}({p}_{i}'')),\tag{14}\end{align*}
View Sourcewhere {y}_{i} = {y}_{i}' + {y}_{i}'' is the ground-truth label and {p}_{i} is the prediction probability. For smooth L_{1} loss, let the two edge servers compute \begin{align*} Smooth_{L_{1}}({x}_{i}') = \begin{cases} \frac {1}{2} \sigma ^{2}\cdot {\texttt {SMul}}({x}_{i}', {x}_{i}'), & |{x}| < \frac {1}{\sigma ^{2}} \\ {sign({x}_{i}')}\cdot {x}_{i}' - \frac {1}{2\sigma ^{2}}, & otherwise, \end{cases}\tag{15}\end{align*}
View Sourceand \begin{align*} Smooth_{L_{1}}({x}_{i}'') = \begin{cases} \frac {1}{2} \sigma ^{2}\cdot {\texttt {SMul}}({x}_{i}'', {x}_{i}''), & |{x}| < \frac {1}{\sigma ^{2}} \\ {sign({x}_{i}'')}\cdot {x}_{i}' - \frac {1}{2\sigma ^{2}}, & otherwise, \end{cases}\tag{16}\end{align*}
View Sourcewhere \begin{equation*} sign({x}) = -2\texttt {SCmp}({x}, 0) + 1.\tag{17}\end{equation*}
View SourceHere, {x}_{i} = {x}_{i}' + {x}_{i}'' is the difference between the predicted and ground-truth regression coordinates {c}_{i} - {t}_{i} . \sigma is a predefined constant. Then, the objective loss L can be computed as \begin{align*} L' = &\frac {1}{N_{c}} \sum _{i=0}^{N_{c}} L_{Log}({p}_{i}', {y}_{i}') \\&\qquad \qquad \,+\, \frac {\lambda }{N_{r}} \sum _{i=0}^{N_{r}} {\texttt {SMul}}({y}_{i}', Smooth_{L_{1}}({c}_{i}' - {t}_{i}')),\tag{18}\end{align*}
View Sourceand \begin{align*} L'' = &\frac {1}{N_{c}} \sum _{i=0}^{N_{c}} L_{Log}({p}_{i}'', {y}_{i}'') \\ & \qquad \quad \,\,+\, \frac {\lambda }{N_{r}} \sum _{i=0}^{N_{r}} {\texttt {SMul}}({y}_{i}'', Smooth_{L_{1}}({c}_{i}'' - {t}_{i}'')),\tag{19}\end{align*}
View Sourcewhere {c}_{i}\in {C}_{reg} and \lambda is a constant that is set to 3 by default. In Section II, the computation method and meaning of the four types of {t}_{i} are listed. They can be locally computed by S_{1} and S_{2} .

D. Secure Regression & Classification
In this stage, SecRCNN invokes SROI to reshape the size of the region proposals produced by SRPN, and two fully-connected layers to generate the final output. As mentioned before, there can be nine types of proposal anchor boxes. The different sizes make it difficult to directly deploy them as the input for the following neural layer computation. Therefore, as shown in Protocol 8, the region proposals are further reshaped into a fixed size w\times h . Then, SecRCNN computes the classification and regression coefficients in a similar way to SRPN. The differences are that the regions are further simplified by SRPN and SCL is changed to SFL. The computation method for SFL is the same as for SCL.

Protocol 8 Secure Regions of Interest Protocol
S_{1} has the feature maps of region proposals {\mathcal {X}}' ; S_{2} has the feature maps of region proposals {\mathcal {X}}'' ; The fixed width w and height h are public parameters.

S_{1} outputs the reshaped region proposals {\mathcal {Y}}' ; S_{2} outputs the reshaped region proposals {\mathcal {Y}}'' ;

S_{1} and S_{2} average divide {\mathcal {X}}' and {\mathcal {X}}'' into w\times h parts, ({\mathcal {X}}_{1}', \ldots, {\mathcal {X}}_{w\times h}') and ({\mathcal {X}}_{1}'', \ldots, {\mathcal {X}}_{w\times h}'') .

for 1 < i < w\times h do

S_{1} and S_{2} compute ({x}', {x}'') \gets \mathop {\arg \max }_{x_{j}\in {\mathcal {X}}_{i}} {\mathcal {X}}_{i}(j) .

Add {x}' and {x}'' into {\mathcal {Y}}' and {\mathcal {Y}}'' .

end for

S_{1} and S_{2} return {\mathcal {Y}}' and {\mathcal {Y}}'' , respectively.

E. Feasibility for Multiparty Computation
For a better understanding of the SecRCNN workflow, only the two-party setting (i.e., two edge servers) is discussed above. However, two parties are sometimes not enough in applications that need to use SecRCNN. Thus, in this section we discuss the feasibility of extending SecRCNN to a multiparty setting (MPC).

To expand to MPC, we have to adapt the secure protocols of SecRCNN to allow for multiparty computation. According to the definitions of the interactive protocols, it can be discovered that all of them are composed of three kinds of basic sub-protocols: SAdd, SCmp and SMul. Therefore, proving the feasibility of SecRCNN for MPC is equivalent to proving that the three protocols support MPC. Among the three protocols, SAdd can be locally completed. Therefore, it is trivial to state the feasibility of SAdd to extend to MPC. SCmp is based on the most significant bit (MSB) protocol, which has been proved to support MPC [21]. Similar to SCmp, the basis of SMul, Beaver’s triplet, has also been originally designed for supporting MPC [22]. In conclusion, if required, it is completely feasible to make SecRCNN to operate in a multiparty setting.

SECTION VI.Theoretical Analysis of SecRCNN
A. Computational Complexity
To evaluate the efficiency of SecRCNN, we analyse the computational complexity of each sub-protocol, as shown in Table I. The input size of each sub-protocol is n ; m is the maximum iteration number of LiVec, HypVec and HypRot; T_{\texttt {Mul}} , T_{\texttt {SMul}} and T_{\texttt {SCmp}} are the runtimes of local multiplication, secure multiplication and secure comparison functions, where T_{\texttt {Mul}} < T_{\texttt {SMul}} < T_{\texttt {SCmp}} . For the division, logarithm and exponentiation sub-protocols, we compare their computation complexities with the existing ones [18].

TABLE I Computational Complexity of Each Protocol in SecRCNN

For LiVec, HypVec and HypRot, all three protocols perform one SCmp and four or six local multiplications at each iteration, which takes \mathcal {O}(nm)(T_{\texttt {SCmp}} + T_{\texttt {Mul}}) time. For SDiv, SLog and SExp, what they have in common is that all of them contain a CORDIC based iterative protocol during their operation processes (LiVec, HypVec or HypRot). The difference is that SExp does not operate SME which takes \mathcal {O}(n)T_{\texttt {Mul}} time. Additionally, SLog does not have to do SMul after completing the iterative protocol. Compared with the existing protocols of the three mathematical functions, our protocols substitute SMul with local multiplication, which can reduce the runtime for iteration. For SNMS, its computational complexity is \mathcal {O}(n\log n + n^{2})T_{\texttt {SCmp}} . \mathcal {O}(n\log n)T_{\texttt {SCmp}} time is spent on quicksort. \mathcal {O}(n^{2})T_{\texttt {SCmp}} time is spent on the candidate region filtering. For SROI, \mathcal {O}(n)T_{\texttt {SCmp}} time is spent on scanning the whole input space and selecting the maximum feature value through SCmp.

The three stages of SecRCNN are completed by invoking the above protocols, or the previously proposed two sub-protocols. Thus, the computational complexity of the three stages can be easily derived from the linear combination of the protocols. For brevity, we omit their analysis.

B. Correctness Analysis of SecRCNN
In SecRCNN, all of the medical images in D are split into two random shares {D}' and {D}'' . Intuitively, we cannot ensure that the outputs of SecRCNN satisfy {f}_{cls} = {f}_{cls}' + {f}_{cls}'' and {f}_{reg} = {f}_{reg}' + {f}_{reg}'' , where {f}_{cls} and {f}_{reg} are the original outputs of Faster R-CNN. Therefore, the following proof is given to state the correctness of SecRCNN.

Theorem 1:
The computation of feature map extraction, proposal region network and classification & regression in SecRCNN is correct.

Proof:
First, the feature map extraction is composed of SCL, SRL and SPL. Among the three types of neural layers, SRL and SPL are implemented by calling the SCmp multiple times. Since the SCmp never changes the input value and only outputs the comparison result 1 or −1, SRL and SPL are both errorless. For SCL, SMul is used to complete the convolutional operations. SMul is a previously proposed protocol and has been proved to be correct. Therefore, SCL is also errorless. Overall, it can be deduced that the feature map extraction of SecRCNN is correct. Second, besides SMul, SCmp and the local computation that cannot influence the correctness, SDiv, SExp and SLog are deployed to compute the softmax function and loss function in SRPN. The CORDIC iteration methods used in SDiv, SExp and SLog can maintain the convergence speed at one bit per iteration. We set the default iteration number to be a small value, like 10. The computation error can reach a degree of no more than 10−10 which is completely negligible for application purposes. According to the preliminary assumption, the three protocols are considered to be correct here. Consequently, SRPN can be proved to be correct. Finally, for the classification and regression process, the involved sub-protocols are identical to SRPN. Naturally, the process is also correct. In conclusion, the correctness of SecRCNN is proved.

C. Security Analysis of SecRCNN
The security analysis of SecRCNN is based on the following definitions and lemmas.

Definition 1:
We say that a protocol \pi is secure if there exists a probabilistic polynomial-time simulator \zeta that can generate a view for the adversary \mathcal {A} in the real world and the view is computationally indistinguishable from its real view.

Lemma 1 [23]:
A protocol is perfectly simulatable if all its sub-protocols are perfectly simulatable.

Lemma 2 [14]:
If a random element r is uniformly distributed on Z_{n} and independent from any variable x \in Z_{n} , then r \pm x is also uniformly random and independent from x .

According to Definition 1 and Lemma 1, to prove the security of SecRCNN, we have to prove that all simulated views of its sub-protocols are computationally distinguishable from their real views. The proofs are given as follows.

Theorem 2:
The protocols LiVec, HypVec and HypRot are secure in the semi-honest model.

Proof:
S_{1} has the same type of real view for LiVec, HypVec and HypRot, view_{1} = \{m, \mathcal {C}, \mathcal {D}, \mathcal {E}, \mathcal {J}\} , where \mathcal {C} = \{{\mu }_{1}',{\mu }_{2}',\ldots,{\mu }_{m}'\} , \mathcal {D} = \{{\upsilon }_{1}', {\upsilon }_{2}', \ldots, {\upsilon }_{m}'\} , \mathcal {E} = \{\theta _{1}', \theta _{2}', \ldots, \theta _{m}'\} , \mathcal {J} = \{{\tau }_{1}', {\tau }_{2}', \ldots, {\tau }_{m}'\} . {\mu }_{1}' , {\upsilon }_{1}' and \theta _{1}' are uniformly random inputs selected in Z_{p} ,. {\tau }_{i}'\in \mathcal {J} are outputs of SCmp. m is a public available constant. Since SCmp is a previously proposed protocol proved to be secure in semi-honest model, all elements {\tau }_{i}'\in \mathcal {J} are uniformly random shares as long as the elements of \mathcal {C}_{it} = \mathcal {C} - \{{\mu }_{1}'\} , \mathcal {D}_{it} = \mathcal {D} - \{{\upsilon }_{1}'\} , \mathcal {E}_{it} = \mathcal {E} - \{\theta _{1}'\} are uniformly random. From the iterative equations mentioned in Section IV, {\mu }_{i+1}' , {\upsilon }_{i+1}' , \theta _{i+1}' , i > 0 are obtained by locally operating first order polynomial about {\mu }_{i}' , {\upsilon }_{i}' , \theta _{i}' . Consequently, based on the inductive method, it is easy to deduce that \mathcal {C}_{it} , \mathcal {D}_{it} and \mathcal {E}_{it} are completely composed of uniformly random values. Furthermore, we can conclude that \mathcal {C} , \mathcal {D} , \mathcal {E} , \mathcal {J} are all uniformly random. Since the output of the three protocols for S_{1} are ({\mu }_{m}' , {\upsilon }_{m}' ) or \theta _{m}' , we can also guarantee that output_{1} = \{{\mu }_{m}', {\upsilon }_{m}'\} or output_{1} = \{\theta _{m}'\} is uniformly random. Therefore, both view_{1} and output_{1} are simulatable by simulator \zeta . And for \mathcal {A} , it is unable to distinguish the real view and simulated view in polynomial time. In the same way, the real view of S_{2} can be proved to be computationally indistinguishable from its simulated view.

Theorem 3:
The protocols SDiv, SLog and SExp are secure in the semi-honest model.

Proof:
In SDiv, the real view of S_{1} is view_{1} = \{{\mu }', {\upsilon }', {\eta }', {\alpha }'\} . {\mu }' and {\upsilon }' are uniformly random inputs selected in Z_{p} ; {\eta }' is the result of {\mu }' multiplied with constants. Therefore, it is trivial to prove that {\eta }' is uniformly random. {\alpha }' is the output of LiVec. According to Theorem 2, {\alpha }' is also a uniformly random share in Z_{p} . The output view of S_{1} is output_{1} = {f'} , {f}' = SMul({\upsilon }', 2^{\epsilon '}\cdot LiVec({\eta }', 1, 0)) . Since SMul is a previously proposed secure protocol and its inputs are uniformly random values from the above analysis, {f}' is a uniformly random share value. As a consequence, view_{1} and output_{1} are simulatable for \zeta and computationally distinguishable from the simulated views for \mathcal {A} . Likewise, the real views of S_{2} are also computationally indistinguishable from its simulated view.

In SLog, the real view of S_{1} is view_{1} = \{{\mu }', {\eta }', {\alpha }'\} . Similar to SDiv, {\mu }' is a uniformly random input, {\eta }' is the product of {\mu }' times constants and {\alpha }' is generated by the HypVec proved to be secure in the proof of Theorem 2. Naturally, the three elements are uniformly random values and simulatable. The output view of S_{1} is output_{1} = {f'} , {f}' = HypVec({\eta }' + 1, {\eta }' - 1, 0) + \epsilon \cdot \log 2 . {f}' is the addition of a uniformly random share and a constant. Thus, {f}' is also uniformly random and simulatable. And it can be deduced that \zeta can generate simulated views in polynomial time which are computationally indistinguishable from the real views for \mathcal {A} . In the same way, it can be proved that the real view S_{2} is computationally indistinguishable from its simulated view.

In SExp, the real view of S_{1} is view_{1} = \{{\mu }', {\alpha }', {\beta }', {\gamma }', {\delta }', R_{n}, a, a', b', {\varrho }' \} , where {\mu }' = {\alpha }' + {\beta }' . {\mu }' is a uniformly random input. From the definition of SExp, Based on Lemma 2, the split shares {\alpha }' , {\beta }' , a' and b' are all uniformly random. Both {\gamma }' and {\delta }' are outputs of HypRot, which are uniformly random according to the proof of Theorem 2. {\varrho }' is the output of SMul, a previously proposed secure protocol. Therefore, it is also uniformly random. Moreover, R_{n} is a constant that cannot influence the security of SExp. The output view of S_{1} is output_{1} = {f'} , {f}' = SMul(HypRot\left(\frac {1}{R_{n}}, 0, {\beta }\right), {\varrho } ). Similar to SDiv, {f}' is the output of SMul and a uniformly random value. As a consequence, view_{1} and output_{1} are simulatable and their simulated views are computationally indistinguishable from them for \mathcal {A} . Similarly, the view of S_{2} is also simulatable and computationally indistinguishable.

The feature map extraction, SRPN and classification & regression of SecRCNN are then proved to be secure.

Theorem 4:
The interactive protocol of feature map extraction, SRPN and classification & regression process in SecRCNN are secure in the semi-honest model.

Proof:
\mathcal {A} eavesdrops on the transmission channels between the two edge servers and records the messages about the three interactive protocols inputs into an input tape tape_{in} and outputs into an output tape tape_{out} . According to the definitions of the interactive protocols, \mathcal {A} have tape_{in} = view_{SCmp}\cup view_{SAdd}\cup view_{SMul}\cup view_{SDiv}\cup view_{SLog}\cup view_{SLog} and tape_{out} = output_{SCmp}\cup output_{SAdd}\cup output_{SMul}\cup output_{SDiv}\cup output_{SLog}\cup output_{SLog} . Here, the elements belonging to the same sub-protocol are pushed into the same view. Let’s hypothesize that there is a polynomial-time algorithm that allows \mathcal {A} to tell whether tape_{in} and tape_{out} are simulated by \zeta . Then, it must be unable for \zeta to find a polynomial-time algorithm to simulate the views tape_{in} and tape_{out} . Based on the previously proved theorems, it is guaranteed that the views of all the sub-protocols are simulatable. Based on Lemma 1, tape_{in} and tape_{out} are simulatable. Furthermore, \zeta is capable of generating views that are computational distinguishable from the real views, which is opposite to the hypothesis. Thus, it can be easily deduced that the hypothesis does not stand and the interactive protocols of SecRCNN are secure in the curious-but-honest model.

SECTION VII.Experiments
In this section, we first evaluate the performance and security of SecRCNN. Then, to further assess the effectiveness and efficiency of SecRCNN, we conduct experiments to evaluate the computation error, runtime and communication overhead of the newly proposed sub-protocols. All the experiments were conducted with the ImageCLEF medical image dataset which is publicly available and provided by the IRMA group from the University Hospital of Aachen, Germany [24]. The original data are encrypted and sent to the two edge servers on a laptop with an Intel(R) Core(TM) i5-7200 CPU @2.50GHz and 8.00GB of RAM. Two computers equipped with NVIDIA GeForce GTX 1080 TI graphic card and 8.00GB of RAM are deployed as the edge servers.

A. Performance Evaluation of SecRCNN
To evaluate the performance of SecRCNN, we randomly selected 2000 medical images for training and 500 images for validation from ImageCLEF. The batch size was set to 128. The model used to extract feature maps was the VGG-16 [12] and implemented according to [25]. p and q , mentioned in Section II-B, were set to two Mersenne primes, p = 2^{31} - 1 and q = 2^{13} - 1 . The Numpy and Tensorflow libraries of Python were utilized in the experiments to accelerate the parallel computations. In addition, the iteration numbers of SDiv, SLog and SExp were all set to be 50 by default. The learning rate was set to 0.001.

First, we compared the performance between Faster R-CNN and SecRCNN in terms of object detection accuracy and training loss. The comparison results are shown from Fig.6 to Fig.8. The loss of Faster R-CNN is composed of two parts, RPN loss and R-CNN loss, given in Fig. 7(a) and Fig. 8(a). From the experimental results, it can be discovered that SecRCNN attains similar performance with Faster R-CNN in the training process. The differences in the accuracy and the two types of losses between Faster R-CNN and SecRCNN are negligible. To further evaluate the computation errors, we sampled the exact error values at an interval of 500, as shown in Fig. 6(b), Fig. 7(b) and Fig. 8(b). In Fig. 6(b), the regression accuracy computation error for RPN was calculated by computing the average computation error of the four box bounding coefficients. As mentioned in Section V, the RPN loss and R-CNN loss can be split into two parts, satisfying RPN Loss = RPN-CLS Loss + RPN-REG Loss and RCNN Loss = RCNN-CLS Loss + RCNN-REG Loss, where RPN-CLS Loss, RPN-REG Loss, RCNN-CLS Loss and RCNN-REG Loss are the classification loss and the regression loss for RPN and RCNN. Therefore, we separately give the computation errors of the RPN loss and the R-CNN loss in Fig. 7(b) and Fig. 8(b). In the three graphs, it can be found that although the computation errors increase along with the training steps, the increasing rate is quite low. After 3000 iterations, the computation error increases by only one order of magnitude and maintains about 10−14, which is completely negligible in real-world applications. The reason for the increase is that the computation error introduced in each round of training is accumulated along with the training process.


Fig. 6.
Performance analysis of SecRCNN with accuracy.

Show All


Fig. 7.
Performance analysis of SecRCNN with RPN loss.

Show All


Fig. 8.
Performance analysis of SecRCNN with R-CNN loss.

Show All

To experimentally investigate the security of SecRCNN, an example of medical image sharing is given in Fig.9. The six images in the first row of Fig.9 are from a Kaggle competition task2 to find the nuclei in divergent images to advance medical discovery. The secretly shared images were generated by splitting the pixels of the original images into two random shares. The shared images are shown in the second and third rows of Fig.9. It can be found that the original images were transformed into two disorganized and meaningless images after being secretly shared. Intuitively, the image shares that look like noise images cannot reveal any information about the original images. By reflecting the random shares into gray values ranged from 0 and 255, the intuitive conclusion is further proved through the gray-level frequency histograms in Fig.10. For simplicity, we only selected the first two columns of Fig.9 to show their gray-level frequency in histograms. The sampling interval of the gray value is 2. It can be observed that for the original image, the gray values with high frequencies are intensively distributed in a specific area. Nevertheless, for the secretly shared images, their gray values are uniformly distributed between 0 to 255. The phenomenon means that the statistical distribution feature of the original medical image is hidden. Thus, it is quite hard for an attacker to exploit the image shares to infer any useful information about the original images. Furthermore, from the images in the last row of Fig.9, it can be discovered that the original images can be recovered by simply adding the corresponding random shares.

Fig. 9. - Examples of medical image shares. First row: six medical images delivered by 
$H_{1}$
 and 
$H_{2}$
. Second row: the random shares of the first row for 
$S_{1}$
. Third row: the random shares of the first row for 
$S_{2}$
. Fourth row: the recovered images by adding the second row and the third row together.
Fig. 9.
Examples of medical image shares. First row: six medical images delivered by H_{1} and H_{2} . Second row: the random shares of the first row for S_{1} . Third row: the random shares of the first row for S_{2} . Fourth row: the recovered images by adding the second row and the third row together.

Show All

Fig. 10. - Gray-level frequencies (histogram) for the first image from 
$H_{1}$
 and the first image from 
$H_{2}$
 in Fig.9. Left: the original images. Middle: the secretly shared images for 
$S_{1}$
. Right: the secretly shared images for 
$S_{2}$
.
Fig. 10.
Gray-level frequencies (histogram) for the first image from H_{1} and the first image from H_{2} in Fig.9. Left: the original images. Middle: the secretly shared images for S_{1} . Right: the secretly shared images for S_{2} .

Show All

Then, we performed an analysis to evaluate the efficiency of SecRCNN. Table II illustrates the protocol runtime and message size of secure RPN and Fast R-CNN for training and testing. In each iteration, there were 128 feature maps pushed into the secure VGG-16 network. 17100 anchors for each map were generated according to the method described in Section V. Under the condition, SecRCNN can finish one iteration with about three minutes runtime and 150 millibyte communication overhead. Compared with model training, the testing needs much less computing resources and communication load. The reason is that for testing, there is no need to spend high computation resource on the computation of the loss functions. In addition, assume that the size of input images is w\times h and the data storage length is \ell . After the feature extraction and region proposal stages, \frac {w}{16}\times \frac {h}{16} features and \frac {w}{16}\times \frac {h}{16}\times 9 anchors are obtained. As mentioned in Section V, three sub-protocols are invoked in SRPN, which takes 6\times 18\times \frac {w}{16}\times \frac {h}{16}\times \ell bits communication overhead. Therefore, it theoretically requires \mathcal {O}(w h \ell) bits to run SRPN. Then, suppose the mini-batch size to be \mathcal {N}_{cls} and the number of anchors proposed by SPRN to be \mathcal {N}_{reg} . \mathcal {O}((\mathcal {N}_{cls} + \mathcal {N}_{reg}) \ell) bits are exchanged during the secure classification and bounding box regression stages.

TABLE II Runtime and Message Size of Each Iteration in SecRCNN

B. Performance Evaluation of the Sub-protocols in SecRCNN
Four factors are important to evaluate the performance of the SDiv, SLog and SExp: 1) the computation errors with different iteration numbers; 2) the computation errors with different input ranges; 3) the runtime with different input lengths; and 4) the communication overhead with different input lengths. Consequently, we compared the performance of the sub-protocols proposed in Section IV with the previously proposed protocols [18]. In the graphs, SecDiv, SecExp, ISEexp, SecLog and ISLog are the abbreviations for the secure division, secure natural exponentiation, improved secure natural exponentiation, secure natural logarithm and improved secure natural logarithm sub-protocols, respectively. The experiment results of SecLog and ISLog are shown in the same column of the histograms because they are based on the identical iterative formula and have similar convergence features, as do SecExp and ISExp.

From Fig. 11(a), Fig. 11(e) and Fig. 11(i), it can be observed that our sub-protocols converge slightly slower than the existing protocols that are Maclaurin Series and Newton-Raphson based [18]. However, they can still reach 10−10 computation error after about 30 iterations, which is enough to satisfy most applications. When the input range changes as shown in Fig. 11(b), Fig. 11(f) and Fig. 11(j), all the newly proposed sub-protocols can at least reach or even exceed the performance of the existing sub-protocols. As shown in Fig. 11(c), Fig. 11(g) and Fig. 11(k), the runtime needed by SDiv, SLog or SExp is obviously less than the previous secure sub-protocols. The decrease is because the messages exchanged in our CORDIC based protocols are greatly reduced, listed in Fig. 11(d), Fig. 11(h) and Fig. 11(l). Data communication is the most time-consuming operation in the additive secret sharing based framework.


Fig. 11.
Effectiveness and efficiency analysis of sub-protocols in SecRCNN.

Show All

SECTION VIII.Related Work
Faster R-CNN is one of the most successful convolutional networks evolved from R-CNN [11], which is the first type of deep learning network applied to the domain of object detection. In recent years, the model has been widely deployed in the fields of autonomous driving [26], biometric authentication [27], and especially medical diagnosis [28]. Due to the state-of-the-art performance of Faster R-CNN, Zhang et al. [29] succeeded in identifying and detecting the adhesion cancer cells in phase-contrast microscopy with limited samples, which was essential to help people against cancer. Later, Ding et al. [30] made it possible to accurately detect pulmonary nodules with computed tomography images for the early screening of lung cancer using Faster R-CNN. Besides cancer, Faster R-CNN also has great clinical significance in the treatment of other diseases. Lo et al. [31] utilized Faster R-CNN to detect the glomeruli in light microscopic images of renal pathology, which is significant for the diagnosis of kidney diseases To reduce the difficulty of examining the transthalamic plane of the fetal head, Lin et al. [32] presented a Faster R-CNN based automatic method to assess the quality of the fetal head in ultrasound images. In addition, by combining Faster R-CNN with DeepLab [33], Tang et al. [34] overcame the challenging task of segmenting the liver from other organ tissues in clinical images and achieved automatic liver segmentation.

To guarantee the privacy of patients’ data while analyzing their pathological images, the most popular solution is digital watermarking. By inserting the binary format watermarking information into the pixel value of the original image, the approach can protect the authenticity and integrity of the medical images. For example, Selvaraj and Varatharajan [36] used the hash function to improve the ability of watermarking to protect the integrity of digital medical images. However, in general, the quality of the watermarked images is reduced to a certain extent [36]. In a situation where multiple healthcare centers would like to cooperate to build a deep learning model, watermarking is no longer suitable. This is because the watermarked images are not computable any more. And since watermark can only provide authenticity of patient ID, it cannot hide the image information from being stolen to build an anonymous database by others. To tackle this problem, Wu [37] utilized the HE to provide reversible hiding of medical images. CaRENets proposed by Chao et al. [38] was also based on the HE and support the inference of tge CNN model on the encrypted medical images. However, because of the high requirements for computation power and storage space, HE is not practical in applications. The other approach for privacy preservation of medical images is called garbled-circuit. Zheng et al. [9] used GC to protect the medical image privacy from the external cloud database. Unfortunately, GC is also unpractical due to the same reason as HE. In addition, the differential privacy (DP) technique can also be used to protect the privacy of medical images [39]. DP is an approach that is specially designed to preserve data privacy while applying the data mining technique. By adding random noises to the image pixels, DP makes a single image unrecognizable for the adversary, but the statistical feature for the whole database is still reserved. Nevertheless, since there has not been a novel algorithm, the existing DP based schemes always make the deep learning model suffer from a dramatic reduction in accuracy [10].

SECTION IX.Conclusion
In this paper, we proposed SecRCNN, a privacy-preserving object detection model for medical image analysis. To reduce the communication overhead of the iterative approximation process, we redesigned the existing sub-protocols of common mathematical functions with the CORDIC algorithms. Then, we proposed a series of interactive protocols to implement the training and inference process of SecRCNN, which included feature extraction, region proposal, classification and bounding box regression. Based on SecRCNN, healthcare centers can collaborate to train a more accurate and more reliable model without concern of privacy disclosure.