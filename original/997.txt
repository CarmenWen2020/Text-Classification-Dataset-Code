As an emerging training model with neural networks, federated learning has received widespread attention due to its ability to update parameters without collecting users' raw data. However, since adversaries can track and derive participants' privacy from the shared gradients, federated learning is still exposed to various security and privacy threats. In this paper, we consider two major issues in the training process over deep neural networks (DNNs): 1) how to protect user's privacy (i.e., local gradients) in the training process and 2) how to verify the integrity (or correctness) of the aggregated results returned from the server. To solve the above problems, several approaches focusing on secure or privacy-preserving federated learning have been proposed and applied in diverse scenarios. However, it is still an open problem enabling clients to verify whether the cloud server is operating correctly, while guaranteeing user's privacy in the training process. In this paper, we propose VerifyNet, the first privacy-preserving and verifiable federated learning framework. In specific, we first propose a double-masking protocol to guarantee the confidentiality of users' local gradients during the federated learning. Then, the cloud server is required to provide the Proof about the correctness of its aggregated results to each user. We claim that it is impossible that an adversary can deceive users by forging Proof, unless it can solve the NP-hard problem adopted in our model. In addition, VerifyNet is also supportive of users dropping out during the training process. The extensive experiments conducted on real-world data also demonstrate the practical performance of our proposed scheme.
SECTION I.Introduction
Deep learning has played a significant part in many applications, e.g., medical prediction [1], [2], autopilot [3], [4], etc. Such deep learning based applications have penetrated into every aspect of our society and gradually changed the habits of human beings in various areas like living, travel, and socializing [5], [6].

Deep learning requires a large number of data which are usually collected from users. However, user’s data may be sensitive in nature or contain some private information. For example, in healthcare systems, the patients may not be willing to share their medical data with a third party service provider (e.g., cloud server) [7]–[8][9]. Recently, federated learning [10], [11] is gradually gaining attention from both academia and industry with its ability of training network without collecting users’ original data, where all users and the cloud server work together only by sharing local gradients and global parameters. However, research shows that attackers can still indirectly obtain the sensitive information including tabs [12], [13] and memberships [14], [15] based on the shared gradients. On the other hand, data integrity breaches in federated learning have also been frequently reported in the media [16], [17]. Particularly, driven by certain illegal interests, a malicious cloud provider may return incorrect results to users. For example, a “lazy” cloud provider may compress the original model with a simpler but less accurate model to reduce its own computation cost, or worse, maliciously forge the aggregated results sent to users. Therefore, protecting user’s privacy and data integrity (especially the correctness of results returned from the server) are two fundamental issues in the training process of federated learning. Hence, it is urgent and meaningful to design a secure federated training protocol, which can efficiently verify the correctness of results returned from the server while protecting user’s data privacy.

In order to solve the above problems, several works focusing on privacy-preserving deep learning have been proposed. Shokri et al. [18] proposed a privacy-preserving deep learning protocol by selectively sharing updated parameters, which can achieve a balance between practicality and security. Trieu Phong et al. [19] proposed a secure deep learning system through the integration of additively homomorphic encryption and gradient descent technology. Recently, Bonawitz et al. [11] put forward a practical and secure architecture for federated learning by exploiting the secret sharing and key agreement protocol, which allows users to be offline during the execution while still guaranteeing high accuracy. However, none of the above solutions support verifying the correctness of results returned from the server. It is closely related between the correctness of results returned from the server and the privacy of users’ local gradients. The risk of users’ privacy being compromised always tends to increase once the adversary has been able to manipulate the data returned to users. For instance, in the well-known white-box attacks [14], [15], adversaries can carefully return crafted results to users for analyzing statistical characteristics of user-uploaded data, and induce users to release more additional sensitive information.

Recently, several schemes [16], [17] have been successively proposed to alleviate data integrity problem under the well-trained neural network. However, these schemes either support a small variety of activation functions or require additional hardware assistance. To the best of our knowledge, there is no existing solution supporting verifiability for a neural network during the training process. Compared with a well-trained neural network, it is obviously more complicated to verify the correctness of the results during the training process, since we have to update the parameters of the entire network in addition to predicting the results. Besides, it is also a challenge of how to support verifiability while tolerating users dropping out during the workflow (due to unreliable networks, device battery issues, etc.) and ensuring the confidentiality of all users’(including dropout) local gradients.

In this paper, we propose VerifyNet, the first privacy-preserving approach supporting verification in the process of training neural networks. We first design a verifiable approach based on the homomorphic hash function and pseudorandom technologies to support the verifiability for each user. Then, we use a variant of secret sharing technology along with key agreement protocol to protect the privacy of users’ local gradients, and deal with the users dropping out problem during the training process. In summary, our contributions can be summarized as follows:

We exploit the homomorphic hash function integrated with pseudorandom technology as the underlying structure of VerifyNet, which allows users to verify the correctness of results returned from the server with acceptable overhead.

We propose a double-masking protocol to guarantee the confidentiality of users’ local gradients during the federated learning. It can endure a certain amount of users exiting for some reasons during training process, and the privacy of these exiting users are still protected.

We give a comprehensive security analysis for our VerifyNet. We claim that the attackers will not get any useful information of users’ local gradients even if the cloud server colludes with multiple users. Besides, extensive experiments conducted on real-world data also demonstrate that our VerifyNet is practical.

The remainder of this paper is organized as follows. In Section II, we outline the problem statement. In Section III and Section IV, we describe the preliminaries and give a technical intuition to explain how we solve the challenges considered in this paper, respectively. In Section V, we describe the technical details of our VerifyNet. Then, we show the security analysis in Section VI. Next, performance evaluation and related works are discussed in VII and VIII, respectively. Finally, Section IX concludes the paper.

SECTION II.Problem Statement
In this section, we first review the main concepts of federated deep learning. Then we describe the system architecture, threat model and design goals.

A. Federated Deep Learning
1) Overview:
Based on the training style, deep learning can be divided into the following two types.

Centralized Training. As shown in Fig. 1(a), traditional centralized training starts with the server asking users to upload their local data (i.e., training samples) to the cloud. Then, the server initializes deep neural networks on the cloud, and trains them with training samples until the optimal parameters are obtained. In the end, the cloud server will release the predictive service interface or return the optimal parameters to the user.

Federated Training. As discussed before, users directly upload local data to the server with potential threats for privacy breaches. Hence, different from the centralized training, in federated training (shown in Fig. 1(b)), each user and server collaborate to train a unified neural network model. To speed up the convergence of the model, each user shares local parameters (i.e., gradients) to the cloud server, which aggregates all gradients and returns the results to each user. Ultimately, the server and each user will get the optimal network parameters. Compared with the centralized training, federated training reduces the risk of user’s privacy being compromised. However, research shows that attackers can still indirectly obtain the sensitive information based on the shared gradients. In addition, driven by certain illegal interests, a malicious cloud provider may return incorrect results to users. Therefore, in this paper, we focus on protecting the privacy of users’ local gradients while verifying the correctness of results returned from the server in the federated training process.


Fig. 1.
General framework for centralized training and federated training. (a) Centralized Training. (b) Federated Training.

Show All

2) Neural Network:
As the underlying structure of deep learning, neural network can be integrated with various technologies to achieve classification, prediction, and regression. As shown in Fig. 2, there is a fully connected neural network with 3 inputs, a hidden layer and 2 outputs. The fully connected means that all neurons between two adjacent layers are connected by variables (called ω in this section) to each other. In general, a neural network can be represented as a function f(x,ω)=y^ , where x denotes the users’ inputs, and y^ is the corresponding outputs via function f with parameter ω .


Fig. 2.
Fully Connected Neural Network.

Show All

3) Federated Learning Updates:
Without loss of generality, assume that each data record is an observation pair ⟨x,y⟩ , and the entire training set is D={⟨xi,yi⟩,i=1,2,⋯T} . A loss function can be defined on the training set as
Lf(D,ω)=1|D|∑⟨xi,yi⟩∈DLf(xi,yi,ω)
View SourceRight-click on figure for MathML and additional features.

where Lf(x,y,ω)=l(y,f(x,ω)) for a specific loss function l . In this paper, loss function is set as l(y,f(x,ω))=l(y,y^)=||y,y^||2 , where ||⋅||2 is the l2 norm of a vector.

The goal of training neural network is to find the optimal parameters ω consequently to minimize the loss function. In our VerifyNet, we adopt stochastic gradient descent [20], [21] to complete this task. Specifically, each parameter is iteratively calculated as follows.
ωj+1←ωj−λ∇Lf(Dj,ωj)
View Sourcewhere ωj indicates the parameters after the j -th iteration. Dj is a random subset of D , and λ is the parameter of learning rate. In our federated learning, each user n∈N holds a private local data set Dn , and trains local set with a certain neural network agreed with all other participants in advance, where D=∑n∈NDn . Concretely, the server selects a random subset Nj⊆N at the j -th iteration, and then each user n∈Nj randomly chooses a subset Djn⊆Dn to execute stochastic gradient descent. Therefore, parameter update can be rewritten as below.
ωj+1←ωj−λ∑n∈Njρjn∑n∈Nj|Djn|
View SourceRight-click on figure for MathML and additional features.where ρjn=|Djn|∇Lf(Djn,ωj) is computed by each user and subsequently shared to the cloud server. Then, the cloud server returns the global parameters ωj+1 to all users.

B. System Architecture
As shown in Fig. 3, our system model consists of three entities, Trusted Authority (TA), User and Cloud Server.

Trusted Authority (TA): The main job of TA is to initialize the entire system, generate public parameters, and assign public and private keys to each participant. Afterwards, it will go offline unless a dispute arises.

User: Each user needs to send his/her encrypted local gradients to the cloud server during each iteration. Besides, the cloud server will also receive some other encrypted information to prepare for generating Proof of its calculated results.

Cloud server: The cloud server aggregates the gradients uploaded by all online users and sends the results along with the Proof to each user, where we require that the cloud server knows nothing but the encrypted gradients and the final results.


Fig. 3.
System Architecture.

Show All

C. Threat Model and Design Goal
There we define a threat model called Honest but Curious Security [22] in our VerifyNet. Specifically, in our VerifyNet, TA is trustworthy and will not collude with any entity. All other participants including the cloud server are considered to be honest-but-curious [11], which means that both the cloud server and users will execute the program according to the agreed agreement, but they may also try to infer other users’ data privacy independently [12], [14], [23]. In particular, we allow that the cloud server colludes with multiple users to get the most offensive capabilities, and also allow the cloud server to forge Proof (There we do not allow collusion to forge Proof) and modify the calculated results for deceiving users.

Our VerifyNet aims to protect the confidentiality of users’ local gradients while supporting strong verifiability to each user, and tolerate users dropping out during the workflow. We claim that it is impossible to succeed in deception unless the adversary can solve the NP-hard problem adopted in our model.

SECTION III.Preliminaries
To facilitate the understanding of the article, we introduce some cryptographic primitives used in our VerifyNet, which will make it easier for readers to understand our approach.

A. Bilinear Pairing
A bilinear pairing can be represented as a map e : G1×G2→GI , where both G1 and G2 are multiplicative cyclic groups with same prime order q . Without loss of generality, we assume that the generator of G1 and G2 are g and h , respectively. Informally, a bilinear pairing e has following properties.

Bilinearity: Given the random numbers a,b∈Z∗q , for any g1∈G1 and g2∈G2 , we have e(ga1,g2b)=e(g1,g2)ab .

Computability: e(g1,g2) can be computed efficiently for any g1∈G1 and g2∈G2 .

Non-degeneracy: e(g,h)≠1 , where g and h are the generator of G1 and G2 , respectively.

B. Homomorphic Hash Functions
Informally, given a message xi∈Zq , a collision-resistant homomorphic hash functions [24], [25] HF:Zq→G1×G2 can be indicated as follows.
HF(xi)=(Ai,Bi)=(gHFδ,ρ(xi),hHFδ,ρ(xi))
View Sourcewhere both δ and ρ are the secret key randomly selected in finite field Zq . HFδ,ρ() is a one-way homomorphic hash function. More precisely, given HF(x1)=(gHFδ,ρ(x1),hHFδ,ρ(x1)) , HF(x2)=(gHFδ,ρ(x2),hHFδ,ρ(x2)) , homomorphic hash function has following properties.

Additivity (in the exponent) can be indicated as HF(x1+x2)←(gHFδ,ρ(x1)+HFδ,ρ(x2) , hHFδ,ρ(x1)+HFδ,ρ(x2))

Multiplying by a constant α can be expressed as HF(αx1)←(gαHFδ,ρ(x1) , hαHFδ,ρ(x1)) .

There are other interested properties of homomorphic hash function. Interested readers can refer to [24], [25] for more details.

C. Pseudorandom Functions
We adopt the pseudorandom functions designed by Dario Fiore et al. [25] in our VerifyNet. Informally, given the secret key K=(K1,K2) , a pseudorandom function PFK:{0,1}∗×{0,1}∗→G1×G2 consists of two other pseudorandom functions, i.e., PFK1:{0,1}∗→Z2q and PFK2:{0,1}∗→Z2q . Given an input (I1,I2) , we have PFK1(I1)=(γI1,νI1) and PFK2(I2)=(γI2,νI2) . Consequently, we have
PFK(I1,I2)=(E,F)=(gγI1γI2+νI1νI2,hγI1γI2+νI1νI2)
View Source

As part of the verification, the pseudorandom functions will be exploited to verify the correctness of the results from the cloud server.

D. Secret Sharing Protocol
In VerifyNet, we utilize the Shamir’s t -out-of-N secret sharing protocol [26] to divide the secret s into N separate parts, where N denotes the number of users in our model, and t is the threshold. This means that any subset of shares greater than t can be used to recover the secret s . Specifically, implementing this secret sharing protocol involves following steps.

S.share(s,t,U)→{(n,sn)}n∈U : Given the threshold t≤|U| and the secret s , output the share sn of s for each user n , where U represents the set of users’ ID (presumed to be distinctive) specified in a finite field F , and |U|=N .

S.recon({(n,sn)}n∈M,t)→s : Input a subset M of shares, where n∈M⊆U and t≤|M| , outputs the secret s .

E. Key Agreement
Diffie-Hellman key agreement [27], [28] is also adopted in our VerifyNet to create the shared key for any two users. Specifically, given a group G with prime order q , the secret/public key of each user n is created as KA.gen(G,g,q)→(SKn,gSKn) , where g is the generator of group G . SKn and gSKn are the secret and public key, respectively. Then, given the public key gSKm of user m , the shared key between user n and user m can be generated as KA.agree(SKn,gSKm)→sn,m . In real-world applications, sn,m is often set to H((gSKm)SKn) for convenience.

SECTION IV.Technical Intuition
As discussed above, in federated learning, each user needs to submit its local gradients to the cloud, and then receives the aggregated results (sum of all local gradients) from the server. However, there are three problems that need to be addressed. Firstly, we need to protect the privacy of the user’s local gradients, because the adversary can indirectly breach user’s sensitive information through these gradient information. Secondly, to prevent malicious spoofing by the server, each user should be able to effectively verify the correctness of the results returned by the server. Thirdly, in real-world scenarios, it is very common for users to be unable to upload data to the server on time due to unreliable networks or device battery issues. Therefore, our proposed protocol should support users offline for some reason in training process. In this section, we give a technical intuition to explain how we solve these three challenges.

A. Single Masking to Protect User’s Gradients
Assume that each user n holds a local gradient xn,(n∈U,|U|=N) , we originally intend to design a single masking protocol to protect the privacy of user’s gradients. Specifically, suppose that all users’ ID in our system are ordered, and any two users n and m agree on a random number rn,m . Then, we can encrypt each user n ’s local gradient xn as follows.
xn^=xn+∑m∈U:n<mrn,m−∑m∈U:n>mrn,m(1)
View Source

Hence, after each user submitting its encrypted gradient xn^ to the server, it can calculate the aggregated gradients ∑n∈Uxn as below.
z=∑n∈Uxn^=∑n∈Uxn(2)
View SourceRight-click on figure for MathML and additional features.

However, this approach has three drawbacks. Firstly, every user needs to negotiate a random number rn,m with all other users, which will result in quadratic communication overhead (O(U2) ). Secondly, this protocol is failure to support users offline during the training process. We note that even if only one user does not upload data on time, the above aggregation operation cannot be successfully completed because the random number added in this user’s gradient cannot be cancelled. Thirdly, verifiability is not supported by above protocol. It is closely related between the correctness of results returned from the server and the privacy of users local gradients. The risk of users’privacy being compromised always tends to increase once the adversary has been able to manipulate the data integrity. Therefore, a secure protocol should also support verifiability of results returned by the server.

B. Double-Masking Potocol Supporting Verifiability
We propose a double-masking protocol to address the problems existing in single masking protocol. We first exploit the pseudorandom generator [29] and Diffie-Hellman key agreement [27], [30] to generate the random number rn,m between two users n and m . Concretely, we first ask TA to randomly create key pairs (NPKn,NSKn) for each user n . Then, we require the cloud server to broadcast all public key NPKn,n∈U to all users. In the end, by exploiting pseudorandom generator and Diffie-Hellman key agreement, each two user n and m can generate the agreed random number denoted as sn,m← KA.agree(NSKn , NPKm ). Hence, each user’s local gradient xn can be encrypted as below.
xn^=xn+∑m∈U:n<mPRG(sn,m)−∑m∈U:n>mPRG(sn,m)(3)
View Sourcewhere PRG(sn,m) is a pseudorandom generator with seed sn,m .

Next, we adopt the threshold secret sharing scheme [26] to support users offline during the training process. In briefly, to offset the random numbers added in the gradients of dropped out users, each user n shares its secret key NSKn to all other users in advance by utilizing the threshold secret sharing scheme. Hence, if a user n cannot submit its data xn^ to the cloud on time, the server can decrypt the random numbers (i.e., PRG(sn,m) ) added in all other users’ xm^(m≠n∈U) by asking more than threshold users to submit user n ’s secret shares. In this way, the random numbers PRG(sn,m) can be recovered and be eventually removed from the xm^ . However, there is still a problem. At some point, some users may delay uploading data to the cloud, which may cause the server to incorrectly determine that these users are offline, and ask other online users to upload shares of those users for removing random numbers. However, just then, these users successfully uploaded their xn^ to the cloud. As a result, since the server have sufficient secret shares of these users, it can get xn by removing all the random numbers sn,m . To address this problem, we add a new random noise call βn in each xn^ . βn will be also shared to all users by utilizing threshold secret sharing scheme. Hence, each user’s local gradient xn is encrypted as below.
xn^=xn+PRG(βn)+∑m∈U:n<mPRG(sn,m)−∑m∈U:n>mPRG(sn,m)(4)
View Source

In this way, once decryption is needed to obtain the aggregated results ∑xn , the cloud server can only receive the shares of βn for all online users, and shares of NSKn for all dropped out users, since these information are enough for the decryption operation.

The double-masking protocol is mainly designed to protect user’s data privacy during training, and support users offline for some reason in training process. However, it lacks consideration in terms of verifiability, i.e., there is no specific verifiable mechanism designed. Therefore, the double-masking protocol is not supportive for verifying the correctness of the aggregated results returned from the server. We want to design a verifiable solution that is highly compatible with our double-masking protocol, and allows each user to easily verify the correctness of results returned from the server without the involvement of trusted third parties. To address this challenge, we exploit the homomorphic hash function integrated with pseudorandom technology as the underlying structure of our verifiable approach, which allows users to verify the correctness of execution performed by the cloud server with acceptable overhead. For the specific verification process, please refer to Section V.

SECTION V.Proposed Scheme
In this section, we present the technical details of our VerifyNet. At a high level view, the purpose of VerifyNet is to address three problems existing in federated training process. One is to protect the privacy of the user’s local gradients in the workflow. Secondly, to prevent malicious spoofing by the server, our VerifyNet supports each user to effectively verify the correctness of the results returned by the server. Thirdly, VerifyNet is also supportive for users offline during the training process.

Fig. 4 shows the detailed description of our VerifyNet, which consists of five rounds to complete above tasks. Specifically, TA first initializes the entire system and generates all the public and private keys needed in our VerifyNet. Then, each user n encrypts its local gradient xn and submits it to the cloud server. After receiving enough message from all online users, the cloud server aggregates the gradients of all online users and returns the results along with Proof to each user. In the end, every user decides to accept or reject the calculation results by verifying the Proof , and returns to the round 0 to start a new iteration.


Fig. 4.
Detailed description of the VerifyNet.

Show All

As shown in Round 4, the specific verification process is as follows.

Proof of correctness:
(A,B)====e(A,h)==e(L,h)======(Πn=1n=|U3|An,Πn=1n=|U3|Bn)(g∑n∈U3HFδ,ρ(xn),h∑n∈U3HFδ,ρ(xn))(gHFδ,ρ(∑n∈U3xn),hHFδ,ρ(∑n∈U3xn))(A′,B′)e(gHFδ,ρ(σ),h)=e(g,hHFδ,ρ(σ))e(g,B)e(g∑n∈U3γnγ+νnν−HFδ,ρ(xn),h)1/de(g,h∑n∈U3γnγ+νnν−HFδ,ρ(xn))1/de(g,Q)e(A,h)⋅e(L,h)de(gHFδ,ρ(σ),h)⋅e(g∑n∈U3γnγ+νnν−HFδ,ρ(xn),h)e(g,h)∑n∈U3γnγ+νnνΦ(5)
View SourceRight-click on figure for MathML and additional features.

If any of the above equations are not valid, reject the aggregated result. Otherwise, accept the result and move to Round 0. The user and the cloud server iteratively run Rounds 0-4 until the entire neural network configuration meets the constraints set in advance.

SECTION VI.Security Analysis
In this section, we first briefly describe the correctness of the verification. Then, we analyze how our VerifyNet guarantees the confidentiality of each user’s local gradients. Other security indicators are beyond the scope of this paper.

A. Correctness of Verification
As shown in Section. V, after receiving the {σ,A,B,L,Q,Ω} from the cloud server, each user first checks whether Φ=e(A,h)⋅e(L,h)d . Based on the l -BDHI assumption [25], Φ=e(A,h)⋅e(L,h)d holds only when ∑n∈U3γnγ+νnν contained in L (in the exponent of g ). If so, each user can infer L=Πn=1n=|U3|Ln=g∑n∈U3γnγ+νnν−HFδ,ρ(σ) , and knows that A=gHFδ,ρ(σ) . Afterwards, based on the DDH assumption [32], each user further checks whether both e(A,h)=e(g,B) and e(L,h)=e(g,Q) hold. If this is true, every user will believe that the cloud server correctly calculated B and Q . Until here, each user has verified that (A,B) is calculated correctly. In the end, if HF(σ)=(A′,B′)=(A,B) holds, every user is convinced that the cloud server did return the correct aggregate result σ=∑n∈U3xn .

Here we omit the detailed proof since it can be easily proved by utilizing l - BDHI [25] and DDH assumption [32].

B. Honest but Curious Security
In this section, we prove that our VerifyNet is secure under the honest but curious setting. In our threat model, the cloud server can collude with any t−1 users to get the most offensive capabilities, but they still know nothing about the local gradients of honest users except the aggregated results. As illustrated above, each user’s local gradient xn is encrypted as
xn^=xn+PRG(βn)+∑m∈U2:n<mPRG(sn,m)−∑m∈U2:n>mPRG(sm,n)
View Source

Moreover, each xn is also used in homomorphic hash functions [24], [25] to generate part of verification information. Since the homomorphic hash functions has been proven to be secure [25], here we mainly discuss the level of privacy protection that xn^ can achieve. Before formally presenting the complete proof process, we introduce some useful symbols that will be used later.

We know that users may drop out at some point of the workfolw. We use Ui⊆U to denote those users who upload data to the cloud server smoothly at round i−1 . Therefore, we have U⊇U1⊇U2⊇U3⊇U4 . The symbol Ui∖Ui+1 is exploited to represent those users who have sent data to the server in Round i−1 , but drop out before uploading data in Round i . As illustrated before, assume that each user n holds a local gradient xn,(n∈U) , we adopt xU′={xn}n∈U′ to indicate a subset U′ of local gradients, where U′⊆U .

In our VerifyNet, the view of a party is defined as its internal state (containing its inputs and randomness) and all the messages received from other parties. It should be noted that a party will immediately stop receiving messages while this party exits the execution at some point.

For simplicity, we use S to represent the cloud server. Given a subset W⊆U∪{S} of parties, the joint view of all parties in W can be denoted as a random variable REALU,t,kW(xU,U1,U2,U3,U4) , where t and k indicate the threshold and security parameter used in our protocol, respectively.

Next we will present two theorems. The first theorem shows that any collusion (excluding the cloud server) less than t users in failure to get other users private information except the result of the aggregation.

THEOREM 1 (Defense against Joint Attacks from Multiple Users):
For all t , k , W⊆U , xU , U with |U|≥t , and U4⊆U3⊆U2⊆U1⊆U , there is a PPT simulator SIM whose output is indistinguishable from the output of REALU,t,kW .
REALU,t,kW(xU,U1,U2,U3,U4)≡SIMU,t,kW(xW,U1,U2,U3,U4)
View Source

Proof:
Because we exclude the involvement of the cloud server, the joint view of parties in set W does not depend on the inputs of other users not in W . Hence, the simulator can generate a perfect simulation by running the protocol with the true inputs of honest but curious users, but replacing the inputs of the honest users with fake data (such as randomly generated number). We stress that the simulated view of users in W is indistinguishable from the output of real view. More concretely, in Round 2, the simulator generates the masked input xn^ for all honest users (not in W ) by utilizing random number (such as 0), instead of using true gradients. Besides, we note that the server just sends a list of all online users’ ID in the round of Unmasking, not the actual value of the specific xn^ , which means that the honest but curious users cannot identify whether the calculation results returned by the cloud server are based on the true gradients of honest users. Therefore, the simulated view of users in W is indistinguishable from the output of real view REALU,t,kW .

THEOREM 2 (Defense against Joint Attacks from The Cloud Server and Multiple Users):
For all t , k , xU , W⊆U∪{S} , |W∖{S}|<t , U with |U|≥t , and U4⊆U3⊆U2⊆U1⊆U , there is a PPT simulator SIM whose output is indistinguishable from the output of REALU,t,kW .
REALU,t,kW(xU,U1,U2,U3,U4)≈SIMU,t,kW(xW,ξ,U1,U2,U3,U4)
View Sourcewhere
ξ={∑n∈U3∖Wxn⊥if|U3|≥totherwise
View Source

Proof:
We use a standard hybrid argument here to prove our THEOREM 2. The main idea is that the simulator SIM executes a series of modifications to our protocol, which ultimately makes the simulated view SIMU,t,kW indistinguishable from the real view \mathbf {REAL}_{\mathcal {W}}^{\mathcal {U}, t, k} . In our hybrid argument, \mathbf {hyb_{i}}, \{i=1,\cdots 9\} indicates a secure modification to the original protocol, which ensures that the modified operation is indistinguishable from the original operation.

In this hybrid, the simulator changes the behavior of all honest users n , where n\in \{ \mathcal {U}_{2} \backslash \mathcal {W}\} . Specifically, for each user n , a uniformly random number v_{n, m} is selected to replace the shared key KA.agree (P_{n}^{SK}, P_{m}^{PK} ) between user n and m in the same set, and to perform the function of encryption and decryption. For example, each honest user utilizes v_{n, m} to generate ciphertext \mathcal {P}_{n,m} mentioned in Round 1, instead of utilizing KA.agree (P_{n}^{SK}, P_{m}^{PK} ). The DDH assumption [32] ensures that this hybrid possesses the indistinguishability from real protocol.

In this hybrid, the simulator replaces all encrypted data (i.e.,the encrypted shares of \beta _{n} and N_{n}^{SK} ) sent by honest users (in the set \{ \mathcal {U}_{2} \backslash \mathcal {W}\} ) to other users with encrypted shares of random values (e.g., 0, with appropriate length). However, all honest users still return the correct shares to the cloud server in the round of Unmasking. Because we just change the content of ciphertext, the properties of symmetric authenticated encryption [11], [31] ensure the indistinguishability between this hybrid with real protocol.

In this hybrid, we first define a subset as below.\begin{equation*} \mathcal {U}^{\ast }= \begin{cases} \mathcal {U}_{2} \backslash \mathcal {W} \quad & \mathrm {if}\; \xi = \perp \\ \mathcal {U}_{2} \backslash \mathcal {U}_{3}\backslash \mathcal {W}\quad & \mathrm {otherwise}\\ \end{cases}\end{equation*}
View SourceThen, in the round of Key Sharing, for all honest users n in the set \mathcal {U}^{\ast } , the simulator replaces all the shares of \beta _{n} with random values (e.g., 0, with appropriate length). It is obvious that the adversary will not get extra share of \beta _{n} , either because the honest users will not reveal their shares of \beta _{n} (resp. |\mathcal {U}_{3}|\geq t, \mathcal {U}^{\ast }= \mathcal {U}_{2} \backslash \mathcal {U}_{3}\backslash \mathcal {W} ), or because all the honest users are offline (resp. |\mathcal {U}_{3}| < t, \mathcal {U}^{\ast }= \mathcal {U}_{2}\backslash \mathcal {W} , where \xi = \perp ). The security of Shamir’s secret sharing scheme guarantees that it is infeasible to recover the secret even possessing any t-1 shares of current secret, which means that even the honest but curious users have |\mathcal {W}| < t shares of \beta _{n} , they still cannot tell whether the shares submitted from honest users comes from the real \beta _{n} .

In this hybrid, instead of generating \textbf {PRG}(\beta _{n}) for all users in the set \mathcal {U}^{\ast } , the simulator uses uniformly random number r_{n} with appropriate size to replace it. It is easy to understand that the simulator just substitutes the output of \textbf {PRG} , where \textbf {PRG} is the Pseudorandom Generator [29] mentioned before. Therefore, the security of Pseudorandom Generator [11] ensures that this hybrid is indistinguishable from the real protocol.

In this hybrid, for each user n in the set \mathcal {U}^{\ast } , the simulator generates the masked input as below:\begin{equation*} \hat {x_{n}}=r_{n}+\sum \limits _{m\in \mathcal {U}_{2}:n < m }\textbf {PRG}(s_{n,m}) -\sum \limits _{m\in \mathcal {U}_{2}:n>m }\textbf {PRG}(s_{m,n})\end{equation*}
View Sourceinstead of utilizing \begin{align*}&\hspace {-2pc}\hat {x_{n}}=x_{n}+\textbf {PRG}(\beta _{n})+\sum \limits _{m\in \mathcal {U}_{2}:n < m }\textbf {PRG}(s_{n,m}) \\& \qquad \qquad \qquad \qquad \quad -\,\sum \limits _{m\in \mathcal {U}_{2}:n>m }\textbf {PRG}(s_{m,n})\end{align*}
View SourceSince \textbf {PRG}(\beta _{n}) has been changed in the previous hybrid with a uniformly random number, we know that x_{n}+r_{n} is also a random value, and it is easy to deduce that the distribution of r_{n} and x_{n}+\textbf {PRG}(\beta _{n}) is indistinguishable.

It should be noted that if \xi = \perp , the simulator has already completed the simulation (describe as \mathbf {hyb_{5}} ) since SIM successfully simulates REAL without knowing x_{n} for all parties n\in \mathcal {W} . Hence for all the simulations that follow, we assume \xi \neq \perp .

In this hybrid, for every user n\in \mathcal {U}_{3}\backslash \mathcal {W} , the simulator substitutes the shares of N_{n}^{SK} with shares of random values (e.g., 0, with appropriate length). Similar to \mathbf {hyb_{3}} , the security of Shamir’s secret sharing protocol guarantees that this hybrid is indistinguishable from the real protocol.

In this hybrid, given a user m'\in \mathcal {U}_{3}\backslash \mathcal {W} , for all other users n\in \mathcal {U}_{3}\backslash \mathcal {W} , the simulator uniformly selects a random number to replace the sharked key (i.e., s_{n,m'}=\textbf {KA.agree}\{N_{n}^{SK}, N_{m}^{PK}\} ) between user n and m' , and this random number will be used as the seed of \textbf {PRG} for both user n and m .

Specifically, a random value s_{n,m'}' is selected for each user n\in \mathcal {U}_{3}\backslash \mathcal {W}\backslash \{m'\} . Instead of sending \begin{align*}&\hspace {-2pc}\hat {x_{n}}=x_{n}+\textbf {PRG}(\beta _{n})+\sum \limits _{m\in \mathcal {U}_{2}:n < m }\textbf {PRG}(s_{n,m})\\&\qquad \quad \qquad \qquad \quad -\,\sum \limits _{m\in \mathcal {U}_{2}:n>m }\textbf {PRG}(s_{m,n})\end{align*}
View SourceSIM submits \begin{align*}&\hspace{-0.4pc}\hat {x_{n}}= x_{n}+r_{n}+\sum \limits _{m\in \mathcal {U}_{2}\backslash \{m'\}:n < m }\textbf {PRG}(s_{n,m})\\&\qquad -\,\sum \limits _{m\in \mathcal {U}_{2}\backslash \{m'\}:n>m }\textbf {PRG}(s_{m,n})+\Delta _{n,m'}\cdot \textbf {PRG}(s_{n,m'}')\end{align*}
View Sourcewhere \Delta _{n,m'}=1 if n < m' . Otherwise, \Delta _{n,m'}=-1 . Correspondingly, we have \begin{equation*} \hat {x_{m'}}=x_{m'}+r_{m'}+\sum \limits _{n\in \mathcal {U}_{2}}\Delta _{n,m'}\cdot \textbf {PRG}(s_{n,m'}')\end{equation*}
View SourceSimilarly, the DDH assumption [32] ensures that this hybrid possesses the indistinguishability from real protocol.

In this hybrid, for the same user m' selected in previous hybrid and all other user n\in \mathcal {U}_{3}\backslash \mathcal {W} , the simulator also uniformly selects a random number r_{n,m'} to replace the computation of \textbf {PRG}(s_{n,m'}') . Similar to \mathbf {hyb_{4}} , it is easy to understand that the simulator just substitutes the output of \textbf {PRG} . Therefore, the security of Pseudorandom Generator [11] ensures that this hybrid is indistinguishable from the real protocol.

In this hybrid, for each user n in the set \mathcal {U}_{3}\backslash \mathcal {W} , the simulator submits \begin{equation*} \hat {x_{n}}=R_{n}+r_{n}+\sum \limits _{m\in \mathcal {U}_{2}\backslash \mathcal {U}_{3}\backslash \mathcal {W}}\Delta _{n,m}\cdot r_{n,m}\end{equation*}
View Sourceinstead of sending \begin{align*}&\hspace {-2pc}\hat {x_{n}}= x_{n}+\textbf {PRG}(\beta _{n})+\sum \limits _{m\in \mathcal {U}_{2}:n < m }\textbf {PRG}(s_{n,m})\\&\qquad \qquad \qquad \qquad \qquad -\,\sum \limits _{m\in \mathcal {U}_{2}:n>m }\textbf {PRG}(s_{m,n})\end{align*}
View Source

where \{R_{n}\}_{n\in \mathcal {U}_{3}\backslash \mathcal {W}} is random value selected by the simulator, and subjected to \sum \limits _{n\in \mathcal {U}_{3}\backslash \mathcal {W}}R_{n}=\sum \limits _{n\in \mathcal {U}_{3}\backslash \mathcal {W}}x_{n}=\xi . Therefore, the simulator has already completed the simulation since SIM successfully simulates REAL without knowing x_{n} for all parties n\in \mathcal {W} . Based on the hybrid 1 to 9, we can infer that the distribution of this hybrid is identical to the real output. Completing the proof.
SECTION VII.Performance Evaluation
We recruit 600 mobile devices to evaluate the performance of our VerifyNet, where most smart devices come with 4GB of RAM and are equipped with Android 6.0 systems. Each mobile device runs the same convolutional neural network offline to obtain the local gradients of all parameters. All the raw data are selected from MNIST database (http://yann.lecun.com/exdb/mnist/) which has a training set of 60,000 examples, and a test set of 10,000 examples. Besides, the “ Cloud” is simulated with a Lenovo server which has Intel(R) Xeon(R) E5-2620 2.10GHZ CPU, 16GB RAM, 256SSD, 1TB mechanical hard disk and runs on the Ubuntu 18.04 operating system. More specifically, we adopt the key agreement protocol based on Elliptic-Curve to achieve the key distribution between two users, and the standard Shamir’s t -out-of-\mathcal {N} secret sharing protocol [26] to generate the shares of secret. In addition, we use AES in counter mode and AES-GCM with 128-bit keys to achieve the authenticated encryption and pseudorandom generator, respectively.

A. Functionality
As shown in TABLE. I, we compare the functionality with the latest work PPML [11], PPDL [19] and SafetyNets [16], since the main works of these schemes are similar to ours. Specifically, we know that both PPML and PPDL guarantee the confidentiality of data privacy during the execution, however, the property of verifiability is not supported by their model. In addition, PPDL is also failure to deal with the problem of users dropping out. On the other hand, SafetyNets is primarily designed from the verifiability perspective, hence the problems of data privacy leakage and users dropping out in the training process are not considered in its protocol. Compared with these schemes, our VerifyNet supports each user to verify the results returned by the cloud server while guaranteeing the confidentiality of user’s local gradients. Besides, similar to PPML, VerifyNet is also robust to users dropping out at any subprocess of whole work process.

TABLE I Detailed Description of the VerifyNet

B. Classification Accuracy
In this section, we select data from MNIST database to test the classification accuracy of our VerifyNet. The experiments were conducted on a CNN network [16], [19], which consists of two convolutional layers and two fully connected layers with 128 neurons each layer. Definitely, in federated learning, the accuracy of model’s outputs is closely related to two factors, i.e., the number of users participating in the training and the size of the local gradients owned by each user. In general, the accuracy of the model’s output is proportional to the number of gradients/users involved in the training, and also proportional to the computation and communication overhead generated by the system. To analyze the relationship between these factors, we record the classification accuracy and running time of our VerifyNet under different number of users/gradients per user. Here we use the symbol |\mathcal {U}| and |\mathcal {G}| to indicate the number of users and gradients per user in our experiments, respectively.

Fig. 5 shows the classification accuracy and running time with the different number of gradients per user, where an iteration means that a parameter update (i.e., Round 0 to Round 4) is completed. For simplicity, here we only consider the case of no users dropping out. Clearly, the increase in the number of gradients facilitates the higher accuracy of the model output, but it also incurs more computation overhead (shown in Fig. 5(b)). However, by comparing classification accuracy with different gradients (See gradients=2000 and gradients=3000, respectively), the number of gradients involved in training is not the more the better, because the accuracy of the model will converge when the number of gradients increases to a certain amount. Therefore, in practical applications, we can empirically choose the appropriate number of gradients to avoid unnecessary overhead. Fig. 6 shows the classification accuracy and running time with the different number of users. Similarly, increasing the number of users in the system is beneficial to improve the model’s classification accuracy, but it also requires additional computation overheads. Note that for the sake of simplicity, we do not record the total amount of data transmitted in the system under different numbers of users/gradients. However, since VerifyNet is an interactive protocol, in theory, our scheme will inevitably generate a certain communication overhead as the number of users/gradients increases.


Fig. 5.
(a) No dropout, |\mathcal {U}|=100 , classification accuracy with the different number of gradients per user. (b) No dropout, |\mathcal {U}|=100 , running time with the different number of gradients per user.

Show All


Fig. 6.
(a) No dropout, |\mathcal {G}|=1000 , classification accuracy with the different number of users. (b) No dropout, |\mathcal {G}|=1000 , running time with the different number of users.

Show All

C. Verification Accuracy
As discussed before, to prevent malicious spoofing by the server, our VerifyNet supports each user to verify the correctness of the results returned by the server. In specific, the cloud server is required to provide the Proof about the correctness of its aggregated results to each user, and each user can reject or accept the results by checking the Proof. To give a simple presentation for the verification accuracy, we simulate 200 users uploading encrypted local data to the server, where all the data are randomly selected from normal distribution N(50, 20) . For simplicity, here we also only consider the case of no users dropping out. Since the randomly selected data are discrete points, their real distribution (N(51.34, 19.37) , red line in Fig. 7) is slightly different from the original ideal distribution. Then, we require the cloud server to calculate the aggregated results along with corresponding Proof for each user. If the verification is passed, the distribution of uploaded data used to generate the aggregated results should be the same as the real data. Based on this, we further use the aggregated results to calculate the mean and variance of the uploaded data. As shown in Fig. 7, we can find that retrieved data distribution is exactly overlapping with the real data distribution, which also confirms that a result returned from the server is correct once its Proof is verified.


Fig. 7.
Verification accuracy.

Show All

D. Probability of Users Dropping Out
Our VerifyNet is robust to users dropping out in training process, because users dropping out is very common due to users’ device battery issues, hardware quality problems and the like occurring in workflow. To evaluate the universality of this phenomenon, we record the number of users who logged out under different number of users/gradients per user in whole system. In specific, we require all users to upload data to the server multiple times within the specified time, and record the average of dropout users under repeated experiments. As shown in Fig. 8, we find that as the number of users/gradients increases, a certain number of users dropping out are inevitable, which is more pronounced as the number of users increases. However, Fig. 8 shows that the proportion of users dropping out is not significant relative to the total number of users. Hence, this also provides a basis for using the secret sharing protocol to manage the problem of users dropping out.


Fig. 8.
Dropout users. (a) |\mathcal {G}|=1000 , with the different number of users. (b) |\mathcal {U}|=100 , with the different number of gradients per user.

Show All

E. Performance Analysis of Client
We analyze the performance of the client from both computation and communication overhead, where we test VerifyNet under different proportions of users dropping out.

1) Computation Overhead:
Fig. 9 shows the running time of each user during verification process. Clearly, the user’s running time increases linearly with the increasing of the number of gradients, but keeps a constant as the number of users increases. One of main reasons is that the verification overhead is only related to the number of gradients owned by each user, since each user n needs to generate (A_{n}, B_{n}, L_{n}, Q_{n}) for newly added gradient x_{n} . Fig. 10 shows the comparison between the computation overhead of verification and the total overhead. For simplicity, we consider no user dropping out and 30% users dropping out in our experiments. We can see that the main computation cost of each user comes from the verification process, regardless of the number of users or gradients. In addition, our VerifyNet maintains good performance in terms of computation overhead. For example, when the number of users is 500 and the total number of gradients in our system is 500000, each user only needs about 17 seconds to complete one iteration of parameter update.

Fig. 9. - Total running time of each user (Verification process). (a) 
$|\mathcal {U}|=100$
, with the different number of gradients per user. (b) 
$|\mathcal {G}|=1000$
, with the different number of users.
Fig. 9.
Total running time of each user (Verification process). (a) |\mathcal {U}|=100 , with the different number of gradients per user. (b) |\mathcal {G}|=1000 , with the different number of users.

Show All

Fig. 10. - Comparison between verification computation overhead and total overhead for each user. (a) No dropout, 
$|\mathcal {U}|=100$
, with the different number of gradients per user. (b) 30% dropout, 
$|\mathcal {U}|=100$
, with the different number of gradients per user. (c) No dropout, 
$|\mathcal {G}|=1000$
, with the different number of users. (d) 30% dropout, 
$|\mathcal {G}|=1000$
, with the different number of users.
Fig. 10.
Comparison between verification computation overhead and total overhead for each user. (a) No dropout, |\mathcal {U}|=100 , with the different number of gradients per user. (b) 30% dropout, |\mathcal {U}|=100 , with the different number of gradients per user. (c) No dropout, |\mathcal {G}|=1000 , with the different number of users. (d) 30% dropout, |\mathcal {G}|=1000 , with the different number of users.

Show All

2) Communication Overhead:
Fig. 11 shows the total transmitted data of each user during verification process. Similar to Fig. 9, the user’s total transmitted data also increases linearly with the increasing of the number of gradients, but keeps a constant as the number of users increases. Fig. 12 shows the comparison between verification communication overhead and total overhead of each user. We can see that the proportion of overhead generated in verification process is not obvious to total overhead, and even can be ignored as the number of users increases. Moreover, experiments demonstrate that our VerifyNet still maintains good performance in terms of communication overhead. For instance, when the number of users is 500 and the total number of gradients in our system is 500000, each user only needs about 70MB to complete one iteration of parameter update.


Fig. 11.
Total transmitted data of each user (Verification process). (a) |\mathcal {U}|=100 , with the different number of gradients per user. (b) |\mathcal {G}|=1000 , with the different number of users.

Show All


Fig. 12.
Comparison between verification communication overhead and total overhead for each user. (a) No dropout, |\mathcal {U}|=100 , with the different number of gradients per user. (b) 30% dropout, |\mathcal {U}|=100 , with the different number of gradients per user. (c) No dropout, |\mathcal {G}|=1000 , with the different number of users. (d) 30% dropout, |\mathcal {G}|=1000 , with the different number of users.

Show All

F. Performance Analysis of Server
1) Computation Overhead:
Fig. 13 shows the running time of verification process of the cloud server. We can see that the server’s running time increases linearly with the increasing of the number of gradients or users. The main reason is that as the number of gradients or users increases, the cloud server needs to generate the Proof of aggregated result for each new added gradients and users. Fig. 14 shows the comparison between verification computation overhead and total overhead of the cloud server. We can find that the proportion of users dropping out greatly determine the trend of overall cost of the cloud server, which is also obvious by comparing with Fig. 14(c) and Fig. 14(d). For example, when no user dropouts, the cloud sever only needs about 75000~ms to complete an iteration of parameter updates, but it will take 220000~ms if 30% of users dropout.

Fig. 13. - Total running time of the cloud server (Verification process). (a) 
$|\mathcal {U}|=100$
, with the different number of gradients per user. (b) 
$|\mathcal {G}|=1000$
, with the different number of users.
Fig. 13.
Total running time of the cloud server (Verification process). (a) |\mathcal {U}|=100 , with the different number of gradients per user. (b) |\mathcal {G}|=1000 , with the different number of users.

Show All


Fig. 14.
Comparison between verification computation overhead and total overhead for the cloud server. (a) No dropout, |\mathcal {U}|=100 , with the different number of gradients per user. (b) 30% dropout, |\mathcal {U}|=100 , with the different number of gradients per user. (c) No dropout, |\mathcal {G}|=1000 , with the different number of users. (d) 30% dropout, |\mathcal {G}|=1000 , with the different number of users.

Show All

2) Communication Overhead:
Fig. 15 shows the total transmitted data of verification process of the cloud server. We can see that as the number of users or gradients increases, the communication overhead of the cloud server also grows linearly. TABLE. I and TABLE. II show the computation and communication overhead of each round, respectively, where the red font indicates the overhead during the verification process. For each user, both the computational and communication overhead are mainly from the Masked Input and Verification, since each user n needs generating \sigma _{n} and verifying Proof for each gradient, and sending the encrypted results to the cloud server. For the cloud server, after receiving all the messages on the Masked Input round, it needs to aggregate the encrypted gradients of all users and restore the secrets of all the online users in the Unmasking round, which results in large computational/communication overheads.

TABLE II Comparison of Functionality
Table II- 
Comparison of Functionality
TABLE III Computation Overhead of Each Round
Table III- 
Computation Overhead of Each Round
Fig. 15. - Total transmitted data of the cloud server (Verification process). (a) 
$|\mathcal {U}|=100$
, with the different number of gradients per user. (b) 
$|\mathcal {G}|=1000$
, with the different number of users.
Fig. 15.
Total transmitted data of the cloud server (Verification process). (a) |\mathcal {U}|=100 , with the different number of gradients per user. (b) |\mathcal {G}|=1000 , with the different number of users.

Show All


Fig. 16.
Comparison between verification communication overhead and total overhead for the cloud server. (a) No dropout, |\mathcal {U}|=100 , with the different number of gradients per user. (b) 30% dropout, |\mathcal {U}|=100 , with the different number of gradients per user. (c) No dropout, |\mathcal {G}|=1000 , with the different number of users. (d) 30% dropout, |\mathcal {G}|=1000 , with the different number of users.

Show All

G. Performance Analysis by Comparing With Existing Approaches
In this section, we analyze the cost of VerifyNet by comparing with the state-of-the-art approaches MDL [33], PPDL [19], SafetyNets [16] and Original Federated Learning model OFL [10], where OFL is the original model for performing federated learning in the plaintext environment. Here we use OFL to describe the performance differences between federated learning in plaintext and ciphertext. MDL [33] and PPDL [19] are consistent with the scenarios considered in our VerifyNet, and their goal is also to protect the privacy of user’s local gradients by using privacy protection techniques. However, they do not consider the verifiability issue of the results returned by the server. Conversely, SafetyNets aims [16] to verify the correctness of results returned from the cloud, which is the first approach for verifiable execution of deep neural networks on untrusted cloud. It can convert certain types of deep neural networks into arithmetic circuits, and then verify the correctness of returned results through multiple interactions with the server.

1) Performance of Encryption Process:
As shown in Fig. 17 and Fig. 18, we record the running time and total transmitted data of VerifyNet, MDL, PPDL and OFL with different number of users/gradients per user. Since verifiable calculations are not considered in MDL and PPDL, the computation and communication overhead required for verification are also excluded from our VerifyNet. In addition, here we only consider the case of no users dropping out because MDL and PPDL are not supportive for users dropping out in training process. We can see that the cost of VerifyNet is significantly smaller than MDL and PPDL, while not much larger than the original solution OFL. This is mainly due to the high efficiency of our double-masking protocol compared with technologies used in MDL and PPDL. Specifically, ElGamal cryptosystem [34] is used in MDL to encrypt users’ local gradients, while guaranteeing multiplicative homomorphism over encrypted domain. However, since encrypting each gradient involves multiple exponential operations, ElGamal is not suitable for federated learning that is driven by large-scale data. LWE-based homomorphic encryption [35] is exploited in PPDL, which is faster than ElGamal cryptosystem. However, its computation/communication overhead also grows significantly as the number of users/gradients per user increases. Compared with MDL and PPDL, we design a double-masking protocol to encrypt users’ local gradients. Since we do not consider the case of users dropping out in training process, each user n only needs to calculate the shares of N_{n}^{PK} once. As a result, the encryption operation is equivalent to adding several random values to each gradient, which greatly reduces the computation and communication overhead in the encryption process.

Fig. 17. - Running time. (a) 
$|\mathcal {U}|=100$
, with the different number of gradients per user. (b) 
$|\mathcal {G}|=1000$
, with the different number of users.
Fig. 17.
Running time. (a) |\mathcal {U}|=100 , with the different number of gradients per user. (b) |\mathcal {G}|=1000 , with the different number of users.

Show All

Fig. 18. - Total transmitted data. (a) 
$|\mathcal {U}|=100$
, with the different number of gradients per user. (b) 
$|\mathcal {G}|=1000$
, with the different number of users.
Fig. 18.
Total transmitted data. (a) |\mathcal {U}|=100 , with the different number of gradients per user. (b) |\mathcal {G}|=1000 , with the different number of users.

Show All

2) Performance of Verification Process:
By comparing with the works SafetyNets [16] and OFL [10], we evaluate the performance of VerifyNet during the verification process. The scenario implemented in SafetyNets is different from our VerifyNet, which aims to verify the correctness of the results returned by the server during the prediction process, and only considers the number of users in whole system is 1. In order to compare the overhead in the same experimental environment, we set |\mathcal {U}| =1, and exploit the verifiable technologies of SafetyNets to accomplish the same task of our VerifyNet. In addition, here we only consider the case of no users dropping out. Then, we record the running time and total transmitted data of three schemes with different number of gradients per user. Fig. 19 shows that the cost of VerifyNet and SafetyNets are significant compared with original model OFL. However, the performance of our VerifyNet is significantly better than SafetyNets. One reason for this is the technical limitations of SafetyNets, and the other reason is the combination of Homomorphic Hash and pseudo-random functions exploited in our proposed protocol. Specifically, SafetyNets uses the Interactive Proof Systems [36] to check the correctness of the calculated result returned by the cloud server. It requires multiple interactions and calculations with the server to complete the verification task, and has been shown to be flawed in computation and communication overheads [37]. However, we exploit the homomorphic hash function integrated with pseudorandom technology as the underlying structure of VerifyNet, which are well known for efficiently processing of data. Hence, our VerifyNet can ensure users to verify the correctness of results returned by the cloud server with relatively low overhead.

Fig. 19. - (a) 
$|\mathcal {U}|$
=1, with the different number of gradients per user. (b) 
$|\mathcal {U}|$
=1, with the different number of gradients per user.
Fig. 19.
(a) |\mathcal {U}| =1, with the different number of gradients per user. (b) |\mathcal {U}| =1, with the different number of gradients per user.

Show All

SECTION VIII.Related Works
In this section, we introduce the latest related works of deep learning in terms of privacy protection and verifiability.

A. Privacy-Preserving Deep Learning
Most deep learning-based privacy protection algorithms focus on protecting users’ data privacy. The main tools used in their protocols are differential privacy, secure multi-party computing [11], [18], and cryptographic primitives [19]. However, the issue of privacy leakage is still not completely addressed. For example, Shokri and Shmatikov [18] proposed a privacy-preserving deep learning approach by utilizing differential privacy to achieve the balance between security and accuracy. Unfortunately, any differential privacy-based strategy has been exposed to be insecure [14] if adversaries utilize the GAN network to attack the protocol. Phong et al. [19] proposed a more secure deep learning system by utilizing additively homomorphic encryption and asynchronous stochastic gradient, but the implementation requires all users to share the same key for expected security level. Recently, Bonawitz et al. [11] designed a federated deep learning approach utilizing secure multi-party computing to protect the local gradient of each user, which is supportive for users offline during the training process.

B. Verifiable Deep Learning
In deep learning, the cloud server may return incorrect results to the user due to unexpected situations. To combat that, Several schemes [16], [17] have been successively proposed to alleviate this problem. For example, Ghodsi et al. [16] designed a framework called SafetyNets. It uses the Interactive Proof Systems [36] to check the correctness of the calculated result returned by the cloud server. Later, Tramer and Boneh [17] proposed a verifiable scheme called Slalom to perform verification by exploiting trusted hardware such as SGX, TrustZone and Sanctum. However, these schemes either support a small variety of activation functions or require additional hardware assistance. More notably, to the best of our knowledge, for a neural network being trained, there is no solution which supports the verifiability to the correctness of computation results from the cloud. Compared with existing approaches, we propose VerifyNet, the first privacy-preserving approach supporting verification in the process of training neural networks. We first utilize homomorphic hash function integrated with pseudorandom technology to support the verifiability for each user. Then, we use a variant of secret sharing technology along with key agreement protocol to protect the privacy of users’ local gradients, and deal with the users dropping out problem during the training process.

SECTION IX.Conclusion
In this paper, we have proposed VerifyNet which supports verification of the server’s calculation results to each user. Besides, VerifyNet is supportive for users dropping out in training process. Security analysis shows the high security of our VerifyNet under the honest but curious security setting. In addition, experiments conducted on real data also demonstrate the practical performance of our proposed scheme. As part of future research work, we will focus on reducing the communication overhead of the entire protocol.